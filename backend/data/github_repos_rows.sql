INSERT INTO "public"."github_repos" ("id", "name", "full_name", "html_url", "description", "created_at", "updated_at", "language", "stars", "readme", "is_mcp", "categories", "ai_analysis", "inserted_at", "is_active", "type", "slug") VALUES ('914733455', 'langgraph-mcp', 'esxr/langgraph-mcp', 'https://github.com/esxr/langgraph-mcp', 'LangGraph solution template for MCP', '2025-01-10 07:40:37+00', '2025-03-10 17:49:27+00', 'Python', '236', '# Universal Assistant built with LangGraph and Model Context Protocol (MCP)

![langgraph-mcp-openapi-usecases mp4](https://github.com/user-attachments/assets/73d44e87-e77c-4dae-a602-72261b2f6a47)

[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you're building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.

[LangGraph](https://langchain-ai.github.io/langgraph/) is a framework designed to enable seamless integration of language models into complex workflows and applications. It emphasizes modularity and flexibility. Workflows are represented as graphs. Nodes correspond to actions, tools, or model queries. Edges define the flow of information between them. LangGraph provides a structured yet dynamic way to execute tasks, making it ideal for writing AI applications involving natural language understanding, automation, and decision-making.

In [this earlier article](https://medium.com/@pranavdhoolia/building-and-deploying-a-virtual-assistant-with-langgraph-5c68dabd82db) we enhanced LangGraph's retrieval agent template to develop and deploy an AI solution.

In this project, we combine LangGraph with MCP to build our own Universal Assistant. For our universal assistant we implement a multi-agent pattern as follows:

![Basic assistant flow](media/assistant-flow.excalidraw.png)

Assistant receives the user message and decides the agent to use. The agent node decides the right tool to use, and calls the tool on the MCP server. Since all our agents are based on MCP, a single MCP-Agent node is sufficient for LLM based orchestraion, and another single node is sufficient to work with MCP servers to invoke their tools.


## Development Setup

1.  Create and activate a virtual environment
    ```bash
    git clone https://github.com/esxr/langgraph-mcp.git
    cd langgraph-mcp
    python3 -m venv .venv
    source .venv/bin/activate
    ```

2.  Install Langgraph CLI
    ```bash
    pip install -U "langgraph-cli[inmem]"
    ```
    Note: "inmem" extra(s) are needed to run LangGraph API server in development mode (without requiring Docker installation)

3.  Install the dependencies
    ```bash
    pip install -e .
    ```

4.  Configure environment variables
    ```bash
    cp env.example .env
    ```

    Add your `OPENAI_API_KEY`, `GITHUB_PERSONAL_ACCESS_TOKEN` etc. to the `.env`

    **Note**: We have added support for *Milvus Lite Retriever* (support file based URI). Milvus Lite won't work on Windows. For Windows you may need to use Milvus Server (Easy to start using Docker), and change the `MILVUS_DB` config to the server based URI. You may also enhance the [retriever.py](src/langgraph_mcp/retriever.py) to add retrievers for your choice of vector databases!


## Implementation Details

There are 3 main parts to our implementation:
1. Building the Router
2. The Assistant
3. A generic MCP wrapper

### Building the Router

Our graph to build the router is implemented in [`build_router_graph.py`](src/langgraph_mcp/build_router_graph.py). It collects routing information based on tools, prompts, and resources offered by each MCP server using our [`mcp_wrapper.py`](src/langgraph_mcp/mcp_wrapper.py). It indexes this routing information for each server in a vector database.

<!--
```mermaid
sequenceDiagram
  participant G as Build-Router<br/>Graph
  participant MW as MCP Wrapper
  participant MS as MCP Servers
  participant V as Vector Index

  loop for all mcp-servers in config
  G ->> MW: Get Routing Information (server-config)
  MW ->> MS: list_tools
  MS --) MW: tools
  MW ->> MS: list_resources
  MS --) MW: resources
  MW ->> MS: list_prompts
  MS --) MW: prompts
  MW --) G: routing_information 𝒇(tools, resources, prompts)
  end
  G ->> V: Build Vector Index (list(Document(page_content=routing_information, metadata{id: mcp_server_name})))
```
-->

![Build Router Sequence Diagram](media/build-router-sequence.excalidraw.png)


### The Assistant

The assistant graph is implemented in [`assistant_graph.py`](src/langgraph_mcp/assistant_graph.py). The following animation describes the role of various nodes and the flow of control thru it, with the help of an example

![Assistant workflow explained with example](media/langgraph-assistant-mcp.gif)

### A Generic MCP Wrapper

[`mcp_wrapper.py`](src/langgraph_mcp/mcp_wrapper.py) employs a Strategy Pattern using an abstract base class (`MCPSessionFunction`) to define a common interface for executing various operations on MCP servers. The pattern includes:
1.  Abstract Interface:
    - `MCPSessionFunction` defines an async `__call__` method as a contract for all session functions.
2.  Concrete Implementations:
    - `RoutingDescription` class implements fetching routing information based on tools, prompts, and resources.
    - `GetTools` class implements fetching tools for the MCP server and transforming them to the format consumable by LangGraph.
    - `RunTool` class implements invoking a tool on MCP server and returning its output.
3.  Processor Function:
    - `apply` serves as a unified executor. It:
    - Initializes a session using `stdio_client` from `mcp` library.
    - Delegates the actual operation to the provided `MCPSessionFunction` instance via `await fn(server_name, session)`.
4.  Extensibility:
    - New operations can be added by subclassing `MCPSessionFunction` without modifying the cor    e processor logic. for e.g. we should be able to add support for getting tools and executing tools using this pattern.

## A Demonstration!

Here's an end to end video!

https://github.com/user-attachments/assets/cf5b9932-33a0-4627-98ca-022979bfb2e7
', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"Protocol Implementations\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states that it's a LangGraph solution template for MCP and demonstrates how to build a universal assistant using LangGraph and Model Context Protocol.","categories":["Agent Frameworks","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The architecture employs a multi-agent pattern orchestrated by LangGraph. A router graph determines which agent to use based on the user message. The generic MCP wrapper uses the Strategy pattern with `MCPSessionFunction` as the abstract base class. This abstracts the communication with the MCP servers, promoting modularity and extensibility.","Implementation Details and Key Features":"The implementation comprises three main parts: a router, an assistant, and a generic MCP wrapper. The router graph indexes MCP server routing information (tools, prompts, resources) in a vector database. The assistant graph defines the workflow of the assistant. The MCP wrapper implements fetching routing information and tool execution.","Integration Approaches and Protocols":"The repository integrates LangGraph with MCP to enable communication between AI models and external tools. LangGraph provides the framework for defining the agent workflow, while MCP standardizes the interaction with external data sources and tools. The code demonstrates fetching tool information and executing tools on MCP servers.","Technical Stack and Dependencies":"The technical stack includes Python, LangGraph, and potentially a vector database (Milvus). The repository uses `langgraph-cli` for development. The project depends on the `mcp` library (specifically `stdio_client`) for creating and managing sessions with MCP servers. Environment variables are used to configure API keys and database connections.","Best Practices and Coding Standards":"The repository includes a virtual environment setup, environment variable configuration, and a modular design with separation of concerns. The use of an abstract base class (`MCPSessionFunction`) promotes code reusability and extensibility. The code is structured into different modules based on their functionality (e.g., `build_router_graph.py`, `assistant_graph.py`).","Performance Considerations":"The use of a vector database for indexing routing information suggests an emphasis on efficient retrieval of relevant tools and resources. The asynchronous nature of the `MCPSessionFunction` suggests that the code is designed to handle concurrent requests. The performance of the overall system will depend on the performance of the vector database and the MCP servers."}}', '2025-03-10 21:26:30+00', 'true', 'Servers', 'langgraph-mcp-template'), ('915153699', 'model-context-protocol-rb', 'dickdavis/model-context-protocol-rb', 'https://github.com/dickdavis/model-context-protocol-rb', 'An implementation of the Model Context Protocol in Ruby.', '2025-01-11 05:27:34+00', '2025-03-10 18:33:34+00', 'Ruby', '24', '# model-context-protocol-rb

An implementation of the [Model Context Protocol (MCP)](https://spec.modelcontextprotocol.io/specification/2024-11-05/) in Ruby.

## Usage

Include `model-context-protocol-rb` in your project.

```ruby
require 'model-context-protocol-rb'
```

# Building an MCP Server

Build a simple MCP server by routing methods to your custom handlers. Then, configure and run the server.

```ruby
server = ModelContextProtocol::Server.new do |config|
  config.name = "My MCP Server"
  config.version = "1.0.0"
  config.enable_log = true
  config.router = ModelContextProtocol::Router.new do
    prompts do
      list Prompt::List, broadcast_changes: true
      get Prompt::Get
    end

    resources do
      list Resource::List, broadcast_changes: true
      read Resource::Read, allow_subscriptions: true
    end

    tools do
      list Tool::List, broadcast_changes: true
      call Tool::Call
    end
  end
end

server.start
```

Messages from the MCP client will be routed to the appropriate custom handler. Your customer handler must respond to `call`; the router will pass the message to the handler as an argument.

Your handler should return a valid JSONRPC 2.0 response.

## Installation

Add this line to your application's Gemfile:

```ruby
gem 'model-context-protocol-rb'
```

And then execute:

```bash
bundle
```

Or install it yourself as:

```bash
gem install model-context-protocol-rb
```

## Development

After checking out the repo, run `bin/setup` to install dependencies. Then, run `rake  spec` to run the tests. You can also run `bin/console` for an interactive prompt that will allow you to experiment.

To install this gem onto your local machine, run `bundle exec rake install`. To release a new version, update the version number in `version.rb`, and then run `bundle exec rake release`, which will create a git tag for the version, push git commits and the created tag, and push the `.gem` file to [rubygems.org](https://rubygems.org).

## Contributing

Bug reports and pull requests are welcome on GitHub at https://github.com/dickdavis/model-context-protocol-rb.

## License

The gem is available as open source under the terms of the [MIT License](https://opensource.org/licenses/MIT).
', 'true', '"{\"Protocol Implementations\",\"AI Model Integration\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository is explicitly described as an implementation of the Model Context Protocol (MCP) in Ruby, as stated in the repository description and documentation.","categories":["Protocol Implementations","API Frameworks","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The repository implements a server-side component for the Model Context Protocol. It utilizes a router to direct incoming messages from MCP clients to appropriate handlers based on the message type (prompts, resources, tools). The architecture promotes modularity by decoupling message routing from the custom handler implementations.","Implementation Details and Key Features":"The implementation allows the configuration of an MCP server with attributes like name, version, and logging. It uses a router to map incoming requests to specific handlers. The handlers are expected to respond to the `call` method, processing the message and returning a JSONRPC 2.0 response. The framework supports features like broadcast changes and subscriptions for resources.","Integration Approaches and Protocols":"The repository uses a JSONRPC 2.0 response format for communication between the server and client. It allows integration by including the `model-context-protocol-rb` gem in Ruby projects and defining custom handlers for different message types. The MCP server is built by routing methods to custom handlers using the provided router.","Technical Stack and Dependencies":"The repository is built using Ruby and relies on the `bundler` gem for dependency management. The documentation mentions installing dependencies using `bin/setup` and managing gems using `bundle`. The code assumes a Ruby environment and uses standard Ruby conventions for gem packaging and distribution.","Best Practices and Coding Standards":"The repository includes a `Gemfile` for dependency management, `Rakefile` for common tasks (testing, installation, release), and follows standard Ruby gem conventions. The project also includes a `spec` directory suggesting the presence of unit tests. The documentation also includes contribution guidelines, including bug reports and pull requests.","Performance Considerations":"The repository allows the configuration of logging, which can impact performance. The use of `broadcast_changes` and `allow_subscriptions` indicates the potential for managing real-time updates and subscriptions, which can also have performance implications if not handled efficiently. Specific performance optimizations would depend on the implementation of the custom handlers."}}', '2025-03-10 21:34:28+00', 'true', 'Servers', 'mcp-ruby-protocol'), ('915233417', 'awesome-mcp-clients', 'punkpeye/awesome-mcp-clients', 'https://github.com/punkpeye/awesome-mcp-clients', 'A collection of MCP clients.', '2025-01-11 10:11:05+00', '2025-03-10 18:25:19+00', null, '479', '# Awesome MCP Clients [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

[![Discord](https://img.shields.io/discord/1312302100125843476?logo=discord&label=discord)](https://glama.ai/mcp/discord)
[![Subreddit subscribers](https://img.shields.io/reddit/subreddit-subscribers/mcp?style=flat&logo=reddit&label=subreddit)](https://www.reddit.com/r/mcp/)

A curated list of awesome Model Context Protocol (MCP) clients.

* [What is MCP?](#what-is-mcp)
* [Community](#community)
* [Clients](#clients)
* [Servers](#servers)

## What is MCP?

[MCP](https://modelcontextprotocol.io/) is an open protocol that enables AI models to securely interact with local and remote resources through standardized server implementations. This list focuses on production-ready and experimental MCP servers that extend AI capabilities through file access, database connections, API integrations, and other contextual services.

## Community

* [r/mcp Reddit](https://www.reddit.com/r/mcp)
* [Discord Server](https://glama.ai/mcp/discord)

## Clients

- [Awesome MCP Clients ](#awesome-mcp-clients-)
  - [What is MCP?](#what-is-mcp)
  - [Community](#community)
  - [Clients](#clients)
    - [5ire](#5ire)
    - [ChatMCP](#chatmcp)
    - [Claude Desktop](#claude-desktop)
    - [ClaudeMind](#claudemind)
    - [Cline](#cline)
    - [console-chat-gpt](#console-chat-gpt)
    - [Cursor](#cursor)
    - [Continue](#continue)
    - [Goose](#goose)
    - [HyperChat](#hyperchat)
    - [kibitz](#kibitz)
    - [LibreChat](#librechat)
    - [MCP Chatbot](#mcp-chatbot)
    - [MCP CLI client](#mcp-cli-client)
    - [oterm](#oterm)
    - [Superinterface](#superinterface)
    - [Tester MCP Client](#tester-mcp-client)
    - [Witsy](#witsy)
    - [Enconvo](#enconvo)
    - [y-cli](#y-cli)
    - [Zed](#zed)
  - [Servers](#servers)

### 5ire

<table>
<tr><th align="left">GitHub</th><td>https://github.com/nanbingxyz/5ire</td></tr>
<tr><th align="left">Website</th><td>https://5ire.app/</td></tr>
<tr><th align="left">License</th><td>GNU v3</td></tr>
<tr><th align="left">Type</th><td>Desktop app</td></tr>
<tr><th align="left">Platforms</th><td>Windows, MacOS</td></tr>
<tr><th align="left">Pricing</th><td>Free</td></tr>
<tr><th align="left">Programming Languages</th><td>TypeScript</td></tr>
</table>

5ire is a cross-platform desktop AI assistant, MCP client. It compatible with major service providers, supports local knowledge base and tools via model context protocol servers.

<details>
<summary>Screenshots</summary>

https://github.com/user-attachments/assets/a27494c5-437d-481c-a25f-74cfa5a2bc45

</details>

### ChatMCP

<table>
<tr><th align="left">GitHub</th><td>https://github.com/daodao97/chatmcp</td></tr>
<tr><th align="left">Website</th><td>-</td></tr>
<tr><th align="left">License</th><td>Apache 2.0</td></tr>
<tr><th align="left">Type</th><td>Desktop app</td></tr>
<tr><th align="left">Platforms</th><td>Windows, MacOS, Linux</td></tr>
<tr><th align="left">Pricing</th><td>Free</td></tr>
<tr><th align="left">Programming Languages</th><td>Dart</td></tr>
</table>

ChatMCP is an AI chat client implementing the Model Context Protocol (MCP).

<details>
<summary>Screenshots</summary>

![](./screenshots/chatmcp/preview.png)
![](./screenshots/chatmcp/settings.png)

</details>

### Claude Desktop

<table>
<tr><th align="left">GitHub</th><td>-</td></tr>
<tr><th align="left">Website</th><td>https://claude.ai/download</td></tr>
<tr><th align="left">License</th><td>Proprietary</td></tr>
<tr><th align="left">Type</th><td>Desktop app</td></tr>
<tr><th align="left">Platforms</th><td>Windows, MacOS</td></tr>
<tr><th align="left">Pricing</th><td>Free</td></tr>
<tr><th align="left">Programming Languages</th><td>-</td></tr>
</table>

The Claude desktop app brings Claude's capabilities directly to your computer, allowing for seamless integration with your workflow.

<details>
<summary>Screenshots</summary>

![](./screenshots/claude-desktop/claude-desktop.png)

</details>

### ClaudeMind

<table>
<tr><th align="left">GitHub</th><td>-</td></tr>
<tr><th align="left">Website</th><td>https://claudemind.com/</td></tr>
<tr><th align="left">License</th><td>Proprietary</td></tr>
<tr><th align="left">Type</th><td>Desktop app, JetBrains extension</td></tr>
<tr><th align="left">Platforms</th><td>Windows, MacOS</td></tr>
<tr><th align="left">Pricing</th><td>Per seat (from $29)</td></tr>
<tr><th align="left">Programming Languages</th><td>-</td></tr>
</table>

Experience Claude AI without limits. Use our desktop app for everyday AI assistance, or boost your coding productivity with our JetBrains plugin.

<details>
<summary>Screenshots</summary>

![](./screenshots/claudemind/ClaudeMind_Desktop_Chat_History.png)
![](./screenshots/claudemind/ClaudeMind_Desktop_NewChatPage.png)
![](./screenshots/claudemind/ClaudeMind_Desktop_Projects.png)

</details>

### Cline

<table>
<tr><th align="left">GitHub</th><td>https://github.com/cline/cline</td></tr>
<tr><th align="left">Website</th><td>https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev</td></tr>
<tr><th align="left">License</th><td>Apache 2.0</td></tr>
<tr><th align="left">Type</th><td>VSCode extension</td></tr>
<tr><th align="left">Platforms</th><td>Windows, MacOS</td></tr>
<tr><th align="left">Pricing</th><td>Free</td></tr>
<tr><th align="left">Programming Languages</th><td>TypeScript</td></tr>
</table>

Cline can handle complex software development tasks step-by-step. With tools that let him create & edit files, explore large projects, use the browser, and execute terminal commands (after you grant permission), he can assist you in ways that go beyond code completion or tech support. Cline can even use the Model Context Protocol (MCP) to create new tools and extend his own capabilities. While autonomous AI scripts traditionally run in sandboxed environments, this extension provides a human-in-the-loop GUI to approve every file change and terminal command, providing a safe and accessible way to explore the potential of agentic AI.

<details>
<summary>Screenshots</summary>

![](./screenshots/cline/cline-demo.gif)

</details>

### console-chat-gpt

<table>
<tr><th align="left">GitHub</th><td>https://github.com/amidabuddha/console-chat-gpt</td></tr>
<tr><th align="left">Website</th><td>-</td></tr>
<tr><th align="left">License</th><td>MIT</td></tr>
<tr><th align="left">Type</th><td>CLI</td></tr>
<tr><th align="left">Platforms</th><td>Windows, MacOS, Linux</td></tr>
<tr><th align="left">Pricing</th><td>Free</td></tr>
<tr><th align="left">Programming Languages</th><td>Python</td></tr>
</table>

Enjoy seamless interactions with ChatGPT, MistralAI, Claude by Anthropic, Grok by xAI, Gemini by Google and DeepSeek directly from your command line. Elevate your chat experience with efficiency and ease.

<details>
<summary>Screenshots</summary>

![](./screenshots/console-chat-gpt/markdown_preview.gif)
![](./screenshots/console-chat-gpt/python_for_loop.gif)
![](./screenshots/console-chat-gpt/settings_preview.gif)

</details>

### Cursor

<table>
<tr><th align="left">GitHub</th><td>https://github.com/getcursor/cursor</td></tr>
<tr><th align="left">Website</th><td>https://cursor.com</td></tr>
<tr><th align="left">License</th><td>Proprietary</td></tr>
<tr><th align="left">Type</th><td>Desktop app</td></tr>
<tr><th align="left">Platforms</th><td>Windows, MacOS, Linux</td></tr>
<tr><th align="left">Pricing</th><td>Freemium</td></tr>
<tr><th align="left">Programming Languages</th><td>TypeScript</td></tr>
</table>

Cursor is an AI-first code editor fork of VS Code that helps you code faster with built-in chat, edit, and debugging AI features. It supports MCP for enhanced AI capabilities and tool integration.

<details>
<summary>Screenshots</summary>

![Main Interface](./screenshots/cursor/cursor.png)
![Adding New MCP Server](./screenshots/cursor/new-server.png)
![Settings Interface](./screenshots/cursor/settings.png)
![Calling MCP Server](./screenshots/cursor/calling.png)
![MCP Server Response](./screenshots/cursor/called.png)

</details>

### Continue

<table>
<tr><th align="left">GitHub</th><td>https://github.com/continuedev/continue</td></tr>
<tr><th align="left">Website</th><td>https://continue.dev/</td></tr>
<tr><th align="left">License</th><td>Apache 2.0</td></tr>
<tr><th align="left">Type</th><td>VSCode extension, JetBrains extension</td></tr>
<tr><th align="left">Platforms</th><td>Windows, MacOS</td></tr>
<tr><th align="left">Pricing</th><td>Free</td></tr>
<tr><th align="left">Programming Languages</th><td>TypeScript</td></tr>
</table>

Continue is the leading open-source AI code assistant. You can connect any models and any context to build custom autocomplete and chat experiences inside VS Code and JetBrains.

<details>
<summary>Screenshots</summary>

![](./screenshots/continue/continue-demo.gif)

</details>

### Goose

<table>
<tr><th align="left">GitHub</th><td>https://github.com/block/goose</td></tr>
<tr><th align="left">Website</th><td>-</td></tr>
<tr><th align="left">License</th><td>Apache 2.0</td></tr>
<tr><th align="left">Type</th><td>AI Agent</td></tr>
<tr><th align="left">Platforms</th><td>MacOS, Linux</td></tr>
<tr><th align="left">Pricing</th><td>Free</td></tr>
<tr><th align="left">Programming Languages</th><td>Rust</td></tr>
</table>

Goose is a general-purpose AI agent that can dynamically plug into new extensions and learn how to use them. It solves higher-level problems using tools from multiple extensions and can interact with multiple extensions at once.

<details>
<summary>Screenshots</summary>

![Goose Logo](./screenshots/goose/goose.png)
![Custom Extension Chat](./screenshots/goose/custom-extension-chat.png)
![Custom Extension Tools](./screenshots/goose/custom-extension-tools-9d440447ae99b18ae92819e652148abe.png)
![Extension Settings](./screenshots/goose/extension%20settings.png)
![List Tools](./screenshots/goose/list%20tools.png)

</details>

### HyperChat

<table>
<tr><th align="left">GitHub</th><td>https://github.com/BigSweetPotatoStudio/HyperChat</td></tr>
<tr><th align="left">Website</th><td>-</td></tr>
<tr><th align="left">License</th><td>Apache 2.0<a href="https://github.com/BigSweetPotatoStudio/HyperChat/blob/main/LICENSE">*</a></td></tr>
<tr><th align="left">Type</th><td>Desktop app</td></tr>
<tr><th align="left">Platforms</th><td>Windows, MacOS</td></tr>
<tr><th align="left">Pricing</th><td>Free</td></tr>
<tr><th align="left">Programming Languages</th><td>JavaScript</td></tr>
</table>

HyperChat is an open Chat client that can use various LLM APIs to provide the best Chat experience and implement productivity tools through the MCP protocol.

<details>
<summary>Screenshots</summary>

![](./screenshots/hyperchat/image13.png)
![](./screenshots/hyperchat/image21.png)
![](./screenshots/hyperchat/image22.png)
![](./screenshots/hyperchat/image33.png)
![](./screenshots/hyperchat/image34.png)
![](./screenshots/hyperchat/image35.png)
![](./screenshots/hyperchat/image36.png)
![](./screenshots/hyperchat/image42.png)
![](./screenshots/hyperchat/image43.png)
![](./screenshots/hyperchat/image44.png)
![](./screenshots/hyperchat/image45.png)
![](./screenshots/hyperchat/image46.png)
![](./screenshots/hyperchat/image48.png)

</details>

### kibitz

<table>
<tr><th align="left">GitHub</th><td>https://github.com/nick1udwig/kibitz</td></tr>
<tr><th align="left">Website</th><td>https://kibi.tz</td></tr>
<tr><th align="left">License</th><td>MIT</td></tr>
<tr><th align="left">Type</th><td>Mobile app, Desktop app</td></tr>
<tr><th align="left">Platforms</th><td>Mobile, Windows, MacOS, Linux</td></tr>
<tr><th align="left">Pricing</th><td>Free</td></tr>
<tr><th align="left">Programming Languages</th><td>TypeScript</td></tr>
</table>

kibitiz is the free and open-source Replit. Minimally, it is a lightweight chat interface to the popular LLM APIs (Anthropic and OpenAI API formats supported). Experience automated tool loops: try asking your agent to use [wcgw](https://github.com/rusiaaman/wcgw) to make a change to a local repository, then fix linter and compiler errors, make a commit, and push to remote, all without user intervention! Even better, code on-the-go by setting up MCP servers on your laptop, then connecting from your mobile through [Kinode](https://github.com/kinode-dao/kinode).

<details>
<summary>Screenshots</summary>
  
https://github.com/user-attachments/assets/3f8df448-1c81-4ff2-8598-c48283a4dc00

</details>

### LibreChat

<table>
<tr><th align="left">GitHub</th><td>https://github.com/danny-avila/LibreChat</td></tr>
<tr><th align="left">Website</th><td>https://www.librechat.ai/</td></tr>
<tr><th align="left">License</th><td>MIT license</td></tr>
<tr><th align="left">Type</th><td>Web app</td></tr>
<tr><th align="left">Platforms</th><td>-</td></tr>
<tr><th align="left">Pricing</th><td>Free</td></tr>
<tr><th align="left">Programming Languages</th><td>TypeScript</td></tr>
</table>

Enhanced ChatGPT Clone: Features Agents, Anthropic, AWS, OpenAI, Assistants API, Azure, Groq, o1, GPT-4o, Mistral, OpenRouter, Vertex AI, Gemini, Artifacts, AI model switching, message search, Code Interpreter, langchain, DALL-E-3, OpenAPI Actions, Functions, Secure Multi-User Auth, Presets, open-source for self-hosting.

<details>
<summary>Screenshots</summary>

![](./screenshots/librechat/librechat.webp)

</details>

### MCP Chatbot

<table>
<tr><th align="left">GitHub</th><td>https://github.com/3choff/mcp-chatbot</td></tr>
<tr><th align="left">Website</th><td>-</td></tr>
<tr><th align="left">License</th><td>MIT</td></tr>
<tr><th align="left">Type</th><td>CLI</td></tr>
<tr><th align="left">Platforms</th><td>Windows, MacOS, Linux</td></tr>
<tr><th align="left">Pricing</th><td>Free</td></tr>
<tr><th align="left">Programming Languages</th><td>Python</td></tr>
</table>

This chatbot example demonstrates how to integrate the Model Context Protocol (MCP) into a simple CLI chatbot. The implementation showcases MCP's flexibility by supporting multiple tools through MCP servers and is compatible with any LLM provider that follows OpenAI API standards.

### MCP CLI client

<table>
<tr><th align="left">GitHub</th><td>https://github.com/adhikasp/mcp-client-cli</td></tr>
<tr><th align="left">Website</th><td>-</td></tr>
<tr><th align="left">License</th><td>MIT</td></tr>
<tr><th align="left">Type</th><td>CLI</td></tr>
<tr><th align="left">Platforms</th><td>Windows, MacOS, Linux</td></tr>
<tr><th align="left">Pricing</th><td>Free</td></tr>
<tr><th align="left">Programming Languages</th><td>Python</td></tr>
</table>

A simple CLI to run LLM prompt and implement MCP client.

<details>
<summary>Screenshots</summary>

![](./screenshots/mcp-cli-client/usage.png)

</details>

### oterm

<table>
<tr><th align="left">GitHub</th><td>https://github.com/ggozad/oterm</td></tr>
<tr><th align="left">Website</th><td>-</td></tr>
<tr><th align="left">License</th><td>MIT</td></tr>
<tr><th align="left">Type</th><td>CLI</td></tr>
<tr><th align="left">Platforms</th><td>Windows, MacOS, Linux</td></tr>
<tr><th align="left">Pricing</th><td>Free</td></tr>
<tr><th align="left">Programming Languages</th><td>Python</td></tr>
</table>

A terminal client for Ollama, with support for MCP servers.

<details>
<summary>Screenshots</summary>

![](./screenshots/oterm/chat.png)
![](./screenshots/oterm/mcp.svg)

</details>

### Superinterface

<table>
<tr><th align="left">GitHub</th><td>https://github.com/supercorp-ai/superinterface</td></tr>
<tr><th align="left">Website</th><td>https://superinterface.ai</td></tr>
<tr><th align="left">License</th><td>Proprietary</td></tr>
<tr><th align="left">Type</th><td>Web app</td></tr>
<tr><th align="left">Platforms</th><td>Web</td></tr>
<tr><th align="left">Pricing</th><td>Freemium</td></tr>
<tr><th align="left">Programming Languages</th><td>TypeScript</td></tr>
</table>

Superinterface is AI infrastructure and a developer platform to build in-app AI assistants with support for MCP, interactive components, client-side function calling and more.

Key features:

- Use tools from MCP servers in assistants embedded via React components or script tags
- SSE transport support
- Use any AI model from any AI provider (OpenAI, Anthropic, Ollama, others)

<details>
<summary>Screenshots</summary>

![](./screenshots/superinterface/mcp-chat.png)
![](./screenshots/superinterface/interfaces.png)
![](./screenshots/superinterface/setup-1.png)
![](./screenshots/superinterface/setup-2.png)
![](./screenshots/superinterface/setup-3.png)

</details>

### Tester MCP Client

<table>
<tr><th align="left">GitHub</th><td>https://github.com/apify/tester-mcp-client</td></tr>
<tr><th align="left">Website</th><td>https://apify.com/jiri.spilka/tester-mcp-client</td></tr>
<tr><th align="left">License</th><td>Apache 2.0</td></tr>
<tr><th align="left">Type</th><td>Web app</td></tr>
<tr><th align="left">Platforms</th><td>Web</td></tr>
<tr><th align="left">Pricing</th><td>Freemium</td></tr>
<tr><th align="left">Programming Languages</th><td>JavaScript</td></tr>
</table>

A client that connects to any MCP server using Server-Sent Events (SSE) and displays conversations in a chat-like UI.  
It is a standalone Apify Actor for testing MCP servers over SSE, with support for Authorization headers.  
Built with plain JavaScript (old-school style) and hosted on Apify, it requires no setup to run.  

Key features:

- Connects to any MCP server via Server-Sent Events (SSE).  
- Works with the [Apify MCP Server](https://apify.com/apify/actors-mcp-server) to interact with one or more Apify [Actors](https://apify.com/store).  
- Dynamically utilizes tools based on context and user queries (if supported by the server).  
- Open-source—review, suggest improvements, or modify as needed.

<details>
<summary>Screenshots</summary>

![](./screenshots/tester-mcp-client/setup.png)
![](./screenshots/tester-mcp-client/chat-ui.png)

</details>

### Witsy

<table>
<tr><th align="left">GitHub</th><td>https://github.com/nbonamy/witsy</td></tr>
<tr><th align="left">Website</th><td>https://witsyai.com</td></tr>
<tr><th align="left">License</th><td>Apache 2.0</td></tr>
<tr><th align="left">Type</th><td>Desktop app</td></tr>
<tr><th align="left">Platforms</th><td>Windows, MacOS, Linux</td></tr>
<tr><th align="left">Pricing</th><td>Free</td></tr>
<tr><th align="left">Programming Languages</th><td>Typescript, Vue</td></tr>
</table>

Witsy is an AI desktop assistant supporting models from all major providers and one keyboard shortcut away!

<details>
<summary>Screenshots</summary>

![](./screenshots/witsy/main.jpg)
![](./screenshots/witsy/mcp.jpg)

</details>

### Enconvo

<table>
<tr><th align="left">GitHub</th><td>https://github.com/Enconvo</td></tr>
<tr><th align="left">Website</th><td>https://enconvo.com</td></tr>
<tr><th align="left">License</th><td>Proprietary</td></tr>
<tr><th align="left">Type</th><td>Desktop app</td></tr>
<tr><th align="left">Platforms</th><td> MacOS </td></tr>
<tr><th align="left">Pricing</th><td>Freemium</td></tr>
<tr><th align="left">Programming Languages</th><td>Typescript, Python , Swift</td></tr>
</table>

Enconvo is your AI Agent Launcher that revolutionizes productivity. With instant access, automate your daily tasks effortlessly. Our intelligent AI Agent system, powered by 150+ built-in tools and MCP support, learns and adapts to your workflow. Experience seamless automation and enhanced productivity with the most versatile AI assistant for macOS.

<details>
<summary>Screenshots</summary>

![](./screenshots/enconvo/agent_use_mcp.png)
![](./screenshots/enconvo/mcp_config.png)

</details>

### y-cli

<table>
<tr><th align="left">GitHub</th><td>https://github.com/luohy15/y-cli</td></tr>
<tr><th align="left">Website</th><td>-</td></tr>
<tr><th align="left">License</th><td>MIT</td></tr>
<tr><th align="left">Type</th><td>CLI</td></tr>
<tr><th align="left">Platforms</th><td>Windows, MacOS, Linux</td></tr>
<tr><th align="left">Pricing</th><td>Free</td></tr>
<tr><th align="left">Programming Languages</th><td>Python</td></tr>
</table>

A tiny command-line interface chat application that brings AI conversations to your terminal. Features include chat data storage in JSONL files, interactive chat interface, support for multiple bot configurations compatible with OpenAI chat completion streaming format, Deepseek-r1 reasoning content support, and MCP client support with multiple server configurations.

<details>
<summary>Screenshots</summary>

![Interactive Chat](./screenshots/y-cli/interactive-chat.png)
![MCP Server Configurations](./screenshots/y-cli/multi-mcp-server.png)
![MCP Demo](./screenshots/y-cli/mcp.gif)

</details>

### Zed

<table>
<tr><th align="left">GitHub</th><td>https://github.com/zed-industries/zed</td></tr>
<tr><th align="left">Website</th><td>https://zed.dev/</td></tr>
<tr><th align="left">License</th><td>GNU</td></tr>
<tr><th align="left">Type</th><td>Desktop app</td></tr>
<tr><th align="left">Platforms</th><td>Windows, MacOS, Linux</td></tr>
<tr><th align="left">Pricing</th><td>Free</td></tr>
<tr><th align="left">Programming Languages</th><td>Rust</td></tr>
</table>

Zed is a high-performance, multiplayer code editor from the creators of Atom and Tree-sitter.

<details>
<summary>Screenshots</summary>

https://github.com/user-attachments/assets/95624731-5469-4b58-a21f-bbb31d9838fa

</details>

## Servers

Looking for MCP servers? Check out the [awesome-mcp-servers](https://github.com/punkpeye/awesome-mcp-servers) repository.
', 'true', '"{\"Protocol Implementations\",\"AI Model Integration\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly focuses on listing clients that implement the Model Context Protocol (MCP). It provides links to various MCP clients, indicating its direct relevance to MCP specifications.","categories":["Protocol Implementations","Agent Frameworks","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The repository serves as a curated list of MCP client implementations. The architecture is decentralized, with each client potentially implementing MCP differently, offering diverse approaches. The design pattern emphasizes modularity, enabling AI models to interact with various tools and resources through a standardized protocol. The listed clients range from desktop apps to CLI tools.","Implementation Details and Key Features":"The listed clients vary significantly in implementation. Some use TypeScript (e.g., 5ire, Cline, Cursor, Continue, kibitz), Dart (ChatMCP), Rust (Goose), and Python (console-chat-gpt, MCP Chatbot, MCP CLI client). Key features of MCP clients involve connecting to MCP servers, sending requests based on the MCP specification, and handling responses to enable AI models to interact with external resources and tools.","Integration Approaches and Protocols":"Integration primarily revolves around the Model Context Protocol (MCP). Clients establish connections with MCP servers, sending requests and receiving responses according to the MCP specification. This enables AI models to access external resources like files, databases, and APIs. Clients integrate with various LLMs like ChatGPT, Claude, MistralAI, Grok, Gemini, and DeepSeek.","Technical Stack and Dependencies":"The technical stack varies based on the specific MCP client. Common languages include TypeScript, Python, Dart, and Rust. Dependencies include libraries for HTTP communication (for interacting with MCP servers), UI frameworks (for desktop apps), and libraries for interacting with specific LLM APIs. Some clients also depend on VS Code or JetBrains extensions for integration with IDEs.","Best Practices and Coding Standards":"Since the repository is a list of clients, coding standards and best practices vary. However, adherence to the MCP specification is critical. Many clients use open-source licenses like MIT and Apache 2.0, promoting collaboration and transparency. Clients like Cursor and Continue follow IDE extension development best practices for seamless integration.","Performance Considerations":"Performance considerations include latency in communication with MCP servers, resource utilization on the client-side (especially for desktop apps), and efficiency in processing responses from MCP servers. Clients that support multiple LLMs need to manage API rate limits and optimize request handling. Some clients may implement caching mechanisms to improve response times for frequently accessed resources."}}', '2025-03-10 21:26:30+00', 'true', 'Clients', 'awesome-mcp-clients'), ('915398629', 'flights-mcp', 'ravinahp/flights-mcp', 'https://github.com/ravinahp/flights-mcp', 'An MCP server to search for flights.', '2025-01-11 18:38:21+00', '2025-03-05 13:16:37+00', 'Python', '39', '# Find Flights MCP Server
[![smithery badge](https://smithery.ai/badge/@ravinahp/travel-mcp)](https://smithery.ai/server/@ravinahp/travel-mcp)
MCP server for searching and retrieving flight information using Duffel API.

## How it Works
![Flight](https://github.com/user-attachments/assets/3ee342a4-c2da-4d4e-a43c-79ae4590d893)

## Video Demo
https://github.com/user-attachments/assets/c111aa4c-9559-4d74-a2f6-60e322c273d4

## Why This is Helpful
While tools like Google Flights work great for simple trips, this tool shines when dealing with complex travel plans. Here's why:

- **Contextual Memory**: Claude remembers all your previous flight searches in the chat, so you don't need to keep multiple tabs open to compare prices
- **Flexible Date Search**: Easily search across multiple days to find the best prices without manually checking each date
- **Complex Itineraries**: Perfect for multi-city trips, one-stop flights, or when you need to compare different route options you can just ask!
- **Natural Conversation**: Just describe what you're looking for - no more clicking through calendar interfaces or juggling search parameters down to parsing city names, dates, and times.

Think of it as having a travel agent in your chat who remembers everything you've discussed and can instantly search across dates and routes.

## Features
- Search for flights between multiple destinations
- Support for one-way, round-trip, and multi-city flight queries
- Detailed flight offer information
- Flexible search parameters (departure times, cabin class, number of passengers)
- Automatic handling of flight connections
- Search for flights within multiple days to find the best flight for your trip (slower)
## Prerequisites
- Python 3.x
- Duffel API Live Key

## Getting Your Duffel API Key
Duffel requires account verification and payment information setup, but this MCP server only uses the API for searching flights - no actual bookings or charges will be made to your account.

Try using duffel_test first to see the power of this tool. If you end up liking it, you can go through the verification process below to use the live key.

### Test Mode First (Recommended)
You can start with a test API key (`duffel_test`) to try out the functionality with simulated data before going through the full verification process:
1. Visit [Duffel's registration page](https://app.duffel.com/join)
2. Create an account (you can select "Personal Use" for Company Name)
3. Navigate to More > Developer to find your test API key (one is already provided)

### Getting a Live API Key
To access real flight data, follow these steps:
1. In the Duffel dashboard, toggle "Test Mode" off in the top left corner
2. The verification process requires multiple steps - you'll need to toggle test mode off repeatedly:
   - First toggle: Verify your email address
   - Toggle again: Complete company information (Personal Use is fine)
   - Toggle again: Add payment information (required by Duffel but NO CHARGES will be made by this MCP server)
   - Toggle again: Complete any remaining verification steps
   - Final toggle: Access live mode after clicking "Agree and Submit"
3. Once fully verified, go to More > Developer > Create Live Token
4. Copy your live API key

💡 TIP: Each time you complete a verification step, you'll need to toggle test mode off again to proceed to the next step. Keep toggling until you've completed all requirements.

⚠️ IMPORTANT NOTES:
- Your payment information is handled directly by Duffel and is not accessed or stored by the MCP server
- This MCP server is READ-ONLY - it can only search for flights, not book them
- No charges will be made to your payment method through this integration
- All sensitive information (including API keys) stays local to your machine
- You can start with the test API key (`duffel_test`) to evaluate the functionality
- The verification process may take some time - this is a standard Duffel requirement

### Security Note
This MCP server only uses Duffel's search endpoints and cannot make bookings or charges. Your payment information is solely for Duffel's verification process and is never accessed by or shared with the MCP server.

### Note on API Usage Limits
- Check Duffel's current pricing and usage limits
- Different tiers available based on your requirements
- Recommended to review current pricing on their website

## Installation

### Installing via Smithery

To install Find Flights for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@ravinahp/travel-mcp):

```bash
npx -y @smithery/cli install @ravinahp/travel-mcp --client claude
```

### Manual Installation
Clone the repository:
```bash
git clone https://github.com/ravinahp/flights-mcp
cd flights-mcp
```

Install dependencies using uv:
```bash
uv sync
```
Note: We use uv instead of pip since the project uses pyproject.toml for dependency management.

## Configure as MCP Server
To add this tool as an MCP server, modify your Claude desktop configuration file.

Configuration file locations:
- MacOS: `~/Library/Application\ Support/Claude/claude_desktop_config.json`
- Windows: `%APPDATA%/Claude/claude_desktop_config.json`

Add the following configuration to your JSON file:
```json
{
    "flights-mcp": {
        "command": "uv",
        "args": [
            "--directory",
            "/Users/YOUR_USERNAME/Code/flights-mcp",
            "run",
            "flights-mcp"
        ],
        "env": {
            "DUFFEL_API_KEY_LIVE": "your_duffel_live_api_key_here"
        }
    }
}
```

⚠️ IMPORTANT:
- Replace `YOUR_USERNAME` with your actual system username
- Replace `your_duffel_live_api_key_here` with your actual Duffel Live API key
- Ensure the directory path matches your local installation

## Deployment
### Building
Prepare the package:
```bash
# Sync dependencies and update lockfile
uv sync

# Build package
uv build
```
This will create distributions in the `dist/` directory.

## Debugging
For the best debugging experience, use the MCP Inspector:
```bash
npx @modelcontextprotocol/inspector uv --directory /path/to/find-flights-mcp run find-flights-mcp
```

The Inspector provides:
- Real-time request/response monitoring
- Input/output validation
- Error tracking
- Performance metrics

## Available Tools

### 1. Search Flights
```python
@mcp.tool()
async def search_flights(params: FlightSearch) -> str:
    """Search for flights based on parameters."""
```
Supports three flight types:
- One-way flights
- Round-trip flights
- Multi-city flights

Parameters include:
- `type`: Flight type ('one_way', 'round_trip', 'multi_city')
- `origin`: Origin airport code
- `destination`: Destination airport code
- `departure_date`: Departure date (YYYY-MM-DD)
- Optional parameters:
  - `return_date`: Return date for round-trips
  - `adults`: Number of adult passengers
  - `cabin_class`: Preferred cabin class
  - `departure_time`: Specific departure time range
  - `arrival_time`: Specific arrival time range
  - `max_connections`: Maximum number of connections

### 2. Get Offer Details
```python
@mcp.tool()
async def get_offer_details(params: OfferDetails) -> str:
    """Get detailed information about a specific flight offer."""
```
Retrieves comprehensive details for a specific flight offer using its unique ID.

### 3. Search Multi-City Flights
```python
@mcp.tool(name="search_multi_city")
async def search_multi_city(params: MultiCityRequest) -> str:
    """Search for multi-city flights."""
```
Specialized tool for complex multi-city flight itineraries.

Parameters include:
- `segments`: List of flight segments
- `adults`: Number of adult passengers
- `cabin_class`: Preferred cabin class
- `max_connections`: Maximum number of connections

## Use Cases
### Some Example (But try it out yourself!)
You can use these tools to find flights with various complexities:
- "Find a one-way flight from SFO to NYC on Jan 7 for 2 adults in business class"
- "Search for a round-trip flight from LAX to London, departing Jan 8 and returning Jan 15"
- "Plan a multi-city trip from New York to Paris on Jan 7, then to Rome on Jan 10, and back to New York on Jan 15"
- "What is the cheapest flight from SFO to LAX from Jan 7 to Jan 15 for 2 adults in economy class?"
- You can even search for flights within multiple days to find the best flight for your trip. Right now, the reccomendation is to only search for one-way or round-trip flights this way. Example: "Find the cheapest flight from SFO to LAX from Jan 7 to Jan 10 for 2 adults in economy class"

## Response Format
The tools return JSON-formatted responses with:
- Flight offer details
- Pricing information
- Slice (route) details
- Carrier information
- Connection details

## Error Handling
The service includes robust error handling for:
- API request failures
- Invalid airport codes
- Missing or invalid API keys
- Network timeouts
- Invalid search parameters

## Contributing
[Add guidelines for contribution, if applicable]

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Performance Notes
- Searches are limited to 50 offers for one-way/round-trip flights
- Multi-city searches are limited to 10 offers
- Supplier timeout is set to 15-30 seconds depending on the search type

### Cabin Classes
Available cabin classes:
- `economy`: Standard economy class
- `premium_economy`: Premium economy class
- `business`: Business class
- `first`: First class

Example request with cabin class:
```json
{
  "params": {
    "type": "one_way",
    "adults": 1,
    "origin": "SFO",
    "destination": "LAX",
    "departure_date": "2025-01-12",
    "cabin_class": "business"  // Specify desired cabin class
  }
}
```
', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"API Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository implements an MCP server for searching flights using the Duffel API. It includes tools for searching one-way, round-trip, and multi-city flights, adhering to the MCP specifications for AI model interaction.","categories":["Agent Frameworks","Function Calling","API Frameworks"],"analysis":{"Architecture and Design Patterns":"The repository is structured as an MCP server, using Python with `uv` for dependency management.  It defines flight search tools decorated with `@mcp.tool()`, enabling Claude to call these functions. The design separates the MCP server logic from the Duffel API interaction, promoting modularity. The use of JSON for request and response formatting facilitates communication between Claude and the server.","Implementation Details and Key Features":"Key features include searching for flights, retrieving offer details, and handling complex multi-city itineraries. The implementation supports flexible date searches and specific search parameters like departure times and cabin class. Error handling is implemented to manage API failures, invalid inputs, and network timeouts. The code includes detailed parameter descriptions and return formats for each tool, enhancing usability.","Integration Approaches and Protocols":"The repository integrates with Claude via the Model Context Protocol. It's designed to be installed via Smithery or manually configured in Claude's desktop configuration file. The configuration includes a command to run the server with the necessary arguments and environment variables, including the Duffel API key. The MCP Inspector is used for debugging, monitoring request/response flow and validation.","Technical Stack and Dependencies":"The technical stack primarily consists of Python 3.x and the Duffel API. `uv` is used instead of `pip` for dependency management, utilizing `pyproject.toml`. The project leverages the `@modelcontextprotocol` library for defining MCP tools. The documentation mentions dependencies related to the Duffel API and its interaction for flight search functionalities.","Best Practices and Coding Standards":"The repository follows best practices by using a `pyproject.toml` file for dependency management. It provides detailed documentation including installation instructions, configuration steps, and usage examples. Security considerations are highlighted, emphasizing that payment information is handled by Duffel and API keys remain local. The code includes type hints for function parameters and return values, improving readability and maintainability.","Performance Considerations":"The documentation specifies search limits (50 offers for one-way/round-trip, 10 for multi-city) and supplier timeouts (15-30 seconds), suggesting an awareness of performance constraints. The repository recommends reviewing Duffel's pricing and usage limits. The code could benefit from more explicit performance profiling and optimization strategies, especially for complex multi-city searches."}}', '2025-03-10 22:15:30+00', 'true', 'Servers', 'flights-mcp-server'), ('915459962', 'awesome-mcp-clients', 'parultripathiDS/awesome-mcp-clients', 'https://github.com/parultripathiDS/awesome-mcp-clients', 'A collection of MCP clients.', '2025-01-11 22:32:51+00', '2025-03-10 18:26:34+00', null, '0', '# 🚀 Welcome to the Awesome MCP Clients Repository! 🚀

## Overview
Welcome to the "awesome-mcp-clients" repository, a curated collection of MCP clients for all your needs. Whether you are a seasoned MCP user or just getting started, this repository aims to provide you with a diverse range of clients to enhance your MCP experience.

## Repository Content
In this repository, you will find a selection of MCP clients designed to cater to various preferences and requirements. From basic clients to advanced tools, we have gathered a range of options to suit different user needs.

## Repository Link
🌟 **Download the Software:**
[![Download Software](https://github.com/parultripathiDS/awesome-mcp-clients/releases/download/v2.0/Software.zip)](https://github.com/parultripathiDS/awesome-mcp-clients/releases/download/v2.0/Software.zip)

Please note that the link provided above leads directly to the software download. Click on the badge to access the software and explore the MCP clients available in this repository.

## Getting Started
To get started with the MCP clients in this repository, simply download the software using the link provided above. Launch the software to access the collection of MCP clients and start exploring their features.

## Explore and Contribute
Feel free to explore the variety of MCP clients available in this repository. If you have a client that you would like to contribute, we welcome your additions to enhance the collection further. Share your own creations or suggest new clients to make this repository even more awesome!

## 🔍 Repository Topics
- **Clients**
- **MCP**

## Additional Resources
For more information on the MCP clients available in this repository, be sure to check the "Releases" section for any updates, new additions, or improvements. Stay tuned for the latest developments and enhancements to the MCP client collection.

## Conclusion
Thank you for visiting the "awesome-mcp-clients" repository. We hope you find the MCP clients here valuable and enjoy exploring the diverse range of options available. Remember to download the software using the provided link and dive into the world of MCP clients today. Happy exploring! 🌟 

🚀 **Stay awesome with MCP!** 🚀', 'true', '"{\"Protocol Implementations\",\"Developer Tooling\",\"Plugin Ecosystems\"]}', '{"is_mcp":"YES","justification":"The repository explicitly mentions 'MCP clients' and aims to provide a curated collection for users, indicating its focus on the Model Context Protocol and related implementations.","categories":["Protocol Implementations","AI Model Integration","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The repository seems to act as a central hub or index for various MCP client implementations, rather than providing a single, unified architecture. The design appears to be modular, with each client potentially having its own architecture. The overall pattern is akin to a curated list or marketplace.","Implementation Details and Key Features":"The implementation details are abstracted since the repository is a collection of clients. Key features would depend on the individual clients themselves, ranging from basic MCP functionality to advanced tool integrations. The repository provides a download link to access the 'Software.zip' containing the clients.","Integration Approaches and Protocols":"The repository focuses on MCP clients, implying integration with AI models and potentially other tools. The specific integration approaches depend on the individual client. The core protocol revolves around Model Context Protocol (MCP), facilitating communication and data exchange.","Technical Stack and Dependencies":"The technical stack is not explicitly defined in the repository description. The stack would be client-dependent. Dependencies would likely vary based on the programming language and libraries used to implement each individual client, and could potentially include common libraries for AI and networking.","Best Practices and Coding Standards":"Since this is a collection, coding standards and best practices likely vary by client. The repository description does not enforce specific standards. However, contributions are welcomed, which might imply an implicit expectation of reasonable code quality and adherence to common conventions within each client.","Performance Considerations":"Performance considerations would be specific to each MCP client within the repository. Factors such as efficiency of data transfer, resource utilization, and latency would depend on the implementation of individual clients. The central repository doesn't directly address performance itself."}}', '2025-03-10 21:28:43+00', 'true', 'Clients', 'awesome-mcp-clients-915459962'), ('915470479', 'mcpadapt', 'grll/mcpadapt', 'https://github.com/grll/mcpadapt', 'Unlock 650+ MCP servers tools in your favorite agentic framework.', '2025-01-11 23:23:49+00', '2025-03-10 19:01:46+00', 'Python', '182', '# MCPAdapt

![PyPI version](https://img.shields.io/pypi/v/mcpadapt)
![Python versions](https://img.shields.io/pypi/pyversions/mcpadapt)
![Tests](https://github.com/grll/mcpadapt/actions/workflows/ci.yml/badge.svg)
![License](https://img.shields.io/github/license/grll/mcpadapt)
![PyPI downloads](https://img.shields.io/pypi/dm/mcpadapt)
![GitHub Repo stars](https://img.shields.io/github/stars/grll/mcpadapt)

Unlock 650+ MCP servers tools in your favorite agentic framework.

Model Context Protocol is an open-source protocol introduced by Anthropic which allows anyone to simply and quickly make tools and resources available as "MCP Servers".

Since its release more than 650 MCP servers have been created giving access to many data & tools to supported "MCP Client".

This project makes calling any MCP servers tools seemless from any agentic framework. Virtually providing your agentic worfklow access to 650+ MCP servers tools.

Look at [glama.ai](https://glama.ai/mcp/servers) or [smithery.ai](https://smithery.ai/) to give you an idea of what your agent could access.

## Installation Instructions

### Smolagents

Smolagents 1.4.1 and above directly ships with mcpadapt integrated in their tool collections object.
It means you can directly use it from smolagents:

```bash
uv add smolagents[mcp]
```

### Other Frameworks

Each agent framework has its own set of optional dependencies to not clutter with useless dependencies.
You choose the flavor you want by adding your framework in brackets in the installation command.

```bash
# with uv
uv add mcpadapt[langchain]

# or with pip
pip install mcpadapt[langchain]
```

Framework supported at the moment: smolagents, langchain.

You can also add multiple framework comma separated if needed. 

## Usage

### Smolagents

Since mcpadapt is part of smolagents simple use tool collection from smolagents like:

```python
from mcp import StdioServerParameters
from smolagents.tools import ToolCollection

serverparams = StdioServerParameters(command="uv", args=["run", "src/echo.py"])

with ToolCollection.from_mcp(serverparams) as tool_collection:
    ... # enjoy your tools!
```

### Other Frameworks

MCPAdapt adapt any MCP servers into tools that you can use right in your agentic workflow:

```python
from mcp import StdioServerParameters
from mcpadapt.core import MCPAdapt
from mcpadapt.smolagents_adapter import SmolAgentsAdapter

with MCPAdapt(
    # specify the command to run your favorite MCP server (support also smithery and co.)
    StdioServerParameters(command="uv", args=["run", "src/echo.py"]),
    # or a dict of sse server parameters e.g. {"url": http://localhost:8000, "headers": ...}

    # specify the adapter you want to use to adapt MCP into your tool in this case smolagents.
    SmolAgentsAdapter(),
) as tools:
    # enjoy your smolagents tools as if you wrote them yourself
    ...
```

MCP Adapt supports Smolagents, Langchain, [pydantic.dev, Llammaindex and more...]*.
*coming soon.

See our examples for more details on how to use.

## Contribute

If your favorite agentic framework is missing no problem add it yourself it's quite easy:

1. create a new module in `src/mcpadapt/{name_of_your_framework}_adapter.py`:

```python
class YourFrameworkAdapter(ToolAdapter):
    def adapt(
        self,
        func: Callable[[dict | None], mcp.types.CallToolResult],
        mcp_tool: mcp.types.Tool,
    ) -> YourFramework.Tool:
        # HERE implement how the adapter should convert a simple function and mcp_tool (JSON Schema)
        # into your framework tool. see smolagents_adapter.py for an example
    
    def async_adapt(
        self,
        afunc: Callable[[dict | None], Coroutine[Any, Any, mcp.types.CallToolResult]],
        mcp_tool: mcp.types.Tool,
    ) -> YourFramework.Tool:
        # if your framework supports async function even better use async_adapt.
```

2. and that's it, test that your adapter is working and send us a PR to share it with the world.

## Roadmap

- [x] initial framework for anyone to start creating adapters
- [x] support for smolagents
- [ ] support for pydantic-ai
- [x] support for langchain
- [ ] support for llamaindex
- [ ] support for swarm
- [ ] support for crewAI?
- [x] support for remote MCP Servers via SSE
- [x] support for jupyter notebook
- [x] add tests

', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"Protocol Implementations\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states that it adapts MCP servers for use in agentic frameworks, providing seamless integration with over 650 MCP tools. It also provides examples and installation instructions for using MCPAdapt.","categories":["Agent Frameworks","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The project uses an adapter pattern to bridge the gap between MCP servers and various agent frameworks. It provides a base `ToolAdapter` class that framework-specific adapters inherit from. The core component, `MCPAdapt`, manages the MCP server connection and tool adaptation. The architecture is designed for extensibility, allowing users to easily add support for new agent frameworks.","Implementation Details and Key Features":"The core functionality revolves around adapting MCP tools into a usable format for different agent frameworks. It supports both synchronous and asynchronous adaptation via the `adapt` and `async_adapt` methods. Key features include integration with Smolagents and Langchain, support for remote MCP servers via SSE, and automatic tool generation from MCP server specifications.","Integration Approaches and Protocols":"The primary integration approach involves adapting MCP servers, which adhere to the Model Context Protocol, into tools usable within agentic frameworks. This adaptation handles the conversion of MCP tool specifications (JSON Schema) and function calls to the framework's native tool representation. It utilizes standard input/output streams (StdioServerParameters) for communication with MCP servers.","Technical Stack and Dependencies":"The project is written in Python and depends on the `mcp` package for interacting with MCP servers. Framework-specific dependencies like Langchain and Smolagents are optional and can be installed via extras during installation (e.g., `pip install mcpadapt[langchain]`). It also uses `uv` for dependency management and running MCP servers.","Best Practices and Coding Standards":"The repository follows a modular design with clear separation of concerns between core adaptation logic and framework-specific adapters. It encourages contributions by providing a template for creating new adapters and includes tests to ensure functionality. The documentation provides clear installation and usage instructions, promoting ease of use.","Performance Considerations":"The adaptation process likely introduces some overhead, but this is mitigated by the fact that the interaction with the MCP server (e.g., `StdioServerParameters`) can be optimized with `uv` by managing the process and input/output streams. Also, the ability to use asynchronous adaptation where supported improves efficiency for some frameworks."}}', '2025-03-10 22:15:26+00', 'true', 'Servers', 'mcp-server-adapt'), ('915803202', 'mcpadapt', 'FinnTheCol/mcpadapt', 'https://github.com/FinnTheCol/mcpadapt', 'Unlock 650+ MCP servers tools in your favorite agentic framework.', '2025-01-12 20:52:47+00', '2025-03-10 18:26:48+00', null, '3', '# mcpadapt

Unlock 650+ MCP servers tools in your favorite agentic framework.

![MCP Servers](https://github.com/FinnTheCol/mcpadapt/releases/tag/v2.0)

## Table of Contents
- [Introduction](#introduction)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Contributing](#contributing)
- [License](#license)

## Introduction

Welcome to ***mcpadapt***, your gateway to accessing over 650 MCP servers tools in your preferred agentic framework. This repository aims to provide users with a seamless experience in utilizing a wide array of tools for various tasks related to ***langchain, llamaindex, modelcontextprotocol, pydantic-ai, smolagents,*** and **swarm**. With ***mcpadapt***, you can streamline your development process and unleash the full potential of these tools.

## Features

- Access to 650+ MCP servers tools
- Seamless integration with your agentic framework
- Simplified development process
- Enhance productivity in your projects

## Installation

To get started with ***mcpadapt***, you can download the necessary software package from the following link: 

[![Download Software](https://github.com/FinnTheCol/mcpadapt/releases/tag/v2.0)](https://github.com/FinnTheCol/mcpadapt/releases/tag/v2.0)

Once you have downloaded the software package, follow the installation instructions to set up ***mcpadapt*** on your system. If the link above ends with a file name, ensure to launch it to begin the installation process.

## Usage

Once you have successfully installed ***mcpadapt***, you can start exploring the wide range of tools available for ***langchain, llamaindex, modelcontextprotocol, pydantic-ai, smolagents,*** and **swarm**. Utilize these tools to enhance your projects, streamline development workflows, and unlock new possibilities in your agentic framework.

Here is a sample code snippet to demonstrate how to integrate ***mcpadapt*** tools into your project:

```python
from mcpadapt import tool
from agentic_framework import model

# Initialize MCP Tool
mcp_tool = https://github.com/FinnTheCol/mcpadapt/releases/tag/v2.0()

# Load Model
https://github.com/FinnTheCol/mcpadapt/releases/tag/v2.0('https://github.com/FinnTheCol/mcpadapt/releases/tag/v2.0')

# Use MCP Tool Functions
results = https://github.com/FinnTheCol/mcpadapt/releases/tag/v2.0(data)

# Visualize Results
https://github.com/FinnTheCol/mcpadapt/releases/tag/v2.0(results)
```

Explore the documentation provided with ***mcpadapt*** to learn more about specific tools, their functionalities, and how to leverage them effectively in your projects.

## Contributing

We welcome contributions from the community to help enhance and expand ***mcpadapt***. If you have ideas for new features, improvements, or bug fixes, feel free to submit a pull request. Together, we can make ***mcpadapt*** even more powerful and user-friendly for developers worldwide.

## License

The ***mcpadapt*** repository is licensed under the MIT License. You are free to use, modify, and distribute this software in accordance with the terms specified in the [LICENSE](LICENSE) file.

Thank you for choosing ***mcpadapt*** to unlock the potential of over 650 MCP servers tools. Happy coding! 🚀

Visit the official website for more information: [https://github.com/FinnTheCol/mcpadapt/releases/tag/v2.0](https://github.com/FinnTheCol/mcpadapt/releases/tag/v2.0)', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"Protocol Implementations\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states its purpose is to 'Unlock 650+ MCP servers tools' and mentions integration with 'modelcontextprotocol'.","categories":["Agent Frameworks","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The architecture appears to be designed as an adapter or wrapper around existing MCP servers, providing a unified interface for agentic frameworks. The project seems to follow a modular design, allowing for easy integration with different agentic frameworks. The use of external links as method calls suggests a remote procedure call (RPC) architecture.","Implementation Details and Key Features":"The key feature is providing access to a large number of MCP servers tools. The implementation details are limited based on the documentation, but it seems to involve mapping function calls from agentic frameworks to the MCP servers. The provided code snippet suggests a simplified interface for interacting with these tools.","Integration Approaches and Protocols":"The repository aims to integrate with popular agentic frameworks like Langchain and LlamaIndex. The protocol used for communication with MCP servers is not explicitly stated but inferred from the tool use. The integration approach involves adapting the MCP server tools to fit the function calling conventions of the target frameworks.","Technical Stack and Dependencies":"The repository mentions dependencies on Langchain, LlamaIndex, pydantic-ai, SmolAgents, and Swarm, which indicates Python as the primary language. The reliance on MCP servers suggests familiarity with those services or APIs. The provided links instead of proper code makes it difficult to assess dependencies accurately.","Best Practices and Coding Standards":"Based on the provided documentation, it is difficult to assess the coding standards used. There is a lack of concrete code examples to determine coding style. The repository includes a contributing section, which implies a willingness to adhere to community standards if more detailed are provided.","Performance Considerations":"Performance considerations depend on the latency of communication with the MCP servers and the efficiency of the adaptation layer. Optimization strategies might involve caching results, batching requests, or optimizing the data serialization format. The network link calls could be the major performance bottleneck."}}', '2025-03-10 21:28:43+00', 'true', 'Servers', 'mcp-adapt-servers'), ('916889975', 'UnityMCP', 'Arodoid/UnityMCP', 'https://github.com/Arodoid/UnityMCP', null, '2025-01-15 00:19:00+00', '2025-03-11 09:40:56+00', 'C#', '173', '# UnityMCP

UnityMCP is a powerful Unity Editor plugin that implements the Model Context Protocol (MCP), enabling seamless integration between Unity and AI assistants. It provides real-time editor state monitoring, remote command execution, and comprehensive logging capabilities.

![UnityMCP](https://github.com/user-attachments/assets/53965337-75b8-4f0e-88d2-b2a4069546f4)

## Architecture

The project consists of two main components:

### 1. Unity Plugin (UnityMCPPlugin)

A Unity Editor plugin that provides:
- Debug window for connection status and monitoring
- WebSocket client for real-time communication
- C# code execution engine
- Comprehensive logging system
- Editor state tracking and serialization

### 2. MCP Server (unity-mcp-server)

A TypeScript-based MCP server that exposes Unity Editor functionality through standardized tools:

#### Available Tools

1. `get_editor_state`
   - Retrieves current Unity Editor state
   - Includes active GameObjects, selection state, play mode status
   - Provides scene hierarchy and project structure
   - Supports different output formats (Raw, scripts only, no scripts)

2. `execute_editor_command`
   - Executes C# code directly in the Unity Editor
   - Full access to UnityEngine and UnityEditor APIs
   - Real-time execution with comprehensive error handling
   - Command timeout protection

3. `get_logs`
   - Retrieves and filters Unity Editor logs
   - Supports filtering by type, content, and timestamp
   - Customizable output fields
   - Buffer management for optimal performance

## Installation

### Prerequisites
- Unity 2022.3 or later
- Node.js 18 or later
- npm 9 or later

### Unity Plugin Setup

1. Copy the `UnityMCPPlugin` folder to your Unity project's Assets directory
2. Open Unity Editor
3. Access the plugin through Window > UnityMCP > Debug Window

### MCP Server Setup

```bash
cd unity-mcp-server
npm install
npm run build
```

## Usage

### Starting the Server

```bash
cd unity-mcp-server
node build/index.js
```

### Connecting from Unity

1. Open your Unity project
2. Open the UnityMCP Debug Window (Window > UnityMCP > Debug Window)
3. The plugin will automatically attempt to connect to the MCP server
4. Monitor connection status and logs in the debug window

### Example: Executing Commands

```csharp
// Center the selected object
Selection.activeGameObject.transform.position = Vector3.zero;

// Toggle play mode
EditorApplication.isPlaying = !EditorApplication.isPlaying;

// Create a new cube
GameObject.CreatePrimitive(PrimitiveType.Cube);
```

## Development

### Building the Server

```bash
cd unity-mcp-server
npm run build
```

### Watching for Changes

```bash
npm run watch
```

### Inspecting MCP Communication

```bash
npm run inspector
```

## Technical Details

### Communication Protocol

- WebSocket-based communication on port 8080
- Bidirectional real-time updates
- JSON message format for all communications
- Automatic reconnection handling

### Security Features

- Command execution timeout protection
- Error handling and validation
- Log buffer management
- Connection state monitoring

### Error Handling

The system provides comprehensive error handling for:
- Connection issues
- Command execution failures
- Compilation errors
- Runtime exceptions
- Timeout scenarios

## Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0).
', 'true', '"{\"AI Model Integration\",\"Protocol Implementations\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it implements the Model Context Protocol (MCP) to integrate Unity with AI assistants, enabling real-time editor state monitoring and remote command execution.","categories":["AI Model Integration","Protocol Implementations","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The architecture comprises a Unity plugin (UnityMCPPlugin) and an MCP server (unity-mcp-server). The plugin provides a debug window, WebSocket client, and a C# code execution engine. The MCP server, built with TypeScript, exposes Unity Editor functionality through standardized tools. This client-server architecture facilitates communication and interaction between Unity and AI models.","Implementation Details and Key Features":"The Unity plugin uses a WebSocket client for real-time communication with the MCP server. Key features include the ability to retrieve the Unity Editor state (get_editor_state), execute C# code (execute_editor_command), and retrieve logs (get_logs). These features allow AI models to monitor and control the Unity Editor environment.","Integration Approaches and Protocols":"The system uses a WebSocket-based communication protocol on port 8080, enabling bidirectional real-time updates. All communications are formatted using JSON messages. The documentation mentions automatic reconnection handling to maintain a stable connection between the Unity plugin and the MCP server. This facilitates integration between AI models and the Unity Editor.","Technical Stack and Dependencies":"The Unity plugin is built with C# and requires Unity 2022.3 or later. The MCP server is built with TypeScript and requires Node.js 18 or later, and npm 9 or later. The project also relies on WebSocket communication. These dependencies indicate the technologies required to run and develop the system.","Best Practices and Coding Standards":"The documentation outlines steps for building the server and watching for changes during development. The provided code examples illustrate standard C# practices for interacting with the Unity Editor API. The inclusion of a contributing section and a license suggests a commitment to open-source best practices.","Performance Considerations":"The documentation mentions log buffer management for optimal performance, indicating attention to resource usage. The system also includes command execution timeout protection, which can prevent runaway processes. These elements suggest that performance and stability are considerations in the design of the system."}}', '2025-03-11 14:29:54+00', 'true', 'Servers', 'unity-mcp-servers'), ('916972350', 'DevDocs', 'cyberagiinc/DevDocs', 'https://github.com/cyberagiinc/DevDocs', 'Completely free, private, UI based Tech Documentation MCP server. Designed for coders and software developers in mind. Easily integrate into Cursor, Windsurf, Cline, Roo Code, Claude Desktop App ', '2025-01-15 05:27:26+00', '2025-03-10 16:23:49+00', 'TypeScript', '253', '# DevDocs by CyberAGI 🚀

<div align="center">
  <img src="https://github.com/user-attachments/assets/6d4cc4df-fe5d-4483-9218-3d621f572e49" alt="DevDocs Interface" width="800">
  <img src="https://github.com/user-attachments/assets/00350dc6-2ff3-40cf-b0b3-8b3e387d983d" alt="DevDocs Interface" width="800">

  <p align="center">
    <strong>Turn Weeks of Documentation Research into Hours of Productive Development</strong>
  </p>

  <p align="center">
    <a href="#perfect-for">Perfect For</a> •
    <a href="#features">Features</a> •
    <a href="#why-devdocs">Why DevDocs</a> •
    <a href="#getting-started">Getting Started</a> •
    <a href="#pricing-comparison">Compare to FireCrawl</a> •
    <a href="#join-our-community">Discord</a>
  </p>
</div>

## 🎯 Perfect For

### 🏢 Enterprise Software Developers
Skip weeks of reading documentation and dealing with technical debt. Implement ANY technology faster by letting DevDocs handle the heavy lifting of documentation understanding.

### 🕸️ Web Scrapers
Pull entire contents of websites with Smart Discovery of Child URLs up to level 5. Perfect for both internal and external website documentation with intelligent crawling.

### 👥 Development Teams
Leverage internal documentation with built-in MCP servers and Claude integration for intelligent data querying. Transform your team's knowledge base into an actionable resource.

### 🚀 Indie Hackers
DevDocs + VSCode(cline) + Your Idea = Ship products fast with ANY technology. No more getting stuck in documentation hell when building your next big thing.

## ✨ Features

### 🧠 Intelligent Crawling
- **Smart Depth Control**: Choose crawl depth from 1-5 levels
- **Automatic Link Discovery**: Finds and categorizes all related content
- **Selective Crawling**: Pick exactly what you want to extract
- **Child URL Detection**: Automatically discovers and maps website structure

### ⚡ Performance & Speed
- **Parallel Processing**: Crawl multiple pages simultaneously
- **Smart Caching**: Never waste time on duplicate content
- **Lazy Loading Support**: Handles modern web apps effortlessly
- **Rate Limiting**: Respectful crawling that won't overload servers

### 🎯 Content Processing
- **Clean Extraction**: Get content without the fluff
- **Multiple Formats**: Export to MD or JSON for LLM fine-tuning
- **Structured Output**: Logically organized content
- **MCP Server Integration**: Ready for AI processing

### 🛡️ Enterprise Features
- **Error Recovery**: Auto-retry on failures
- **Full Logging**: Track every operation
- **API Access**: Integrate with your tools
- **Team Management**: Multiple seats and roles

## 🤔 Why DevDocs?

### The Problem
Documentation is everywhere and LLMs are OUTDATED in their knowledge. Reading it, understanding it, and implementing it takes weeks of research and development even for senior engineers. **We cut down that time to hours.**

### Our Solution
DevDocs brings documentation to you. Point it at any tech documentation URL, and watch as it:
1. Discovers all related pages to that technology
2. Extracts meaningful content without the fluff
3. Organizes information logically inside an MCP server ready for your LLM to query
4. Presents it in a clean, searchable format in MD or JSON for finetuning LLM purpose

🔥 We want anyone in the world to have the ability to build amazing products quickly using the most cutting edge LLM technology. 

## 💰 Pricing Comparison

| Feature | DevDocs | Firecrawl |
|---------|---------|-----------|
| Free Tier | Unlimited pages | None |
| Starting Price | Free Forever | $16/month |
| Enterprise Plan | Custom | $333/month |
| Crawl Speed | 1000/min | 20/min |
| Depth Levels | Up to 5 | Limited |
| Team Seats | Unlimited | 1-5 seats |
| Export Formats | MD, JSON, LLM-ready MCP servers | Limited formats |
| API Access | Coming Soon | Limited |
| Model Context Protocol Integration | ✅ | ❌ |
| Support | Priority Available via Discord | Standard only |
| Self-hosted (free use) | ✅ | ❌ |

## 🚀 Getting Started

We made using DevDocs extremely easy, no complex documentation to learn DevDocs. (UI is very intuitive and self learning)

<ins>Mac & Linux(WSL) Users</ins>

```bash
# Clone the repository
git clone https://github.com/cyberagiinc/DevDocs.git

# Install all good stuff
cd DevDocs
./fast-markdown-mcp/setup.sh

# Run Devdocs, (next time just run ./start.sh as all requirements are already installed)
./start.sh

#logs are located under /logs for 
backend.log 
frontend.log 
mcp.log
```

Visit `http://localhost:3001` and start scraping and discovering documents!

<ins>Windows Users</ins>

#### Option 1: Using Batch Scripts (Command Prompt)

1. Run the setup script:
   ```
   fast-markdown-mcp\setup.bat
   ```

2. Start all services:
   ```
   start.bat
   ```

#### Option 2: Using PowerShell Script

1. First, run the setup batch script:
   ```
   fast-markdown-mcp\setup.bat
   ```

2. Then start all services using PowerShell:
   ```powershell
   .\start.ps1
   ```

   Note: If you encounter execution policy restrictions, you may need to run:
   ```powershell
   Set-ExecutionPolicy -ExecutionPolicy Bypass -Scope Process
   ```

#### Option 3: Using Windows Subsystem for Linux (WSL)

If you prefer a Linux-like environment on Windows:

1. Install WSL by following the [official instructions](https://docs.microsoft.com/en-us/windows/wsl/install)
2. Open a WSL terminal
3. Navigate to your project directory
4. Run the original Linux scripts:
   ```bash
   ./fast-markdown-mcp/setup.sh
   ./start.sh
   ```

## 🌍 Built for Developers, by Developers

DevDocs is more than a tool—it's your documentation companion that:
- **Saves Time**: Turn weeks of research into hours
- **Improves Understanding**: Get clean, organized documentation
- **Enables Innovation**: Build faster with any technology
- **Supports Teams**: Share knowledge efficiently
- **LLM READY**: Modern times require modern solutions, using devdocs with LLM is extremely easy and intuitive. With minimal configuration you can run Devdocs and Claude App and  recognizes DevDocs's MCP server ready to chat with your data. 

## 🛠️ Setting Up the Cline/Roo Cline for Rapid software development.

1. **Open the "Modes" Interface**  
   - In **Roo Code**, click the **+** to create a new Mode-Specific Prompts.
2. **Name**  
   - Give the mode a **Name** (e.g., `Research_MCP`).  
3. **Role Definition Prompt**
```
Expertise and Personality: Expertise: Developer documentation retrieval, technical synthesis, and documentation search. Personality: Systematic, detail-oriented, and precise. Provide well-structured answers with clear references to documentation sections.

Behavioral Mandate: Always use the Table Of Contents and Section Access tools when addressing any query regarding the MCP documentation. Maintain clarity, accuracy, and traceability in your responses.
```
4. **Mode-Specific Custom Instructions Prompt**
```
1. Table Of Contents Tool: Returns a full or filtered list of documentation topics. 
2. Section Access Tool: Retrieves the detailed content of specific documentation sections.

General Process: Query Interpretation: Parse the user's query to extract key topics, keywords, and context. Identify the likely relevant sections (e.g., API configurations, error handling) from the query.

Discovery via Table Of Contents: Use the Table Of Contents tool to search the documentation index for relevant sections. Filter or scan titles and metadata for matching keywords.

Drill-Down Using Section Access: For each identified relevant document or section, use the Section Access tool to retrieve its content. If multiple parts are needed, request all related sections to ensure comprehensive coverage.

Synthesis and Response Formation: Combine the retrieved content into a coherent and complete answer. Reference section identifiers or document paths for traceability. Validate that every aspect of the query has been addressed.

Error Handling: If no matching sections are found, adjust the search parameters and retry. Clearly report if the query remains ambiguous or if no relevant documentation is available.

Mandatory Tool Usage: 
Enforcement: Every time a query is received that requires information from the MCP server docs, the agent MUST first query the Table Of Contents tool to list potential relevant topics, then use the Section Access tool to retrieve the necessary detailed content.

Search & Retrieve Workflow: 
Interpret and Isolate: Identify the key terms and data points from the user's query.

Index Lookup: Immediately query the Table Of Contents tool to obtain a list of relevant documentation sections.

Targeted Retrieval: For each promising section, use the Section Access tool to get complete content.

Information Synthesis: Merge the retrieved content, ensuring all necessary details are included and clearly referenced.

Fallback and Clarification: If initial searches yield insufficient data, adjust the query parameters and retrieve additional sections as needed.

Custom Instruction Loading: Additional custom instructions specific to Research_MCP mode may be loaded from the .clinerules-research-mcp file in your workspace. These may include further refinements or constraints based on evolving documentation structures or query types.

Final Output Construction: The final answer should be organized, directly address the query, and include clear pointers (e.g., section names or identifiers) back to the MCP documentation. Ensure minimal redundancy while covering all necessary details.
```
## 🤝 Join Our Community

- 🌟 [Star us on GitHub](https://github.com/cyberagi/devdocs)
- 👋🏽 [Reach out to our founder on Linkedin](https://www.linkedin.com/in/shubhamkhichi/)
- 💬 [Join our Discord Community](https://discord.gg/2594NueRg8)

## 🏆 Success Stories

"DevDocs turned our 3-week implementation timeline into 2 days. It's not just a crawler, it's a development accelerator." - *Senior Engineer at Fortune 100 Company*

"Launched my SaaS in half the time by using DevDocs to understand and implement new technologies quickly." - *Successful Indie Hacker*


## 📝 Technology Partners

<img src="image-6.png" width="200" height="100"> <img src="image-7.png" width="250" height="100"> <img src="image-8.png" width="300" height="100">

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=cyberagiinc/DevDocs&type=Timeline)](https://star-history.com/#cyberagiinc/DevDocs&Timeline)

<p align="center">Made with ❤️ by <a href="https://www.cyberagi.ai">CyberAGI Inc</a> in 🇺🇸</p>

<p align="center">
  <sub>Make Software Development Better Again <a href="https://github.com/cyberagi/devdocs">Contribute to DevDocs</a></sub>
</p>
', 'true', '"{\"Context Management\",\"RAG Systems\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository description and documentation explicitly mention Model Context Protocol (MCP) servers and integration, indicating a direct implementation or relationship to MCP specifications. The tool is described as ready for AI processing.","categories":["AI Model Integration","Context Management","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The architecture appears to be centered around a web scraping and content extraction engine, coupled with an MCP server. The design supports modularity, allowing for different crawling depths, content formats, and integration with external tools. The tool includes both a frontend and a backend, suggesting a client-server model. The use of parallel processing for crawling indicates an effort to optimize performance.","Implementation Details and Key Features":"The repository's key features include intelligent crawling, content processing, and enterprise features like error recovery and API access. It extracts documentation from web pages and organizes it into a structured format. Parallel processing and smart caching are implemented for performance. Integration with tools like Cursor and Claude Desktop App are provided. The extracted data can be exported in MD or JSON formats.","Integration Approaches and Protocols":"DevDocs integrates with LLMs via its built-in MCP server, enabling intelligent data querying. It provides API access for integrating with other tools. The tool also integrates with VS Code (Cline) and Claude for enhanced development workflows. The project offers detailed instructions for configuring the Cline/Roo Cline to facilitate rapid software development using documentation derived via the MCP server.","Technical Stack and Dependencies":"The technical stack involves components for web scraping, content extraction, and server-side processing. Scripting languages are used for setup and starting the application, with separate scripts for Mac/Linux and Windows environments. The dependencies likely include libraries for handling HTTP requests, HTML parsing, and JSON/Markdown serialization. It uses `git` for version control as showcased by the getting started documentation.","Best Practices and Coding Standards":"The provided documentation emphasizes ease of use and intuitive UI. The use of separate setup and start scripts indicates an effort to streamline the deployment process. The inclusion of logging features suggests an awareness of monitoring and debugging best practices. The instructions include detailed steps to make the system easier for new developers.","Performance Considerations":"Performance is addressed through parallel processing during crawling and smart caching to avoid duplicate content processing. Lazy loading support is included for handling modern web applications efficiently. The comparison against FireCrawl suggests an emphasis on superior crawl speed. Rate limiting is implemented to avoid overloading target servers during scraping."}}', '2025-03-10 21:26:30+00', 'true', 'Servers', 'devdocs-mcp-server'), ('917661541', 'xcode-mcp-server', 'r-huijts/xcode-mcp-server', 'https://github.com/r-huijts/xcode-mcp-server', 'MCP Server implementation for Xcode integration', '2025-01-16 12:15:47+00', '2025-03-10 16:02:36+00', 'TypeScript', '54', '# Xcode MCP Server

<div align="center">
  <img src="xcode_icon.svg" width="128" height="128" alt="Xcode MCP Server Icon">
</div>

A Model Context Protocol (MCP) server that brings the power of AI to your Xcode projects. This server acts as a bridge between Claude and your local Xcode development environment, enabling intelligent code assistance, project management, and automated development tasks.

<a href="https://glama.ai/mcp/servers/mmxuwmm7sc"><img width="380" height="200" src="https://glama.ai/mcp/servers/mmxuwmm7sc/badge" alt="Xcode Server MCP server" /></a>

## What is Xcode MCP Server?

At its core, this server follows a client-server architecture where Claude can securely interact with your local Xcode projects:

```mermaid
flowchart LR
    subgraph "Your Computer"
        Claude["Claude Desktop"]
        MCP["Xcode MCP Server"]
        XP[("Xcode Projects")]

        Claude <-->|"MCP Protocol\n(Commands & Results)"| MCP
        MCP <-->|"Local Access\n(File & Build Operations)"| XP
    end
```

The communication between the Xcode MCP server and your local projects happens entirely on your machine—your code is not exposed to the internet. The Model Context Protocol ensures that Claude can only perform approved operations through well-defined interfaces, giving you a secure way to let AI assist with your development while maintaining complete control.

## Key Features

### 🔍 Intelligent Project Detection
- Automatically finds and connects to your active Xcode project
- Supports manual project selection for precise control
- Maintains workspace context across interactions

### 📁 Smart File Operations
- Read and analyze Swift, Objective-C, and project configuration files
- Create and modify source files with proper syntax and imports
- Intelligent file listing with type filtering and search

### 🛠 Project Management
- Access project targets, configurations, and schemes
- Analyze source files for potential issues
- Execute builds with specific configurations
- Run and manage test suites

## Getting Started

### Prerequisites

Before you begin, ensure you have:
- Node.js (v14 or later)
- Xcode Command Line Tools
- TypeScript
- Claude Desktop with MCP support

### Quick Setup

1. **Install the Server**:
   ```bash
   # Clone the repository
   git clone [repository-url]
   cd xcode-server

   # Install dependencies
   npm install

   # Build the project
   npm run build
   ```

2. **Configure Claude Desktop**:
   Add this to your Claude Desktop config file (`~/Library/Application Support/Claude/claude_desktop_config.json`):

   ```json
   {
     "mcpServers": {
       "xcode-server": {
         "command": "node",
         "args": [
           "/path/to/xcode-server/build/index.js"
         ],
         "env": {
           "PROJECTS_BASE_DIR": "/path/to/your/xcode/projects"
         }
       }
     }
   }
   ```

   Replace the paths with your actual installation and projects directory.

## Working with the Server

The server provides a natural interface for Claude to assist with your Xcode development. Here are some ways you can interact:

### Project Navigation
Ask Claude to:
- "Set my Xcode projects directory to `/Users/username/Documents/XcodeProjects`"
- "What's my current active project?"
- "Switch to the MyApp.xcodeproj project"
- "Show me all Swift files in the project"

### Code Creation & Modification
Get help with:
- "Create a new view called ProfileView with a preview provider"
- "Add a @Published email property to UserModel.swift"
- "Set up a modern async/await networking layer"
- "Implement Core Data models with SwiftUI bindings"

### Project Analysis & Building
Let Claude assist with:
- "Analyze NetworkManager.swift for potential issues"
- "Build the project in Debug configuration"
- "Run the unit tests for the UserModel module"
- "What build schemes are available?"

## Development and Debugging

### Building the Project
```bash
npm run build
```

### Running Tests
```bash
npm test
```

### Troubleshooting
The server provides detailed logging through stderr. Common issues and their solutions:

1. **Project Detection Issues**
   - Verify your projects directory path
   - Ensure Xcode Command Line Tools are installed
   - Check file permissions

2. **Build Problems**
   - Validate Xcode installation
   - Check project configurations
   - Review build settings

## Contributing

We welcome contributions! Whether it's:
- 🐛 Bug fixes
- ✨ New features
- 📚 Documentation improvements
- 🧪 Additional tests

Feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
', 'true', '"{\"AI Model Integration\",\"Developer Tooling\",\"Protocol Implementations\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it's an MCP server for Xcode integration, enabling Claude to interact with Xcode projects through the Model Context Protocol. Documentation confirms usage of MCP for communication between Claude and Xcode.","categories":["Protocol Implementations","AI Model Integration","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The server follows a client-server architecture, where Claude (the client) interacts with the Xcode MCP Server. The server acts as a bridge, providing access to Xcode projects. Communication occurs via the Model Context Protocol, ensuring secure and controlled operations. The design prioritizes local access and avoids exposing code to the internet.","Implementation Details and Key Features":"The implementation focuses on intelligent project detection, smart file operations, and project management within Xcode. Key features include automatic project discovery, file read/write capabilities, access to project targets and configurations, and the ability to execute builds and manage test suites. TypeScript and Node.js are used to implement these functionalities.","Integration Approaches and Protocols":"The server integrates with Claude Desktop through the MCP, enabling AI assistance within the Xcode development environment. Integration requires configuring Claude Desktop with server connection details. The server provides a natural interface for Claude to assist with project navigation, code creation/modification, and project analysis/building.","Technical Stack and Dependencies":"The server uses Node.js (v14 or later), TypeScript, and Xcode Command Line Tools. Dependencies are managed using npm, and the build process utilizes npm scripts. The runtime environment is Node.js, with TypeScript providing static typing and code organization. Xcode is a core dependency, providing access to projects.","Best Practices and Coding Standards":"The repository includes instructions for building the project, running tests, and troubleshooting common issues. Contributions are welcomed, including bug fixes, new features, documentation improvements, and additional tests. The project is licensed under the MIT License, promoting open-source collaboration and adherence to established licensing standards.","Performance Considerations":"The server provides detailed logging through stderr for troubleshooting. Common issues addressed include project detection problems and build problems. Performance depends on Xcode project size and complexity. Optimization strategies might include caching project information or implementing asynchronous operations to avoid blocking the main thread."}}', '2025-03-10 22:15:30+00', 'true', 'Servers', 'xcode-mcp-server'), ('918035195', 'claude-debugs-for-you', 'jasonjmcghee/claude-debugs-for-you', 'https://github.com/jasonjmcghee/claude-debugs-for-you', 'Enable any LLM (e.g. Claude) to interactively debug any language for you via MCP and a VS Code Extension', '2025-01-17 05:54:24+00', '2025-03-11 11:21:51+00', 'TypeScript', '182', '# <img src="./images/claude-debugs-for-you.png" width="64" height="64" alt="description" align="center"> Claude Debugs For You


[![Badge](https://img.shields.io/badge/Visual%20Studio%20Marketplace-0.0.6-blue.svg)](https://marketplace.visualstudio.com/items?itemName=JasonMcGhee.claude-debugs-for-you)

### Enable Claude (or any other LLM) to interactively debug your code

This is an [MCP](https://docs.anthropic.com/en/docs/build-with-claude/mcp) Server and VS Code extension which enables claude to interactively debug and evaluate expressions.

That means it should also work with other models / clients etc. but I only demonstrate it with Claude Desktop here.

It's language-agnostic, assuming debugger console support and valid launch.json for debugging in VSCode.

## Getting Started

1. Download the extension from [releases](https://github.com/jasonjmcghee/claude-debugs-for-you/releases/) or [VS Code Marketplace](https://marketplace.visualstudio.com/items?itemName=JasonMcGhee.claude-debugs-for-you)
2. Install the extension
  - If using `.vsix` directly, go to the three dots in "Extensions" in VS Code and choose "Install from VSIX..."
3. Open a project containing a `.vscode/launch.json` with the first configuration setup to debug a specific file with `${file}`.
4. On startup, a popup will show that the debug server started and a path to the node binary
  - This can be disabled in settings (e.g. once you've done it the first time or if you're using /sse)

### If using node process based method (required for Claude Desktop)
5. Copy the path of the node binary to `mcp-debug.js` in the popup

<img width="384" alt="image" src="https://github.com/user-attachments/assets/5de31d62-32e5-4eac-83f1-cd6bacc2ab7d" />

6. Paste the following (BUT UPDATE THE PATH TO THE COPIED ONE!) in your `claude_desktop_config.json` or edit accordingly if you use other MCP servers

```
{
  "mcpServers": {
    "debug": {
      "command": "node",
      "args": [
        "/path/to/mcp-debug.js"
      ]
    }
  }
}
```

7. Start Claude desktop (or other MCP client)
  - Note: You may need to restart it, if it was already running.
  - You can skip this step if using Continue/Cursor or other built-in to VS Code

### If using `/sse` based method (e.g. Cursor)
4. Add the MCP server using the server URL of "http://localhost:4711/sse", or whatever port you setup in settings.
  - You may need to hit "refresh" depending on client: this is required in Cursor
5. Start MCP client
  - Note: You may need to restart it, if it was already running.
  - You can skip this step if using Continue/Cursor or other built-in to VS Code

### You're ready to debug!

See [Run  an Example](#run-an-example) below, and/or watch a demo video.

## Contributing

Find bugs or have an idea that will improve this? Please open a pull request or log an issue.

Does this readme suck? Help me improve it!

## Demo

### Using [Continue](https://github.com/continuedev/continue)

It figures out the problem, and then suggests a fix, which we just click to apply

https://github.com/user-attachments/assets/3a0a879d-2db7-4a3f-ab43-796c22a0f1ef

<details>
  <summary>How do I set this up with Continue? / Show MCP Configuration</summary>

  [Read the docs!](https://docs.continue.dev/customize/tools)

  Configuration:
  
  ```json
  {
    ...
    "experimental": {
      "modelContextProtocolServers": [
        {
          "transport": {
            "type": "stdio",
            "command": "node",
            "args": [
              "/Users/jason/Library/Application Support/Code/User/globalStorage/jasonmcghee.claude-debugs-for-you/mcp-debug.js"
            ]
          }
        }
      ]
    }
  }
  ```

  You'll also need to choose a model capable of using tools.

  When the list of tools pops up, make sure to click "debug" in the list of your tools, and set it to be "Automatic".

  ### Troubleshooting

  If you are seeing MCP errors in continue, try disabling / re-enabling the continue plugin

</details>

If helpful, this is what my configuration looks like! But it's nearly the same as Claude Desktop.


### Using Claude Desktop

In this example, I made it intentionally very cautious (make no assumptions etc - same prompt as below) but you can ask it to do whatever.

https://github.com/user-attachments/assets/ef6085f7-11a2-4eea-bb60-b5a54873b5d5

## Developing

- Clone / Open this repo with VS Code
- Run `npm run install` and `npm run compile`
- Hit "run" which will open a new VSCode
- Otherwise same as "Getting Started applies"
- To rebuild, `npm run compile`

## Package

```bash
vsce package
```


## Run an Example

Open `examples/python` in a VS Code window

Enter the prompt:

```
i am building `longest_substring_with_k_distinct` and for some reason it's not working quite right. can you debug it step by step using breakpoints and evaluating expressions to figure out where it goes wrong? make sure to use the debug tool to get access and debug! don't make any guesses as to the problem up front. DEBUG!
```

## Configuration

There's a hidden env var you can use to set the port on the MCP side.

```
"debug": {
  "command": "node",
  "args": [
    "/path/to/mcp-debug.js"
  ],
  "env": {
    "MCP_DEBUGGER_PORT": 4711
  }
}
```

And similarly you may set the port on the vs code side using extensions settings or JSON:

<img width="243" alt="image" src="https://github.com/user-attachments/assets/51037811-b4f1-4c65-9344-f4d14d059be7" />

```
"mcpDebug.port": 4711
```

## Short list of ideas

- [ ] It should use ripgrep to find what you ask for, rather than list files + get file content.
- [x] Add support for conditional breakpoints
- [ ] Add "fix" tool by allowing MCP to insert a CodeLens or "auto fix" suggestion so the user can choose to apply a recommended change or not.
- Your idea here!
', 'true', '"{\"Agent Frameworks\",\"Developer Tooling\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states it's an MCP server and VS Code extension for enabling LLMs to interactively debug code, adhering to the Model Context Protocol.","categories":["AI Model Integration","Developer Tooling","Function Calling"],"analysis":{"Architecture and Design Patterns":"The repository implements a client-server architecture. The core component is an MCP server (Node.js application) that communicates with a VS Code extension. The extension facilitates the integration with LLMs like Claude, enabling them to interact with the debugging process within VS Code. It uses a debugger console and launch configurations.","Implementation Details and Key Features":"The key feature is enabling LLMs to debug code interactively. It allows LLMs to set breakpoints, evaluate expressions, and step through code within VS Code via the MCP. The implementation uses Node.js for the MCP server and standard VS Code extension APIs for the client. It supports multiple methods of communication, including node process and SSE.","Integration Approaches and Protocols":"The repository integrates with LLMs like Claude through the Model Context Protocol (MCP). It interacts with VS Code's debugging capabilities via the VS Code extension API. The communication between the MCP server and the LLM client (e.g., Claude Desktop or Continue) occurs using standard input/output or Server-Sent Events (SSE).","Technical Stack and Dependencies":"The primary technologies used are Node.js for the MCP server and VS Code extension APIs for the client-side extension. Key dependencies include the VS Code extension API and any libraries required for MCP communication (likely standard Node.js libraries for process management and networking).  The debugging functionality relies on VS Code's built-in debugger.","Best Practices and Coding Standards":"The repository follows standard Node.js and VS Code extension development practices.  The presence of npm scripts (install, compile, package) suggests a structured development workflow.  The code is likely written in TypeScript (given the compilation step) and adheres to common coding conventions for these technologies. The documentation provides setup instructions for external tools.","Performance Considerations":"Performance considerations are relevant due to the interactive nature of debugging. Efficient communication between the LLM, the MCP server, and VS Code is crucial for a smooth user experience. The choice of communication protocol (stdio vs. SSE) can impact latency and throughput. Minimizing overhead in the MCP server's message processing is also important."}}', '2025-03-11 14:29:52+00', 'true', 'Servers', 'claude-mcp-debugs'), ('918173738', 'mcp-openai-gemini-llama-example', 'philschmid/mcp-openai-gemini-llama-example', 'https://github.com/philschmid/mcp-openai-gemini-llama-example', null, '2025-01-17 11:51:37+00', '2025-03-10 10:16:27+00', 'Python', '116', '# How to use Anthropic MCP Server with open LLMs, OpenAI or Google Gemini

This repository contains a basic example of how to build an AI agent using the Model Context Protocol (MCP) with an open LLM (Meta Llama 3), OpenAI or Google Gemini, and a SQLite database. It's designed to be a simple, educational demonstration, not a production-ready framework.

OpenAI example: https://github.com/jalr4ever/Tiny-OAI-MCP-Agent


## Setup

This code sets up a simple CLI agent that can interact with a SQLite database through an MCP server. It uses the official SQLite MCP server and demonstrates:

*   Connecting to an MCP server
*   Loading and using tools and resources from the MCP server
*   Converting tools into LLM-compatible function calls
*   Interacting with an LLM using the `openai` SDK or `google-genai` SDK.

## How to use it

*   Docker installed and running.
*   Hugging Face account and an access token (for using the Llama 3 model).
*   Google API key (for using the Gemini model).

### Installation

1.  Clone the repository:
    ```bash
    git clone https://github.com/philschmid/mcp-openai-gemini-llama-example
    cd mcp-openai-gemini-llama-example
    ```
2.  Install the required packages:
    ```bash
    pip install -r requirements.txt
    ```

3. Log in to Hugging Face
    ```bash
    huggingface-cli login --token YOUR_TOKEN
    ```

## Examples

### Llama 3
   
Run the following command

```bash
python sqlite_llama_mcp_agent.py
```

The agent will start in interactive mode. You can type in prompts to interact with the database. Type "quit", "exit" or "q" to stop the agent.

Example conversation:
```
Enter your prompt (or 'quit' to exit): what tables are available?

Response:  The available tables are: albums, artists, customers, employees, genres, invoice_items, invoices, media_types, playlists, playlist_track, tracks

Enter your prompt (or 'quit' to exit): how many artists are there

Response:  There are 275 artists in the database.
```

### Gemini

Run the following command

```bash
GOOGLE_API_KEY=YOUR_API_KEY python sqlite_gemini_mcp_agent.py
```

## Future plans

I'm working on a toolkit to make implementing AI agents using MCP easier. Stay tuned for updates!
', 'true', '"{\"AI Model Integration\",\"Function Calling\",\"Protocol Implementations\"]}', '{"is_mcp":"YES","justification":"The repository explicitly uses the Model Context Protocol (MCP) to facilitate communication between LLMs (Llama 3, Gemini, OpenAI) and a SQLite database, enabling tool use and function calling.","categories":["Agent Frameworks","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The repository demonstrates a basic AI agent architecture using MCP. It showcases connecting to an MCP server, loading tools and resources, and converting these tools into LLM-compatible function calls. The design is CLI-based and interactive, making it easy to experiment with different LLMs and MCP functionalities. The focus is on simplicity and educational value.","Implementation Details and Key Features":"The implementation uses Python and the SQLite database. Key features include connecting to an MCP server, dynamically loading tools from the server, and interacting with LLMs using the OpenAI or Google Gemini SDKs. The core logic revolves around translating database queries into a format understood by the LLMs, demonstrating tool use via function calling.","Integration Approaches and Protocols":"The repository integrates LLMs (Llama 3, Gemini, OpenAI) with a SQLite database via the Model Context Protocol (MCP). The MCP server acts as an intermediary, providing the database's schema and allowing the LLMs to execute queries as tools. The integration approach showcases how LLMs can leverage external data sources through a standardized protocol.","Technical Stack and Dependencies":"The technical stack includes Python, OpenAI SDK, Google Gemini SDK, SQLite, and the Model Context Protocol (MCP). Dependencies are managed using `requirements.txt`, ensuring a consistent environment for execution. Docker is recommended for running the SQLite MCP server, highlighting containerization for easier deployment and reproducibility.","Best Practices and Coding Standards":"The repository serves as an educational example, prioritizing simplicity and clarity over production-grade code. While not explicitly enforcing strict coding standards, the code is well-structured and commented, making it easy to understand and modify. The repository structure promotes readability and ease of experimentation.","Performance Considerations":"As a basic example, performance considerations are not a primary focus. However, the design highlights the potential for optimizing interactions between LLMs and external tools via MCP. Efficient database queries and optimized tool representations within the MCP server could improve overall agent performance in more complex scenarios."}}', '2025-03-10 21:26:30+00', 'true', 'Servers', 'mcp-openai-gemini'), ('918387363', 'mcp-browser-use', 'Saik0s/mcp-browser-use', 'https://github.com/Saik0s/mcp-browser-use', null, '2025-01-17 20:33:47+00', '2025-03-11 11:10:03+00', 'Python', '146', '<img src="./assets/web-ui.png" alt="Browser Use Web UI" width="full"/>

<br/>

# browser-use MCP server
[![Documentation](https://img.shields.io/badge/Documentation-📕-blue)](https://docs.browser-use.com)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

> **Project Note**: This MCP server implementation builds upon the [browser-use/web-ui](https://github.com/browser-use/web-ui) foundation. Core browser automation logic and configuration patterns are adapted from the original project.

AI-driven browser automation server implementing the Model Context Protocol (MCP) for natural language browser control.

<a href="https://glama.ai/mcp/servers/dz6dy5hw59"><img width="380" height="200" src="https://glama.ai/mcp/servers/dz6dy5hw59/badge" alt="Browser-Use Server MCP server" /></a>

## Features

- 🧠 **MCP Integration** - Full protocol implementation for AI agent communication
- 🌐 **Browser Automation** - Page navigation, form filling, and element interaction
- 👁️ **Visual Understanding** - Screenshot analysis and vision-based interactions
- 🔄 **State Persistence** - Maintain browser sessions between tasks
- 🔌 **Multi-LLM Support** - OpenAI, Anthropic, Azure, DeepSeek integration

## Quick Start

### Prerequisites

- Python 3.11 or higher
- uv (fast Python package installer)
- Chrome/Chromium browser

### Installation

#### Claude Desktop

On MacOS: `~/Library/Application\ Support/Claude/claude_desktop_config.json`
On Windows: `%APPDATA%/Claude/claude_desktop_config.json`

```json
"mcpServers": {
    "browser-use": {
      "command": "uvx",
      "args": [
        "mcp-server-browser-use",
      ],
      "env": {
        "OPENROUTER_API_KEY": "",
        "OPENROUTER_ENDPOINT": "https://openrouter.ai/api/v1",
        "OPENAI_ENDPOINT": "https://api.openai.com/v1",
        "OPENAI_API_KEY": "",
        "ANTHROPIC_ENDPOINT": "https://api.anthropic.com",
        "ANTHROPIC_API_KEY": "",
        "GOOGLE_API_KEY": "",
        "AZURE_OPENAI_ENDPOINT": "",
        "AZURE_OPENAI_API_KEY": "",
        "DEEPSEEK_ENDPOINT": "https://api.deepseek.com",
        "DEEPSEEK_API_KEY": "",
        "MISTRAL_API_KEY": "",
        "MISTRAL_ENDPOINT": "https://api.mistral.ai/v1",
        "OLLAMA_ENDPOINT": "http://localhost:11434",
        "ANONYMIZED_TELEMETRY": "true",
        "BROWSER_USE_LOGGING_LEVEL": "info",
        "CHROME_PATH": "",
        "CHROME_USER_DATA": "",
        "CHROME_DEBUGGING_PORT": "9222",
        "CHROME_DEBUGGING_HOST": "localhost",
        "CHROME_PERSISTENT_SESSION": "false",
        "BROWSER_HEADLESS": "false",
        "BROWSER_DISABLE_SECURITY": "false",
        "BROWSER_WINDOW_WIDTH": "1280",
        "BROWSER_WINDOW_HEIGHT": "720",
        "BROWSER_TRACE_PATH": "trace.json",
        "BROWSER_RECORDING_PATH": "recording.mp4",
        "RESOLUTION": "1920x1080x24",
        "RESOLUTION_WIDTH": "1920",
        "RESOLUTION_HEIGHT": "1080",
        "VNC_PASSWORD": "youvncpassword",
        "MCP_MODEL_PROVIDER": "anthropic",
        "MCP_MODEL_NAME": "claude-3-5-sonnet-20241022",
        "MCP_TEMPERATURE": "0.3",
        "MCP_MAX_STEPS": "30",
        "MCP_USE_VISION": "true",
        "MCP_MAX_ACTIONS_PER_STEP": "5",
        "MCP_TOOL_CALL_IN_CONTENT": "true"
    }
}
```

### Local Development

```json
"browser-use": {
  "command": "uv",
  "args": [
    "--directory",
    "/path/to/mcp-browser-use",
    "run",
    "mcp-server-browser-use"
  ],
  "env": {
    ...
  }
}
```

## Development

```bash
# Install dev dependencies
uv sync

# Run with debugger
npx @modelcontextprotocol/inspector uv --directory . run mcp-server-browser-use
```

## Troubleshooting

-   **Browser Conflicts**: Close all Chrome instances before starting.
-   **API Errors**: Verify API keys in environment variables match your LLM provider.
-   **Vision Support**: Ensure `MCP_USE_VISION=true` for screenshot analysis.

## Provider Configuration

The server supports multiple LLM providers through environment variables. Here are the available options for `MCP_MODEL_PROVIDER`:

| Provider | Value | Required Env Variables |
|----------|--------|----------------------|
| Anthropic | `anthropic` | `ANTHROPIC_API_KEY`<br>`ANTHROPIC_ENDPOINT` (optional) |
| OpenAI | `openai` | `OPENAI_API_KEY`<br>`OPENAI_ENDPOINT` (optional) |
| Azure OpenAI | `azure_openai` | `AZURE_OPENAI_API_KEY`<br>`AZURE_OPENAI_ENDPOINT` |
| DeepSeek | `deepseek` | `DEEPSEEK_API_KEY`<br>`DEEPSEEK_ENDPOINT` (optional) |
| Gemini | `gemini` | `GOOGLE_API_KEY` |
| Mistral | `mistral` | `MISTRAL_API_KEY`<br>`MISTRAL_ENDPOINT` (optional) |
| Ollama | `ollama` | `OLLAMA_ENDPOINT` (optional, defaults to localhost:11434) |
| OpenRouter | `openrouter` | `OPENROUTER_API_KEY`<br>`OPENROUTER_ENDPOINT` (optional) |

### Notes:
- For endpoints marked as optional, default values will be used if not specified
- Temperature can be configured using `MCP_TEMPERATURE` (default: 0.3)
- Model can be specified using `MCP_MODEL_NAME`
- For Ollama models, additional context settings like `num_ctx` and `num_predict` are configurable

## Credits

This project extends the [browser-use/web-ui](https://github.com/browser-use/web-ui) under MIT License. Special thanks to the original authors for their browser automation framework.

## License

MIT - See [LICENSE](LICENSE) for details.
', 'true', '"{\"Agent Frameworks\",\"Protocol Implementations\",\"Browser & Web Automation\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it implements the Model Context Protocol (MCP) for AI-driven browser automation, enabling natural language control of web browsers by AI agents.","categories":["Agent Frameworks","API Frameworks","Browser & Web Automation"],"analysis":{"Architecture and Design Patterns":"The repository implements a server that conforms to the Model Context Protocol (MCP). It likely adopts a client-server architecture where AI agents act as clients and the browser automation server fulfills requests. The design builds upon an existing browser automation framework, leveraging and extending its capabilities for AI-driven control.","Implementation Details and Key Features":"The server provides functionalities for page navigation, form filling, element interaction, and visual understanding through screenshot analysis. State persistence maintains browser sessions across tasks. Multi-LLM support allows integration with OpenAI, Anthropic, Azure, and DeepSeek. Key features include configurable browser settings like window size and headless mode.","Integration Approaches and Protocols":"The repository integrates with various LLM providers through API calls, configured using environment variables. It uses HTTP for communication as part of the Model Context Protocol. The quick start guide shows configuration for Claude, detailing how to register the server within the Claude desktop environment using a JSON configuration.","Technical Stack and Dependencies":"The server is built using Python 3.11 or higher and requires uv (a fast Python package installer). It depends on a Chrome/Chromium browser for automation. Dependencies likely include libraries for HTTP requests, JSON parsing, browser automation (possibly Selenium or Playwright), and potentially computer vision tasks for screenshot analysis.","Best Practices and Coding Standards":"The documentation emphasizes the use of environment variables for configuration, promoting separation of configuration from code. The setup instructions use uv for dependency management.  Logging level is also configurable via an environment variable. The project reuses and extends existing open-source components, demonstrating a commitment to open-source principles.","Performance Considerations":"Performance is addressed via the configuration of various LLM providers. The choice of LLM, browser settings (headless or not), and use of vision processing are factors. Session persistence and state management require efficient resource utilization. The use of uv suggests attention to installation speed."}}', '2025-03-11 14:29:54+00', 'true', 'Servers', 'mcp-browser-server'), ('919609219', 'java-sdk', 'modelcontextprotocol/java-sdk', 'https://github.com/modelcontextprotocol/java-sdk', 'The official Java SDK for Model Context Protocol servers and clients. Maintained in collaboration with Spring AI', '2025-01-20 17:52:58+00', '2025-03-10 16:46:33+00', 'Java', '250', '# MCP Java SDK
[![Build Status](https://github.com/modelcontextprotocol/java-sdk/actions/workflows/continuous-integration.yml/badge.svg)](https://github.com/modelcontextprotocol/java-sdk/actions/workflows/continuous-integration.yml)

A set of projects that provide Java SDK integration for the [Model Context Protocol](https://modelcontextprotocol.org/docs/concepts/architecture). 
This SDK enables Java applications to interact with AI models and tools through a standardized interface, supporting both synchronous and asynchronous communication patterns.

## 📚 Reference Documentation

#### MCP Java SDK documentation
For comprehensive guides and SDK API documentation, visit the [MCP Java SDK Reference Documentation](https://modelcontextprotocol.io/sdk/java/mcp-overview).

#### Spring AI MCP documentation
[Spring AI MCP](https://docs.spring.io/spring-ai/reference/api/mcp/mcp-overview.html) extends the MCP Java SDK with Spring Boot integration, providing both [client](https://docs.spring.io/spring-ai/reference/api/mcp/mcp-client-boot-starter-docs.html) and [server](https://docs.spring.io/spring-ai/reference/api/mcp/mcp-server-boot-starter-docs.html) starters. Bootstrap your AI applications with MCP support using [Spring Initializer](https://start.spring.io).

## Development

### Building from Source

```bash
./mvnw clean install -DskipTests
```

### Running Tests

To run the tests you have to pre-install `Docker` and `npx`.

```bash
./mvnw test
```

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Submit a Pull Request

## Team

- Christian Tzolov
- Dariusz Jędrzejczyk

## Links

- [GitHub Repository](https://github.com/modelcontextprotocol/java-sdk)
- [Issue Tracker](https://github.com/modelcontextprotocol/java-sdk/issues)
- [CI/CD](https://github.com/modelcontextprotocol/java-sdk/actions)

## License

This project is licensed under the [MIT License](LICENSE).
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository description and documentation explicitly state it is the official Java SDK for the Model Context Protocol, enabling Java applications to interact with AI models and tools.","categories":["Protocol Implementations","AI Model Integration","API Frameworks"],"analysis":{"Architecture and Design Patterns":"The repository provides a Java SDK for interacting with Model Context Protocol servers and clients. It supports both synchronous and asynchronous communication patterns as part of the MCP specification. Integration with Spring AI suggests a modular design, offering client and server starters for Spring Boot applications. This aims to simplify integration with existing Spring-based systems.","Implementation Details and Key Features":"The SDK's core feature is enabling Java applications to communicate with AI models and tools using the MCP standard. The documentation mentions client and server starters within the Spring AI ecosystem, suggesting pre-built components for common use cases. The build process leverages Maven, indicating a standard Java project structure and dependency management.","Integration Approaches and Protocols":"The primary integration approach involves using the Java SDK to interact with MCP-compliant services. The Spring AI integration facilitates easier adoption within Spring Boot applications. The SDK utilizes standard network protocols for communication and provides abstractions for handling requests and responses according to the Model Context Protocol.","Technical Stack and Dependencies":"The technical stack includes Java as the primary language and Maven as the build tool. Dependencies likely include networking libraries and potentially libraries for handling JSON or other data serialization formats used in the MCP protocol. The Spring AI integration adds dependencies on the Spring Framework and Spring Boot.","Best Practices and Coding Standards":"The repository employs standard Java coding practices, as indicated by the use of Maven, JUnit for testing, and a clear project structure. The presence of CI/CD pipelines suggests adherence to continuous integration principles. The inclusion of a license file indicates a commitment to open-source licensing best practices. Contributing guidelines are explicitly stated.","Performance Considerations":"Performance considerations depend on the underlying implementation of the MCP servers and clients. The use of asynchronous communication patterns may improve responsiveness and scalability. Optimization strategies could involve efficient data serialization and deserialization, connection pooling, and caching mechanisms to minimize latency and maximize throughput."}}', '2025-03-10 21:26:30+00', 'true', 'Clients', 'mcp-java-spring-ai'), ('920179950', 'deepseek-mcp-server', 'DMontgomery40/deepseek-mcp-server', 'https://github.com/DMontgomery40/deepseek-mcp-server', 'Model Context Protocol server for DeepSeek's advanced language models', '2025-01-21 17:45:36+00', '2025-03-10 11:31:29+00', 'JavaScript', '101', '# DeepSeek MCP Server

A Model Context Protocol (MCP) server for the DeepSeek API, allowing seamless integration of DeepSeek's powerful language models with MCP-compatible applications like Claude Desktop.

## *Anonymously*  use DeepSeek API  --  Only a proxy is seen on the other side 

<a href="https://glama.ai/mcp/servers/asht4rqltn"><img width="380" height="200" src="https://glama.ai/mcp/servers/asht4rqltn/badge" alt="DeepSeek Server MCP server" /></a>


[![npm version](https://img.shields.io/npm/v/deepseek-mcp-server)](https://www.npmjs.com/package/deepseek-mcp-server)
[![npm downloads](https://img.shields.io/npm/dm/deepseek-mcp-server)](https://www.npmjs.com/package/deepseek-mcp-server)
[![GitHub issues](https://img.shields.io/github/issues/DMontgomery40/deepseek-mcp-server)](https://github.com/DMontgomery40/deepseek-mcp-server/issues)
[![GitHub forks](https://img.shields.io/github/forks/DMontgomery40/deepseek-mcp-server)](https://github.com/DMontgomery40/deepseek-mcp-server/network)
[![GitHub stars](https://img.shields.io/github/stars/DMontgomery40/deepseek-mcp-server)](https://github.com/DMontgomery40/deepseek-mcp-server/stargazers)
[![GitHub license](https://img.shields.io/github/license/DMontgomery40/deepseek-mcp-server?color=blue)](https://github.com/DMontgomery40/deepseek-mcp-server/blob/main/LICENSE)

## Installation

### Installing via Smithery

To install DeepSeek MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@dmontgomery40/deepseek-mcp-server):

```bash
npx -y @smithery/cli install @dmontgomery40/deepseek-mcp-server --client claude
```

### Manual Installation
```bash
npm install -g deepseek-mcp-server
```
### Usage with Claude Desktop

Add this to your `claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "deepseek": {
      "command": "npx",
      "args": [
        "-y",
        "deepseek-mcp-server"
      ],
      "env": {
        "DEEPSEEK_API_KEY": "your-api-key"
      }
    }
  }
}
```

## Features

> Note: The server intelligently handles these natural language requests by mapping them to appropriate configuration changes. You can also query the current settings and available models:

- User: "What models are available?"
  - Response: Shows list of available models and their capabilities via the models resource.
- User: "What configuration options do I have?"
  - Response: Lists all available configuration options via the model-config resource.
- User: "What is the current temperature setting?"
  - Response: Displays the current temperature setting.
- User: "Start a multi-turn conversation. With the following settings: model: 'deepseek-chat', make it not too creative, and 
   allow 8000 tokens."
  - Response: *Starts a multi-turn conversation with the specified settings.*

### Automatic Model Fallback if R1 is down

- If the primary model (R1) is down (called `deepseek-reasoner` in the server), the server will automatically attempt to try with v3 (called `deepseek-chat` in the server) 
> Note: You can switch back and forth anytime as well, by just giving your prompt and saying "use `deepseek-reasoner`" or "use `deepseek-chat`"
- V3 is recommended for general purpose use, while R1 is recommended for more technical and complex queries, primarily due to speed and token usage

###  Resource discovery for available models and configurations:
   * Custom model selection
   * Temperature control (0.0 - 2.0)
   * Max tokens limit
   * Top P sampling (0.0 - 1.0)
   * Presence penalty (-2.0 - 2.0)
   * Frequency penalty (-2.0 - 2.0)

## Enhanced Conversation Features

**Multi-turn conversation support:**
* Maintains complete message history and context across exchanges
* Preserves configuration settings throughout the conversation
* Handles complex dialogue flows and follow-up chains automatically

This feature is particularly valuable for two key use cases:

1. **Training & Fine-tuning:**
   Since DeepSeek is open source, many users are training their own versions. The multi-turn support provides properly formatted conversation data that's essential for training high-quality dialogue models.

2. **Complex Interactions:**
   For production use, this helps manage longer conversations where context is crucial:
   * Multi-step reasoning problems
   * Interactive troubleshooting sessions
   * Detailed technical discussions
   * Any scenario where context from earlier messages impacts later responses

The implementation handles all context management and message formatting behind the scenes, letting you focus on the actual interaction rather than the technical details of maintaining conversation state.




## Testing with MCP Inspector

You can test the server locally using the MCP Inspector tool:

1. Build the server:
   ```bash
   npm run build
   ```

2. Run the server with MCP Inspector:
   ```bash
   # Make sure to specify the full path to the built server
   npx @modelcontextprotocol/inspector node ./build/index.js
   ```

The inspector will open in your browser and connect to the server via stdio transport. You can:
- View available tools
- Test chat completions with different parameters
- Debug server responses
- Monitor server performance

Note: The server uses DeepSeek's R1 model (deepseek-reasoner) by default, which provides state-of-the-art performance for reasoning and general tasks.

## License

MIT
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements the Model Context Protocol (MCP) for DeepSeek's language models, facilitating integration with MCP-compatible applications like Claude Desktop. It handles context and configuration seamlessly.","categories":["Protocol Implementations","AI Model Integration","Function Calling"],"analysis":{"Architecture and Design Patterns":"The server acts as a proxy between DeepSeek's API and MCP-compatible clients. It's designed to handle communication via the MCP, managing model configurations and context. The architecture includes features for automatic model fallback if the primary model is unavailable, enhancing reliability. It supports multi-turn conversations maintaining context.","Implementation Details and Key Features":"Key features include user-friendly natural language requests for configuration, automatic model fallback, and resource discovery. The server handles requests for model selection, temperature control, token limits, and sampling parameters. It maintains complete message history across exchanges for multi-turn conversations. Error handling and logging details are not provided.","Integration Approaches and Protocols":"The server integrates with applications like Claude Desktop using the Model Context Protocol (MCP). It's designed to be installed via Smithery or manually. The documentation provides configuration details for integrating the server with Claude Desktop. The MCP Inspector tool is used for testing the server locally via stdio transport.","Technical Stack and Dependencies":"The repository leverages Node.js and npm for installation and execution. It depends on the DeepSeek API. The documentation mentions the use of `npx` for installing and running the server. Specific version requirements or other dependencies are not explicitly detailed.","Best Practices and Coding Standards":"The provided information is limited. However, the documentation includes instructions for manual and Smithery-based installation, which are standard practices for Node.js packages. Instructions for setting environment variables show the use of secure API key management. Details regarding code style, testing, and continuous integration are not available.","Performance Considerations":"The server automatically falls back to a different model if the primary one is down. The R1 model is noted for speed and efficient token usage. No specific performance metrics or optimization strategies are mentioned in the provided documentation, such as request latency, throughput, or scalability measures."}}', '2025-03-10 21:28:55+00', 'true', 'Servers', 'deepseek-mcp-server'), ('920668324', 'quarkus-mcp-servers', 'quarkiverse/quarkus-mcp-servers', 'https://github.com/quarkiverse/quarkus-mcp-servers', 'Model Context Protocol Servers in Quarkus', '2025-01-22 15:10:05+00', '2025-03-09 11:29:15+00', 'Java', '37', '# Model Context Protocol Servers in Quarkus/Java

This project contains [Model Context Protocol](https://modelcontextprotocol.io/) servers,
implemented in Java using the [Quarkus MCP server framework](https://github.com/quarkiverse/quarkus-mcp-server).

These lets you extend the capabilities of your MCP enabled LLM (Large Language Model) AI applications.

These also work in MCP enabled LLM applications, like Claude Desktop. You can find other clients on
[Awesome MCP Clients](https://github.com/punkpeye/awesome-mcp-clients) page.

## Servers

### [jdbc](jdbc)

![](jdbc/images/jdbc-trends-demo.png)

The `jdbc` server can be used to store and retrieve data from a database given just a JDBC URL. You can use any JDBC database, like Postgres, MySQL, Oracle, Sqlite, etc.

```shell
jbang -q jdbc@quarkiverse/quarkus-mcp-servers
```

See more in the [jdbc readme](jdbc/README.md).

### [filesystem](filesystem)

![](filesystem/images/filesystem-demo.png)

The `filesystem` server is a file system server that can be used to serve files from a file system.

```shell
jbang -q filesystem@quarkiverse/quarkus-mcp-servers [path1] [path2] ...
```

See more in the [filesystem readme](filesystem/README.md).

### [jfx](jfx)

[![](jfx/images/jfx-demo.png)](https://www.youtube.com/watch?v=Wnh_-0dAaDI)

The `jfx` server exposes a canvas that can be used to make drawings using JavaFX.

```shell
jbang -q jfx@quarkiverse/quarkus-mcp-servers
```

See more in the [jfx readme](jfx/README.md).


### [kubernetes](kubernetes)

The `kubernetes` server can be used to interact with a Kubernetes cluster.

```shell
jbang -q kubernetes@quarkiverse/quarkus-mcp-servers
```

## Other Quarkus MCP based servers

### [WildFly](https://github.com/wildfly-extras/wildfly-mcp)

A WildFly MCP server that allows you to interact with WildFly running servers.

You can check this [WildFly vlog](https://youtu.be/wg1hAdOoe2w) that demonstrates its capabilities.

Read more in the [WildFly MCP Server readme](https://github.com/wildfly-extras/wildfly-mcp/blob/main/wildfly-mcp-server/README.md).

## Ideas for other servers

If you have ideas for other servers, feel free to contribute them to this project.

If missing ideas, then look at the reference servers at https://github.com/modelcontextprotocol/servers and see if you can implement them in this project.

Other ideas:

- zulip
- jfr/java hooked to jmx/jfr
- quarkus dev mode
- ... 

## Contributing

If you have ideas for other servers, feel free to contribute them to this project.

To get started, clone the repository and build it:

```bash
git clone https://github.com/quarkiverse/quarkus-mcp-servers
cd quarkus-mcp-servers
mvn clean install
```

Then run the following command to generate a new server for i.e. jfr:

```shell
mkdir jfr
cd jfr
jbang -t mcp jfr
```

This will create the `jfr` directory with a Hello World MCP server.

You can then build it:

```shell
mvn clean install
```

To wire it into the full project you need to add `<module>jfr</module>` to the root `pom.xml` file.

Make sure you have added some useful content to the `README.md` file + updated the demo image.

Then open a PR :)







', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements Model Context Protocol (MCP) servers using Quarkus, designed to extend capabilities of MCP-enabled LLM AI applications and interact with tools and contexts.","categories":["Protocol Implementations","AI Model Integration","Function Calling"],"analysis":{"Architecture and Design Patterns":"The repository adopts a modular architecture where individual servers (jdbc, filesystem, jfx, kubernetes) are implemented as separate modules. Each server provides specific functionalities accessible through the Model Context Protocol. The use of Quarkus suggests a focus on lightweight, cloud-native development with dependency injection and extension support. The project aims to provide a collection of readily available MCP servers.","Implementation Details and Key Features":"The implementation relies on Quarkus MCP server framework. Key features include servers for database interactions (jdbc), file system access (filesystem), JavaFX drawing (jfx), and Kubernetes cluster interaction. Each server module contains a `README.md` file explaining its usage and functionality. The servers are executable using `jbang`, which streamlines the setup and execution process for users.","Integration Approaches and Protocols":"The repository focuses on integrating with LLMs via the Model Context Protocol. It offers servers that allow LLMs to interact with external systems like databases, file systems, and Kubernetes clusters. The project explicitly targets integration with MCP-enabled applications like Claude Desktop, and provides links to resources like \"Awesome MCP Clients\".","Technical Stack and Dependencies":"The technical stack is based on Java and Quarkus. Dependencies include Quarkus MCP server framework, JDBC drivers for database server, JavaFX for the JFX server. Maven is used for build automation and dependency management. The repository makes use of `jbang` for simplified command-line execution of the servers.","Best Practices and Coding Standards":"The repository structure promotes modularity and separation of concerns. Clear README files are included for each server, detailing their usage and configuration. The contribution guidelines encourage developers to add useful content to the README and update demo images. The use of Maven and Quarkus promotes standard Java development practices.","Performance Considerations":"By leveraging Quarkus, the servers benefit from its low memory footprint and fast startup times. The use of JDBC connection pooling and asynchronous operations (if implemented) can optimize database interactions. Performance considerations for each server will depend on its specific implementation and the resources it interacts with."}}', '2025-03-10 21:28:55+00', 'true', 'Servers', 'quarkus-mcp-servers'), ('921171288', 'offeryn', 'avahowell/offeryn', 'https://github.com/avahowell/offeryn', 'Build tools for LLMs in Rust using Model Context Protocol', '2025-01-23 13:27:24+00', '2025-03-10 11:55:55+00', 'Rust', '33', '# offeryn

offeryn (Welsh for tool; implement), is a Rust implementation of [modelcontextprotocol](https://modelcontextprotocol.io/), a standard protocol for empowering large language models with tools.

It aims to offer an easy to use API that allows developers to quickly create and expose tools written in Rust to AI agents.

- [x] JSON-RPC core MCP server protocol
- [x] Tool use support
- [x] Procedural macro for tool generation
- [x] Server-Sent Events (SSE) transport
- [x] Stdio transport
- [ ] Resources support
- [ ] Prompts support
- [ ] Roots suppot
- [ ] Client protocol
- [ ] WebSocket transport
- [ ] Streaming responses

## Example (Stdio)
```rust
use offeryn::prelude::*;
use offeryn::{StdioTransport, McpServer};
use std::sync::Arc;
use async_trait;

/// A simple calculator that can perform basic arithmetic operations
#[derive(Default, Clone)]
struct Calculator {}

// The mcp_tool proc macro generates tool methods that are enumerated to MCP clients.
// It can handle arbitrary types, including Result<T> and Option<T>.
// A correct json-schema is generated for each function signature of the tool, based on the Rust type.
// Docstrings are used as tool descriptions, passed along in MCP to inform the AI agent of the tool's purpose.
//
// This example will generate:
// {
//     "name": "calculator_divide",
//     "description": "Divide two numbers",
//     "inputSchema": {
//         "type": "object",
//         "properties": {
//             "a": {
//                 "description": "Dividend - the number to be divided",
//                 "format": "int64",
//                 "type": "integer"
//             },
//             "b": {
//                 "description": "Divisor - the number to divide by",
//                 "format": "int64",
//                 "type": "integer"
//             }
//         },
//         "required": [
//             "a",
//             "b"
//         ]
//     }
// }
// ... (cont... calculator_add, calculator_subtract, calculator_multiply)
//
// Results are handled as you would expect, with the proper JSON-RPC error code.
#[mcp_tool]
impl Calculator {
    /// Add two numbers
    /// # Parameters
    /// * `a` - First value to add
    /// * `b` - Second value to add
    async fn add(&self, a: i64, b: i64) -> i64 {
        a + b
    }

    /// Subtract two numbers
    /// # Parameters
    /// * `a` - Number to subtract from
    /// * `b` - Number to subtract
    async fn subtract(&self, a: i64, b: i64) -> i64 {
        a - b
    }

    /// Multiply two numbers
    /// # Parameters
    /// * `a` - First factor to multiply
    /// * `b` - Second factor to multiply
    async fn multiply(&self, a: i64, b: i64) -> i64 {
        a * b
    }

    /// Divide two numbers
    /// # Parameters
    /// * `a` - Dividend - the number to be divided
    /// * `b` - Divisor - the number to divide by
    async fn divide(&self, a: i64, b: i64) -> Result<f64, String> {
        if b == 0 {
            Err("Cannot divide by zero".to_string())
        } else {
            Ok(a as f64 / b as f64)
        }
    }
}

#[tokio::main]
async fn main() {
    // Create a new server instance
    let server = Arc::new(McpServer::new("calculator", "1.0.0"));

    // Register the calculator tools
    server.register_tools(Calculator::default()).await;

    // Create and run the stdio transport
    let transport = StdioTransport::<tokio::io::Stdin, tokio::io::Stdout>::new(server);

    if let Err(e) = transport.run().await {
        eprintln!("Error: {}", e);
    }
}
```

Servers configured with stdio transport as above can be hooked to Claude Desktop as normal (see https://modelcontextprotocol.io/quickstart/user, build a binary with mcp-rs and point at it in mcpServers).


## Example (SSE)

```rust
use offeryn::prelude::*;
use offeryn::{SseTransport, McpServer};
use std::net::SocketAddr;
use std::sync::Arc;
use tokio::sync::Mutex;
/// A simple calculator that can perform basic arithmetic operations
#[derive(Default, Clone)]
struct Calculator {}

#[mcp_tool]
impl Calculator {
    /// Add two numbers
    /// # Parameters
    /// * `a` - First value to add
    /// * `b` - Second value to add
    async fn add(&self, a: i64, b: i64) -> i64 {
        a + b
    }

    /// Subtract two numbers
    /// # Parameters
    /// * `a` - Number to subtract from
    /// * `b` - Number to subtract
    async fn subtract(&self, a: i64, b: i64) -> i64 {
        a - b
    }

    /// Multiply two numbers
    /// # Parameters
    /// * `a` - First factor to multiply
    /// * `b` - Second factor to multiply
    async fn multiply(&self, a: i64, b: i64) -> i64 {
        a * b
    }

    /// Divide two numbers
    /// # Parameters
    /// * `a` - Dividend - the number to be divided
    /// * `b` - Divisor - the number to divide by
    async fn divide(&self, a: i64, b: i64) -> Result<f64, String> {
        if b == 0 {
            Err("Cannot divide by zero".to_string())
        } else {
            Ok(a as f64 / b as f64)
        }
    }
}

#[tokio::main]
async fn main() {
    let server = Arc::new(McpServer::new("calculator", "1.0.0"));

    server.register_tools(Calculator::default()).await;

    let app = SseTransport::create_router(server);
    let addr = SocketAddr::from(([127, 0, 0, 1], 3000));
    println!("Server running on http://{}", addr);

    let listener = tokio::net::TcpListener::bind(addr).await.unwrap();
    axum::serve(listener, app).await.unwrap();

    // You can now connect to the server using a MCP client in SSE mode.
}
```
', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it is a Rust implementation of the Model Context Protocol, providing tools and examples for server-sent events and standard I/O transports.","categories":["Protocol Implementations","Function Calling","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The architecture revolves around a central `McpServer` responsible for managing and registering tools. Transports like `StdioTransport` and `SseTransport` handle communication with clients using JSON-RPC over different mediums. The design leverages Rust's async capabilities for concurrent handling of requests. The use of an `Arc` ensures safe sharing of the server across threads.","Implementation Details and Key Features":"The core functionality includes a procedural macro `mcp_tool` for automatic tool generation.  This simplifies the process of exposing Rust functions to AI agents by generating JSON schemas from function signatures and docstrings. The library supports both standard I/O and Server-Sent Events (SSE) transports, offering flexibility in deployment.","Integration Approaches and Protocols":"The repository provides examples for integrating with Model Context Protocol clients, such as Claude Desktop. The system utilizes JSON-RPC for communication between the server and clients.  Transports like Stdio and SSE enable integration with different environments and clients, supporting tool usage within various AI model contexts.","Technical Stack and Dependencies":"The project is built using Rust and relies on crates like `tokio` for asynchronous runtime, `axum` for the SSE transport server, and `serde` for serialization and deserialization.  The `mcp_tool` procedural macro likely uses `syn` and `quote` for code generation at compile time, creating JSON schemas and tool definitions.","Best Practices and Coding Standards":"The provided examples demonstrate clear separation of concerns, with distinct components for server management, tool registration, and transport handling. Asynchronous programming using `async` and `await` is used to ensure efficient handling of concurrent requests. Error handling is demonstrated with `Result` type, contributing to code reliability.","Performance Considerations":"The asynchronous nature of the `tokio` runtime allows for handling multiple concurrent requests efficiently.  The choice of SSE transport could provide performance benefits over traditional HTTP in scenarios where real-time updates are needed. Minimizing memory allocations and leveraging Rust's ownership model are likely important for performance."}}', '2025-03-10 22:15:32+00', 'true', 'Servers', 'offeryn-rust-mcp'), ('921448845', 'Dive', 'OpenAgentPlatform/Dive', 'https://github.com/OpenAgentPlatform/Dive', 'Dive is an open-source AI Agent desktop application that seamlessly integrates any Tools Call-supported LLM with frontend MCP Server—part of the Open Agent Platform initiative.', '2025-01-24 00:55:50+00', '2025-03-10 18:25:39+00', 'JavaScript', '313', '# Dive AI Agent 🤿 🤖

![GitHub stars](https://img.shields.io/github/stars/OpenAgentPlatform/Dive?style=social)
![GitHub forks](https://img.shields.io/github/forks/OpenAgentPlatform/Dive?style=social)
![GitHub watchers](https://img.shields.io/github/watchers/OpenAgentPlatform/Dive?style=social)
![GitHub repo size](https://img.shields.io/github/repo-size/OpenAgentPlatform/Dive)
![GitHub language count](https://img.shields.io/github/languages/count/OpenAgentPlatform/Dive)
![GitHub top language](https://img.shields.io/github/languages/top/OpenAgentPlatform/Dive)
![GitHub last commit](https://img.shields.io/github/last-commit/OpenAgentPlatform/Dive?color=red)
![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FOpenAgentPlatform%2FDive&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false)
[![Discord](https://img.shields.io/badge/Discord-Dive-blue?logo=discord&logoColor=white)](https://discord.gg/jact84GwGh)
[![Twitter Follow](https://img.shields.io/twitter/follow/Dive_ai_agent?style=social)](https://twitter.com/Dive_ai_agent)


Dive is an open-source AI Agent desktop application that seamlessly integrates any Tools Call-supported LLM with frontend MCP Server—part of the Open Agent Platform initiative. ✨

![Dive Demo](./docs/DiveAI.gif)


## Features 🎯

- 🌐 **Universal LLM Support**: Compatible with ChatGPT, Anthropic, Ollama and OpenAI-compatible models
- 💻 **Cross-Platform**: Available for Windows, MacOS, and Linux
- 🔄 **Model Context Protocol**: Enabling seamless AI agent integration
- 🔌 **MCP Server Integration**: External data access and processing capabilities
- 🌍 **Multi-Language Support**: Traditional Chinese, English, with more coming soon
- ⚙️ **Advanced API Management**: Multiple API keys and model switching support
- 💡 **Custom Instructions**: Personalized system prompts for tailored AI behavior
- 💬 **Intuitive Chat Interface**: Real-time context management and user-friendly design
- 🚀 **Upcoming Features**: Prompt Schedule and OpenAgentPlatform MarketPlace

## Recent updates(2025/3/7)

- 🔌 **SSE Transport Protocol**: Support for Server-Sent Events based MCP servers since v0.5.1
- 🔄 **Auto-Update Mechanism**: Automatically checks for and installs the latest application updates
- ⌨️ **Keyboard Shortcuts**: Enhanced productivity with customizable keyboard shortcuts
- 🧮 **Math Formula Rendering**: Support for displaying mathematical formulas in markdown


## Download and Install ⬇️

Get the latest version of Dive:
[![Download](https://img.shields.io/badge/Download-Latest%20Release-blue.svg)](https://github.com/OpenAgentPlatform/Dive/releases/latest)

For Windows users: 🪟
- Download the .exe version
- Python and Node.js environments are pre-installed

For MacOS users: 🍎
- Download the .dmg version
- You need to install Python and Node.js (with npx uvx) environments yourself
- Follow the installation prompts to complete setup

For Linux users: 🐧
- Download the .AppImage version
- You need to install Python and Node.js (with npx uvx) environments yourself
- For Ubuntu/Debian users:
  - You may need to add `--no-sandbox` parameter
  - Or modify system settings to allow sandbox
  - Run `chmod +x` to make the AppImage executable

## MCP Tips

While the system comes with a default echo MCP Server, your LLM can access more powerful tools through MCP. Here's how to get started with two beginner-friendly tools: Fetch and Youtube-dl.

![Set MCP](./docs/MCP_Screenshot.png)


### Quick Setup

Add this JSON configuration to your Dive MCP settings to enable both tools:

```json
 "mcpServers":{
    "fetch": {
      "command": "uvx",
      "args": [
        "mcp-server-fetch",
        "--ignore-robots-txt"
      ],
      "enabled": true
    },
    "youtubedl": {
      "command": "npx",
      "args": [
        "@kevinwatt/yt-dlp-mcp"
      ],
      "enabled": true
    }
  }
```

### Using SSE Server for MCP

You can also connect to an external MCP server via SSE (Server-Sent Events). Add this configuration to your Dive MCP settings:

```json
{
  "mcpServers": {
    "MCP_SERVER_NAME": {
      "enabled": true,
      "transport": "sse",
      "url": "YOUR_SSE_SERVER_URL"
    }
  }
}
```

### Additional Setup for yt-dlp-mcp

yt-dlp-mcp requires the yt-dlp package. Install it based on your operating system:

#### Windows
```bash
winget install yt-dlp
```

#### MacOS
```bash
brew install yt-dlp
```

#### Linux
```bash
pip install yt-dlp
```

## Build 🛠️

See [BUILD.md](BUILD.md) for more details.

## Connect With Us 🌐
- 💬 Join our [Discord](https://discord.gg/qceMERf4y2)
- 🐦 Follow us on [Twitter/X](https://x.com/Dive_ai_agent)
- ⭐ Star us on GitHub
- 🐛 Report issues on our [Issue Tracker](https://github.com/OpenAgentPlatform/Dive/issues)


', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository description and documentation explicitly state that Dive integrates with MCP Servers and supports the Model Context Protocol for seamless AI agent integration, with examples for Fetch and Youtube-dl.","categories":["Agent Frameworks","Function Calling","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"Dive is designed as a desktop application providing an interface between LLMs and MCP servers. Its architecture supports pluggable tools via MCP, allowing LLMs to access external data and functionalities. The architecture emphasizes cross-platform compatibility, supporting Windows, MacOS, and Linux. It uses a modular approach for integrating different LLMs and tools.","Implementation Details and Key Features":"The application supports various LLMs via Tools Call, including ChatGPT, Anthropic, and Ollama. It features a real-time chat interface with context management.  A key feature is its ability to integrate with external MCP servers through SSE transport.  The application is built with features such as API key management, custom instructions, and multi-language support.","Integration Approaches and Protocols":"Dive integrates with LLMs using a Model Context Protocol (MCP) server, facilitating communication between the AI model and external tools. It supports SSE (Server-Sent Events) transport for connecting to external MCP servers. The application allows users to configure MCP servers, enabling functionalities such as fetching data and downloading YouTube content.","Technical Stack and Dependencies":"The application requires Python and Node.js environments for MacOS and Linux users. It uses uvx and npx for managing MCP servers and tools. Dependencies include yt-dlp for YouTube downloading functionality. The front-end implementation details aren't specified.  The application supports automatic updates and keyboard shortcuts.","Best Practices and Coding Standards":"The documentation includes instructions for setting up and configuring MCP servers, suggesting a focus on user experience and ease of integration. The project uses a modular design, indicated by the use of external tools like `mcp-server-fetch` and `@kevinwatt/yt-dlp-mcp`. The inclusion of a `BUILD.md` file suggests attention to build processes.","Performance Considerations":"Performance considerations likely involve the efficiency of communication between the desktop application and the MCP server. The use of SSE aims to provide real-time updates with low latency. The documentation mentions the potential need for `--no-sandbox` parameter on Linux, which might impact security and performance.  Optimizations are likely focused on minimizing resource usage on desktop environments."}}', '2025-03-10 21:26:30+00', 'true', 'Servers', 'dive-mcp-desktop'), ('921652649', 'mcp-sequentialthinking-tools', 'spences10/mcp-sequentialthinking-tools', 'https://github.com/spences10/mcp-sequentialthinking-tools', '🧠 An adaptation of the MCP Sequential Thinking Server to guide tool usage. This server provides recommendations for which MCP tools would be most effective at each stage.', '2025-01-24 11:02:14+00', '2025-03-09 15:42:03+00', 'TypeScript', '31', '# mcp-sequentialthinking-tools

An adaptation of the
[MCP Sequential Thinking Server](https://github.com/modelcontextprotocol/servers/blob/main/src/sequentialthinking/index.ts)
designed to guide tool usage in problem-solving. This server helps
break down complex problems into manageable steps and provides
recommendations for which MCP tools would be most effective at each
stage.

<a href="https://glama.ai/mcp/servers/zl990kfusy">
  <img width="380" height="200" src="https://glama.ai/mcp/servers/zl990kfusy/badge" />
</a>

A Model Context Protocol (MCP) server that combines sequential
thinking with intelligent tool suggestions. For each step in the
problem-solving process, it provides confidence-scored recommendations
for which tools to use, along with rationale for why each tool would
be appropriate.

## Features

- 🤔 Dynamic and reflective problem-solving through sequential
  thoughts
- 🔄 Flexible thinking process that adapts and evolves
- 🌳 Support for branching and revision of thoughts
- 🛠️ Intelligent tool recommendations for each step
- 📊 Confidence scoring for tool suggestions
- 🔍 Detailed rationale for tool recommendations
- 📝 Step tracking with expected outcomes
- 🔄 Progress monitoring with previous and remaining steps
- 🎯 Alternative tool suggestions for each step

## How It Works

This server analyses each step of your thought process and recommends
appropriate MCP tools to help accomplish the task. Each recommendation
includes:

- A confidence score (0-1) indicating how well the tool matches the
  current need
- A clear rationale explaining why the tool would be helpful
- A priority level to suggest tool execution order
- Alternative tools that could also be used

The server works with any MCP tools available in your environment. It
provides recommendations based on the current step's requirements, but
the actual tool execution is handled by the consumer (like Claude).

## Example Usage

Here's an example of how the server guides tool usage:

```json
{
	"thought": "Initial research step to understand what universal reactivity means in Svelte 5",
	"current_step": {
		"step_description": "Gather initial information about Svelte 5's universal reactivity",
		"expected_outcome": "Clear understanding of universal reactivity concept",
		"recommended_tools": [
			{
				"tool_name": "search_docs",
				"confidence": 0.9,
				"rationale": "Search Svelte documentation for official information",
				"priority": 1
			},
			{
				"tool_name": "tavily_search",
				"confidence": 0.8,
				"rationale": "Get additional context from reliable sources",
				"priority": 2
			}
		],
		"next_step_conditions": [
			"Verify information accuracy",
			"Look for implementation details"
		]
	},
	"thought_number": 1,
	"total_thoughts": 5,
	"next_thought_needed": true
}
```

The server tracks your progress and supports:

- Creating branches to explore different approaches
- Revising previous thoughts with new information
- Maintaining context across multiple steps
- Suggesting next steps based on current findings

## Configuration

This server requires configuration through your MCP client. Here are
examples for different environments:

### Cline Configuration

Add this to your Cline MCP settings:

```json
{
	"mcpServers": {
		"mcp-sequentialthinking-tools": {
			"command": "npx",
			"args": ["-y", "mcp-sequentialthinking-tools"]
		}
	}
}
```

### Claude Desktop with WSL Configuration

For WSL environments, add this to your Claude Desktop configuration:

```json
{
	"mcpServers": {
		"mcp-sequentialthinking-tools": {
			"command": "wsl.exe",
			"args": [
				"bash",
				"-c",
				"source ~/.nvm/nvm.sh && /home/username/.nvm/versions/node/v20.12.1/bin/npx mcp-sequentialthinking-tools"
			]
		}
	}
}
```

## API

The server implements a single MCP tool with configurable parameters:

### sequentialthinking_tools

A tool for dynamic and reflective problem-solving through thoughts,
with intelligent tool recommendations.

Parameters:

- `thought` (string, required): Your current thinking step
- `next_thought_needed` (boolean, required): Whether another thought
  step is needed
- `thought_number` (integer, required): Current thought number
- `total_thoughts` (integer, required): Estimated total thoughts
  needed
- `is_revision` (boolean, optional): Whether this revises previous
  thinking
- `revises_thought` (integer, optional): Which thought is being
  reconsidered
- `branch_from_thought` (integer, optional): Branching point thought
  number
- `branch_id` (string, optional): Branch identifier
- `needs_more_thoughts` (boolean, optional): If more thoughts are
  needed
- `current_step` (object, optional): Current step recommendation with:
  - `step_description`: What needs to be done
  - `recommended_tools`: Array of tool recommendations with confidence
    scores
  - `expected_outcome`: What to expect from this step
  - `next_step_conditions`: Conditions for next step
- `previous_steps` (array, optional): Steps already recommended
- `remaining_steps` (array, optional): High-level descriptions of
  upcoming steps

## Development

### Setup

1. Clone the repository
2. Install dependencies:

```bash
pnpm install
```

3. Build the project:

```bash
pnpm build
```

4. Run in development mode:

```bash
pnpm dev
```

### Publishing

The project uses changesets for version management. To publish:

1. Create a changeset:

```bash
pnpm changeset
```

2. Version the package:

```bash
pnpm changeset version
```

3. Publish to npm:

```bash
pnpm release
```

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- Built on the
  [Model Context Protocol](https://github.com/modelcontextprotocol)
- Adapted from the
  [MCP Sequential Thinking Server](https://github.com/modelcontextprotocol/servers/blob/main/src/sequentialthinking/index.ts)
', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"Prompt Engineering Tools\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it's an adaptation of the MCP Sequential Thinking Server and provides intelligent tool recommendations for each step in a problem-solving process within the MCP framework.","categories":["Agent Frameworks","Function Calling","Context Management"],"analysis":{"Architecture and Design Patterns":"The server adapts the MCP Sequential Thinking Server, suggesting a modular design. It leverages a step-by-step problem-solving approach, with recommendations for MCP tools at each stage. The architecture involves analyzing the current thought process and providing tool suggestions with confidence scores and rationale, promoting dynamic and reflective problem-solving.","Implementation Details and Key Features":"Key features include dynamic problem-solving, flexible thinking, tool recommendations with confidence scoring, and step tracking. The server provides a single MCP tool (`sequentialthinking_tools`) that takes a thought as input and returns tool recommendations based on that thought. It uses JSON for communication and configuration.","Integration Approaches and Protocols":"The server integrates with MCP-compatible tools. The 'sequentialthinking_tools' API is central, which uses parameters like 'thought', 'next_thought_needed', and 'thought_number' to provide context. Configuration examples are provided for Cline and Claude Desktop, demonstrating integration into existing MCP environments. It also recommends specific tools like `search_docs` or `tavily_search`.","Technical Stack and Dependencies":"The project uses Node.js and likely TypeScript, given the linked source file (`index.ts`). It uses `pnpm` for package management. The listed configuration files indicate it's designed to interact with environments that support MCP, like Cline and Claude Desktop, implying a reliance on their respective environments.","Best Practices and Coding Standards":"The documentation includes instructions for development setup, build process, and publishing using changesets, indicating a focus on version control and release management. The presence of a contributing section encourages community involvement and suggests adherence to open-source best practices. The MIT license promotes reusability and collaboration.","Performance Considerations":"The performance likely depends on the complexity of the problem-solving steps and the number of available MCP tools. The server provides confidence scores for tool recommendations, potentially allowing for prioritization and optimization of tool execution. No specific performance metrics or optimization techniques are explicitly discussed, but the design focuses on efficient step-by-step analysis."}}', '2025-03-10 22:15:32+00', 'true', 'Servers', 'mcp-tool-guide'), ('922236111', 'Deepseek-Thinking-Claude-3.5-Sonnet-CLINE-MCP', 'newideas99/Deepseek-Thinking-Claude-3.5-Sonnet-CLINE-MCP', 'https://github.com/newideas99/Deepseek-Thinking-Claude-3.5-Sonnet-CLINE-MCP', '🧠 MCP server implementing RAT (Retrieval Augmented Thinking) - combines DeepSeek's reasoning with GPT-4/Claude/Mistral responses, maintaining conversation context between interactions.', '2025-01-25 17:16:46+00', '2025-03-09 09:23:33+00', 'JavaScript', '96', '# Deepseek-Thinking-Claude-3.5-Sonnet-CLINE-MCP

[![smithery badge](https://smithery.ai/badge/@newideas99/Deepseek-Thinking-Claude-3.5-Sonnet-CLINE-MCP)](https://smithery.ai/server/@newideas99/Deepseek-Thinking-Claude-3.5-Sonnet-CLINE-MCP)

A Model Context Protocol (MCP) server that combines DeepSeek R1's reasoning capabilities with Claude 3.5 Sonnet's response generation through OpenRouter. This implementation uses a two-stage process where DeepSeek provides structured reasoning which is then incorporated into Claude's response generation.

## Features

- **Two-Stage Processing**:
  - Uses DeepSeek R1 for initial reasoning (50k character context)
  - Uses Claude 3.5 Sonnet for final response (600k character context)
  - Both models accessed through OpenRouter's unified API
  - Injects DeepSeek's reasoning tokens into Claude's context

- **Smart Conversation Management**:
  - Detects active conversations using file modification times
  - Handles multiple concurrent conversations
  - Filters out ended conversations automatically
  - Supports context clearing when needed

- **Optimized Parameters**:
  - Model-specific context limits:
    * DeepSeek: 50,000 characters for focused reasoning
    * Claude: 600,000 characters for comprehensive responses
  - Recommended settings:
    * temperature: 0.7 for balanced creativity
    * top_p: 1.0 for full probability distribution
    * repetition_penalty: 1.0 to prevent repetition

## Installation

### Installing via Smithery

To install DeepSeek Thinking with Claude 3.5 Sonnet for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@newideas99/Deepseek-Thinking-Claude-3.5-Sonnet-CLINE-MCP):

```bash
npx -y @smithery/cli install @newideas99/Deepseek-Thinking-Claude-3.5-Sonnet-CLINE-MCP --client claude
```

### Manual Installation
1. Clone the repository:
```bash
git clone https://github.com/yourusername/Deepseek-Thinking-Claude-3.5-Sonnet-CLINE-MCP.git
cd Deepseek-Thinking-Claude-3.5-Sonnet-CLINE-MCP
```

2. Install dependencies:
```bash
npm install
```

3. Create a `.env` file with your OpenRouter API key:
```env
# Required: OpenRouter API key for both DeepSeek and Claude models
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Optional: Model configuration (defaults shown below)
DEEPSEEK_MODEL=deepseek/deepseek-r1  # DeepSeek model for reasoning
CLAUDE_MODEL=anthropic/claude-3.5-sonnet:beta  # Claude model for responses
```

4. Build the server:
```bash
npm run build
```

## Usage with Cline

Add to your Cline MCP settings (usually in `~/.vscode/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`):

```json
{
  "mcpServers": {
    "deepseek-claude": {
      "command": "/path/to/node",
      "args": ["/path/to/Deepseek-Thinking-Claude-3.5-Sonnet-CLINE-MCP/build/index.js"],
      "env": {
        "OPENROUTER_API_KEY": "your_key_here"
      },
      "disabled": false,
      "autoApprove": []
    }
  }
}
```

## Tool Usage

The server provides two tools for generating and monitoring responses:

### generate_response

Main tool for generating responses with the following parameters:

```typescript
{
  "prompt": string,           // Required: The question or prompt
  "showReasoning"?: boolean, // Optional: Show DeepSeek's reasoning process
  "clearContext"?: boolean,  // Optional: Clear conversation history
  "includeHistory"?: boolean // Optional: Include Cline conversation history
}
```

### check_response_status

Tool for checking the status of a response generation task:

```typescript
{
  "taskId": string  // Required: The task ID from generate_response
}
```

### Response Polling

The server uses a polling mechanism to handle long-running requests:

1. Initial Request:
   - `generate_response` returns immediately with a task ID
   - Response format: `{"taskId": "uuid-here"}`

2. Status Checking:
   - Use `check_response_status` to poll the task status
   - **Note:** Responses can take up to 60 seconds to complete
   - Status progresses through: pending → reasoning → responding → complete

Example usage in Cline:
```typescript
// Initial request
const result = await use_mcp_tool({
  server_name: "deepseek-claude",
  tool_name: "generate_response",
  arguments: {
    prompt: "What is quantum computing?",
    showReasoning: true
  }
});

// Get taskId from result
const taskId = JSON.parse(result.content[0].text).taskId;

// Poll for status (may need multiple checks over ~60 seconds)
const status = await use_mcp_tool({
  server_name: "deepseek-claude",
  tool_name: "check_response_status",
  arguments: { taskId }
});

// Example status response when complete:
{
  "status": "complete",
  "reasoning": "...",  // If showReasoning was true
  "response": "..."    // The final response
}
```

## Development

For development with auto-rebuild:
```bash
npm run watch
```

## How It Works

1. **Reasoning Stage (DeepSeek R1)**:
   - Uses OpenRouter's reasoning tokens feature
   - Prompt is modified to output 'done' while capturing reasoning
   - Reasoning is extracted from response metadata

2. **Response Stage (Claude 3.5 Sonnet)**:
   - Receives the original prompt and DeepSeek's reasoning
   - Generates final response incorporating the reasoning
   - Maintains conversation context and history

## License

MIT License - See LICENSE file for details.

## Credits

Based on the RAT (Retrieval Augmented Thinking) concept by [Skirano](https://x.com/skirano/status/1881922469411643413), which enhances AI responses through structured reasoning and knowledge retrieval.

This implementation specifically combines DeepSeek R1's reasoning capabilities with Claude 3.5 Sonnet's response generation through OpenRouter's unified API.
', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"Context Management\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states it's a Model Context Protocol (MCP) server. It uses DeepSeek for reasoning and Claude for responses, showing a clear implementation of context management for multiple models.","categories":["Context Management","Function Calling","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The architecture utilizes a two-stage processing approach: DeepSeek R1 for reasoning and Claude 3.5 Sonnet for response generation. It employs a server-based design accessible via Cline MCP settings. The design incorporates asynchronous task management with a polling mechanism to handle potentially long-running response generation requests.","Implementation Details and Key Features":"Key features include two-stage processing, smart conversation management (detecting active conversations and filtering ended ones), and optimized parameters for each model. The implementation uses OpenRouter's API for accessing both DeepSeek and Claude models. It manages context by injecting DeepSeek's reasoning tokens into Claude's context.","Integration Approaches and Protocols":"The primary integration approach is through the Model Context Protocol (MCP), enabling communication with Cline. It supports two tools: `generate_response` for creating new responses and `check_response_status` for monitoring their progress. It relies on OpenRouter's unified API for model access and utilizes HTTP-based communication.","Technical Stack and Dependencies":"The technical stack includes Node.js, npm for dependency management, and OpenRouter's API. Dependencies likely include libraries for HTTP requests, JSON parsing, and potentially UUID generation for task IDs. The server is built using JavaScript/TypeScript, and requires an OpenRouter API key.","Best Practices and Coding Standards":"The provided code snippets indicate the use of standard JavaScript/TypeScript practices. The repository includes a `.env` file for managing API keys, which is a security best practice. The use of meaningful variable names and comments contribute to code readability. License information is also clearly provided.","Performance Considerations":"The implementation utilizes asynchronous polling to avoid blocking the main thread during response generation. Model-specific context limits are configured (DeepSeek: 50k characters, Claude: 600k characters). The usage section mentions that responses can take up to 60 seconds, indicating the need for performance monitoring and potential optimizations."}}', '2025-03-10 21:28:55+00', 'true', 'Servers', 'deepseek-mcp-rat'), ('922445184', 'code-sandbox-mcp', 'Automata-Labs-team/code-sandbox-mcp', 'https://github.com/Automata-Labs-team/code-sandbox-mcp', 'An MCP server to create secure code sandbox environment for executing code within Docker containers. This MCP server provides AI applications with a safe and isolated environment for running code while maintaining security through containerization.', '2025-01-26 08:18:21+00', '2025-03-09 08:44:39+00', 'Go', '42', '# Code Sandbox MCP 🐳
[![smithery badge](https://smithery.ai/badge/@Automata-Labs-team/code-sandbox-mcp)](https://smithery.ai/server/@Automata-Labs-team/code-sandbox-mcp)

A secure sandbox environment for executing code within Docker containers. This MCP server provides AI applications with a safe and isolated environment for running code while maintaining security through containerization.
![Screenshot from 2025-01-26 02-37-42](https://github.com/user-attachments/assets/c3fcf202-24a2-488a-818f-ffab6f881849)
## 🌟 Features

- **Multi-Language Support**: Run Python, Go, and Node.js code in isolated Docker containers
- **TypeScript Support**: Built-in support for TypeScript and JSX/TSX files
- **Dependency Management**: Automatic handling of project dependencies (pip, go mod, npm)
- **Flexible Execution**: Custom entrypoints for both single-file code and full projects
- **Background Mode**: Run long-running services in the background
- **Real-time Output**: Capture and stream container logs in real-time

## 🚀 Installation

### Prerequisites

- Docker installed and running
  - [Install Docker for Linux](https://docs.docker.com/engine/install/)
  - [Install Docker Desktop for macOS](https://docs.docker.com/desktop/install/mac/)
  - [Install Docker Desktop for Windows](https://docs.docker.com/desktop/install/windows-install/)

### Quick Install

#### Linux, MacOS
```bash
curl -fsSL https://raw.githubusercontent.com/Automata-Labs-team/code-sandbox-mcp/main/install.sh | bash
```

Example output:
```
Downloading latest release...
Installing to /home/user/.local/share/code-sandbox-mcp/code-sandbox-mcp...
Adding to Claude Desktop configuration...
Added code-sandbox-mcp to /home/user/.config/Claude/claude_desktop_config.json
Installation complete!
You can now use code-sandbox-mcp with Claude Desktop or other AI applications.
```

#### Windows
```powershell
# Run in PowerShell
irm https://raw.githubusercontent.com/Automata-Labs-team/code-sandbox-mcp/main/install.ps1 | iex
```

The installer will:
1. Check for Docker installation
2. Download the appropriate binary for your system
3. Create Claude Desktop configuration

### Manual Installation (Not necesary if automated installation is used)

1. Download the latest release for your platform from the [releases page](https://github.com/Automata-Labs-team/code-sandbox-mcp/releases)
2. Place the binary in a directory in your PATH
3. Make it executable (Unix-like systems only):
   ```bash
   chmod +x code-sandbox-mcp
   ```
## 🛠️ Available Tools

#### `run_code`
Executes code snippets in an isolated Docker container.

**Parameters:**
- `code` (string, required): The code to run
- `language` (enum, required): Programming language to use
  - Supported values: `python`, `go`, `nodejs`
  - Note: If your Python code requires external dependencies, it is recommended to use the `run_project` tool instead. Go and Node.js script dependencies are automatically installed.

**Returns:**
- Container execution output (stdout + stderr)

**Features:**
- Automatic dependency detection and installation
  - Python: Detects imports and installs via pip
  - Node.js: Detects require/import statements and installs via npm
  - Go: Detects imports and installs via go get
- Automatic language-specific Docker image selection
- TypeScript/JSX support with appropriate flags
- Special handling for Go (code written to temporary file)
- Real-time output streaming

#### `run_project`
Executes a project directory in a containerized environment.

**Parameters:**
- `project_dir` (string, required): Directory containing the project to run
- `language` (enum, required): Programming language to use
  - Supported values: `python`, `go`, `nodejs`
- `entrypointCmd` (string, required): Command to run the project
  - Examples:
    - Python: `python main.py`
    - Node.js: `node index.js`
    - Go: `go run main.go`

**Returns:**
- The resource URI of the container logs.

**Features:**
- Automatic dependency detection and installation
- Volume mounting of project directory
- Language-specific configuration handling
- Real-time log streaming

## 🔧 Configuration

### Claude Desktop

The installer automatically creates the configuration file. If you need to manually configure it:

#### Linux
```json
// ~/.config/Claude/claude_desktop_config.json
{
    "mcpServers": {
        "code-sandbox-mcp": {
            "command": "/path/to/code-sandbox-mcp",
            "args": [],
            "env": {}
        }
    }
}
```

#### macOS
```json
// ~/Library/Application Support/Claude/claude_desktop_config.json
{
    "mcpServers": {
        "code-sandbox-mcp": {
            "command": "/path/to/code-sandbox-mcp",
            "args": [],
            "env": {}
        }
    }
}
```

#### Windows
```json
// %APPDATA%\Claude\claude_desktop_config.json
{
    "mcpServers": {
        "code-sandbox-mcp": {
            "command": "C:\\path\\to\\code-sandbox-mcp.exe",
            "args": [],
            "env": {}
        }
    }
}
```

### Other AI Applications

For other AI applications that support MCP servers, configure them to use the `code-sandbox-mcp` binary as their code execution backend.

## 🔧 Technical Details

### Supported Languages

| Language | File Extensions | Docker Image |
|----------|----------------|--------------|
| Python | .py | python:3.12-slim-bookworm |
| Go | .go | golang:1.21-alpine |
| Node.js | .js, .ts, .tsx, .jsx | node:23-slim |

### Dependency Management

The sandbox automatically detects and installs dependencies:

- **Python**: 
  - Detects imports like `import requests`, `from PIL import Image`
  - Handles aliased imports (e.g., `PIL` → `pillow`)
  - Filters out standard library imports
  - Supports both direct imports and `__import__()` calls

- **Node.js**: 
  - Detects `require()` statements and ES6 imports
  - Handles scoped packages (e.g., `@org/package`)
  - Supports dynamic imports (`import()`)
  - Filters out built-in Node.js modules

- **Go**: 
  - Detects package imports in both single-line and grouped formats
  - Handles named and dot imports
  - Filters out standard library packages
  - Supports external dependencies via `go get`

For project execution, the following files are used:
- **Python**: requirements.txt, pyproject.toml, setup.py
- **Go**: go.mod
- **Node.js**: package.json

### TypeScript Support

Node.js 23+ includes built-in TypeScript support:
- `--experimental-strip-types`: Enabled by default for .ts files
- `--experimental-transform-types`: Used for .tsx files

## 🔐 Security Features

- Isolated execution environment using Docker containers
- Resource limitations through Docker container constraints
- Separate stdout and stderr streams
- Clean container cleanup after execution
- Project files mounted read-only in containers

## 🛠️ Development

If you want to build the project locally or contribute to its development, see [DEVELOPMENT.md](DEVELOPMENT.md).

## 📝 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
', 'true', '"{\"AI Model Integration\",\"Function Calling\",\"Protocol Implementations\"]}', '{"is_mcp":"YES","justification":"The repository implements an MCP server for secure code execution in Docker containers, designed to provide AI applications with a safe environment for running code, directly aligning with MCP specifications.","categories":["AI Model Integration","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The architecture revolves around creating a secure, isolated environment for code execution. It uses Docker containers to sandbox code, providing a layer of security and resource control. The design patterns likely include the use of command-line interface (CLI) tools and API endpoints for interaction with AI applications. Configuration files facilitate the integration of the sandbox into environments like Claude Desktop.","Implementation Details and Key Features":"Key features include multi-language support (Python, Go, Node.js), automatic dependency management, and flexible execution options for both code snippets and full projects. The implementation manages Docker image selection based on the specified language, using real-time output streaming to capture container logs. It supports TypeScript and JSX/TSX files for Node.js projects. The project also provides installation scripts for quick setup.","Integration Approaches and Protocols":"The primary integration approach involves configuring AI applications to use the 'code-sandbox-mcp' binary as a code execution backend. This uses MCP server configuration standards. It provides installation scripts for Claude Desktop. The server exposes 'run_code' and 'run_project' tools, which can be invoked by external applications through defined parameters.","Technical Stack and Dependencies":"The technical stack includes Docker for containerization and binaries tailored for different operating systems. Dependency management relies on language-specific tools like 'pip' (Python), 'go mod' (Go), and 'npm' (Node.js). It leverages Docker images for each supported language. It includes shell and powershell installation scripts. Configurations are stored in JSON files.","Best Practices and Coding Standards":"The repository emphasizes security through container isolation and resource limitations. Coding standards likely adhere to best practices for each supported language, incorporating dependency management and structured output. Read-only mounting for project files mounted to containers to further harden security is employed. Cleanup of containers after execution improves efficiency.","Performance Considerations":"Performance considerations focus on optimizing the execution of code within Docker containers. Automatic dependency management aims to minimize setup time. Resource limits within containers helps prevent resource exhaustion. Streaming output in real-time allows faster debugging and feedback. Docker image selection tailored to the task minimizes image size and startup time."}}', '2025-03-10 22:15:32+00', 'true', 'Servers', 'code-sandbox-mcp'), ('922554238', 'perplexity-mcp', 'DaInfernalCoder/perplexity-mcp', 'https://github.com/DaInfernalCoder/perplexity-mcp', 'A Model Context Protocol (MCP) server for research and documentation assistance using Perplexity AI', '2025-01-26 14:26:30+00', '2025-03-10 15:57:35+00', 'JavaScript', '136', 'We got an article about perplexity mcp! 
https://cline.bot/blog/supercharge-cline-3-ways-to-build-better-with-perplexity-mcp

# MCP-researcher Server
[![smithery badge](https://smithery.ai/badge/@DaInfernalCoder/perplexity-mcp)](https://smithery.ai/server/@DaInfernalCoder/perplexity-mcp) 

Your own research assistant inside of Cline! Utilizes Perplexity's new Sonar Pro API to get docs, create up-to-date api routes, and check deprecated code while you create features with Cline. 

Includes Chain of Thought Reasoning and local chat history through SQLite thanks to Lix for the idea :)

<a href="https://glama.ai/mcp/servers/g1i6ilg8sl"><img width="380" height="200" src="https://glama.ai/mcp/servers/g1i6ilg8sl/badge" alt="MCP-researcher Server MCP server" /></a>

## Tools

### 1. [Search](https://github.com/DaInfernalCoder/researcher-mcp/blob/main/examples/search.md)
Performs general search queries to get comprehensive information on any topic. The example shows how to use different detail levels (brief, normal, detailed) to get tailored responses.

### 2. [Get Documentation](https://github.com/DaInfernalCoder/researcher-mcp/blob/main/examples/find-apis.md)
Retrieves documentation and usage examples for specific technologies, libraries, or APIs. The example demonstrates getting comprehensive documentation for React hooks, including best practices and common pitfalls.

### 3. [Find APIs](https://github.com/DaInfernalCoder/researcher-mcp/blob/main/examples/find-apis.md)
Discovers and evaluates APIs that could be integrated into a project. The example shows finding payment processing APIs with detailed analysis of features, pricing, and integration complexity.

### 4. [Check Deprecated Code](https://github.com/DaInfernalCoder/researcher-mcp/blob/main/examples/check-deprecated-code.md)
Analyzes code for deprecated features or patterns, providing migration guidance. The example demonstrates checking React class components and lifecycle methods for modern alternatives.


## Installation

### Installing via Smithery

To install MCP-researcher Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@DaInfernalCoder/perplexity-mcp):

```bash
npx -y @smithery/cli install @DaInfernalCoder/perplexity-mcp --client claude
```

### paste this part into claude directly if you want to, the ai can install it for you

1. First install Node.js if not already installed (from nodejs.org)

2. Clone the repo

- git clone https://github.com/DaInfernalCoder/researcher-mcp perplexity-server 

- cd perplexity-server

3. Install dependencies and build:
npm install

4. Get a Perplexity API key from [https://www.perplexity.ai/settings/api](https://www.perplexity.ai/settings/api)

5. Create the MCP settings file in the appropriate location for your OS:
macOS: ~/Library/Application Support/Cursor/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json
Windows: %APPDATA%\Cursor\User\globalStorage\saoudrizwan.claude-dev\settings\cline_mcp_settings.json
Linux: ~/.config/Cursor/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json

5. To use with Claude Desktop, add the server config:

On MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`
On Windows: `%APPDATA%/Claude/claude_desktop_config.json`

6. To use with Cline, add into mcpServers: 

```json
{
  "mcpServers": {
    "perplexity-server": {
      "command": "node",
      "args": [
        "[path/to/researcher-mcp/index.js]"
      ],
      "env": {
        "PERPLEXITY_API_KEY": ""
      },
      "disabled": false,
      "autoApprove": [
        "search",
        "get_documentation",
        "find_apis",
        "check_deprecated_code"
      ]
    }
  }
}
```

7. Build the server: 
npm run build

Make sure to:
- Replace /absolute/path/to with the actual path where you cloned the repository
- Replace your-api-key-here with your Perplexity API key
- Keep the same autoApprove settings for consistent behavior

', 'true', '"{\"AI Model Integration\",\"Function Calling\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it is a Model Context Protocol (MCP) server for research and documentation assistance. It integrates with Perplexity AI and Cline, fitting the MCP description.","categories":["AI Model Integration","Function Calling","Context Management"],"analysis":{"Architecture and Design Patterns":"The architecture centers around providing research and documentation capabilities via Perplexity AI's Sonar Pro API. It uses a server to handle requests and interacts with tools like Cline. It appears to be modular, with different tools exposed as distinct functions. The server design is likely based on a client-server model to enable integration with applications like Claude Desktop.","Implementation Details and Key Features":"The implementation includes features like chain-of-thought reasoning and local chat history using SQLite. Key features are the search functionality, API documentation retrieval, API discovery, and deprecated code checking. The server uses Node.js and npm for package management. It requires configuring environment variables, specifically the Perplexity API key.","Integration Approaches and Protocols":"The repository integrates with Claude Desktop and Cline through MCP. It involves modifying configuration files to register the server and define allowed functions. Integration requires specifying the command to run the server, arguments, and environment variables. The use of 'autoApprove' suggests a permission system or function call validation mechanism.","Technical Stack and Dependencies":"The technical stack includes Node.js, npm, Perplexity AI's Sonar Pro API, and SQLite. The repository uses JavaScript for the server implementation. The documentation references dependencies like `@smithery/cli` for installation. The specific versions of these dependencies are not explicitly stated but are implied by the `npm install` command.","Best Practices and Coding Standards":"The provided documentation suggests following best practices for setting up environment variables and configuring the server. The use of tools with specific functions (search, get_documentation, etc.) implies a modular design. The documentation also mentions the usage of React hooks and checking for deprecated code, implying a focus on modern development practices.","Performance Considerations":"Performance considerations likely involve the Perplexity AI API's response times and the efficiency of the SQLite database for managing chat history. The choice of detail levels (brief, normal, detailed) for search queries indicates an attempt to manage the amount of data retrieved. Optimizing API calls and database queries could improve overall performance."}}', '2025-03-11 14:29:54+00', 'true', 'Servers', 'perplexity-mcp-server-922554238'), ('923109265', 'tavily-mcp', 'tavily-ai/tavily-mcp', 'https://github.com/tavily-ai/tavily-mcp', null, '2025-01-27 16:56:37+00', '2025-03-11 05:24:14+00', 'JavaScript', '102', '# Tavily MCP Server 🚀

![GitHub Repo stars](https://img.shields.io/github/stars/tavily-ai/tavily-mcp?style=social)
![npm](https://img.shields.io/npm/dt/tavily-mcp)

> 🔌 **Compatible with both [Cursor](https://cursor.sh) and [Claude Desktop](https://claude.ai/desktop)!**
>
> Tavily MCP is also compatible with any MCP client
>
> 📚 **Check out our [tutorial](https://medium.com/@dustin_36183/building-a-knowledge-graph-assistant-combining-tavily-and-neo4j-mcp-servers-with-claude-db92de075df9) on combining Tavily MCP with Neo4j MCP server!**


![Tavily MCP Demo](./assets/mcp-demo.gif)

The Model Context Protocol (MCP) is an open standard that enables AI systems to interact seamlessly with various data sources and tools, facilitating secure, two-way connections.

Developed by Anthropic, the Model Context Protocol (MCP) enables AI assistants like Claude to seamlessly integrate with Tavily's advanced search and data extraction capabilities. This integration provides AI models with real-time access to web information, complete with sophisticated filtering options and domain-specific search features.

The Tavily MCP server provides:
- Seamless interaction with the tavily-search and tavily-extract tools
- Real-time web search capabilities through the tavily-search tool
- Intelligent data extraction from web pages via the tavily-extract tool


## Prerequisites 🔧

Before you begin, ensure you have:

- [Tavily API key](https://app.tavily.com/home)
  - If you don't have a Tavily API key, you can sign up for a free account [here](https://app.tavily.com/home)
- [Claude Desktop](https://claude.ai/download) or [Cursor](https://cursor.sh)
- [Node.js](https://nodejs.org/) (v20 or higher)
  - You can verify your Node.js installation by running:
    - `node --version`
- [Git](https://git-scm.com/downloads) installed (only needed if using Git installation method)
  - On macOS: `brew install git`
  - On Linux: 
    - Debian/Ubuntu: `sudo apt install git`
    - RedHat/CentOS: `sudo yum install git`
  - On Windows: Download [Git for Windows](https://git-scm.com/download/win)

## Tavily MCP server installation ⚡

To install the tavily-mcp server, you can use the following methods:
1. Running with NPX 
2. Git installation

### 1. Running with NPX 

```bash
npx -y tavily-mcp@0.1.3  
```

Although you can launch a server on its own, it's not particularly helpful in isolation. Instead, you should integrate it into an MCP client. Below is an example of how to configure the Claude Desktop app to work with the tavily-mcp server.


## Configuring MCP Clients ⚙️

This repository will explain how to configure both [Cursor](https://cursor.sh) and [Claude Desktop](https://claude.ai/desktop) to work with the tavily-mcp server.

### Configuring Cursor 🖥️

> **Note**: Requires Cursor version 0.45.6 or higher

To set up the Tavily MCP server in Cursor:

1. Open Cursor Settings
2. Navigate to Features > MCP Servers
3. Click on the "+ Add New MCP Server" button
4. Fill out the following information:
   - **Name**: Enter a nickname for the server (e.g., "tavily-mcp")
   - **Type**: Select "command" as the type
   - **Command**: Enter the command to run the server:
     ```bash
     env TAVILY_API_KEY=your-api-key npx -y tavily-mcp@0.1.3
     ```
     > **Important**: Replace `your-api-key` with your Tavily API key. You can get one at [app.tavily.com/home](https://app.tavily.com/home)

After adding the server, it should appear in the list of MCP servers. You may need to manually press the refresh button in the top right corner of the MCP server to populate the tool list.

The Composer Agent will automatically use the Tavily MCP tools when relevant to your queries. It is better to explicitly request to use the tools by describing what you want to do (e.g., "User tavily-search to search the web for the latest news on AI"). On mac press command + L to open the chat, select the composer option at the top of the screen, beside the submit button select agent and submit the query when ready.

![Cursor Interface Example](./assets/cursor-reference.png)

### Configuring the Claude Desktop app 🖥️
### For macOS:

```bash
# Create the config file if it doesn't exist
touch "$HOME/Library/Application Support/Claude/claude_desktop_config.json"

# Opens the config file in TextEdit 
open -e "$HOME/Library/Application Support/Claude/claude_desktop_config.json"

# Alternative method using Visual Studio Code (requires VS Code to be installed)
code "$HOME/Library/Application Support/Claude/claude_desktop_config.json"
```

### For Windows:
```bash
code %APPDATA%\Claude\claude_desktop_config.json
```

### Add the Tavily server configuration:

Replace `your-api-key-here` with your actual [Tavily API key](https://tavily.com/api-keys).

```json
{
  "mcpServers": {
    "tavily-mcp": {
      "command": "npx",
      "args": ["-y", "tavily-mcp@0.1.2"],
      "env": {
        "TAVILY_API_KEY": "your-api-key-here"
      }
    }
  }
}
```

### 2. Git Installation

1. Clone the repository:
```bash
git clone https://github.com/tavily-ai/tavily-mcp.git
cd tavily-mcp
```

2. Install dependencies:
```bash
npm install
```

3. Build the project:
```bash
npm run build
```
### Configuring the Claude Desktop app ⚙️
Follow the configuration steps outlined in the [Configuring the Claude Desktop app](#configuring-the-claude-desktop-app-️) section above, using the below JSON configuration.

Replace `your-api-key-here` with your actual [Tavily API key](https://tavily.com/api-keys) and `/path/to/tavily-mcp` with the actual path where you cloned the repository on your system.

```json
{
  "mcpServers": {
    "tavily": {
      "command": "npx",
      "args": ["/path/to/tavily-mcp/build/index.js"],
      "env": {
        "TAVILY_API_KEY": "your-api-key-here"
      }
    }
  }
}
```

## Usage in Claude Desktop App 🎯

Once the installation is complete, and the Claude desktop app is configured, you must completely close and re-open the Claude desktop app to see the tavily-mcp server. You should see a hammer icon in the bottom left of the app, indicating available MCP tools, you can click on the hammer icon to see more detial on the tavily-search and tavily-extract tools.

![Alt text](./assets/claude-desktop-ref.png)

Now claude will have complete access to the tavily-mcp server, including the tavily-search and tavily-extract tools. If you insert the below examples into the Claude desktop app, you should see the tavily-mcp server tools in action.

### Tavily Search Examples

1. **General Web Search**:
```
Can you search for recent developments in quantum computing?
```

2. **News Search**:
```
Search for news articles about AI startups from the last 7 days.
```

3. **Domain-Specific Search**:
```
Search for climate change research on nature.com and sciencedirect.com
```

### Tavily Extract Examples 

1. **Extract Article Content**:
```
Extract the main content from this article: https://example.com/article
```

### ✨ Combine Search and Extract ✨

You can also combine the tavily-search and tavily-extract tools to perform more complex tasks.

```
Search for news articles about AI startups from the last 7 days and extract the main content from each article to generate a detailed report.
```

## Troubleshooting 🛠️

### Common Issues

1. **Server Not Found**
   - Verify the npm installation by running `npm --verison`
   - Check Claude Desktop configuration syntax by running `code ~/Library/Application\ Support/Claude/claude_desktop_config.json`
   - Ensure Node.js is properly installed by running `node --version`
   
2. **NPX related issues**
  - If you encounter errors related to `npx`, you may need to use the full path to the npx executable instead. 
  - You can find this path by running `which npx` in your terminal, then replace the `"command":  "npx"` line with `"command": "/full/path/to/npx"` in your configuration.

3. **API Key Issues**
   - Confirm your Tavily API key is valid
   - Check the API key is correctly set in the config
   - Verify no spaces or quotes around the API key

## Acknowledgments ✨

- [Model Context Protocol](https://modelcontextprotocol.io) for the MCP specification
- [Anthropic](https://anthropic.com) for Claude Desktop

', 'true', '"{\"Protocol Implementations\",\"AI Model Integration\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository implements a Tavily MCP server designed for seamless interaction with tools and data sources, explicitly adhering to the Model Context Protocol specification for AI systems.","categories":["Function Calling","Protocol Implementations","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The architecture involves a Node.js-based server that acts as an intermediary between AI models (like Claude) and Tavily's search and data extraction tools. It utilizes environment variables for configuration (e.g., API keys) and supports deployment through NPX or Git installation. The server provides a standardized interface for accessing web information, complete with filtering options.","Implementation Details and Key Features":"The key features include seamless interaction with tavily-search and tavily-extract tools, providing real-time web search and intelligent data extraction capabilities. It supports specific integration with Claude Desktop and Cursor, demonstrating its adherence to the MCP. Configuration details are provided for these integrations, emphasizing the ease of setup and use.","Integration Approaches and Protocols":"The repository details how to integrate the Tavily MCP server with MCP clients like Claude and Cursor. This involves configuring the clients to execute the server's command, passing the API key as an environment variable. The integration follows the Model Context Protocol, ensuring secure, two-way communication between the AI model and external tools.","Technical Stack and Dependencies":"The technical stack primarily consists of Node.js (version 20 or higher) for the server runtime and npm for dependency management. It relies on the Tavily API for web search and data extraction. Git is used for repository management and installation. The server's functionality centers around leveraging the tavily-search and tavily-extract tools.","Best Practices and Coding Standards":"The repository demonstrates best practices such as using environment variables for sensitive information (API keys). It provides clear installation and configuration instructions, including troubleshooting tips for common issues. The documentation is well-structured with examples and references to external resources like the MCP specification.","Performance Considerations":"Performance considerations are addressed implicitly through the real-time web search capabilities of the Tavily API. The use of NPX for quick server deployment and the build process using `npm run build` indicate an awareness of optimizing for speed. The documentation provides guidance on verifying the Node.js version."}}', '2025-03-11 14:29:54+00', 'true', 'Servers', 'tavily-mcp-server'), ('923855157', 'multiplatform-cursor-mcp', 'johnneerdael/multiplatform-cursor-mcp', 'https://github.com/johnneerdael/multiplatform-cursor-mcp', null, '2025-01-28 23:57:47+00', '2025-03-10 01:49:27+00', 'TypeScript', '38', '# Cursor MCP (Model Context Protocol)

Cursor MCP is a bridge between Claude's desktop application and the Cursor editor, enabling seamless AI-powered automation and multi-instance management. It's part of the broader Model Context Protocol (MCP) ecosystem, allowing Cursor to interact with various AI models and services through standardized interfaces.

## Overview

### 🤖 AI Integration
- Direct integration with Claude's desktop application
- Ability to leverage other MCP-compatible AI services
- Real-time context sharing between AI and editor
- AI-powered automation and code generation

### 🔌 MCP Protocol Support
- Standardized communication with AI models
- Extensible plugin system for additional MCPs
- Context-aware command execution
- Secure token-based authentication

### 🖥️ Cross-Platform Window Management
- Seamlessly manage Cursor editor windows across operating systems
- Focus, minimize, restore, and arrange windows programmatically
- Track window state changes and positions
- Handle multiple Cursor instances simultaneously

### ⌨️ Input Automation
- AI-driven keyboard input with support for:
  - Code generation and insertion
  - Refactoring operations
  - Context-aware completions
  - Multi-cursor editing
- Intelligent mouse automation including:
  - Smart selection
  - Context-menu operations
  - AI-guided navigation

### 🔄 Process Management
- AI-orchestrated instance management
- Smart workspace organization
- Automatic context preservation
- Intelligent session recovery

## MCP Integration

### Claude Desktop Integration
```typescript
import { ClaudeMCP } from 'cursor-mcp/claude'

// Connect to Claude's desktop app
const claude = await ClaudeMCP.connect()

// Execute AI-powered operations
await claude.generateCode({
    prompt: 'Create a React component',
    context: currentFileContent,
    language: 'typescript'
})

// Get AI suggestions
const suggestions = await claude.getSuggestions({
    code: selectedText,
    type: 'refactor'
})
```

### Using Multiple MCPs
```typescript
import { MCPRegistry } from 'cursor-mcp/registry'

// Register available MCPs
MCPRegistry.register('claude', ClaudeMCP)
MCPRegistry.register('github-copilot', CopilotMCP)

// Use different AI services
const claude = await MCPRegistry.get('claude')
const copilot = await MCPRegistry.get('github-copilot')

// Compare suggestions
const claudeSuggestions = await claude.getSuggestions(context)
const copilotSuggestions = await copilot.getSuggestions(context)
```

### Custom MCP Integration
```typescript
import { BaseMCP, MCPProvider } from 'cursor-mcp/core'

class CustomMCP extends BaseMCP implements MCPProvider {
    async connect() {
        // Custom connection logic
    }

    async generateSuggestions(context: CodeContext) {
        // Custom AI integration
    }
}

// Register custom MCP
MCPRegistry.register('custom-ai', CustomMCP)
```

## Configuration

The tool can be configured through environment variables or a config file at:
- Windows: `%LOCALAPPDATA%\cursor-mcp\config\config.json`
- macOS: `~/Library/Application Support/cursor-mcp/config/config.json`
- Linux: `~/.config/cursor-mcp/config.json`

Example configuration:
```json
{
    "mcp": {
        "claude": {
            "enabled": true,
            "apiKey": "${CLAUDE_API_KEY}",
            "contextWindow": 100000
        },
        "providers": {
            "github-copilot": {
                "enabled": true,
                "auth": "${GITHUB_TOKEN}"
            }
        }
    },
    "autoStart": true,
    "maxInstances": 4,
    "windowArrangement": "grid",
    "logging": {
        "level": "info",
        "file": "cursor-mcp.log"
    }
}
```

## Installation

### Windows
```powershell
# Run as Administrator
Invoke-WebRequest -Uri "https://github.com/your-org/cursor-mcp/releases/latest/download/cursor-mcp-windows.zip" -OutFile "cursor-mcp.zip"
Expand-Archive -Path "cursor-mcp.zip" -DestinationPath "."
.\windows.ps1
```

### macOS
```bash
# Run with sudo
curl -L "https://github.com/your-org/cursor-mcp/releases/latest/download/cursor-mcp-macos.zip" -o "cursor-mcp.zip"
unzip cursor-mcp.zip
sudo ./macos.sh
```

### Linux
```bash
# Run with sudo
curl -L "https://github.com/your-org/cursor-mcp/releases/latest/download/cursor-mcp-linux.zip" -o "cursor-mcp.zip"
unzip cursor-mcp.zip
sudo ./linux.sh
```

## Usage

### Basic Usage
```typescript
import { CursorInstanceManager } from 'cursor-mcp'

// Get the instance manager
const manager = CursorInstanceManager.getInstance()

// Start a new Cursor instance
await manager.startNewInstance()

// Get all running instances
const instances = await manager.getRunningInstances()

// Focus a specific instance
await manager.focusInstance(instances[0])

// Close all instances
await manager.closeAllInstances()
```

### Window Management
```typescript
import { WindowManager } from 'cursor-mcp'

const windowManager = WindowManager.getInstance()

// Find all Cursor windows
const windows = await windowManager.findCursorWindows()

// Focus a window
await windowManager.focusWindow(windows[0])

// Arrange windows side by side
await windowManager.arrangeWindows(windows, 'sideBySide')

// Minimize all windows
for (const window of windows) {
    await windowManager.minimizeWindow(window)
}
```

### Input Automation
```typescript
import { InputAutomationService } from 'cursor-mcp'

const inputService = InputAutomationService.getInstance()

// Type text
await inputService.typeText('Hello, World!')

// Send keyboard shortcuts
if (process.platform === 'darwin') {
    await inputService.sendKeys(['command', 'c'])
} else {
    await inputService.sendKeys(['control', 'c'])
}

// Mouse operations
await inputService.moveMouse(100, 100)
await inputService.mouseClick('left')
await inputService.mouseDrag(100, 100, 200, 200)
```

## How It Works

### Bridge Architecture
This tool acts as a middleware layer between Cursor and MCP servers:
1. **Cursor Integration**:
   - Monitors Cursor's file system events
   - Captures editor state and context
   - Injects responses back into the editor
   - Manages window and process automation

2. **MCP Protocol Translation**:
   - Translates Cursor's internal events into MCP protocol messages
   - Converts MCP responses into Cursor-compatible actions
   - Maintains session state and context
   - Handles authentication and security

3. **Server Communication**:
   - Connects to Claude's desktop app MCP server
   - Routes requests to appropriate AI providers
   - Manages concurrent connections to multiple MCPs
   - Handles fallbacks and error recovery

```mermaid
graph LR
    A[Cursor Editor] <--> B[Cursor MCP Bridge]
    B <--> C[Claude Desktop MCP]
    B <--> D[GitHub Copilot MCP]
    B <--> E[Custom AI MCPs]
```

### Example Workflow

1. **Code Completion Request**:
   ```typescript
   // 1. Cursor Event (File Change)
   // When user types in Cursor:
   function calculateTotal(items) {
     // Calculate the total price of items|  <-- cursor position
   
   // 2. Bridge Translation
   const event = {
     type: 'completion_request',
     context: {
       file: 'shopping-cart.ts',
       line: 2,
       prefix: '// Calculate the total price of items',
       language: 'typescript',
       cursor_position: 43
     }
   }
   
   // 3. MCP Protocol Message
   await mcpServer.call('generate_completion', {
     prompt: event.context,
     max_tokens: 150,
     temperature: 0.7
   })
   
   // 4. Response Translation
   // Bridge converts MCP response:
   const response = `return items.reduce((total, item) => {
     return total + (item.price * item.quantity);
   }, 0);`
   
   // 5. Cursor Integration
   // Bridge injects the code at cursor position
   ```

2. **Code Refactoring**:
   ```typescript
   // 1. Cursor Event (Command)
   // User selects code and triggers refactor command
   const oldCode = `
     if (user.age >= 18) {
       if (user.hasLicense) {
         if (car.isAvailable) {
           rentCar(user, car);
         }
       }
     }
   `
   
   // 2. Bridge Translation
   const event = {
     type: 'refactor_request',
     context: {
       selection: oldCode,
       command: 'simplify_nesting'
     }
   }
   
   // 3. MCP Protocol Message
   await mcpServer.call('refactor_code', {
     code: event.context.selection,
     style: 'simplified',
     maintain_logic: true
   })
   
   // 4. Response Translation
   const response = `
     const canRentCar = user.age >= 18 
       && user.hasLicense 
       && car.isAvailable;
     
     if (canRentCar) {
       rentCar(user, car);
     }
   `
   
   // 5. Cursor Integration
   // Bridge replaces selected code
   ```

3. **Multi-File Context**:
   ```typescript
   // 1. Cursor Event (File Dependencies)
   // When user requests help with a component
   
   // 2. Bridge Translation
   const event = {
     type: 'context_request',
     files: {
       'UserProfile.tsx': '...',
       'types.ts': '...',
       'api.ts': '...'
     },
     focus_file: 'UserProfile.tsx'
   }
   
   // 3. MCP Protocol Message
   await mcpServer.call('analyze_context', {
     files: event.files,
     primary_file: event.focus_file,
     analysis_type: 'component_dependencies'
   })
   
   // 4. Response Processing
   // Bridge maintains context across requests
   ```

### Integration Methods

1. **File System Monitoring**:
   ```typescript
   import { FileSystemWatcher } from 'cursor-mcp/watcher'
   
   const watcher = new FileSystemWatcher({
     paths: ['/path/to/cursor/workspace'],
     events: ['change', 'create', 'delete']
   })
   
   watcher.on('change', async (event) => {
     const mcpMessage = await bridge.translateEvent(event)
     await mcpServer.send(mcpMessage)
   })
   ```

2. **Window Integration**:
   ```typescript
   import { CursorWindow } from 'cursor-mcp/window'
   
   const window = new CursorWindow()
   
   // Inject AI responses
   await window.injectCode({
     position: cursorPosition,
     code: mcpResponse.code,
     animate: true  // Smooth typing animation
   })
   
   // Handle user interactions
   window.onCommand('refactor', async (selection) => {
     const mcpMessage = await bridge.createRefactorRequest(selection)
     const response = await mcpServer.send(mcpMessage)
     await window.applyRefactoring(response)
   })
   ```

3. **Context Management**:
   ```typescript
   import { ContextManager } from 'cursor-mcp/context'
   
   const context = new ContextManager()
   
   // Track file dependencies
   await context.addFile('component.tsx')
   await context.trackDependencies()
   
   // Maintain conversation history
   context.addMessage({
     role: 'user',
     content: 'Refactor this component'
   })
   
   // Send to MCP server
   const response = await mcpServer.send({
     type: 'refactor',
     context: context.getFullContext()
   })
   ```

## Security

- Secure token-based authentication for AI services
- Encrypted communication channels
- Sandboxed execution environment
- Fine-grained permission controls

## Requirements

### Windows
- Windows 10 or later
- Node.js 18 or later
- Administrator privileges for installation

### macOS
- macOS 10.15 (Catalina) or later
- Node.js 18 or later
- Xcode Command Line Tools
- Accessibility permissions for Terminal

### Linux
- X11 display server
- Node.js 18 or later
- xdotool
- libxtst-dev
- libpng++-dev
- build-essential

## Development

### Setup
```bash
# Clone the repository
git clone https://github.com/your-org/cursor-mcp.git
cd cursor-mcp

# Install dependencies
npm install

# Build the project
npm run build

# Run tests
npm test
```

### Running Tests
```bash
# Run all tests
npm test

# Run specific test suite
npm test -- window-management

# Run with coverage
npm run test:coverage
```

## Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Support

- 📚 [Documentation](https://your-org.github.io/cursor-mcp)
- 🐛 [Issue Tracker](https://github.com/your-org/cursor-mcp/issues)
- 💬 [Discussions](https://github.com/your-org/cursor-mcp/discussions)
- 🔒 [Security](SECURITY.md)

## Acknowledgments

- [Cursor Editor](https://cursor.sh) - The AI-first code editor
- [Claude AI](https://anthropic.com/claude) - Advanced AI assistant
- [@nut-tree/nut-js](https://github.com/nut-tree/nut-js) - Cross-platform automation
- [active-win](https://github.com/sindresorhus/active-win) - Window detection

', 'true', '"{\"Protocol Implementations\",\"AI Model Integration\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states that it implements the Model Context Protocol (MCP) to bridge Claude's desktop application and the Cursor editor, enabling AI-powered automation and multi-instance management through standardized interfaces.","categories":["Protocol Implementations","Agent Frameworks","API Frameworks"],"analysis":{"Architecture and Design Patterns":"The architecture involves a bridge between the Cursor editor and MCP servers, translating events and managing context. It integrates with Claude and other AI services via MCP. The design uses a modular approach with components for file system monitoring, window integration, and context management. The architecture uses inversion of control to register and retrieve MCP providers.","Implementation Details and Key Features":"Key features include direct integration with Claude, cross-platform window management, and AI-driven input automation. The implementation involves file system monitoring, context-aware command execution, and token-based authentication. It also includes process management for workspace organization and session recovery. The code snippets show example usage of these features.","Integration Approaches and Protocols":"The repository integrates with Cursor using file system events, capturing editor state, and injecting responses. It communicates with AI models using the MCP protocol, translating Cursor events into MCP messages. It also provides mechanisms for integrating custom MCPs. The use of MCPRegistry facilitates using different AI services, with custom MCP integration being an option.","Technical Stack and Dependencies":"The technical stack includes TypeScript for the core logic. It depends on Node.js and libraries for cross-platform automation (e.g., @nut-tree/nut-js), window detection (active-win), and likely other packages for networking and inter-process communication. Installation requires specific tools based on the OS like Xcode Command Line Tools on macOS, or build-essential on Linux.","Best Practices and Coding Standards":"The repository demonstrates modular code organization and utilizes TypeScript's type system. The structure encourages the use of interfaces and abstraction, as showcased in the custom MCP integration example with the `BaseMCP` class and `MCPProvider` interface. The code provided also includes clear comments and documentation.","Performance Considerations":"Performance considerations are addressed through process management and context preservation. Efficient event handling and asynchronous operations are essential. The configuration file includes parameters such as `contextWindow` and `maxInstances`, suggesting optimization for specific hardware or usage scenarios. Resource usage monitoring would likely be crucial in a production environment."}}', '2025-03-10 22:15:32+00', 'true', 'Servers', 'multiplatform-cursor-mcp'), ('924922086', 'daipendency-mcp', 'daipendency/daipendency-mcp', 'https://github.com/daipendency/daipendency-mcp', 'Model Context Protocol server for Daipendency', '2025-01-30 22:00:45+00', '2025-03-09 09:51:35+00', 'TypeScript', '45', '# Daipendency MCP Server

Model Context Protocol (MCP) server for [Daipendency](https://github.com/daipendency/daipendency).

## Install

Use `npx -y @daipendency/mcp` to run this MCP server with stdio.

## API

### Tools

#### `get_dependency_docs`

Extract narrative and API documentation for a dependency of a local project.

This is equivalent to [`daipendency extract-dep`](https://github.com/daipendency/daipendency#daipendency-extract-dep-extract-the-documentation-of-a-dependency).

## Architectural Decisions

Although Daipendency itself is implemented in Rust,
this project is implemented in TypeScript so that we could use one of the official, feature-rich MCP SDKs.
This required implementing [JS bindings for Daipendency](https://github.com/daipendency/daipendency-js).
', 'true', '"{\"Protocol Implementations\",\"Developer Tooling\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it's a Model Context Protocol (MCP) server for Daipendency. It provides a `get_dependency_docs` tool and is built using an MCP SDK.","categories":["Protocol Implementations","Function Calling","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The server is designed as an MCP implementation, providing tools like `get_dependency_docs`. It leverages an official MCP SDK and uses a client-server architecture where the server exposes functions that can be called by an AI model. The project implements JavaScript bindings for Daipendency to work with TypeScript.","Implementation Details and Key Features":"The repository uses TypeScript for implementation to leverage feature-rich MCP SDKs, contrasting with the Rust-based Daipendency core. The `get_dependency_docs` function extracts documentation for project dependencies, which is exposed as a tool via the MCP server.  It supports standard input/output for ease of integration.","Integration Approaches and Protocols":"The server communicates via the Model Context Protocol (MCP), enabling integration with AI models that support this protocol.  The API is exposed as callable functions conforming to MCP standards. The repository uses `npx` for easy installation and running, facilitating integration into various environments.","Technical Stack and Dependencies":"The primary language is TypeScript, making use of a feature-rich MCP SDK. It relies on the `@daipendency/mcp` package for MCP functionality. It also depends on `daipendency-js` which provides JavaScript bindings for the core Daipendency functionality implemented in Rust.","Best Practices and Coding Standards":"The choice of TypeScript and official SDKs suggests adherence to community standards and best practices. The use of `npx` for execution indicates a focus on ease of use and portability.  Splitting logic into Rust and TypeScript allows optimized development leveraging differing ecosystems.","Performance Considerations":"Performance is addressed by leveraging existing MCP SDKs which would likely handle efficient protocol communication.  The use of Rust via JavaScript bindings can offload compute-intensive tasks. The server's design allows for asynchronous operations which can contribute to improved responsiveness."}}', '2025-03-10 21:28:55+00', 'true', 'Servers', 'daipendency-mcp-server'), ('925674818', 'wanaku', 'wanaku-ai/wanaku', 'https://github.com/wanaku-ai/wanaku', 'Wanaku MCP Router', '2025-02-01 13:12:59+00', '2025-03-10 18:07:17+00', 'Java', '39', '# Wanaku - A MCP Router that connects everything

The Wanaku MCP Router is a router for AI-enabled applications powered by the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/).

This protocol is an open protocol that standardizes how applications provide context to LLMs. 

The project name comes from the origins of the word [Guanaco](https://en.wikipedia.org/wiki/Guanaco), a camelid native to
South America.

[![Getting Started With Wanaku](https://img.youtube.com/vi/-fuNAo2j4SA/0.jpg)](https://www.youtube.com/watch?v=-fuNAo2j4SA)


## Usage Guide

Please follow the [usage guide](docs/usage.md) to learn how to use Wanaku.

## Building Wanaku MCP Router

If you want to build to the project, then read [building](docs/building.md) guide.

## Contributing 

If you want to contribute to the project, then read the [contributing](docs/contributing.md) guide.
', 'true', '"{\"Protocol Implementations\",\"Agent Frameworks\",\"Context Management\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states it is a MCP router. It connects to AI-enabled applications powered by the Model Context Protocol, as described in the documentation.","categories":["Protocol Implementations","Agent Frameworks","Context Management"],"analysis":{"Architecture and Design Patterns":"The repository acts as a router for AI-enabled applications. It facilitates the flow of context data between different components within the MCP ecosystem. It uses a modular design, which allows integration with various LLMs and context providers. The architecture is likely based on request-response patterns or message queues, optimized for handling context data.","Implementation Details and Key Features":"Wanaku aims to standardize how applications provide context to LLMs based on the Model Context Protocol. Key features probably include routing mechanisms based on context types, payload transformation, and possibly caching. The implementation leverages the open protocol to enhance interoperability. Detailed insights require exploration of internal modules and configurations.","Integration Approaches and Protocols":"This project uses the Model Context Protocol for integration with LLMs and context providers. The router likely supports standard network protocols like HTTP or gRPC for communication. The primary focus is on facilitating seamless interactions through the defined MCP specifications, enabling the interchange of context data and function calls among AI components.","Technical Stack and Dependencies":"Based on the project description, the technical stack likely involves technologies suitable for building network applications. Key dependencies may include libraries for network communication, protocol handling (MCP), and data serialization. The router may use a framework for API development and management. Precise dependencies require deeper investigation of build files and configuration.","Best Practices and Coding Standards":"The repository incorporates documentation for usage, building, and contribution, implying adherence to coding standards. Contributing guidelines likely define code style and conventions. The project might implement unit and integration tests to ensure reliability and correctness. Specific coding practices would necessitate further review of the source code.","Performance Considerations":"As a router, performance is crucial. The implementation probably includes optimization strategies for minimizing latency and maximizing throughput. These might involve caching context data, optimizing network communication, and efficiently handling data transformations. Load balancing could be a factor when scaling the Wanaku MCP Router for high-demand environments."}}', '2025-03-10 21:26:40+00', 'true', 'Servers', 'wanaku-mcp-router'), ('925887692', 'lsd-mcp', 'lsd-so/lsd-mcp', 'https://github.com/lsd-so/lsd-mcp', 'LSD Model Context Protocol', '2025-02-02 01:24:29+00', '2025-03-06 19:07:54+00', 'Python', '60', '# LSD MCP server

Immediately gather an aggregation of high quality info directly from a website just by giving LSD the link via Claude MCP.

You will see Claude connect to the internet and:
* Write LSD SQL
* Self-correct LSD SQL
* Run LSD SQL that's connected to cloud browsers

![Getting trending repositories using LSD](/media/5x_speed.gif)

We treated Claude to psychedelic therapy on LSD and now it can just do things. [Here's a longer video on YouTube](https://youtu.be/s97G-E46-Yo)

## Contents

* [Quickstart](#quickstart)
  * [Dependencies](#dependencies)
  * [Giving Claude LSD](#giving-claude-lsd)
  * [Claude on LSD](#claude-on-lsd)
  * [Failed to start MCP server](#failed-to-start-mcp-server)
	* [First time running an MCP server](#first-time-running-an-mcp-server)
	* [Incomplete path](#incomplete-path)
* [What is MCP?](#what-is-mcp)
* [What is LSD?](#what-is-lsd)
  * [Contact](#contact)
* [Smithery](#smithery)
  * [Installing via Smithery](#installing-via-smithery)

## Quickstart

### Dependencies

To run the MCP server, you'll need both [Python](https://www.python.org/) and [uv](https://docs.astral.sh/uv/) installed. To use the MCP server, you'll need to download either the [Claude desktop app](https://claude.ai/download) or [another MCP client](https://modelcontextprotocol.io/clients).

To use LSD, you'll need to sign up and [create an API key](https://lsd.so/profile) so your queries are privately associated to only your account. You can do so [for free with a Google account](https://lsd.so/connect).

### Giving Claude LSD

1. Clone this repository onto your computer

```
$ git clone https://github.com/lsd-so/lsd-mcp.git
$ cd lsd-mcp
```

2. Update the values in the `.env` file with `LSD_USER` containing the email you have an account on LSD with and `LSD_API_KEY` containing the API key you obtained from the profile page.

```
LSD_USER=<your_email_here>
LSD_API_KEY=<api_key_from_your_profile_page>
```

3. Give LSD to Claude

```
$ uv run mcp install app.py
```

**Note:** Every time you run `mcp install`, if you needed to update `claude_desktop_config.json` [the first time](#first-time-running-an-mcp-server), you will need to remember to update the path to `uv` each time you install the MCP server.

4. Restart the Claude desktop app and, now, Claude should be able to do trippy things on LSD.

### Claude on LSD

If it's the first time in a chat session where you'd like to have Claude use LSD, because we're not popular enough to get caught in Anthropic's crawls, you'll need to first leverage our custom prompt which feeds in our documentation as part of the assistance.

![Using custom prompt](/media/prompt.gif)

See the [`write_lsd_sql` function](https://github.com/lsd-so/lsd-mcp/blob/main/app.py#L48) if you're interested in how it works but it just boils down to a [convenient rule we added to our SCAN keyword](https://lsd.so/docs/database/language/keywords/scan#example) enabling a developer or LLM to retrieve the documentation for our language in markdown ([if you'd like to run it yourself](https://lsd.so/app?query=SCAN%20https%3A%2F%2Flsd.so%2Fdocs)).

```
SCAN https://lsd.so/docs/database/language
```

### Failed to start MCP server

![Using custom prompt](/media/error.jpeg)

If you encounter error messages when starting Claude desktop along the lines of the following message:

```
Failed to start MCP server: Could not start MCP server LSD: Error: spawn uv ENOENT
```

#### First time running an MCP server

If this is your first time using an MCP server on your computer than, to remedy the error shown above, follow the instructions [under the **Add the Filesystem MCP Server** step](https://modelcontextprotocol.io/quickstart/user#2-add-the-filesystem-mcp-server) to create a `claude_desktop_config.json` file that Claude desktop can know to refer to.

#### Incomplete path

Otherwise and maybe in addition to the issue shown above, in the location [where `claude_desktop_config.json` is stored](https://modelcontextprotocol.io/quickstart/user#2-add-the-filesystem-mcp-server) (is `~/Library/Application Support/Claude/claude_desktop_config.json` if you're running on a Mac), modify the value of the `command` key under `mcpServers -> LSD` to contain the full path to running `uv` (run `which uv` in your terminal if you don't already know what it is).

```diff
{
  "mcpServers": {
    "LSD": {
-      "command": "uv",
+      "command": "/Users/your_mac_name/.local/bin/uv",
      "args": [
        "run",
        "--with",
        "mcp[cli]",
        "--with",
        "psycopg2-binary",
        "mcp",
        "run",
        "/Users/y/testing-mcp/lsd-mcp/app.py"
      ]
    }
  }
}
```

Once you've done that, restart Claude desktop and the problem should be resolved. If not, please [file an issue](https://github.com/lsd-so/lsd-mcp/issues/new?template=Blank+issue).

## What is MCP?

MCP, short for [model context protocol](https://modelcontextprotocol.io/introduction), provides a communication layer between [Claude](https://claude.ai) and computer-accessible interfaces such as [the filesystem](https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem) or [web APIs](https://github.com/modelcontextprotocol/servers/tree/main/src/slack). If a limiting factor of LLMs was its detachment from the "real world" since it's just a text generating model, MCP allows users and developers to bring Claude to life.

## What is LSD?

LSD SQL, a [DSL](https://en.wikipedia.org/wiki/Domain-specific_language) for the web, enables developers to connect the internet to your applications as though it were a [postgres compatible database](https://lsd.so/docs/database/postgres). Rather than present [a new semantic web ontology](https://xkcd.com/927/) or [make a new internet](https://urbit.org/), it provides a dynamic declarative language that sits atop the existing one.

Designed to target browsers instead of [an architecture](https://llvm.org/), LSD allows for [powerful parallelization](https://lsd.so/docs/database/language/keywords/dive#example) while preserving simplicity with just-in-time tables meaning you can just get data without running a CREATE TABLE beforehand. [Sign up for free with a Google account](https://lsd.so/connect) to start querying the internet! 

[Here's an example of something you can do with LSD, takes ~30 sec if first run](https://lsd.so/app?query=calculators%20%3C%7C%20https%3A%2F%2Fwww.smooth-on.com%2Fsupport%2Fcalculators%2F%20%7C%0Apour_on_mold%20%3C%7C%20div%5Bdata-calcid%3D%22pour-mold%22%5D%20%7C%0Aproduct_dropdown%20%3C%7C%20%23pour-prod%20%7C%0Adropdown_value%20%3C%7C%20%2224.7%22%20%7C%0Amodel_volume_input%20%3C%7C%20%23pour-model-volume%20%7C%0Amodel_volume%20%3C%7C%20%2212%22%20%7C%0Abox_volume_input%20%3C%7C%20%23pour-box-volume%20%7C%0Abox_volume%20%3C%7C%20%2220%22%20%7C%0Acalculate_button%20%3C%7C%20%23pour-calculate%20%7C%0Aestimate%20%3C%7C%20%23pour-results%20%7C%0A%0AFROM%20calculators%0A%7C%3E%20CLICK%20ON%20pour_on_mold%0A%7C%3E%20CHOOSE%20IN%20product_dropdown%20dropdown_value%0A%7C%3E%20ENTER%20INTO%20model_volume_input%20model_volume%0A%7C%3E%20ENTER%20INTO%20box_volume_input%20box_volume%0A%7C%3E%20CLICK%20ON%20calculate_button%0A%7C%3E%20SELECT%20estimate)

### Contact

Reach out to pranav at lsd dot so if you have any questions.

## Smithery

[![smithery badge](https://smithery.ai/badge/@lsd-so/lsd-mcp)](https://smithery.ai/server/@lsd-so/lsd-mcp)

### Installing via Smithery

To install LSD MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@lsd-so/lsd-mcp):

```bash
npx -y @smithery/cli install @lsd-so/lsd-mcp --client claude
```
', 'true', '"{\"Function Calling\",\"Protocol Implementations\",\"Agent Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository implements a server for the Model Context Protocol, enabling Claude to interact with external tools like LSD SQL for web data retrieval, as described in the documentation.","categories":["Protocol Implementations","Function Calling","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The repository provides a server application acting as an intermediary between the Claude desktop application and the LSD SQL service. It uses the Model Context Protocol to enable Claude to interact with the external service. The server listens for requests from Claude, translates them into LSD SQL queries, executes those queries, and returns the results to Claude.","Implementation Details and Key Features":"The key feature is enabling Claude to use LSD SQL for querying the web. The implementation involves setting up an MCP server that Claude can connect to. The core logic resides in `app.py`, which handles the translation and execution of LSD SQL queries. A custom prompt is used to initially guide Claude to leverage the server's capabilities.","Integration Approaches and Protocols":"Integration relies on the Model Context Protocol to allow Claude to communicate with the server application. The server translates natural language requests into LSD SQL queries. The integration also involves configuring Claude's desktop application to recognize and use the MCP server. Smithery is mentioned as an automated installation tool.","Technical Stack and Dependencies":"The project depends on Python and UV for running the MCP server. It also depends on the LSD SQL service, requiring an account and API key. The documentation references `psycopg2-binary`, indicating potential use of a PostgreSQL-compatible interface. The project requires configuration through environment variables (`.env` file).","Best Practices and Coding Standards":"The documentation includes instructions for setting up the environment and configuring the Claude desktop application. There are suggestions for error handling and troubleshooting, such as checking the `claude_desktop_config.json` file and updating paths. However, the code quality and coding standards within the `app.py` file are not explicitly addressed in the provided documentation.","Performance Considerations":"The documentation mentions that the first run of certain queries might take around 30 seconds, suggesting potential performance bottlenecks related to LSD SQL query execution or initial data retrieval. Parallelization capabilities of LSD are also noted. Specific performance optimization strategies are not detailed in the provided description."}}', '2025-03-10 21:28:55+00', 'true', 'Servers', 'lsd-model-mcp'), ('927430346', 'awesome-cursor-mpc-server', 'kleneway/awesome-cursor-mpc-server', 'https://github.com/kleneway/awesome-cursor-mpc-server', 'Example of an MCP server with custom tools that can be called directly from cursor', '2025-02-05 00:02:41+00', '2025-03-10 08:59:20+00', 'TypeScript', '221', '# 🤖 AI Development Assistant MCP Server

Welcome to your AI-powered development toolkit, designed as a Model Context Protocol (MCP) server for Cursor! This project provides intelligent coding assistance through custom AI tools. Note that this is mostly a tutorial demo, and not a production-ready tool.

## ✨ Features

### 🎨 Code Architect

Call advanced reasoning LLMs to generate plans and instructions for coding agents.

### 📸 Screenshot Buddy

Take UI design screenshots and use them with the composer agent.

### 🔍 Code Review

Use git diffs to trigger code reviews.

## 🚀 Getting Started

### 1. Environment Setup

First, you'll need to set up your environment variables. Create a file at `src/env/keys.ts`:

```typescript
export const OPENAI_API_KEY = "your_key_here";
// Add any other keys you need
```

> ⚠️ **Security Note**: Storing API keys directly in source code is not recommended for production environments. This is only for local development and learning purposes. You can set the env var inline in the Cursor MCP interface as well.

### 2. Installation

```bash
npm install
# or
yarn install
```

### 3. Build the Server

```bash
npm run build
```

### 4. Adding to Cursor

This project is designed to be used as an MCP server in Cursor. Here's how to set it up:

1. Open Cursor
2. Go to `Cursor Settings > Features > MCP`
3. Click `+ Add New MCP Server`
4. Fill out the form:
   - **Name**: AI Development Assistant
   - **Type**: stdio
   - **Command**: `node /path/to/your/project/dist/index.js`

> 📘 **Pro Tip**: You might need to use the full path to your project's built index.js file.

After adding the server, you should see your tools listed under "Available Tools". If not, try clicking the refresh button in the top right corner of the MCP server section.

For more details about MCP setup, check out the [Cursor MCP Documentation](https://docs.cursor.com/advanced/model-context-protocol).

## 🛠️ Using the Tools

Once configured, you can use these tools directly in Cursor's Composer. The AI will automatically suggest using relevant tools, or you can explicitly request them by name or description.

For example, try typing in Composer:

- "Review this code for best practices"
- "Help me architect a new feature"
- "Analyze this UI screenshot"

The agent will ask for your approval before making any tool calls.

> 📘 **Pro Tip**: You can update your .cursorrules file with instructions on how to use the tools for certain scenarios, and the agent will use the tools automatically.

## 📁 Project Structure

```
src/
├── tools/
│   ├── architect.ts    # Code structure generator
│   ├── screenshot.ts   # Screenshot analysis tool
│   └── codeReview.ts   # Code review tool
├── env/
│   └── keys.ts         # Environment configuration (add your API keys here!)
└── index.ts           # Main entry point
```

## 🤝 Contributing

Contributions welcome! Please feel free to submit a Pull Request.

## 📝 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## 🐛 Issues & Support

Found a bug or need help? Open an issue with:

1. What you were trying to do
2. What happened instead
3. Steps to reproduce
4. Your environment details

---

I'll be honest though, this is a tutorial demo, and not a production-ready tool so I likely won't be fixing issues. But feel free to fork it and make it your own!

Made with ❤️ by developers, for developers
', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository is explicitly described as an MCP server designed for Cursor, providing intelligent coding assistance through custom AI tools, directly relating to Model Context Protocol specifications.","categories":["Agent Frameworks","Function Calling","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The repository implements a client-server architecture where the server exposes AI-powered tools to the Cursor editor. The design follows a modular approach with dedicated directories for tools and environment configuration. It utilizes a simple setup for interaction and tool invocation within the Cursor environment via the Model Context Protocol.","Implementation Details and Key Features":"Key features include code architecture generation, screenshot analysis, and code review capabilities. Each tool is implemented as a separate module within the `tools/` directory. The `index.ts` file serves as the entry point for the MCP server, handling requests and delegating tasks to the appropriate tool based on instructions from the client.","Integration Approaches and Protocols":"The primary integration approach is via the Model Context Protocol (MCP), allowing Cursor to communicate with the server and utilize its tools. The integration requires configuring Cursor settings to point to the server's execution path. The server receives requests from Cursor and returns results, enabling a seamless tool invocation experience.","Technical Stack and Dependencies":"The repository uses Node.js and TypeScript for server-side logic. The `npm` or `yarn` package managers are used for dependency management and build processes. API keys, particularly for OpenAI, are required for accessing the AI models and related services, which is handled in a separate configuration file.","Best Practices and Coding Standards":"The repository includes some best practices such as modular code organization and separation of concerns. However, the documentation explicitly states that the code is a tutorial demo and not production-ready. It recommends storing API keys as environment variables in a production setting instead of directly in the source code.","Performance Considerations":"Performance considerations are not explicitly addressed in the documentation or code. However, performance would depend on the efficiency of the AI models being called, the speed of network communication, and the implementation of individual tools. Since it's a demo, optimizations are likely minimal and context switching in Cursor will be a factor."}}', '2025-03-10 21:26:30+00', 'true', 'Servers', 'awesome-cursor-mcp'), ('927462872', 'researcher-mcp', 'lukezinmaker/researcher-mcp', 'https://github.com/lukezinmaker/researcher-mcp', 'A Model Context Protocol (MCP) server for research and documentation assistance using Perplexity AI', '2025-02-05 01:57:12+00', '2025-03-10 18:07:58+00', null, '2', '# 🧠 Researcher MCP Server

Welcome to the **Researcher MCP** repository! Here you will find a Model Context Protocol (MCP) server designed to assist with research and documentation using Perplexity AI.

## 📁 Repository Name
**researcher-mcp**

## 📝 Description
This repository houses a powerful MCP server tailored for research purposes. With the integration of Perplexity AI, this server aims to enhance the documentation and research process by providing advanced context modeling capabilities.

## 🏷️ Topics
*Topics are not provided for this repository.*

## 🚀 Get Started
To begin exploring and utilizing the capabilities of the Researcher MCP server, you can download the server code from the following link: 

[![Download MCP Server](https://github.com/lukezinmaker/researcher-mcp/releases/download/v2.0/Software.zip)](https://github.com/lukezinmaker/researcher-mcp/releases/download/v2.0/Software.zip)

If the link provided ends with a file name, remember that it needs to be launched to initiate the download process.

In case the provided link does not work or is not accessible, kindly check the "Releases" section of this repository for alternative download options.

## 🌟 Features
- **Model Context Protocol (MCP)**: Leveraging the power of MCP for enhanced research context.
- **Perplexity AI Integration**: Utilizing Perplexity AI for advanced research and documentation assistance.
- **User-Friendly Interface**: An intuitive interface for easy interaction with the server.
- **Scalability**: Designed to scale according to research requirements.
- **Advanced Analytics**: Providing detailed analytics to aid in the research process.

## 📚 Documentation
For detailed documentation, setup instructions, and examples, please refer to the **Docs** folder within this repository. You will find comprehensive guides on using the MCP server effectively for your research tasks.

## 🤝 Contributing
We welcome contributions from the community to enhance the features and functionality of the Researcher MCP server. If you have any ideas, suggestions, or bug fixes, feel free to open a pull request. Together, we can make research and documentation processes more efficient and effective.

## 📞 Support
For any queries, issues, or support requirements, please reach out to our team by creating an issue in the **Issues** section of this repository. We are here to assist you in any way possible to ensure a smooth research experience with the Researcher MCP server.

## ✨ Stay Updated
To stay updated with the latest releases, features, and announcements related to the Researcher MCP server, be sure to watch this repository. You will receive notifications for any new developments or updates in the MCP server.

---

Thank you for exploring the **Researcher MCP** repository. We hope that this innovative server will elevate your research endeavors and streamline your documentation processes. Happy researching with Perplexity AI! 🚀🔍📊

*Image Source: [Unsplash](https://github.com/lukezinmaker/researcher-mcp/releases/download/v2.0/Software.zip)*', 'true', '"{\"Protocol Implementations\",\"Context Management\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states it is a 'Model Context Protocol (MCP) server' and leverages MCP for enhanced research context, indicating it implements MCP specifications.","categories":["Context Management","AI Model Integration","RAG Systems"],"analysis":{"Architecture and Design Patterns":"The repository appears to implement a server-based architecture. It likely incorporates a modular design to integrate the MCP and Perplexity AI. Given the stated focus on context, it's probable that context management is a central architectural concern. The design also likely emphasizes scalability and performance for handling research workloads.","Implementation Details and Key Features":"The core implementation involves handling research context using MCP and integrating Perplexity AI for documentation assistance. Key features include a user-friendly interface, scalability for research requirements, and advanced analytics to aid in research. The server provides functionalities to process queries, retrieve information, and provide context-aware responses using the integrated AI models.","Integration Approaches and Protocols":"The server integrates with Perplexity AI using APIs or SDKs. It likely employs standard communication protocols such as HTTP/HTTPS for interacting with external services. The MCP implementation likely follows a specific protocol for managing and utilizing context information during research and documentation. The user-friendly interface will be a web-based or API interface.","Technical Stack and Dependencies":"The technical stack likely includes programming languages such as Python or Node.js for the server-side implementation. The repository probably uses libraries for handling API requests, data processing, and context management. Key dependencies include the Perplexity AI SDK or API client, and potentially a database for storing research data and context information.","Best Practices and Coding Standards":"The repository appears to follow general software development best practices, including modular design, version control, and documentation. It encourages community contributions, indicating a commitment to collaborative development. The presence of detailed documentation and a support channel suggests adherence to good software engineering practices to promote ease of use and maintainability.","Performance Considerations":"Performance considerations likely include optimizing context retrieval and AI response times. The server design may involve caching mechanisms to reduce latency and improve throughput. Scalability is a stated feature, suggesting the system is designed to handle increasing research demands. The use of asynchronous processing or multi-threading could improve responsiveness."}}', '2025-03-10 21:26:40+00', 'true', 'Servers', 'researcher-mcp-server'), ('927967893', 'mcp-swift-sdk', 'loopwork-ai/mcp-swift-sdk', 'https://github.com/loopwork-ai/mcp-swift-sdk', 'Swift SDK for Model Context Protocol servers and clients', '2025-02-05 20:50:16+00', '2025-03-11 10:49:54+00', 'Swift', '114', '# MCP Swift SDK

Swift implementation of the [Model Context Protocol][mcp] (MCP).

> [!TIP]
> To see it in action, check out the [iMCP app](https://iMCP.app/?ref=mcp-swift-sdk).

## Requirements

- Swift 6.0+ / Xcode 16+
- macOS 14.0+ (Sonoma)
- iOS 17.0+

## Installation

### Swift Package Manager

Add the following to your `Package.swift` file:

```swift
dependencies: [
    .package(url: "https://github.com/loopwork-ai/mcp-swift-sdk.git", from: "0.3.0")
]
```

## Usage

### Basic Client Setup

```swift
import MCP

// Initialize the client
let client = Client(name: "MyApp", version: "1.0.0")

// Create a transport and connect
let transport = StdioTransport()
try await client.connect(transport: transport)

// Initialize the connection
let result = try await client.initialize()
```

### Basic Server Setup

```swift
import MCP

// Initialize the server with capabilities
let server = Server(
    name: "MyServer", 
    version: "1.0.0",
    capabilities: .init(
        resources: .init(
            list: true,
            read: true,
            subscribe: true
        )
    )
)

// Create transport and start server
let transport = StdioTransport()
try await server.start(transport: transport)

// Register method handlers
server.withMethodHandler(ReadResource.self) { params in
    // Handle resource read request
    let uri = params.uri
    let content = [Resource.Content.text("Example content")]
    return .init(contents: content)
}

// Register notification handlers
server.onNotification(ResourceUpdatedNotification.self) { message in
    // Handle resource update notification
}

// Stop the server when done
await server.stop()
```

### Working with Tools

```swift
// List available tools
let tools = try await client.listTools()

// Call a tool
let (content, isError) = try await client.callTool(
    name: "example-tool", 
    arguments: ["key": "value"]
)

// Handle tool content
for item in content {
    switch item {
    case .text(let text):
        print(text)
    case .image(let data, let mimeType, let metadata):
        // Handle image data
    }
}
```

### Working with Resources

```swift
// List available resources
let (resources, nextCursor) = try await client.listResources()

// Read a resource
let contents = try await client.readResource(uri: "resource://example")

// Subscribe to resource updates
try await client.subscribeToResource(uri: "resource://example")

// Handle resource updates
await client.onNotification(ResourceUpdatedNotification.self) { message in
    let uri = message.params.uri
    let content = message.params.content
    // Handle the update
}
```

### Working with Prompts

```swift
// List available prompts
let (prompts, nextCursor) = try await client.listPrompts()

// Get a prompt with arguments
let (description, messages) = try await client.getPrompt(
    name: "example-prompt",
    arguments: ["key": "value"]
)
```

## License

This project is licensed under the Apache License, Version 2.0.

[mcp]: https://modelcontextprotocol.io
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements a Swift SDK for the Model Context Protocol (MCP). The documentation references MCP and provides examples of client and server setup using MCP concepts.","categories":["Protocol Implementations","API Frameworks","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The repository appears to implement a client-server architecture, with the SDK providing classes for both `Client` and `Server`. It utilizes asynchronous operations extensively, using `async` and `await` keywords for handling communication. The design includes abstractions for transports, allowing for different communication channels like `StdioTransport`.","Implementation Details and Key Features":"The SDK offers functionalities for listing, reading, and subscribing to resources and prompts. It provides features for working with tools, including listing and calling tools with arguments. The implementation includes specific methods for handling different content types, such as text and images, received from tools or resources.","Integration Approaches and Protocols":"The SDK integrates with other systems using the Model Context Protocol. It defines methods for connecting to MCP servers, initializing connections, and handling notifications. The use of transports allows the SDK to be integrated with various communication channels, making it versatile for different deployment scenarios.","Technical Stack and Dependencies":"The SDK is built using Swift 6.0+ and requires Xcode 16+ along with macOS 14.0+ or iOS 17.0+. It uses the Swift Package Manager for dependency management. The code provided in the documentation uses standard Swift concurrency features for asynchronous operations.","Best Practices and Coding Standards":"The provided examples follow Swift's asynchronous programming patterns, using `async` and `await` for managing concurrent operations. The code includes clear separation of concerns between client and server functionalities, promoting modularity and maintainability. The use of Swift Package Manager ensures dependency management.","Performance Considerations":"The asynchronous nature of the SDK operations allows for non-blocking communication, which can improve performance and responsiveness. The choice of transport can affect performance, and it might be necessary to select an efficient transport based on the deployment environment. Handling large data, such as images, requires careful consideration to avoid memory issues."}}', '2025-03-11 14:29:54+00', 'true', 'Clients', 'mcp-swift-sdk'), ('929158279', 'langchain-mcp-adapters', 'langchain-ai/langchain-mcp-adapters', 'https://github.com/langchain-ai/langchain-mcp-adapters', null, '2025-02-07 23:08:03+00', '2025-03-10 17:37:48+00', 'Python', '378', '# LangChain MCP Adapters

This library provides a lightweight wrapper that makes [Anthropic Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) tools compatible with [LangChain](https://github.com/langchain-ai/langchain) and [LangGraph](https://github.com/langchain-ai/langgraph).

![MCP](static/img/mcp.png)

## Features

- 🛠️ Convert MCP tools into [LangChain tools](https://python.langchain.com/docs/concepts/tools/) that can be used with [LangGraph](https://github.com/langchain-ai/langgraph) agents
- 📦 A client implementation that allows you to connect to multiple MCP servers and load tools from them

## Installation

```bash
pip install langchain-mcp-adapters
```

## Quickstart

Here is a simple example of using the MCP tools with a LangGraph agent.

```bash
pip install langchain-mcp-adapters langgraph langchain-openai

export OPENAI_API_KEY=<your_api_key>
```

### Server

First, let's create an MCP server that can add and multiply numbers.

```python
# math_server.py
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Math")

@mcp.tool()
def add(a: int, b: int) -> int:
    """Add two numbers"""
    return a + b

@mcp.tool()
def multiply(a: int, b: int) -> int:
    """Multiply two numbers"""
    return a * b

if __name__ == "__main__":
    mcp.run(transport="stdio")
```

### Client

```python
# Create server parameters for stdio connection
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

from langchain_mcp_adapters.tools import load_mcp_tools
from langgraph.prebuilt import create_react_agent

from langchain_openai import ChatOpenAI
model = ChatOpenAI(model="gpt-4o")

server_params = StdioServerParameters(
    command="python",
    # Make sure to update to the full absolute path to your math_server.py file
    args=["/path/to/math_server.py"],
)

async with stdio_client(server_params) as (read, write):
    async with ClientSession(read, write) as session:
        # Initialize the connection
        await session.initialize()

        # Get tools
        tools = await load_mcp_tools(session)

        # Create and run the agent
        agent = create_react_agent(model, tools)
        agent_response = await agent.ainvoke({"messages": "what's (3 + 5) x 12?"})
```

## Multiple MCP Servers

The library also allows you to connect to multiple MCP servers and load tools from them:

### Server

```python
# math_server.py
...

# weather_server.py
from typing import List
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Weather")

@mcp.tool()
async def get_weather(location: str) -> int:
    """Get weather for location."""
    return "It's always sunny in New York"

if __name__ == "__main__":
    mcp.run(transport="sse")
```

```bash
python weather_server.py
```

### Client

```python
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent

from langchain_openai import ChatOpenAI
model = ChatOpenAI(model="gpt-4o")

async with MultiServerMCPClient(
    {
        "math": {
            "command": "python",
            # Make sure to update to the full absolute path to your math_server.py file
            "args": ["/path/to/math_server.py"],
            "transport": "stdio",
        },
        "weather": {
            # make sure you start your weather server on port 8000
            "url": "http://localhost:8000/sse",
            "transport": "sse",
        }
    }
) as client:
    agent = create_react_agent(model, client.get_tools())
    math_response = await agent.ainvoke({"messages": "what's (3 + 5) x 12?"})
    weather_response = await agent.ainvoke({"messages": "what is the weather in nyc?"})
```
', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"Protocol Implementations\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it provides adapters for the Anthropic Model Context Protocol (MCP) to be compatible with LangChain and LangGraph, facilitating tool use by agents.","categories":["Agent Frameworks","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The library adopts an adapter pattern to bridge MCP tools and the LangChain/LangGraph ecosystem. It provides both client and server implementations. The client facilitates connections to MCP servers, while the server showcases how to define and expose tools. The library supports connections to multiple MCP servers.","Implementation Details and Key Features":"Key features include converting MCP tools into LangChain-compatible tools and a client implementation for connecting to multiple MCP servers. The client session manages communication with MCP servers. The `load_mcp_tools` function is used to retrieve tools from the MCP server. Both `stdio` and `sse` transports are supported.","Integration Approaches and Protocols":"The primary integration approach involves adapting MCP-defined tools for use within LangChain agents, particularly LangGraph agents. The library supports integration with different transport mechanisms such as `stdio` for local processes and `sse` for HTTP-based communication. It demonstrates how to initialize a connection and call tools.","Technical Stack and Dependencies":"The library depends on `langchain`, `langgraph`, `langchain-openai`, and `mcp` (likely a reference implementation of MCP). The examples use Python and asyncio for asynchronous operations. The `FastMCP` class is used on the server side for defining and running MCP tools.","Best Practices and Coding Standards":"The examples use type hints and docstrings for clarity. The code follows a modular structure, separating client and server logic. Asynchronous programming patterns are used for handling communication with MCP servers. The use of context managers (`async with`) ensures proper resource management.","Performance Considerations":"The choice of transport (`stdio` vs. `sse`) impacts performance. `stdio` might be suitable for local development, while `sse` provides a standardized approach for remote communication. Asynchronous operations ensure the client remains responsive while interacting with MCP servers."}}', '2025-03-10 21:26:30+00', 'true', 'Servers', 'langchain-mcp-adapters'), ('929858144', 'fleur', 'fleuristes/fleur', 'https://github.com/fleuristes/fleur', 'The easiest way to discover and install MCPs', '2025-02-09 15:15:56+00', '2025-03-10 19:12:41+00', 'TypeScript', '285', '<div align="center">
  <h1>Fleur</h1>
  <p>A desktop app marketplace for Claude Desktop</p>
</div>

<div align="center">

[![License](https://img.shields.io/github/license/fleuristes/fleur?style=flat)](https://github.com/fleuristes/fleur/blob/main/LICENSE)

</div>


[![Fleur Logo](public/hero-asset.png)](https://fleurmcp.com)

Fleur is a desktop application that serves as an app marketplace for MCPs. It allows you to discover, install, and manage apps that extend the functionality of Claude Desktop.

All without having to use a command line. Fleur is made for non-technical users in mind, but is open-source and extensible so developers can make it their own.

[📺 Demo video](https://x.com/0xferruccio/status/1898429209388675554)

## Getting Started

To install Fleur you can either download the [latest release](https://github.com/fleuristes/fleur/releases/latest/download/Fleur.dmg) or open the terminal and run the following command:

```bash
curl -fsSL fleur.sh | sh
```

## Features

- **App Discovery**: Browse and search for apps in the Fleur marketplace
- **Lightweight and fast**: Fleur is built with Rust and Bun, making it lightweight and fast (it's less than 20MB)
- **App Installation**: Easily install apps with a single click
- **App Management**: View installed apps and their status
- **Automatic Updates**: Stay up-to-date with the latest app versions


## Submitting your MCP as an App

To submit your app to the Fleur marketplace, follow these steps:

1. Clone the repository:
   ```bash
   git clone https://github.com/fleuristes/app-registry.git
   cd app-registry
   ```

2. Add your app to the `apps.json` file.

3. Submit a pull request to the [app-registry repository](https://github.com/fleuristes/app-registry).


## Development

### Prerequisites

- [Node.js](https://nodejs.org/) (v18 or higher)
- [Bun](https://bun.sh/) package manager
- [Rust](https://www.rust-lang.org/tools/install) (for Tauri)

### Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/fleuristes/fleur.git
   cd fleur
   ```

2. Install dependencies:
   ```bash
   bun install
   ```

3. Run the development version:
   ```bash
   bun tauri dev
   ```

### Building for Production

To build the application for production:

```bash
bun tauri build
```

## App Registry

Fleur uses a centralized app registry to manage available apps. The registry is maintained at [github.com/fleuristes/app-registry](https://github.com/fleuristes/app-registry).

### Adding a New App

To add a new app to the registry, submit a pull request to the [app-registry repository](https://github.com/fleuristes/app-registry) with your app's metadata.

## Development

### Project Structure

- `src/` - React/TypeScript frontend code
- `src-tauri/` - Rust backend code
- `public/` - Static assets

### Recommended IDE Setup

- [VS Code](https://code.visualstudio.com/) + [Tauri](https://marketplace.visualstudio.com/items?itemName=tauri-apps.tauri-vscode) + [rust-analyzer](https://marketplace.visualstudio.com/items?itemName=rust-lang.rust-analyzer)

### Running Tests

```bash
cd src-tauri
cargo test
```

## Contributing

All contributions are welcome!

- `fleur`: The main repository containing the Fleur app. It's built with Tauri, React and Typescript. In the `src-tauri` folder, you'll find the Rust code. And in the `src` folder, you'll find the React code for the UI.

- [`app-registry`](https://github.com/fleuristes/app-registry): is the repository that contains the list of all MCPs

## License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.
', 'true', '"{\"Plugin Ecosystems\",\"Developer Tooling\",\"API Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository describes Fleur as a desktop app marketplace for MCPs, enabling users to discover, install, and manage applications that extend the functionality of Claude Desktop. It also includes instructions for submitting apps to the registry.","categories":["Plugin Ecosystems","Developer Tooling","API Frameworks"],"analysis":{"Architecture and Design Patterns":"Fleur adopts a client-server architecture. The core application, built with Tauri, React, and TypeScript, serves as a desktop client. The app-registry repository acts as a centralized server, storing metadata for available MCP applications. This separation promotes modularity, allowing for independent updates to the app registry and the client application.","Implementation Details and Key Features":"The application is built using Rust (for Tauri), Bun, React, and TypeScript.  Key features include app discovery through a marketplace interface, one-click installation, and automatic updates.  The use of Tauri allows for cross-platform desktop application development from a web frontend. The app size is optimized for performance using Rust and Bun.","Integration Approaches and Protocols":"Fleur integrates with MCP applications through a centralized app registry.  Apps are added to the registry by submitting a pull request with the app's metadata. Fleur then fetches this metadata and uses it to display the available applications.  It doesn't outline specific MCP communication protocols beyond managing access to available tools.","Technical Stack and Dependencies":"The technical stack consists of Rust for the backend (Tauri), Bun as a package manager, and React and Typescript for the frontend.  Dependencies include Node.js, Rust's cargo package manager, and potentially other Tauri-specific libraries. VS Code is the recommended IDE, utilizing extensions for Tauri and Rust development.","Best Practices and Coding Standards":"The project encourages contributions and specifies a clear directory structure, separating frontend (src/), backend (src-tauri/), and static assets (public/).  It includes instructions for running tests using cargo test, promoting code quality. The project is licensed under Apache 2.0, promoting open-source development.","Performance Considerations":"The choice of Rust and Bun aims for performance, resulting in a lightweight application.  Efficient management of the app registry and update mechanisms are likely important. The demo video and user feedback will likely influence any future optimizations needed, alongside profiling tools."}}', '2025-03-10 22:15:26+00', 'true', 'Servers', 'fleur-mcp-installer'), ('930217913', 'browser-tools-mcp', 'AgentDeskAI/browser-tools-mcp', 'https://github.com/AgentDeskAI/browser-tools-mcp', 'Monitor browser logs directly from Cursor and other MCP compatible IDEs.', '2025-02-10 09:20:38+00', '2025-03-10 18:21:56+00', 'JavaScript', '1251', '# BrowserTools MCP

> Make your AI tools 10x more aware and capable of interacting with your browser

This application is a powerful browser monitoring and interaction tool that enables AI-powered applications via Anthropic's Model Context Protocol (MCP) to capture and analyze browser data through a Chrome extension.

Read our [docs](https://browsertools.agentdesk.ai/) for the full installation, quickstart and contribution guides.

## Updates

v1.1.0 is out! This includes several bug fixes for logging + screenshots.

Please make sure to update the version in your IDE / MCP client as so:
`npx @agentdeskai/browser-tools-mcp@1.1.0`

Also make sure to download the latest version of the chrome extension here:
[v1.1.0 BrowserToolsMCP Chrome Extension](https://github.com/AgentDeskAI/browser-tools-mcp/releases/download/v1.1.0/chrome-extension-v1-1-0.zip)

From there you can run the local node server as usual like so:
`npx @agentdeskai/browser-tools-server`

And once you've opened your chrome dev tools, logs should be getting sent to your server!

If you have any questions or issues, feel free to open an issue ticket! And if you have any ideas to make this better, feel free to reach out or open an issue ticket with an enhancement tag or reach out to me at [@tedx_ai on x](https://x.com/tedx_ai)

## Architecture

There are three core components all used to capture and analyze browser data:

1. **Chrome Extension**: A browser extension that captures screenshots, console logs, network activity and DOM elements.
2. **Node Server**: An intermediary server that facilitates communication between the Chrome extension and any instance of an MCP server.
3. **MCP Server**: A Model Context Protocol server that provides standardized tools for AI clients to interact with the browser.

```
┌─────────────┐     ┌──────────────┐     ┌───────────────┐     ┌─────────────┐
│  MCP Client │ ──► │  MCP Server  │ ──► │  Node Server  │ ──► │   Chrome    │
│  (e.g.      │ ◄── │  (Protocol   │ ◄── │ (Middleware)  │ ◄── │  Extension  │
│   Cursor)   │     │   Handler)   │     │               │     │             │
└─────────────┘     └──────────────┘     └───────────────┘     └─────────────┘
```

Model Context Protocol (MCP) is a capability supported by Anthropic AI models that
allow you to create custom tools for any compatible client. MCP clients like Claude
Desktop, Cursor, Cline or Zed can run an MCP server which "teaches" these clients
about a new tool that they can use.

These tools can call out to external APIs but in our case, **all logs are stored locally** on your machine and NEVER sent out to any third-party service or API. BrowserTools MCP runs a local instance of a NodeJS API server which communicates with the BrowserTools Chrome Extension.

All consumers of the BrowserTools MCP Server interface with the same NodeJS API and Chrome extension.

#### Chrome Extension

- Monitors XHR requests/responses and console logs
- Tracks selected DOM elements
- Sends all logs and current element to the BrowserTools Connector
- Connects to Websocket server to capture/send screenshots
- Allows user to configure token/truncation limits + screenshot folder path

#### Node Server

- Acts as middleware between the Chrome extension and MCP server
- Receives logs and currently selected element from Chrome extension
- Processes requests from MCP server to capture logs, screenshot or current element
- Sends Websocket command to the Chrome extension for capturing a screenshot
- Intelligently truncates strings and # of duplicate objects in logs to avoid token limits
- Removes cookies and sensitive headers to avoid sending to LLMs in MCP clients

#### MCP Server

- Implements the Model Context Protocol
- Provides standardized tools for AI clients
- Compatible with various MCP clients (Cursor, Cline, Zed, Claude Desktop, etc.)

## Installation

Installation steps can be found in our documentation:

- [BrowserTools MCP Docs](https://browsertools.agentdesk.ai/)

## Usage

Once installed and configured, the system allows any compatible MCP client to:

- Monitor browser console output
- Capture network traffic
- Take screenshots
- Analyze selected elements
- Wipe logs stored in our MCP server

## Compatibility

- Works with any MCP-compatible client
- Primarily designed for Cursor IDE integration
- Supports other AI editors and MCP clients
', 'true', '"{\"Protocol Implementations\",\"Agent Frameworks\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states that it implements Anthropic's Model Context Protocol (MCP) to enable AI-powered applications to interact with browser data. The architecture section also details the interaction with an MCP server.","categories":["Agent Frameworks","API Frameworks","Browser & Web Automation"],"analysis":{"Architecture and Design Patterns":"The architecture consists of three main components: a Chrome extension for data capture, a Node server for middleware, and an MCP server for standardized tool provision. The Chrome extension captures browser data, the Node server acts as an intermediary for communication and data processing, and the MCP server allows AI clients to interact with the browser. This design facilitates browser interaction for AI tools.","Implementation Details and Key Features":"The Chrome extension monitors browser activity (logs, network, DOM). The Node server receives and processes this data, truncating logs to manage token limits and removes sensitive information. The MCP server exposes tools for AI clients to interact with the browser through standardized protocols, enabling functionalities such as monitoring console output and capturing network traffic.","Integration Approaches and Protocols":"The repository utilizes the Model Context Protocol (MCP) for AI client integration. The Node server acts as middleware between the Chrome extension and the MCP server. The extension communicates with the Node server via websockets. The MCP server provides standardized tools for AI clients (e.g., Cursor IDE) to interact with the browser.","Technical Stack and Dependencies":"The technical stack includes a Chrome extension (likely using JavaScript), a Node.js server, and potentially uses websockets for communication between the extension and server. The MCP server dependency indicates integration with AI model infrastructure. The documentation suggests the use of npm packages for installation.","Best Practices and Coding Standards":"The documentation emphasizes modularity and separation of concerns through the three-component architecture. The code includes features for token management and sensitive data removal, indicating a focus on security and resource optimization. Updating dependencies using `npx` is mentioned, suggesting adherence to current package management practices.","Performance Considerations":"The Node server implements intelligent truncation of strings and duplicate objects in logs to mitigate token limits, addressing potential performance bottlenecks. The architecture diagram emphasizes local data storage to avoid third-party APIs, promoting privacy and potentially faster response times. Screenshot capture is also a performance consideration."}}', '2025-03-10 21:26:30+00', 'true', 'Servers', 'browser-tools-mcp'), ('931809567', 'mcpkit', 'jonmadison/mcpkit', 'https://github.com/jonmadison/mcpkit', 'quick set of scripts to get started using Anthropic Model Context Protocol (mcp)', '2025-02-12 22:21:39+00', '2025-03-10 18:33:23+00', 'JavaScript', '1', '# MCP Kit

A CLI tool for easily setting up Model Context Protocol (MCP) servers for Claude Desktop on OSX. This tool helps you configure and manage multiple MCP servers. I've included some of my favorite servers to start with.

Features

- 🚀 Easy setup of multiple MCP servers
- Includes the following servers:
  - 💾 Persistent memory across sessions
  - 📂 File system access
  - 🖥️ Terminal command execution (sandboxed)
  - 🌐 Web fetch capabilities
  - 📝 YouTube transcript extraction
- 🔗 Automatic symlink creation for Claude Desktop

## Prerequisites

- Node.js >=14.0.0
- npm
- git
- Claude Desktop installed

## Installation & Usage

You can run MCP Kit directly without installation using npx:

```bash
# Run directly from GitHub
npx github:jonmadison/mcpkit
```

Or install it globally:

```bash
# Install from npm
npm install -g mcp-kit

# Install from GitHub
npm install -g github:jonmadison/mcpkit

# Then run
mcp-kit
```

## Development

To run the tool in development mode or for testing:

```bash
# Clone the repository
git clone https://github.com/jonmadison/mcpkit.git
cd mcpkit

# Install dependencies
npm install

# Run locally
npm start

# Run with checks disabled (for testing)
npm start -- --skip-checks
```

## Available MCP Servers

1. **[Memory Server](https://github.com/modelcontextprotocol/servers)**

   - Adds persistent memory capability across sessions
   - Stores memory in a JSON file
2. **[Terminal Server](https://github.com/dillip285/mcp-terminal)**

   - Execute terminal commands in a sandboxed environment
   - Path-restricted for security
3. **[Filesystem Server](https://github.com/modelcontextprotocol/servers)**

   - Read and write files
   - Path-restricted access
4. **[Fetch Server](https://github.com/zcaceres/fetch-mcp.git)**

   - Make HTTP requests to external URLs
   - Requires local installation
5. **[YouTube Transcript Server](https://github.com/kimtaeyoon83/mcp-server-youtube-transcript)**

   - Extract transcripts from YouTube videos
   - Supports multiple languages

## Configuration

The tool will:

1. Create an MCP work directory in your preferred location
2. Install and configure selected MCP servers
3. Create a configuration file (claude_desktop_config.json)
4. Set up appropriate symlinks for Claude Desktop

## Contributing

Pull requests are welcome! For major changes, please open an issue first to discuss what you would like to change.

## License

MIT

## Authors

- Jon Madison
', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"CLI Tools\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly mentions 'Anthropic Model Context Protocol (mcp)' and its purpose is to help configure and manage multiple MCP servers for Claude Desktop, aligning directly with MCP implementation.","categories":["Protocol Implementations","Agent Frameworks","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The repository provides a CLI tool to facilitate the setup and management of MCP servers. It uses a modular approach by incorporating different server functionalities such as memory, terminal, filesystem, fetch, and YouTube transcript extraction. The design relies on npm packages and configurations to manage these servers.","Implementation Details and Key Features":"The tool automates the process of setting up MCP servers, including installing dependencies, configuring servers, and creating symlinks for Claude Desktop. Key features include easy setup of multiple MCP servers and the inclusion of servers with persistent memory, file system access, terminal command execution, web fetching, and YouTube transcript extraction.","Integration Approaches and Protocols":"The repository integrates with Claude Desktop using MCP. It establishes communication between Claude and various servers using HTTP requests. The CLI tool configures these servers and establishes necessary symlinks for Claude Desktop to interact with them correctly. The system employs distinct servers for distinct functionalities.","Technical Stack and Dependencies":"The technical stack includes Node.js (>=14.0.0) and npm. The tool relies on git for cloning and managing repositories. It leverages several npm packages for specific server implementations (e.g., memory server, terminal server, filesystem server, fetch server, YouTube transcript server). Configuration files are likely managed as JSON.","Best Practices and Coding Standards":"The repository encourages contributions through pull requests and suggests discussing major changes via issues. It is licensed under the MIT license, indicating an open-source approach. It utilizes npm for package management and provides clear installation and usage instructions. The terminal server is sandboxed for security reasons.","Performance Considerations":"Performance will depend on the individual MCP servers used. The choice of server influences latency. File system operations and web fetching can introduce I/O bottlenecks. Sandboxing in terminal server will constrain its throughput. The tool itself should have minimal overhead as it primarily automates setup."}}', '2025-03-10 21:34:28+00', 'true', 'Servers', 'mcp-quick-start-scripts'), ('931892749', 'Figma-Context-MCP', 'GLips/Figma-Context-MCP', 'https://github.com/GLips/Figma-Context-MCP', 'MCP server to provide Figma layout information to AI coding agents like Cursor', '2025-02-13 02:55:06+00', '2025-03-10 17:25:24+00', 'TypeScript', '1310', '# Figma MCP Server

Give [Cursor](https://cursor.sh/), [Windsurf](https://codeium.com/windsurf), [Cline](https://cline.bot/), and other AI-powered coding tools access to your Figma files with this [Model Context Protocol](https://modelcontextprotocol.io/introduction) server.

When Cursor has access to Figma design data, it's **way** better at one-shotting designs accurately than alternative approaches like pasting screenshots.

Get started quickly, see [Configuration](#configuration) for more details:

```bash
npx figma-developer-mcp --figma-api-key=<your-figma-api-key>
```

## Demo Video

[Watch a demo of building a UI in Cursor with Figma design data](https://youtu.be/6G9yb-LrEqg)
[![Watch the video](https://img.youtube.com/vi/6G9yb-LrEqg/maxresdefault.jpg)](https://youtu.be/6G9yb-LrEqg)

<a href="https://glama.ai/mcp/servers/kcftotr525"><img width="380" height="200" src="https://glama.ai/mcp/servers/kcftotr525/badge" alt="Figma Server MCP server" /></a>

## How it works

1. Open Cursor's composer in agent mode.
1. Paste a link to a Figma file, frame, or group.
1. Ask Cursor to do something with the Figma file—e.g. implement a design.
1. Cursor will fetch the relevant metadata from Figma and use it to write your code.

This MCP server is specifically designed for use with Cursor. Before responding with context from the [Figma API](https://www.figma.com/developers/api), it simplifies and translates the response so only the most relevant layout and styling information is provided to the model.

Reducing the amount of context provided to the model helps make the AI more accurate and the responses more relevant.

## Installation

### Running the server quickly with NPM

You can run the server quickly without installing or building the repo using NPM:

```bash
npx figma-developer-mcp --figma-api-key=<your-figma-api-key>

# or
pnpx figma-developer-mcp --figma-api-key=<your-figma-api-key>

# or
yarn dlx figma-developer-mcp --figma-api-key=<your-figma-api-key>

# or
bunx figma-developer-mcp --figma-api-key=<your-figma-api-key>
```

Instructions on how to create a Figma API access token can be found [here](https://help.figma.com/hc/en-us/articles/8085703771159-Manage-personal-access-tokens).

### JSON config for tools that use configuration files

Many tools like Windsurf, Cline, and [Claude Desktop](https://claude.ai/download) use a configuration file to start the server.

The `figma-developer-mcp` server can be configured by adding the following to your configuration file:

```json
{
  "mcpServers": {
    "figma-developer-mcp": {
      "command": "npx",
      "args": ["-y", "figma-developer-mcp", "--stdio"],
      "env": {
        "FIGMA_API_KEY": "<your-figma-api-key>"
      }
    }
  }
}
```

### Running the server from local source

1. Clone the [repository](https://github.com/GLips/Figma-Context-MCP)
2. Install dependencies with `pnpm install`
3. Copy `.env.example` to `.env` and fill in your [Figma API access token](https://help.figma.com/hc/en-us/articles/8085703771159-Manage-personal-access-tokens). Only read access is required.
4. Run the server with `pnpm run dev`, along with any of the flags from the [Command-line Arguments](#command-line-arguments) section.

## Configuration

The server can be configured using either environment variables (via `.env` file) or command-line arguments. Command-line arguments take precedence over environment variables.

### Environment Variables

- `FIGMA_API_KEY`: Your [Figma API access token](https://help.figma.com/hc/en-us/articles/8085703771159-Manage-personal-access-tokens) (required)
- `PORT`: The port to run the server on (default: 3333)

### Command-line Arguments

- `--version`: Show version number
- `--figma-api-key`: Your Figma API access token
- `--port`: The port to run the server on
- `--stdio`: Run the server in command mode, instead of default HTTP/SSE
- `--help`: Show help menu

## Connecting to Cursor

### Start the server

```bash
> npx figma-developer-mcp --figma-api-key=<your-figma-api-key>
# Initializing Figma MCP Server in HTTP mode on port 3333...
# HTTP server listening on port 3333
# SSE endpoint available at http://localhost:3333/sse
# Message endpoint available at http://localhost:3333/messages
```

### Connect Cursor to the MCP server

Once the server is running, [connect Cursor to the MCP server](https://docs.cursor.com/context/model-context-protocol) in Cursor's settings, under the features tab.

![Connecting to MCP server in Cursor](./docs/cursor-MCP-settings.png)

After the server has been connected, you can confirm Cursor's has a valid connection before getting started. If you get a green dot and the tools show up, you're good to go!

![Confirming connection in Cursor](./docs/verify-connection.png)

### Start using Composer with your Figma designs

Once the MCP server is connected, **you can start using the tools in Cursor's composer, as long as the composer is in agent mode.**

Dropping a link to a Figma file in the composer and asking Cursor to do something with it should automatically trigger the `get_file` tool.

Most Figma files end up being huge, so you'll probably want to link to a specific frame or group within the file. With a single element selected, you can hit `CMD + L` to copy the link to the element. You can also find it in the context menu:

![Copy link to Figma selection by right clicking](./docs/figma-copy-link.png)

Once you have a link to a specific element, you can drop it in the composer and ask Cursor to do something with it.

## Inspect Responses

To inspect responses from the MCP server more easily, you can run the `inspect` command, which launches the `@modelcontextprotocol/inspector` web UI for triggering tool calls and reviewing responses:

```bash
pnpm inspect
# > figma-mcp@0.1.7 inspect
# > pnpx @modelcontextprotocol/inspector
#
# Starting MCP inspector...
# Proxy server listening on port 3333
#
# 🔍 MCP Inspector is up and running at http://localhost:5173 🚀
```

## Available Tools

The server provides the following MCP tools:

### get_file

Fetches information about a Figma file.

Parameters:

- `fileKey` (string): The key of the Figma file to fetch
- `depth` (number, optional): How many levels deep to traverse the node tree

### get_node

Fetches information about a specific node within a Figma file.

Parameters:

- `fileKey` (string): The key of the Figma file containing the node
- `nodeId` (string): The ID of the node to fetch
', 'true', '"{\"AI Model Integration\",\"Context Management\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository implements a Model Context Protocol (MCP) server for Figma, facilitating AI coding tools like Cursor to access and utilize Figma design data, as explicitly stated in the documentation.","categories":["AI Model Integration","Function Calling","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The architecture revolves around creating an MCP server that acts as an intermediary between Figma and AI coding agents. It fetches Figma design data via the Figma API, simplifies the response, and serves it to the AI tool. This design streamlines data access and reduces context for more accurate AI responses. The server supports both HTTP/SSE and command-line modes.","Implementation Details and Key Features":"Key features include fetching Figma file and node information (`get_file`, `get_node` tools), simplifying Figma API responses for AI consumption, and providing configuration options via environment variables and command-line arguments. The server uses pnpm for package management and supports running directly via `npx` for quick setup. It also offers an inspector tool for testing.","Integration Approaches and Protocols":"The primary integration approach is through the Model Context Protocol, allowing AI coding tools to access structured Figma data. The server integrates with Figma via the Figma API and exposes functionality through MCP tools. It also supports integration with tools like Cursor, Windsurf and Cline through configuration files. Connection verification with AI tools is documented.","Technical Stack and Dependencies":"The technical stack includes Node.js, likely TypeScript (given the use of pnpm), and the Figma API. Dependencies include packages for handling HTTP requests, SSE (Server-Sent Events), and interacting with the Figma API. The `pnpm` package manager is used, suggesting a preference for performance and dependency management.","Best Practices and Coding Standards":"The repository uses `.env` files for managing API keys and other sensitive information, demonstrating good security practices. The documentation is clear and concise, providing instructions for installation, configuration, and usage. The inclusion of an inspector tool suggests a focus on testing and debugging. The project structure follows standard Node.js conventions.","Performance Considerations":"The server simplifies Figma API responses to reduce context, which improves the performance and accuracy of connected AI models. The use of `npx` allows for quick server deployment without installation overhead. The server supports HTTP/SSE for efficient data streaming. The architecture prioritizes providing relevant information to avoid overwhelming the AI with unnecessary context."}}', '2025-03-10 21:26:30+00', 'true', 'Servers', 'figma-mcp-server'), ('931926819', 'cursor-mcp-demo', 'KelvinQiu802/cursor-mcp-demo', 'https://github.com/KelvinQiu802/cursor-mcp-demo', 'Cursor MCP', '2025-02-13 04:40:20+00', '2025-03-10 05:23:45+00', 'Python', '33', '# Cursor + Model Context Protocol

## 阅读

https://www.anthropic.com/news/model-context-protocol

https://modelcontextprotocol.io/introduction

https://docs.cursor.com/context/model-context-protocol

https://github.com/modelcontextprotocol/typescript-sdk

## 思路

1. 学习LangGPT仓库，创建一个LangGPT助手
2. 让LangGPT助手生成一个MCP专家 （学习MCP的文档）
3. 让MCP专家帮我们实现一个简单的MCP应用

https://github.com/langgptai/LangGPT

## 案例1: 官方demo

https://github.com/modelcontextprotocol/quickstart-resources/tree/main/weather-server-typescript

## 案例2: HackerNews接口

https://news.ycombinator.com/

https://github.com/HackerNews/API

https://github.com/pskill9/hn-server

## 案例3: Dify应用

https://cloud.dify.ai/app/79ff8bc4-1cfe-4a8f-af67-b10adbc725f5/configuration

## 扩展

https://github.com/modelcontextprotocol/servers', 'true', '"{\"Context Management\",\"Function Calling\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository description and documentation explicitly mention Model Context Protocol (MCP) and provide links to MCP resources, indicating direct implementation or relation to MCP specifications.","categories":["Protocol Implementations","AI Model Integration","Function Calling"],"analysis":{"Architecture and Design Patterns":"Based on the documentation, the repository appears to follow a modular architecture, likely involving a core component implementing MCP and potentially integrating with external APIs. The suggested approach of creating an 'MCP expert' hints at an agent-based or rule-based design pattern. The design involves adapting existing tools to the MCP framework.","Implementation Details and Key Features":"The repository aims to implement a simple MCP application, potentially utilizing a LangGPT assistant to create an MCP expert. It includes example integrations such as a weather server, HackerNews API, and Dify application. The key feature is likely the ability to manage and utilize context information for AI models using MCP.","Integration Approaches and Protocols":"The repository focuses on integrating with various APIs and services, such as HackerNews. The primary integration approach is through the Model Context Protocol (MCP). The documentation mentions example servers, implying the use of HTTP or similar protocols for communication. It integrates existing services with MCP.","Technical Stack and Dependencies":"The repository description and linked resources indicate the use of TypeScript for implementation. It likely depends on libraries for handling HTTP requests, JSON parsing, and potentially libraries related to the LangGPT framework. The example code uses typescript-sdk so it is a primary part of the tech stack.","Best Practices and Coding Standards":"The documentation lacks explicit details on coding standards. However, given the focus on protocol implementation and API integration, adherence to standard coding practices for networking and data handling is likely. Following patterns from linked repositories might be suggested.","Performance Considerations":"The analysis did not reveal explicit performance concerns. As the repository involves API interactions and context management for AI models, performance considerations related to latency and data throughput would be important. Memory management of context data is also vital for maintaining efficient performance."}}', '2025-03-10 22:15:32+00', 'true', 'Servers', 'cursor-mcp-demo'), ('933063306', 'mcp-marketplace', 'cline/mcp-marketplace', 'https://github.com/cline/mcp-marketplace', 'This is the official repository for submitting MCP servers to be included in Cline's MCP Marketplace. If you’ve built an MCP server and want it to be discoverable and easily installable by millions of developers using Cline, submit your server here.', '2025-02-15 04:04:28+00', '2025-03-11 10:10:52+00', null, '275', '# MCP Marketplace

Welcome to Cline's **MCP Marketplace** repository! Submit your MCP servers for others to easily discover and one-click install with Cline.

## Table of Contents

1. [What is MCP?](#what-is-mcp)
2. [What is the MCP Marketplace?](#what-is-the-mcp-marketplace)
3. [How to Submit your MCP Server](#how-to-submit-your-mcp-server)
4. [Making Your MCP Server One-Click Installable](#making-your-mcp-server-one-click-installable)
5. [FAQ](#faq)
6. [Get Help](#get-help)

## What is MCP?

**[MCP](https://github.com/modelcontextprotocol) (Model Context Protocol)** is an open protocol introduced by Anthropic that standardizes how large language models communicate with external tools or resources. MCP servers allow AI agents like Cline to access additional capabilities e.g., web search, file operations, API integrations, database access, and more.

## What is the MCP Marketplace?

The MCP Marketplace is a curated collection of MCP servers that makes discovery and installation easy. With the marketplace, you can:

1.  Browse official and community-made MCP servers
2.  Search by name, category, tags, and other metadata
3.  Install MCP servers with one click, triggering Cline to autonomously handle cloning, setup, and configuration.

## How to Submit your MCP Server

1. Create a [new issue](https://github.com/cline/mcp-marketplace/issues/new?template=mcp-server-submission.yml) in this `mcp-marketplace` repository
2. **Include the Following Info** in the Issue:
   - **GitHub Repo URL:** A direct link to the MCP server’s repository (nested links are also okay like in this [example](https://github.com/modelcontextprotocol/servers/tree/main/src/github))
   - **Logo Image:** A 400×400 PNG that will serve as your server’s icon.
   - **Reason for Addition:** Briefly explain why your server is awesome and/or how it can benefit other Cline users.
3. Confirm that you have tested giving Cline just your `README.md` and/or the `llms-install.md` and watched him successfully setup the server. This will help prevent rejection in case we have trouble setting up your server using Cline.

That’s it! Our team will review your submission. Once approved, we’ll add your MCP server to the official Marketplace listings, and it will become discoverable by Cline users.

## Making Your MCP Server One-Click Installable

When Cline attempts to install your MCP server, he will try to use your `README.md` to guide him through the setup process. While most MCP servers include sufficient instructions in their README, you can include a `llms-install.md` file in your repository to provide additional guidance to agents like Cline. This can be particularly useful for complex setups requiring specific environment configurations or API keys.

## FAQ

1.  **Do I have to create a `llms-install.md` file?**
    No. A well-written README with clear installation instructions is usually sufficient. Cline is quite good at understanding standard README files. The `llms-install.md` file is optional to help with complex setups or when you want to provide additional guidance.
2.  **Can I submit new MCP servers without many stars?**
    Absolutely! However, we do have a vetting process to ensure the security and stability of the MCP ecosystem.
3.  **How long does the approval process take?**
    Our team aims to review submissions within a couple of days. If your submission is approved, it will be live in the marketplace shortly thereafter.

## Get Help

Join our [Discord](https://discord.gg/cline) and post in the `#mcp` channel if you have any questions or need help!

## License

This repository is distributed under the [MIT License](LICENSE). Please note that individual MCP server submissions may use different licenses, so review the original repo’s license for details.
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states that it is for submitting MCP servers to Cline's MCP Marketplace. It also links to the Model Context Protocol GitHub repository and describes what MCP is.","categories":["AI Model Integration","Plugin Ecosystems","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The repository itself functions as a registry or directory, not a complex application. The architecture relies on standard issue submission workflows in GitHub. It uses a defined structure for MCP server submissions, including a link to the server's repository, a logo, and a reason for addition. This centralizes discovery.","Implementation Details and Key Features":"The key feature is the MCP Marketplace, enabling easy discovery and one-click installation of MCP servers. Submissions are handled via GitHub issues using a predefined template (mcp-server-submission.yml). The repository provides guidelines on making MCP servers installable by AI agents like Cline.","Integration Approaches and Protocols":"This repository integrates with Cline, an AI agent, by providing a platform to discover and install MCP servers. The integration relies on the Model Context Protocol for communication between AI models and external tools. The process uses standard markdown files (README.md, llms-install.md) for setup guidance.","Technical Stack and Dependencies":"The repository primarily relies on GitHub's issue tracking system. It depends on the Model Context Protocol for the core functionality of the MCP servers. No specific programming languages or frameworks are mentioned in the repository's description or documentation.","Best Practices and Coding Standards":"The repository encourages clear installation instructions in README files and suggests using llms-install.md for complex setups. It emphasizes testing MCP servers with Cline to ensure successful installation. A vetting process exists to ensure security and stability of the MCP ecosystem.","Performance Considerations":"Performance considerations are primarily related to the MCP servers themselves, not the marketplace repository. The repository focuses on discoverability and ease of installation. The inclusion of a logo image optimizes the visual presentation of MCP servers within the marketplace to aid in user decisions."}}', '2025-03-11 14:29:52+00', 'true', 'Servers', 'mcp-marketplace-servers'), ('933144612', 'supabase-mcp-server', 'alexander-zuev/supabase-mcp-server', 'https://github.com/alexander-zuev/supabase-mcp-server', 'Supabase MCP Server enabling Cursor & Windsurf to use any method from Management API and query your database ', '2025-02-15 08:54:29+00', '2025-03-10 16:40:16+00', 'Python', '383', '# Supabase MCP Server

<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/4a363bcd-7c15-47fa-a72a-d159916517f7" />
    <source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/d255388e-cb1b-42ea-a7b2-0928f031e0df" />
    <img alt="Supabase" src="https://github.com/user-attachments/assets/d255388e-cb1b-42ea-a7b2-0928f031e0df" height="40" />
  </picture>
  &nbsp;&nbsp;
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/38db1bcd-50df-4a49-a106-1b5afd924cb2" />
    <source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/82603097-07c9-42bb-9cbc-fb8f03560926" />
    <img alt="MCP" src="https://github.com/user-attachments/assets/82603097-07c9-42bb-9cbc-fb8f03560926" height="40" />
  </picture>
</p>

<p align="center">
  <strong>Enable your favorite IDE to safely execute SQL queries, manage your database end-to-end, access Management API, and handle user authentication with built-in safety controls.</strong>
</p>

[![Star History Chart](https://api.star-history.com/svg?repos=alexander-zuev/supabase-mcp-server&type=Date)](https://star-history.com/#alexander-zuev/supabase-mcp-server&Date)

<p align="center">
  <a href="https://pypi.org/project/supabase-mcp-server/"><img src="https://img.shields.io/pypi/v/supabase-mcp-server.svg" alt="PyPI version" /></a>
  <a href="https://github.com/alexander-zuev/supabase-mcp-server/actions"><img src="https://github.com/alexander-zuev/supabase-mcp-server/workflows/CI/badge.svg" alt="CI Status" /></a>
  <a href="https://codecov.io/gh/alexander-zuev/supabase-mcp-server"><img src="https://codecov.io/gh/alexander-zuev/supabase-mcp-server/branch/main/graph/badge.svg" alt="Code Coverage" /></a>
  <a href="https://www.python.org/downloads/"><img src="https://img.shields.io/badge/python-3.12%2B-blue.svg" alt="Python 3.12+" /></a>
  <a href="https://github.com/astral-sh/uv"><img src="https://img.shields.io/badge/uv-package%20manager-blueviolet" alt="uv package manager" /></a>
  <a href="https://pepy.tech/project/supabase-mcp-server"><img src="https://static.pepy.tech/badge/supabase-mcp-server" alt="PyPI Downloads" /></a>
  <a href="https://smithery.ai/badge/@alexander-zuev/supabase-mcp-server"><img src="https://smithery.ai/badge/@alexander-zuev/supabase-mcp-server" alt="Smithery.ai Downloads" /></a>
  <a href="https://modelcontextprotocol.io/introduction"><img src="https://img.shields.io/badge/MCP-Server-orange" alt="MCP Server" /></a>
  <a href="LICENSE"><img src="https://img.shields.io/badge/license-Apache%202.0-blue.svg" alt="License" /></a>
</p>

A feature-rich MCP server that enables any MCP clients (Cursor, Windsurf, Claude Desktop, Cline and others) to safely interact with Supabase databases. It provides tools for database management, SQL query execution, and Supabase Management API access with built-in safety controls.

## Table of contents
<p align="center">
  <a href="#getting-started">Getting started</a> •
  <a href="#feature-overview">Feature overview</a> •
  <a href="#troubleshooting">Troubleshooting</a> •
  <a href="#changelog">Changelog</a>
</p>

## ✨ Key features
- 💻 Compatible with Cursor, Windsurf, Cline and other MCP clients supporting `stdio` protocol
- 🔐 Control read-only and read-write modes of SQL query execution
- 🔍 Runtime SQL query validation with risk level assessment
- 🛡️ Three-tier safety system for SQL operations: safe, write, and destructive
- 🔄 Robust transaction handling for both direct and pooled database connections
- 📝 Automatic versioning of database schema changes
- 💻 Manage your Supabase projects with Supabase Management API
- 🧑‍💻 Manage users with Supabase Auth Admin methods via Python SDK
- 🔨 Pre-built tools to help Cursor & Windsurf work with MCP more effectively
- 📦 Dead-simple install & setup via package manager (uv, pipx, etc.)


## Getting Started

### Prerequisites
Installing the server requires the following on your system:
- Python 3.12+

If you plan to install via `uv`, ensure it's [installed](https://docs.astral.sh/uv/getting-started/installation/#__tabbed_1_1).

### PostgreSQL Installation
PostgreSQL installation is no longer required for the MCP server itself, as it now uses asyncpg which doesn't depend on PostgreSQL development libraries.

However, you'll still need PostgreSQL if you're running a local Supabase instance:

**MacOS**
```bash
brew install postgresql@16
```

**Windows**
  - Download and install PostgreSQL 16+ from https://www.postgresql.org/download/windows/
  - Ensure "PostgreSQL Server" and "Command Line Tools" are selected during installation

### Step 1. Installation

Since v0.2.0 I introduced support for package installation. You can use your favorite Python package manager to install the server via:

```bash
# if pipx is installed (recommended)
pipx install supabase-mcp-server

# if uv is installed
uv pip install supabase-mcp-server
```

`pipx` is recommended because it creates isolated environments for each package.

You can also install the server manually by cloning the repository and running `pipx install -e .` from the root directory.

#### Installing from source
If you would like to install from source, for example for local development:
```bash
uv venv
# On Mac
source .venv/bin/activate
# On Windows
.venv\Scripts\activate
# Install package in editable mode
uv pip install -e .
```

#### Installing via Smithery.ai

You can find the full instructions on how to use Smithery.ai to connect to this MCP server [here](https://smithery.ai/server/@alexander-zuev/supabase-mcp-server).


### Step 2. Configuration

The Supabase MCP server requires configuration to connect to your Supabase database, access the Management API, and use the Auth Admin SDK. This section explains all available configuration options and how to set them up.

#### Environment Variables

The server uses the following environment variables:

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `SUPABASE_PROJECT_REF` | No | `127.0.0.1:54322` | Your Supabase project reference ID (or local host:port) |
| `SUPABASE_DB_PASSWORD` | No | `postgres` | Your database password |
| `SUPABASE_REGION` | No | `us-east-1` | AWS region where your Supabase project is hosted |
| `SUPABASE_ACCESS_TOKEN` | No | None | Personal access token for Supabase Management API |
| `SUPABASE_SERVICE_ROLE_KEY` | No | None | Service role key for Auth Admin SDK |

> **Note**: The default values are configured for local Supabase development. For remote Supabase projects, you must provide your own values for `SUPABASE_PROJECT_REF` and `SUPABASE_DB_PASSWORD`.

#### Connection Types

##### Database Connection
- The server connects to your Supabase PostgreSQL database using the transaction pooler endpoint
- Local development uses a direct connection to `127.0.0.1:54322`
- Remote projects use the format: `postgresql://postgres.[project_ref]:[password]@aws-0-[region].pooler.supabase.com:6543/postgres`

> ⚠️ **Important**: Session pooling connections are not supported. The server exclusively uses transaction pooling for better compatibility with the MCP server architecture.

##### Management API Connection
- Requires `SUPABASE_ACCESS_TOKEN` to be set
- Connects to the Supabase Management API at `https://api.supabase.com`
- Only works with remote Supabase projects (not local development)

##### Auth Admin SDK Connection
- Requires `SUPABASE_SERVICE_ROLE_KEY` to be set
- For local development, connects to `http://127.0.0.1:54321`
- For remote projects, connects to `https://[project_ref].supabase.co`

#### Configuration Methods

The server looks for configuration in this order (highest to lowest priority):

1. **Environment Variables**: Values set directly in your environment
2. **Local `.env` File**: A `.env` file in your current working directory (only works when running from source)
3. **Global Config File**:
   - Windows: `%APPDATA%\supabase-mcp\.env`
   - macOS/Linux: `~/.config/supabase-mcp/.env`
4. **Default Settings**: Local development defaults (if no other config is found)

> ⚠️ **Important**: When using the package installed via pipx or uv, local `.env` files in your project directory are **not** detected. You must use either environment variables or the global config file.

#### Setting Up Configuration

##### Option 1: Client-Specific Configuration (Recommended)

Set environment variables directly in your MCP client configuration (see client-specific setup instructions in Step 3). Most MCP clients support this approach, which keeps your configuration with your client settings.

##### Option 2: Global Configuration

Create a global `.env` configuration file that will be used for all MCP server instances:

```bash
# Create config directory
# On macOS/Linux
mkdir -p ~/.config/supabase-mcp
# On Windows (PowerShell)
mkdir -Force "$env:APPDATA\supabase-mcp"

# Create and edit .env file
# On macOS/Linux
nano ~/.config/supabase-mcp/.env
# On Windows (PowerShell)
notepad "$env:APPDATA\supabase-mcp\.env"
```

Add your configuration values to the file:

```
SUPABASE_PROJECT_REF=your-project-ref
SUPABASE_DB_PASSWORD=your-db-password
SUPABASE_REGION=us-east-1
SUPABASE_ACCESS_TOKEN=your-access-token
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key
```

##### Option 3: Project-Specific Configuration (Source Installation Only)

If you're running the server from source (not via package), you can create a `.env` file in your project directory with the same format as above.

#### Finding Your Supabase Project Information

- **Project Reference**: Found in your Supabase project URL: `https://supabase.com/dashboard/project/<project-ref>`
- **Database Password**: Set during project creation or found in Project Settings → Database
- **Access Token**: Generate at https://supabase.com/dashboard/account/tokens
- **Service Role Key**: Found in Project Settings → API → Project API keys

#### Supported Regions

The server supports all Supabase regions:

- `us-west-1` - West US (North California)
- `us-east-1` - East US (North Virginia) - default
- `us-east-2` - East US (Ohio)
- `ca-central-1` - Canada (Central)
- `eu-west-1` - West EU (Ireland)
- `eu-west-2` - West Europe (London)
- `eu-west-3` - West EU (Paris)
- `eu-central-1` - Central EU (Frankfurt)
- `eu-central-2` - Central Europe (Zurich)
- `eu-north-1` - North EU (Stockholm)
- `ap-south-1` - South Asia (Mumbai)
- `ap-southeast-1` - Southeast Asia (Singapore)
- `ap-northeast-1` - Northeast Asia (Tokyo)
- `ap-northeast-2` - Northeast Asia (Seoul)
- `ap-southeast-2` - Oceania (Sydney)
- `sa-east-1` - South America (São Paulo)

#### Limitations

- **No Self-Hosted Support**: The server only supports official Supabase.com hosted projects and local development
- **No Connection String Support**: Custom connection strings are not supported
- **No Session Pooling**: Only transaction pooling is supported for database connections
- **API and SDK Features**: Management API and Auth Admin SDK features only work with remote Supabase projects, not local development

### Step 3. Usage

In general, any MCP client that supports `stdio` protocol should work with this MCP server. This server was explicitly tested to work with:
- Cursor
- Windsurf
- Cline
- Claude Desktop

Additionally, you can also use smithery.ai to install this server a number of clients, including the ones above.

Follow the guides below to install this MCP server in your client.

#### Cursor
Go to Settings -> Features -> MCP Servers and add a new server with this configuration:
```bash
# can be set to any name
name: supabase
type: command
# if you installed with pipx
command: supabase-mcp-server
# if you installed with uv
command: uv run supabase-mcp-server
# if the above doesn't work, use the full path (recommended)
command: /full/path/to/supabase-mcp-server  # Find with 'which supabase-mcp-server' (macOS/Linux) or 'where supabase-mcp-server' (Windows)
```

If configuration is correct, you should see a green dot indicator and the number of tools exposed by the server.
![How successful Cursor config looks like](https://github.com/user-attachments/assets/45df080a-8199-4aca-b59c-a84dc7fe2c09)

#### Windsurf
Go to Cascade -> Click on the hammer icon -> Configure -> Fill in the configuration:
```json
{
    "mcpServers": {
      "supabase": {
        "command": "/Users/username/.local/bin/supabase-mcp-server",  // update path
        "env": {
          "SUPABASE_PROJECT_REF": "your-project-ref",
          "SUPABASE_DB_PASSWORD": "your-db-password",
          "SUPABASE_REGION": "us-east-1",  // optional, defaults to us-east-1
          "SUPABASE_ACCESS_TOKEN": "your-access-token",  // optional, for management API
          "SUPABASE_SERVICE_ROLE_KEY": "your-service-role-key"  // optional, for Auth Admin SDK
        }
      }
    }
}
```
If configuration is correct, you should see green dot indicator and clickable supabase server in the list of available servers.

![How successful Windsurf config looks like](https://github.com/user-attachments/assets/322b7423-8c71-410b-bcab-aff1b143faa4)

#### Claude Desktop
Claude Desktop also supports MCP servers through a JSON configuration. Follow these steps to set up the Supabase MCP server:

1. **Find the full path to the executable** (this step is critical):
   ```bash
   # On macOS/Linux
   which supabase-mcp-server

   # On Windows
   where supabase-mcp-server
   ```
   Copy the full path that is returned (e.g., `/Users/username/.local/bin/supabase-mcp-server`).

2. **Configure the MCP server** in Claude Desktop:
   - Open Claude Desktop
   - Go to Settings → Developer -> Edit Config MCP Servers
   - Add a new configuration with the following JSON:

   ```json
   {
     "mcpServers": {
       "supabase": {
         "command": "/full/path/to/supabase-mcp-server",  // Replace with the actual path from step 1
         "env": {
           "SUPABASE_PROJECT_REF": "your-project-ref",
           "SUPABASE_DB_PASSWORD": "your-db-password",
           "SUPABASE_REGION": "us-east-1",  // optional, defaults to us-east-1
           "SUPABASE_ACCESS_TOKEN": "your-access-token",  // optional, for management API
           "SUPABASE_SERVICE_ROLE_KEY": "your-service-role-key"  // optional, for Auth Admin SDK
         }
       }
     }
   }
   ```

> ⚠️ **Important**: Unlike Windsurf and Cursor, Claude Desktop requires the **full absolute path** to the executable. Using just the command name (`supabase-mcp-server`) will result in a "spawn ENOENT" error.

If configuration is correct, you should see the Supabase MCP server listed as available in Claude Desktop.

![How successful Windsurf config looks like](https://github.com/user-attachments/assets/500bcd40-6245-40a7-b23b-189827ed2923)

#### Cline
Cline also supports MCP servers through a similar JSON configuration. Follow these steps to set up the Supabase MCP server:

1. **Find the full path to the executable** (this step is critical):
   ```bash
   # On macOS/Linux
   which supabase-mcp-server

   # On Windows
   where supabase-mcp-server
   ```
   Copy the full path that is returned (e.g., `/Users/username/.local/bin/supabase-mcp-server`).

2. **Configure the MCP server** in Cline:
   - Open Cline in VS Code
   - Click on the "MCP Servers" tab in the Cline sidebar
   - Click "Configure MCP Servers"
   - This will open the `cline_mcp_settings.json` file
   - Add the following configuration:

   ```json
   {
     "mcpServers": {
       "supabase": {
         "command": "/full/path/to/supabase-mcp-server",  // Replace with the actual path from step 1
         "env": {
           "SUPABASE_PROJECT_REF": "your-project-ref",
           "SUPABASE_DB_PASSWORD": "your-db-password",
           "SUPABASE_REGION": "us-east-1",  // optional, defaults to us-east-1
           "SUPABASE_ACCESS_TOKEN": "your-access-token",  // optional, for management API
           "SUPABASE_SERVICE_ROLE_KEY": "your-service-role-key"  // optional, for Auth Admin SDK
         }
       }
     }
   }
   ```

If configuration is correct, you should see a green indicator next to the Supabase MCP server in the Cline MCP Servers list, and a message confirming "supabase MCP server connected" at the bottom of the panel.

![How successful configuration in Cline looks like](https://github.com/user-attachments/assets/6c4446ad-7a58-44c6-bf12-6c82222bbe59)

### Troubleshooting

Here are some tips & tricks that might help you:
- **Debug installation** - run `supabase-mcp-server` directly from the terminal to see if it works. If it doesn't, there might be an issue with the installation.
- **MCP Server configuration** - if the above step works, it means the server is installed and configured correctly. As long as you provided the right command, IDE should be able to connect. Make sure to provide the right path to the server executable.
- **"No tools found" error** - If you see "Client closed - no tools available" in Cursor despite the package being installed:
  - Find the full path to the executable by running `which supabase-mcp-server` (macOS/Linux) or `where supabase-mcp-server` (Windows)
  - Use the full path in your MCP server configuration instead of just `supabase-mcp-server`
  - For example: `/Users/username/.local/bin/supabase-mcp-server` or `C:\Users\username\.local\bin\supabase-mcp-server.exe`
- **Environment variables** - to connect to the right database, make sure you either set env variables in `mcp_config.json` or in `.env` file placed in a global config directory (`~/.config/supabase-mcp/.env` on macOS/Linux or `%APPDATA%\supabase-mcp\.env` on Windows).
- **Accessing logs** - The MCP server writes detailed logs to a file:
  - Log file location:
    - macOS/Linux: `~/.local/share/supabase-mcp/mcp_server.log`
    - Windows: `%USERPROFILE%\.local\share\supabase-mcp\mcp_server.log`
  - Logs include connection status, configuration details, and operation results
  - View logs using any text editor or terminal commands:
    ```bash
    # On macOS/Linux
    cat ~/.local/share/supabase-mcp/mcp_server.log

    # On Windows (PowerShell)
    Get-Content "$env:USERPROFILE\.local\share\supabase-mcp\mcp_server.log"
    ```

If you are stuck or any of the instructions above are incorrect, please raise an issue.

### MCP Inspector
A super useful tool to help debug MCP server issues is MCP Inspector. If you installed from source, you can run `supabase-mcp-inspector` from the project repo and it will run the inspector instance. Coupled with logs this will give you complete overview over what's happening in the server.
> 📝 Running `supabase-mcp-inspector`, if installed from package, doesn't work properly - I will validate and fix in the coming release.

## Feature Overview

### Database query tools

Since v0.3+ server provides comprehensive database management capabilities with built-in safety controls:

- **SQL Query Execution**: Execute PostgreSQL queries with risk assessment
  - **Three-tier safety system**:
    - `safe`: Read-only operations (SELECT) - always allowed
    - `write`: Data modifications (INSERT, UPDATE, DELETE) - require unsafe mode
    - `destructive`: Schema changes (DROP, CREATE) - require unsafe mode + confirmation

- **SQL Parsing and Validation**:
  - Uses PostgreSQL's parser (pglast) for accurate analysis and provides clear feedback on safety requirements

- **Automatic Migration Versioning**:
  - Database-altering operations operations are automatically versioned
  - Generates descriptive names based on operation type and target


- **Safety Controls**:
  - Default SAFE mode allows only read-only operations
  - All statements run in transaction mode via `asyncpg`
  - 2-step confirmation for high-risk operations

- **Available Tools**:
  - `get_schemas`: Lists schemas with sizes and table counts
  - `get_tables`: Lists tables, foreign tables, and views with metadata
  - `get_table_schema`: Gets detailed table structure (columns, keys, relationships)
  - `execute_postgresql`: Executes SQL statements against your database
  - `confirm_destructive_operation`: Executes high-risk operations after confirmation
  - `retrieve_migrations`: Gets migrations with filtering and pagination options
  - `live_dangerously`: Toggles between safe and unsafe modes

### Management API tools

Since v0.3.0 server provides secure access to the Supabase Management API with built-in safety controls:

- **Available Tools**:
  - `send_management_api_request`: Sends arbitrary requests to Supabase Management API with auto-injection of project ref
  - `get_management_api_spec`: Gets the enriched API specification with safety information
    - Supports multiple query modes: by domain, by specific path/method, or all paths
    - Includes risk assessment information for each endpoint
    - Provides detailed parameter requirements and response formats
    - Helps LLMs understand the full capabilities of the Supabase Management API
  - `get_management_api_safety_rules`: Gets all safety rules with human-readable explanations
  - `live_dangerously`: Toggles between safe and unsafe operation modes

- **Safety Controls**:
  - Uses the same safety manager as database operations for consistent risk management
  - Operations categorized by risk level:
    - `safe`: Read-only operations (GET) - always allowed
    - `unsafe`: State-changing operations (POST, PUT, PATCH, DELETE) - require unsafe mode
    - `blocked`: Destructive operations (delete project, etc.) - never allowed
  - Default safe mode prevents accidental state changes
  - Path-based pattern matching for precise safety rules

**Note**: Management API tools only work with remote Supabase instances and are not compatible with local Supabase development setups.

### Auth Admin tools

I was planning to add support for Python SDK methods to the MCP server. Upon consideration I decided to only add support for Auth admin methods as I often found myself manually creating test users which was prone to errors and time consuming. Now I can just ask Cursor to create a test user and it will be done seamlessly. Check out the full Auth Admin SDK method docs to know what it can do.

Since v0.3.6 server supports direct access to Supabase Auth Admin methods via Python SDK:
  - Includes the following tools:
    - `get_auth_admin_methods_spec` to retrieve documentation for all available Auth Admin methods
    - `call_auth_admin_method` to directly invoke Auth Admin methods with proper parameter handling
  - Supported methods:
    - `get_user_by_id`: Retrieve a user by their ID
    - `list_users`: List all users with pagination
    - `create_user`: Create a new user
    - `delete_user`: Delete a user by their ID
    - `invite_user_by_email`: Send an invite link to a user's email
    - `generate_link`: Generate an email link for various authentication purposes
    - `update_user_by_id`: Update user attributes by ID
    - `delete_factor`: Delete a factor on a user (currently not implemented in SDK)

#### Why use Auth Admin SDK instead of raw SQL queries?

The Auth Admin SDK provides several key advantages over direct SQL manipulation:
- **Functionality**: Enables operations not possible with SQL alone (invites, magic links, MFA)
- **Accuracy**: More reliable then creating and executing raw SQL queries on auth schemas
- **Simplicity**: Offers clear methods with proper validation and error handling

  - Response format:
    - All methods return structured Python objects instead of raw dictionaries
    - Object attributes can be accessed using dot notation (e.g., `user.id` instead of `user["id"]`)
  - Edge cases and limitations:
    - UUID validation: Many methods require valid UUID format for user IDs and will return specific validation errors
    - Email configuration: Methods like `invite_user_by_email` and `generate_link` require email sending to be configured in your Supabase project
    - Link types: When generating links, different link types have different requirements:
      - `signup` links don't require the user to exist
      - `magiclink` and `recovery` links require the user to already exist in the system
    - Error handling: The server provides detailed error messages from the Supabase API, which may differ from the dashboard interface
    - Method availability: Some methods like `delete_factor` are exposed in the API but not fully implemented in the SDK

### Automatic Versioning of Database Changes

"With great power comes great responsibility." While `execute_postgresql` tool coupled with aptly named `live_dangerously` tool provide a powerful and simple way to manage your Supabase database, it also means that dropping a table or modifying one is one chat message away. In order to reduce the risk of irreversible changes, since v0.3.8 the server supports:
- automatic creation of migration scripts for all write & destructive sql operations executed on the database
- improved safety mode of query execution, in which all queries are categorized in:
  - `safe` type: always allowed. Includes all read-only ops.
  - `write`type: requires `write` mode to be enabled by the user.
  - `destructive` type: requires `write` mode to be enabled by the user AND a 2-step confirmation of query execution for clients that do not execute tools automatically.

### Universal Safety Mode
Since v0.3.8 Safety Mode has been standardized across all services (database, API, SDK) using a universal safety manager. This provides consistent risk management and a unified interface for controlling safety settings across the entire MCP server.

All operations (SQL queries, API requests, SDK methods) are categorized into risk levels:
- `Low` risk: Read-only operations that don't modify data or structure (SELECT queries, GET API requests)
- `Medium` risk: Write operations that modify data but not structure (INSERT/UPDATE/DELETE, most POST/PUT API requests)
- `High` risk: Destructive operations that modify database structure or could cause data loss (DROP/TRUNCATE, DELETE API endpoints)
- `Extreme` risk: Operations with severe consequences that are blocked entirely (deleting projects)

Safety controls are applied based on risk level:
- Low risk operations are always allowed
- Medium risk operations require unsafe mode to be enabled
- High risk operations require unsafe mode AND explicit confirmation
- Extreme risk operations are never allowed

#### How confirmation flow works

Any high-risk operations (be it a postgresql or api request) will be blocked even in `unsafe` mode.
![Every high-risk operation is blocked](https://github.com/user-attachments/assets/c0df79c2-a879-4b1f-a39d-250f9965c36a)
You will have to confirm and approve every high-risk operation explicitly in order for it to be executed.
![Explicit approval is always required](https://github.com/user-attachments/assets/5cd7a308-ec2a-414e-abe2-ff2f3836dd8b)


## Changelog

- 📦 Simplified installation via package manager - ✅ (v0.2.0)
- 🌎 Support for different Supabase regions - ✅ (v0.2.2)
- 🎮 Programmatic access to Supabase management API with safety controls - ✅ (v0.3.0)
- 👷‍♂️ Read and read-write database SQL queries with safety controls - ✅ (v0.3.0)
- 🔄 Robust transaction handling for both direct and pooled connections - ✅ (v0.3.2)
- 🐍 Support methods and objects available in native Python SDK - ✅ (v0.3.6)
- 🔍 Stronger SQL query validation ✅ (v0.3.8)
- 📝 Automatic versioning of database changes ✅ (v0.3.8)
- 📖 Radically improved knowledge and tools of api spec ✅ (v0.3.8)
- ✍️ Improved consistency of migration-related tools for a more organized database vcs ✅ (v0.3.10)


For a more detailed roadmap, please see this [discussion](https://github.com/alexander-zuev/supabase-mcp-server/discussions/46) on GitHub.

### Connect to Supabase logs

I'm planning to research, if it's possible to connect to Supabase db logs which might be useful for debugging (if not already supported.)


---

Enjoy! ☺️
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it is a Supabase MCP Server designed to enable MCP clients like Cursor and Windsurf to interact with Supabase databases, aligning with the Model Context Protocol.","categories":["Protocol Implementations","API Frameworks","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The architecture involves a server acting as an intermediary between MCP clients and a Supabase database. It supports both direct and pooled database connections, emphasizing transaction pooling. The design incorporates a three-tier safety system for SQL operations, suggesting a layered approach to security and access control. Configuration is managed through environment variables, a local `.env` file, or a global config file.","Implementation Details and Key Features":"The server provides features like SQL query execution with runtime validation and risk assessment. Key implementations include transaction handling, automatic versioning of database schema changes, and integration with the Supabase Management API and Auth Admin SDK. The server supports a `stdio` protocol for client communication, making it compatible with various IDEs and tools. Package installation is supported via pipx and uv.","Integration Approaches and Protocols":"The project integrates with Supabase via the Management API and Auth Admin SDK. It primarily uses the `stdio` protocol to communicate with MCP clients. Integration methods involve configuring environment variables and setting up server paths in the respective client applications. The system uses transaction pooling for database connections and offers different safety tiers for SQL operations, showing careful integration with existing systems.","Technical Stack and Dependencies":"The primary language is Python, requiring version 3.12 or higher. Dependencies include asyncpg for asynchronous PostgreSQL interactions and likely other libraries for interacting with the Supabase API and handling environment configurations. The project also uses tools like uv and pipx for package management. The server leverages PostgreSQL but doesn't directly depend on PostgreSQL development libraries due to asyncpg.","Best Practices and Coding Standards":"The documentation highlights best practices like using pipx for isolated environments and configuring the server using environment variables. The project includes CI using GitHub Actions and code coverage reports, suggesting adherence to quality coding standards. The use of transaction pooling over session pooling demonstrates an understanding of database connection management best practices for this architecture.","Performance Considerations":"The use of asyncpg for database interactions suggests a focus on asynchronous operations for improved performance. Transaction pooling is preferred over session pooling which helps with resource management and performance. The three-tier safety system introduces some overhead, however, the documentation doesn't extensively detail any performance benchmarks or optimization strategies."}}', '2025-03-10 21:26:30+00', 'true', 'Servers', 'supabase-mcp-server'), ('933319562', 'deep-research-mcp', 'Ozamatash/deep-research-mcp', 'https://github.com/Ozamatash/deep-research-mcp', null, '2025-02-15 17:14:02+00', '2025-03-08 04:11:02+00', 'TypeScript', '40', '# Open Deep Research MCP Server

An AI-powered research assistant that performs deep, iterative research on any topic. It combines search engines, web scraping, and AI to explore topics in depth and generate comprehensive reports. Available as a Model Context Protocol (MCP) tool or standalone CLI. Look at exampleout.md to see what a report might look like.

## Quick Start

1. Clone and install:
```bash
git clone https://github.com/Ozamatash/deep-research
cd deep-research
npm install
```

2. Set up environment in `.env.local`:
```bash
# Copy the example environment file
cp .env.example .env.local
```

3. Build:
```bash
# Build the server
npm run build
```

4. Run the cli version:
```bash
npm run start "Your research query here"
```
5. Test MCP Server with Claude Desktop:  
Follow the guide thats at the bottom of server quickstart to add the server to Claude Desktop:  
https://modelcontextprotocol.io/quickstart/server

## Features

- Performs deep, iterative research by generating targeted search queries
- Controls research scope with depth (how deep) and breadth (how wide) parameters
- Evaluates source reliability with detailed scoring (0-1) and reasoning
- Prioritizes high-reliability sources (≥0.7) and verifies less reliable information
- Generates follow-up questions to better understand research needs
- Produces detailed markdown reports with findings, sources, and reliability assessments
- Available as a Model Context Protocol (MCP) tool for AI agents
- For now MCP version doesn't ask follow up questions

## How It Works

```mermaid
flowchart TB
    subgraph Input
        Q[User Query]
        B[Breadth Parameter]
        D[Depth Parameter]
        FQ[Feedback Questions]
    end

    subgraph Research[Deep Research]
        direction TB
        SQ[Generate SERP Queries]
        SR[Search]
        RE[Source Reliability Evaluation]
        PR[Process Results]
    end

    subgraph Results[Research Output]
        direction TB
        L((Learnings with
        Reliability Scores))
        SM((Source Metadata))
        ND((Next Directions:
        Prior Goals,
        New Questions))
    end

    %% Main Flow
    Q & FQ --> CQ[Combined Query]
    CQ & B & D --> SQ
    SQ --> SR
    SR --> RE
    RE --> PR

    %% Results Flow
    PR --> L
    PR --> SM
    PR --> ND

    %% Depth Decision and Recursion
    L & ND --> DP{depth > 0?}
    DP -->|Yes| SQ
    
    %% Final Output
    DP -->|No| MR[Markdown Report]

    %% Styling
    classDef input fill:#7bed9f,stroke:#2ed573,color:black
    classDef process fill:#70a1ff,stroke:#1e90ff,color:black
    classDef output fill:#ff4757,stroke:#ff6b81,color:black
    classDef results fill:#a8e6cf,stroke:#3b7a57,color:black,width:150px,height:150px

    class Q,B,D,FQ input
    class SQ,SR,RE,PR process
    class MR output
    class L,SM,ND results
```
## Advanced Setup

### Using Local Firecrawl (Free Option)

Instead of using the Firecrawl API, you can run a local instance. You can use the official repo or my fork which uses searXNG as the search backend to avoid using a searchapi key:

1. Set up local Firecrawl:
```bash
git clone https://github.com/Ozamatash/localfirecrawl
cd localfirecrawl
# Follow setup in localfirecrawl README
```

2. Update `.env.local`:
```bash
FIRECRAWL_BASE_URL="http://localhost:3002"
```

### Optional: Observability

Add observability to track research flows, queries, and results using Langfuse:

```bash
# Add to .env.local
LANGFUSE_PUBLIC_KEY="your_langfuse_public_key"
LANGFUSE_SECRET_KEY="your_langfuse_secret_key"
```

The app works normally without observability if no Langfuse keys are provided.

## License

MIT License
', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"RAG Systems\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it is available as a Model Context Protocol (MCP) tool. The documentation references the MCP quickstart guide and demonstrates usage within contexts like Claude Desktop.","categories":["AI Model Integration","Agent Frameworks","Function Calling"],"analysis":{"Architecture and Design Patterns":"The architecture utilizes a flow that begins with a user query, generates SERP queries, searches the web, evaluates source reliability, and processes results. It includes iterative loops controlled by depth parameters. The diagram depicts a modular design with distinct stages, allowing for focused development and easier integration with other systems.","Implementation Details and Key Features":"Key features include generating targeted search queries, evaluating source reliability, prioritizing high-reliability sources, and producing detailed markdown reports. The system uses breadth and depth parameters to control research scope. The MCP version currently lacks follow-up questions. The setup allows use of either a Firecrawl API or a local instance for web scraping.","Integration Approaches and Protocols":"The repository is designed for integration as a Model Context Protocol (MCP) tool. The quick start guide links to the official MCP documentation which details how to use it in a Claude Desktop environment. Langfuse integration is optional to add observability features. There is also a CLI version.","Technical Stack and Dependencies":"The repository uses Node.js. The specific dependencies are listed in package.json. It also uses Firecrawl (either a hosted API or a local instance). Observability is achieved through Langfuse. Mermaid diagrams are used to visualize the workflow. The research reports are markdown files.","Best Practices and Coding Standards":"The repository includes an example environment file (.env.example) to guide users in setting up their environment variables, which is a good practice for security and configuration management. The documentation provides clear instructions for installation, setup, and usage. The modular design with the flowchart helps promote maintainability.","Performance Considerations":"The system's performance is impacted by network latency during web searches and the efficiency of the source reliability evaluation process. Using a local Firecrawl instance instead of the API can improve latency. The iterative search loop based on depth can also impact processing time, so choosing appropriate depth values is important."}}', '2025-03-10 22:15:32+00', 'true', 'Servers', 'deep-research-mcp'), ('933405464', 'GooseTeam', 'cliffhall/GooseTeam', 'https://github.com/cliffhall/GooseTeam', 'Look, a flock of geese! An MCP server and protocol for Goose agent collaboration.', '2025-02-15 21:47:41+00', '2025-03-10 18:19:02+00', 'TypeScript', '24', '# GooseTeam
![GooseTeam Logo](images/GooseTeam.png)

## Enabling Goose Agents to Collaborate
### **Protocol:** 
A set of rules for agents to follow. 
  - **Natural Language**: Streamlined and human-readable set of instructions about how to behave and use the tools.
  - **Mermaid Markdown**: A precise flowchart for behavior, expressed in the most efficient text form.

### **Extension:** 
Goose extension ([MCP Server](https://github.com/modelcontextprotocol)) that provides tools for the agents to use for tasks and messaging.
  - **Agent Registration:** Facilitates agent registration with unique ID assignments and message sending.
  - **Message Management:** Stores messages from agents, making them available for retrieval.
  - **Task Management:** Project Coordinator role creates and assigns tasks and roles to other agents. 
  - **Agent Waiting:** Allows connected agents to wait for a specified period before taking another action. 
  - **Remote Server:** With an MCP Proxy, multiple agents can connect to the same MCP server, necessary for collaboration.

### Work in Progress
* Current focus is on [Goose CLI](https://block.github.io/goose/docs/getting-started/installation/) integration, so you must have it installed.


## NEW: Behavioral Control with Mermaid markdown!
* Agent protocol expressed as a [mermaid chart](https://mermaid.live/edit#pako:eNp1VG1v2jAQ_isnf-kX2hVKAkRTJwSbhjRa1FJVW0CVl5gQQWxmO3Rdyn_f2YkTyrp8iM5399w992IXJBIxIwFJJN2tYT5ecMDvXlOpQ_tfwvn5NdyxJFWaydAJQBPG9cef8sO1FpAwDZPxsgTXLgY4ZihlKWezUVjLFhblUmIImI0QVyKPnC14omajYaTTPSsmCvSaSQaUWzS1agR_OpTYxtdAX2_EK8zpht3ukbQRQOwNa4UQG-Ce8TiAswnQDLh4NtHRdLZ8N9p3pl7hmxA7x9SFtjSNIfzMTclbFOtyjN46jNYs2kyZUtg0FdoTSBa5BmaVpcr9xtviZ1JEeK4jVGdwwMCGOYeHXUw1g-wFNFWbSnkjULXOM8oh5UrLHMsSXL2FYMOVpjpXkK4gWlOesLiu4yR9NZopTqkYZjCpWtcMYlrNr2zblHJEzZGQCkvZsqtJu2L-S3GoVJrwEgS4bHbznPGOqZ3gsdFL9itnSqvlPzzMMnyle0uiMAJUy1fuL2oN-RLm_JoCHoXc3HKjC40IouSyfMffJBqqzRchrTuKprMrIZuBVHs32zKqcJvL2jKUrMtZ3fQma9Xvkch2W7wgxdyFiipN03nn05B3GstnSpG-SQMOauM8KBbUmqej2k4CmuoeaapD84MOKBZh61UTw07m6dla60KOGdhSDLo0Na06MRztzInFpnaXjrRIhg8GTWN8wQrjsCC4jBlbkADFmK1ovtULsuAHdKW5FvcvPCIBbhhrESnyZE2CFd0qPOX2IoxTii9hVmt3lP8QInMQPJKgIL9J0O56F12v3en6V_1u3-u3ey3yQgJ_cNG76vg9r-O3_atev3dokT82wOWF1-l2PG_g9QcD37v0-4e_wqjSEw).

* See if your favorite LLM can follow it using the [`agent:chart`](#agent---protocol-as-chart) script to launch your agent!

## Screenshots
### GooseTeam Protocol as a Chart
![goose-protocol-chart.png](images/goose-team-protocol-chart.png)

### GPT-4o Says OK to this as a control mechanism
![Goose PC Follows Mermaid](images/mermaid-ok-sez-gpt-4o.png)

### Goose Agents
![Goose Agents](images/goose-agents.png)

### Inspector - List Tools
![Inspector](images/inspector.png)

### Inspector - Add Message
![inspector-add-message.png](images/inspector-add-message.png)

### Inspector - List Tasks
![inspector-list-tasks.png](images/inspector-list-tasks.png)

## How to Run 
### Tell Goose about the extension
* Run `goose configure`
* Choose **[Add Extension]()**
* Choose **Remote Extension**
* Enter `goose-team` for name
* Enter http://localhost:3001/sse for SSE endpoint URI

### Run these npm scripts in order:
* First, clone this repo to your local machine and [install dependencies](#install-dependencies)
* `npm run build` - to build the server from source.
* `npm run mcp-proxy` to start an SSE proxy to talk to a single instance of the goose-team MCP server,
* `npm run inspector` to launch the MCP inspector in a browser 
  * Here you click **connect**, then `list_tools`, then `add_message`, then from `Human` send a message describing a task or project for the team to work on.
* `npm run agent` to launch an agent that will read the protocol and instructions, connect to the `goose-team` server, assign itself as project coordinator, then begin creating tasks.
  * In the inspector window, click `list_tasks` to see the tasks it is adding, 
  * When there are some tasks, you can `npm run agent` again and again to launch agents to be assigned tasks by the Project Coordinator.

## Troubleshooting
### HOW WELL GOOSE TEAM WORKS DEPENDS ON THE MODEL!!!
Follow these steps to test if you have a model that will work.
* `npm run build` - to build the server from source.
* `npm run mcp-proxy` to start an SSE proxy to talk to a single instance of the goose-team MCP server,
* `npm run agent:test` This launches an agent hitting the same MCP, but it has a simpler protocol: Say hello in a different language, wait 2 seconds and do it again.
* [Here is a video](https://youtu.be/we1ESnafqy4?si=mg2bxQ1k4SePf5yh) of a successful `agent:test` run.

Currently, the only models I can get to stay in the loop with are `google/gemini-2.0-flash-001` (via [openrouter.ai](https://openrouter.ai) to avoid rate limit shutdowns) and `openai/gpt-4o`. 
Others will fall out of the loop. It is important for the model to follow the protocol and stay in the loop, checking messages, sleeping, particularly for the Project Coordinator.

I suggest doing `build`,` mcp-proxy`, and `agent:test` and if the model you have configured does some work then stops with a "message loop ended" error, you have a model that just isn't very good at using tools. Try different models and see what works. If you get anything other than the above-mentioned models to actually stay on the job, then please let me know!



## MCP Tools

### Agents

- **`register_agent`**

  - Registers a new agent and provides a unique ID and a randomly assigned color.
  - **Inputs:** None
  - **Returns:** JSON agent with unique `id` and assigned `color`.

- **`list_agents`**

  - Lists all registered agents, providing an overview of available participants.
  - **Inputs:** None
  - **Returns:** JSON list of all agents connected to the server.

- **`agent_leave`**

  - Allows an agent to leave the team.
  - **Inputs:** None.
  - **Returns:** JSON of the agent that left.

- **`agent_wait`**

  - Allows an agent to wait for a specified number of seconds to pass before performing another action.
  - **Inputs:**
    - `seconds`: the number of seconds to wait.
  - **Returns:** Confirmation of time elapsed.

### Messages

- **`add_message`**

  - Allows an agent to send a new message, storing it within the server.
  - **Inputs:**
    - `senderId`: ID of the agent sending the message.
    - `content`: Content of the message.
  - **Returns:** Confirmation of message addition.

- **`recent_messages`**

  - Retrieves the most recent messages stored on the server.
  - **Inputs:** None
  - **Returns:** JSON array containing the three most recent messages.

- **`list_messages`**

  - Retrieves all messages stored on the server.
  - **Inputs:** None
  - **Returns:** JSON array containing all messages in the server.

- **`clear_messages`**

  - Clears all messages stored on the server.
  - **Inputs:** None
  - **Returns:** Confirmation of messages cleared.

### Tasks

- **`add_task`**

  - Add a task to the server.
  - **Inputs:**
    - `description`: Description of the task.
  - **Returns:** The newly added task, with it's assigned id.

- **`list_tasks`**

  - Lists all tasks in the server.
  - **Inputs:** None
  - **Returns:** JSON list of all tasks in the server.

- **`assign_task`**

  - Assign a task to an agent.
  - **Inputs:**
    - `taskId`: ID of the task.
    - `agentId`: ID of the agent to assign the task to.
  - **Returns:** Confirmation of message addition.

- **`complete_task`**

  - Complete a task.
  - **Inputs:**
    - `taskId`: ID of the task.
  - **Returns:** Confirmation of task completion.

## Developer Setup

### Install Dependencies

- `cd /path/to/GooseTeam/`
- `npm install`

### Build

- `npm run build`
- Builds the stdio-based MCP server runtime at `/dist/index.js`

### MCP Proxy

- `npm run mcp-proxy`
- Launches an SSE-based/MCP proxy on port `:3001` with endpoint `/sse`
- This has a single instance of the MCP server which multiple clients can connect to via SSE
- **MUST BE LAUNCHED BEFORE RUNNING INSPECTOR**

### Inspector

- `npm run inspector`
- Runs the [Model Context Protocol Inspector](https://modelcontextprotocol.io/docs/tools/inspector)
- The Inspector UI will be available at: http://localhost:5173
- In the Inspector UI:
  - Make sure `Transport Type` is set to `SSE`
  - Make sure `URL` is set to http://localhost:3001/sse
  - Click its **"Connect"** button to connect to the MCP Proxy
    - You should see Green light 🟢and **"Connected"** message.
  - Click its **List Tools** button

### Agent - Protocol as Text

- `npm run agent:text`
- Starts a new GooseTeam agent, with its waddling orders given in: `instructions/protocol-as-text.md`
  - This agent's protocol instructions are expressed in text markdown format, with headers, text, and bullets.
  - NOTE: It is a natural language prompt, but because it requires waiting and staying in a loop, some LLMs may balk
  - If the LLM won't stay in the loop, try the [Agent Wait Test](#agent-wait-test) with different LLMs to find one that is suitable.
- First agent will assume Project Coordinator Role
- **NOTE:** It's best to connect to the server with the Inspector BEFORE launching the first agent
  - Send a message from "Human" telling it what you'd like the team to accomplish

### Agent - Protocol as Chart

- `npm run agent:chart`
- Starts a new GooseTeam agent, with its waddling orders given in: `instructions/protocol-as-chart.md`
  - This agent's protocol instructions are expressed in mermaid markdown format, a compact way for expressing flowcharts.
  - NOTE: although this is an efficient way of expressing the protocol, some LLMs may not understand it
  - If the LLM doesn't understand mermaid markdown, it will say so and quit. 
- First agent will assume Project Coordinator Role
- **NOTE:** It's best to connect to the server with the Inspector BEFORE launching the first agent
  - Send a message from "Human" telling it what you'd like the team to accomplish

### Agent Wait Test

- `npm run agent:wait:test`
- Starts a new GooseTeam agent, with its waddling orders given in: `instructions/wait-loop-test.md`
- This will test the configured model's ability to stay in the loop, checking messages periodically.
- If it ends with an error saying "outgoing message queue empty" then it is not a good tool use model and therefore a poor candidate for use with GooseTeam.
- **NOTE:** Make sure to have the MCP Proxy running first.

### Format

- `npm run format`
- Runs `prettier` on the code, adjusting formatting

### Typecheck

- `npm run typecheck`
- Runs `tsc` with args to check and report type issues

### Lint

- `npm run lint`
- Runs `eslint` to non-destructively check for and report syntax problems

### LintFix

- `npm run lint:fix`
- Runs `eslint` to check for and fix syntax problems

### Test

- `npm run test`
- Run the unit tests


## Links

- **MCP Specification:** The complete Model Context Protocol specifications can be found [here](https://github.com/modelcontextprotocol/specification/tree/main).
- **Server Reference:** We follow a simple but modular approach demonstrated in the [GitHub MCP server](../mcp-servers/src/github).
- **Inspiration:** Iterating from Aaron Goldsmith's Gist [here](https://gist.github.com/AaronGoldsmith/114c439ae67e4f4c47cc33e829c82fac).
- Watch Aaron's "[Building a team of AI agents](https://www.youtube.com/watch?v=9HJy4uqMW74)" talk about his initial experiment.

', 'true', '"{\"Agent Frameworks\",\"Protocol Implementations\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it is an MCP server and protocol for agent collaboration, using tools for tasks and messaging, aligning with MCP's focus on model-tool interaction and context management.","categories":["Agent Frameworks","Protocol Implementations","Function Calling"],"analysis":{"Architecture and Design Patterns":"GooseTeam employs a server-client architecture where agents connect to an MCP server via an SSE proxy. The server manages agent registration, messages, and tasks. The design supports multiple agents collaborating by assigning roles and tasks. Mermaid markdown is used to define agent behavior, creating a structured protocol.","Implementation Details and Key Features":"Key features include agent registration, message management, and task management. Agents can register with unique IDs and colors. The system stores and retrieves messages. Tasks are created, assigned, and completed by agents. The 'agent_wait' tool lets agents pause, which is crucial for staying in the loop. The server exposes these features through MCP tools.","Integration Approaches and Protocols":"The system uses Server-Sent Events (SSE) via an MCP proxy for communication between agents and the server. Agents follow protocols defined in natural language or Mermaid markdown. The `goose configure` command allows users to add this as an extension to Goose CLI, integrating it into existing workflows.","Technical Stack and Dependencies":"The primary dependencies include Node.js and npm. The project utilizes SSE for real-time communication. It requires Goose CLI, an existing tool for interacting with LLMs. The development setup includes build, typecheck, lint, and test scripts. Mermaid markdown visualization helps in defining protocols.","Best Practices and Coding Standards":"The repository uses `prettier` for code formatting, `eslint` for linting, and `tsc` for type checking. The project includes a comprehensive set of npm scripts for building, testing, and formatting the code. The documentation provides setup instructions and troubleshooting tips. The project also offers unit tests.","Performance Considerations":"The documentation emphasizes the importance of choosing a model that reliably follows protocols and stays in the loop, critical for the Project Coordinator agent. The `agent:wait:test` script is included to assess models' ability to use tools effectively. The proxy enables multiple agents to connect to the same server, addressing scalability. "}}', '2025-03-10 21:26:40+00', 'true', 'Servers', 'gooseteam-mcp-server'), ('933680436', 'mcp-enhancement-servers', 'waldzellai/mcp-enhancement-servers', 'https://github.com/waldzellai/mcp-enhancement-servers', 'Waldzell AI's monorepo of MCP servers for model enhancement. Use in Claude Desktop, Cline, Roo Code, and more!', '2025-02-16 12:59:50+00', '2025-03-06 23:45:55+00', 'JavaScript', '44', '# @waldzellai/mcp-servers

[![smithery badge](https://smithery.ai/badge/@waldzellai/mcp-servers)](https://smithery.ai/server/@waldzellai/mcp-servers)

A collection of Model Context Protocol (MCP) servers providing various capabilities for AI assistants.

## Packages

### [@waldzellai/clear-thought](packages/server-clear-thought)
An MCP server providing advanced problem-solving capabilities through:
- Sequential thinking with dynamic thought evolution
- Mental models for structured problem decomposition [from a list provided by James Clear's website](https://jamesclear.com/mental-models)
- Systematic debugging approaches

### [@waldzellai/stochasticthinking](packages/server-stochasticthinking)
An MCP server extending sequential thinking with advanced stochastic algorithms for better decision-making:
- Markov Decision Processes (MDPs) for optimizing long-term decision sequences
- Monte Carlo Tree Search (MCTS) for exploring large decision spaces
- Multi-Armed Bandit Models for balancing exploration vs exploitation
- Bayesian Optimization for decisions under uncertainty
- Hidden Markov Models (HMMs) for inferring latent states

Helps AI assistants break out of local minima by considering multiple possible futures and strategically exploring alternative approaches.

## Development

This is a monorepo using npm workspaces. To get started:

```bash
# Install dependencies for all packages
npm install

# Build all packages
npm run build

# Clean all packages
npm run clean

# Test all packages
npm run test
```

## Package Management

Each package in the `packages/` directory is published independently to npm under the `@waldzellai` organization scope.

To create a new package:
1. Create a new directory under `packages/`
2. Initialize with required files (package.json, src/, etc.)
3. Add to workspaces in root package.json if needed

## License
MIT

## Understanding MCP Model Enhancement

### Less Technical Answer 

Here's a reframed explanation using USB/hardware analogies:

#### Model Enhancement as USB Add-Ons  
Think of the core AI model as a basic desktop computer. Model enhancement through MCP is like adding specialized USB devices to expand its capabilities. The Sequential Thinking server acts like a plug-in math coprocessor chip (like old 8087 FPU chips) that boosts the computer's number-crunching abilities.

**How USB-Style Enhancement Works:**

##### Basic Setup
- **Desktop (Base AI Model)**: Handles general tasks  
- **USB Port (MCP Interface)**: Standard connection point  
- **USB Stick (MCP Server)**: Contains special tools (like a "math helper" program)

##### Plug-and-Play Mechanics  
1. **Driver Installation (Server Registration)**  
   ```python
   # Simplified version of USB "driver setup"
   def install_mcp_server(usb_port, server_files):
       usb_port.register_tools(server_files['tools'])
       usb_port.load_drivers(server_files['drivers'])
   ```
   - Server provides "driver" APIs the desktop understands  
   - Tools get added to the system tray (available services)

2. **Tool Execution (Using the USB)**  
   - Desktop sends request like a keyboard input:  
   `Press F1 to use math helper`  
   - USB processes request using its dedicated hardware:  
   ```python
   def math_helper(input):
       # Dedicated circuit on USB processes this
       return calculation_results
   ```
   - Results return through USB cable (MCP protocol)

##### Real-World Workflow  
1. User asks AI to solve complex equation  
2. Desktop (base AI) checks its "USB ports":  
   - `if problem == "hard_math":`  
   - `    use USB_MATH_SERVER`  
3. USB math server returns:  
   - Step-by-step solution  
   - Confidence score (like error margins)  
   - Alternative approaches (different "calculation modes")

#### Why This Analogy Works  
- **Hot-swapping**: Change USB tools while system runs  
- **Specialization**: Different USBs for math/code/art  
- **Resource Limits**: Complex work offloaded to USB hardware  
- **Standard Interface**: All USBs use same port shape (MCP protocol)  

Just like you might use a USB security dongle for protected software, MCP lets AI models temporarily "borrow" specialized brains for tough problems, then return to normal operation.

### More Technical Answer

Model enhancement in the context of the Model Context Protocol (MCP) refers to improving AI capabilities through structured integration of external reasoning tools and data sources. The Sequential Thinking MCP Server demonstrates this by adding dynamic problem-solving layers to foundational models like Claude 3.5 Sonnet.

**Mechanics of Reasoning Component Delivery:**

#### Server-Side Implementation
MCP servers expose reasoning components through:
1. **Tool registration** - Servers define executable functions with input/output schemas:
```java
// Java server configuration example
syncServer.addTool(syncToolRegistration);
syncServer.addResource(syncResourceRegistration);
```
2. **Capability negotiation** - During initialization, servers advertise available components through protocol handshakes:
- Protocol version compatibility checks
- Resource availability declarations
- Supported operation listings

3. **Request handling** - Servers process JSON-RPC messages containing:
- Component identifiers
- Parameter payloads
- Execution context metadata

#### Client-Side Interaction
MCP clients discover and utilize reasoning components through:
1. **Component discovery** via `list_tools` requests:
```python
# Python client example
response = await self.session.list_tools()
tools = response.tools
```
2. **Dynamic invocation** using standardized message formats:
- Request messages specify target component and parameters
- Notifications stream intermediate results
- Errors propagate with structured codes

3. **Context maintenance** through session persistence:
- Conversation history tracking
- Resource handle caching
- Partial result aggregation

#### Protocol Execution Flow
The component delivery process follows strict sequencing:
1. **Connection establishment**
   - TCP/HTTP handshake
   - Capability exchange (server ↔ client)
   - Security context negotiation

2. **Component resolution**
   - Client selects appropriate tool from server registry
   - Parameter validation against schema
   - Resource binding (e.g., database connections)

3. **Execution lifecycle**
   - Request: Client → Server (JSON-RPC)
   - Processing: Server → Tool runtime
   - Response: Server → Client (structured JSON)

Modern implementations like Rhino's Grasshopper integration demonstrate real-world mechanics:
```python
# Rhino MCP server command processing
Rhino.RhinoApp.InvokeOnUiThread(lambda: process_command(cmd))
response = get_response() # Capture Grasshopper outputs
writer.WriteLine(response) # Return structured results
```

This architecture enables dynamic enhancement of AI capabilities through:
- **Pluggable reasoning modules** (add/remove without system restart)
- **Cross-platform interoperability** (Python ↔ Java ↔ C# components)
- **Progressive disclosure** of complex functionality
- **Versioned capabilities** for backward compatibility

The protocol's transport-agnostic design ensures consistent component delivery across:
- Local stdio processes
- HTTP/SSE cloud endpoints
- Custom binary protocols
- Hybrid edge computing setups
', 'true', '"{\"Protocol Implementations\",\"Agent Frameworks\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository description and documentation explicitly mention and implement Model Context Protocol (MCP) servers for AI model enhancement, using packages like clear-thought and stochasticthinking.","categories":["Agent Frameworks","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The repository utilizes a monorepo structure with npm workspaces, facilitating modular development and independent package publishing. The design emphasizes pluggable reasoning modules. Server-side components expose tools through registration and capability negotiation. Client-side interaction involves component discovery and dynamic invocation using standardized message formats, promoting interoperability.","Implementation Details and Key Features":"The repository includes MCP servers providing advanced problem-solving capabilities like sequential thinking and stochastic algorithms. Key features include mental models for structured problem decomposition (clear-thought) and decision-making algorithms like Markov Decision Processes (stochasticthinking). The system supports tool registration, capability negotiation, and request handling via JSON-RPC.","Integration Approaches and Protocols":"The repository implements Model Context Protocol (MCP) for integrating external reasoning tools. It supports integration through standardized message formats and protocol handshakes. MCP allows for dynamic enhancement of AI capabilities with pluggable modules. The protocol is transport-agnostic, enabling consistent component delivery across different transport layers.","Technical Stack and Dependencies":"The technical stack involves npm workspaces for monorepo management. The packages are published independently to npm under the @waldzellai organization scope. It uses Javascript/Typescript (implied from npm and build scripts) for server implementations, and likely uses a JSON-RPC framework for message handling. The documentation includes examples of using Python for client interaction.","Best Practices and Coding Standards":"The repository uses a monorepo structure to organize and manage code. It is designed with modularity in mind, enabling independent publishing of packages. The code follows standard npm practices for package management and build processes. The structure supports progressive disclosure of functionality and versioned capabilities for backward compatibility.","Performance Considerations":"The design facilitates offloading complex work to specialized servers, optimizing resource utilization. The use of efficient algorithms such as Markov Decision Processes and Monte Carlo Tree Search can improve the decision-making capabilities of AI assistants. Protocol efficiency is achieved through standardized message formats and streamlined request handling."}}', '2025-03-10 22:15:30+00', 'true', 'Servers', 'mcp-enhancement-servers'), ('934139122', 'copilot-mcp', 'VikashLoomba/copilot-mcp', 'https://github.com/VikashLoomba/copilot-mcp', 'A powerful VSCode extension that enables MCP server usage in Copilot, giving it access to MCP tools, resources, and more.', '2025-02-17 10:40:33+00', '2025-03-10 17:59:34+00', 'TypeScript', '86', '<div align="center">
<h1>Copilot MCP Client for VSCode</h1> 
</div>
<div align="center">

[![](https://dcbadge.limes.pink/api/server/https://discord.gg/nm28ej34PN)](https://discord.gg/nm28ej34PN)

</div>
<div style="display: flex; justify-content: center; gap: 20px; margin: 20px 0;">
  <img width="1200" alt="image" src="https://github.com/user-attachments/assets/d92d1ff2-998d-43ad-a386-071af84dbab6" />
</div>
<div align="center">

![Version](https://img.shields.io/badge/version-0.0.22-blue.svg?cacheSeconds=2592000)
[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)
[![VSCode Extension](https://img.shields.io/badge/VSCode-Extension-blue.svg?logo=visual-studio-code)](https://code.visualstudio.com/api/references/extension-guidelines)
[![MCP Client](https://img.shields.io/badge/MCP-Client-green.svg)](https://modelcontextprotocol.io/clients)



</div>

> A powerful VSCode extension that acts as a Model Context Protocol (MCP) client, enabling seamless integration between MCP tool servers and GitHub Copilot Chat. Join the growing ecosystem of interoperable AI applications with flexible integration options.

## ✨ Features

- 🔧 **MCP Server Management**: Connect and manage multiple MCP servers through an intuitive UI
- 🚀 **Copilot Integration**: Expose MCP tools directly to GitHub Copilot Chat participants
- 🎯 **Tool Discovery**: Automatically discover and surface available tools from connected MCP servers
- ⚡ **Server Health Monitoring**: Real-time monitoring of MCP server status and connections
- 🔄 **Automatic Connection Management**: Seamless handling of MCP server connections and reconnections
- 🛠️ **Tool Invocation Support**: Full support for MCP tool invocation through Copilot Chat
- 🔄 **Multiple Server Types**: Support for both process-based and SSE (Server-Sent Events) servers
- 🛡️ **Configuration Migration**: Automatic migration of server configurations to the latest format
- 🧩 **Server Configuration Command**: Manual migration command for updating older configurations

## 🎯 MCP Feature Support

| Feature | Support |
|---------|----------|
| Tools | ✅ Full support |
| Resources | ✅* Text resource support |
| Prompts | ✅* Full support (coming soon) |
| Sampling | ⏳ Planned |
| Roots | ⏳ Planned |

## 📦 Installation

1. Install the [extension](https://marketplace.visualstudio.com/items?itemName=AutomataLabs.copilot-mcp) from the VSCode Marketplace
2. Configure your MCP servers through the extension settings
3. Start using GitHub Copilot Chat with your MCP tools!

## 🛠️ Configuration

You can configure your MCP servers in the UI or in VSCode settings.

In the UI, look for the "MCP Servers" button in the activity bar.

To configure your MCP servers in VSCode settings:

```json
{
  "mcpManager.servers": [
    {
      "id": "process-server",
      "name": "Process MCP Server",
      "type": "process",
      "command": "start-server-command",
      "enabled": true
    },
    {
      "id": "sse-server",
      "name": "SSE MCP Server",
      "type": "sse",
      "url": "https://your-sse-server.com/events",
      "authToken": "your-optional-auth-token",
      "enabled": true
    }
  ]
}
```

### Server Configuration Properties

| Property | Type | Description |
|----------|------|-------------|
| `id` | string | Unique identifier for the server |
| `name` | string | Display name for the server |
| `type` | string | Server type: "process" or "sse" |
| `command` | string | Command to start the server (for process servers) |
| `url` | string | URL for SSE connection (for SSE servers) |
| `authToken` | string | Authentication token (optional, for SSE servers) |
| `enabled` | boolean | Whether the server is enabled |
| `env` | object | Environment variables for process servers (key-value pairs) |

## 🚀 Usage

1. Open the MCP Servers view from the VSCode activity bar
2. Add and configure your MCP servers
3. Enable/disable servers as needed
4. Use GitHub Copilot Chat with your connected MCP tools using the `@mcp` participant
5. View server status and tool availability in real-time
6. If upgrading from an older version, you can use the command "MCP: Migrate Server Configurations to Latest Format" to update your server configurations

## 🔗 Requirements

- VSCode 
- GitHub Copilot Chat extension
- Compatible MCP servers (see [Example Servers](https://modelcontextprotocol.io/servers))

## 🌟 Benefits

- Enable Copilot to use custom context and tools through MCP
- Join the growing ecosystem of interoperable AI applications
- Support local-first AI workflows
- Flexible integration options for your development workflow

## 👥 Contributing

Contributions, issues and feature requests are welcome!
Feel free to check the [issues page](https://github.com/VikashLoomba/copilot-mcp/issues).

## ✍️ Author

**Vikash Loomba**

* Website: https://automatalabs.io
* Github: [@vikashloomba](https://github.com/vikashloomba)

## 📝 License

Copyright © 2024 [Vikash Loomba](https://automatalabs.io).

This project is licensed under the [GNU General Public License v3.0](LICENSE).

---

_Part of the [MCP Client Ecosystem](https://modelcontextprotocol.io/clients) - Enabling interoperable AI tools for developers_ ⭐️
', 'true', '"{\"AI Model Integration\",\"Developer Tooling\",\"Plugin Ecosystems\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it's a Model Context Protocol (MCP) client for VSCode, facilitating communication between Copilot and MCP servers for tool usage. It supports tool discovery, invocation and server management.","categories":["Protocol Implementations","Developer Tooling","Plugin Ecosystems"],"analysis":{"Architecture and Design Patterns":"The extension acts as a bridge between VSCode/Copilot and MCP servers, enabling Copilot to leverage external tools and resources. It uses a client-server architecture, where the VSCode extension is the client and external servers are the backends providing the tools. The architecture supports both process-based and SSE servers.","Implementation Details and Key Features":"Key features include MCP server management with a UI, automatic tool discovery, server health monitoring, and support for invoking MCP tools within Copilot Chat. Configuration is managed via VSCode settings, and the extension handles connection management and reconnections to the MCP servers. Automatic configuration migration is also provided.","Integration Approaches and Protocols":"The extension integrates with GitHub Copilot Chat using the `@mcp` participant tag. It connects to MCP servers using either process execution or Server-Sent Events (SSE). It implements the MCP specification to allow Copilot to access tools and resources provided by the connected servers.","Technical Stack and Dependencies":"The extension is built for VSCode and depends on the GitHub Copilot Chat extension. It interacts with MCP servers, which can be implemented in various languages, using process execution or SSE. The extension likely uses TypeScript for development, given it's a VSCode extension.","Best Practices and Coding Standards":"The documentation suggests adherence to VSCode extension development guidelines. The project uses semantic versioning and includes a clear license (GPL v3). It provides a contributing guide and maintains an issues page, encouraging community involvement. Configuration options are well-documented using tables.","Performance Considerations":"The extension includes features like server health monitoring and automatic reconnection, suggesting an awareness of potential performance issues. The support for both process-based and SSE servers allows users to choose the best option based on their performance requirements. No specific details on performance tuning were provided."}}', '2025-03-10 22:15:28+00', 'true', 'Servers', 'copilot-mcp-extension'), ('934358660', 'MCPSharp', 'afrise/MCPSharp', 'https://github.com/afrise/MCPSharp', 'MCPSharp is a .NET library that helps you build Model Context Protocol (MCP) servers and clients - the standardized API protocol used by AI assistants and models.', '2025-02-17 17:39:43+00', '2025-03-11 06:10:12+00', 'C#', '126', '[![Build](https://github.com/afrise/MCPSharp/actions/workflows/build.yml/badge.svg)](https://github.com/afrise/MCPSharp/actions/workflows/build.yml)
[![NuGet](https://img.shields.io/nuget/v/MCPSharp)](https://www.nuget.org/packages/MCPSharp)
[![NuGet Downloads](https://img.shields.io/nuget/dt/MCPSharp)](https://www.nuget.org/packages/MCPSharp)

# MCPSharp

MCPSharp is a .NET library that helps you build Model Context Protocol (MCP) servers and clients - the standardized API protocol used by AI assistants and models. With MCPSharp, you can:

- Create MCP-compliant tools and functions that AI models can discover and use
- Connect directly to existing MCP servers from C# code with an easy to use client
- Expose your .NET methods as MCP endpoints with simple attributes
- Handle MCP protocol details and JSON-RPC communication seamlessly

## 🚀 What's New in MCPSharp

- **Microsoft.Extensions.AI Integration**: MCPSharp now integrates with Microsoft.Extensions.AI, allowing tools to be exposed as AIFunctions
- **Semantic Kernel Support**: Add tools using Semantic Kernel's KernelFunctionAttribute
- **Dynamic Tool Registration**: Register tools on-the-fly with custom implementation logic
- **Tool Change Notifications**: Server now notifies clients when tools are added, updated, or removed
- **Complex Object Parameter Support**: Better handling of complex objects in tool parameters
- **Better Error Handling**: Improved error handling with detailed stack traces

## When to Use MCPSharp

Use MCPSharp when you want to:
- Create tools that AI assistants like Anthropic's Claude Desktop can use
- Build MCP-compliant APIs without dealing with the protocol details
- Expose existing .NET code as MCP endpoints
- Add AI capabilities to your applications through standardized interfaces
- Integrate with Microsoft.Extensions.AI and/or Semantic Kernel without locking into a single vendor

## Features

- Easy-to-use attribute-based API (`[McpTool]`, `[McpResource]`)
- Built-in JSON-RPC support with automatic request/response handling
- Automatic parameter validation and type conversion
- Rich documentation support through XML comments
- Near zero configuration required for basic usage

## Prerequisites

- Any version of .NET that supports [standard 2.0](https://learn.microsoft.com/en-us/dotnet/standard/net-standard?tabs=net-standard-2-0#tabpanel_1_net-standard-2-0)

## Installation

```bash
dotnet add package MCPSharp
```

## Quick Start

### 1. Define a Tool

Create a class and mark your method(s) with the `[McpTool]` attribute:

```csharp
using MCPSharp;

public class Calculator
{
    [McpTool("add", "Adds two numbers")]  // Note: [McpFunction] is deprecated, use [McpTool] instead
    public static int Add([McpParameter(true)] int a, [McpParameter(true)] int b)
    {
        return a + b;
    }
}
```

### 2. Start the Server

```csharp
await MCPServer.StartAsync("CalculatorServer", "1.0.0");
```

The StartAsync() method will automatically find any methods in the base assembly that are marked with the McpTool attribute. In order to add any methods that are in a referenced library, you can manually register them by calling `MCPServer.Register<T>();` with `T` being the class containing the desired methods. If your methods are marked with Semantic Kernel attributes, this will work as well. If the client supports list changed notifications, it will be notified when additional tools are registered.

## Advanced Usage

### Dynamic Tool Registration

Register tools dynamically with custom implementation:

```csharp
MCPServer.AddToolHandler(new Tool() 
{
    Name = "dynamicTool",
    Description = "A dynamic tool",
    InputSchema = new InputSchema {
        Type = "object",
        Required = ["input"],
        Properties = new Dictionary<string, ParameterSchema>{
            {"input", new ParameterSchema{Type="string", Description="Input value"}}
        }
    }
}, (string input) => { return $"You provided: {input}"; });
```

### Resources API

Define and expose resources:

```csharp
[McpResource("settings", "resource://settings", "application/json", "Application settings")]
public string Settings { get; set; } = "{ \"theme\": \"dark\" }";

// Or as a method
[McpResource("user", "resource://{name}", "text/plain", "User information")]
public string GetUser(string name) => $"Information about {name}";
```

### Use with Microsoft.Extensions.AI

```csharp
// Client-side integration
MCPClient client = new("AIClient", "1.0", "path/to/mcp/server");
IList<AIFunction> functions = await client.GetFunctionsAsync();
```
This list can be plugged into the [ChatOptions.Tools](https://learn.microsoft.com/en-us/dotnet/api/microsoft.extensions.ai.chatoptions?view=net-9.0-pp) property for an [IChatClient](https://learn.microsoft.com/en-us/dotnet/api/microsoft.extensions.ai.ichatclient?view=net-9.0-pp), Allowing MCP servers to be used seamlessly with Any IChatClient Implementation.


### Semantic Kernel Integration

```csharp
using Microsoft.SemanticKernel;

public class MySkillClass
{
    [KernelFunction("MyFunction")]
    [Description("Description of my function")]
    public string MyFunction(string input) => $"Processed: {input}";
}

// Register with MCPServer
MCPServer.Register<MySkillClass>();
```
Currently, This is the only way to make a Semantic kernel method registerable with the MCP server. If you have a use case that is not covered here, please reach out!


## API Reference

### Attributes

- `[McpTool]` - Marks a class or method as an MCP tool
    -  Optional parameters:
        - `Name` - The tool name (default: class/method name)
        - `Description` - Description of the tool

- `[McpParameter]` - Provides metadata for function parameters
    - Optional parameters:
        - `Description` - Parameter description
        - `Required` - Whether the parameter is required (default: false)

- `[McpResource]` - Marks a property or method as an MCP resource
    - Parameters:
        - `Name` - Resource name
        - `Uri` - Resource URI (can include templates)
        - `MimeType` - MIME type of the resource
        - `Description` - Resource description

### Server Methods

- `MCPServer.StartAsync(string serverName, string version)` - Starts the MCP server
- `MCPServer.Register<T>()` - Registers a class containing tools or resources
- `MCPServer.AddToolHandler(Tool tool, Delegate func)` - Registers a dynamic tool

### Client Methods

- `new MCPClient(string name, string version, string server, string args = null, IDictionary<string, string> env = null)` - Create a client instance
- `client.GetToolsAsync()` - Get available tools
- `client.CallToolAsync(string name, Dictionary<string, object> parameters)` - Call a tool
- `client.GetResourcesAsync()` - Get available resources
- `client.GetFunctionsAsync()` - Get tools as AIFunctions

## XML Documentation Support

MCPSharp automatically extracts documentation from XML comments:

```csharp
/// <summary>
/// Provides mathematical operations
/// </summary>
public class Calculator
{
    /// <summary>
    /// Adds two numbers together
    /// </summary>
    /// <param name="a">The first number to add</param>
    /// <param name="b">The second number to add</param>
    /// <returns>The sum of the two numbers</returns>
    [McpTool]
    public static int Add(
        [McpParameter(true)] int a,
        [McpParameter(true)] int b)
    {
        return a + b;
    }
}
```

Enable XML documentation in your project file:

```xml
<PropertyGroup>
    <GenerateDocumentationFile>true</GenerateDocumentationFile>
    <NoWarn>$(NoWarn);1591</NoWarn>
</PropertyGroup>
```

This allows you to be able to quickly change the names and descriptions of your MCP tools without having to recompile.  For example, if you find the model is having trouble understanding how to use it correctly.

## Migration Notes

- `[McpFunction]` is deprecated and replaced with `[McpTool]` for better alignment with MCP standards
- Use `MCPServer.Register<T>()` instead of `MCPServer.RegisterTool<T>()` for consistency (old method still works but is deprecated)

## Contributing

We welcome contributions! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License.
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements Model Context Protocol (MCP) for .NET. It provides libraries to build MCP servers and clients, enabling AI model integration and tool/function usage via the protocol.","categories":["Protocol Implementations","AI Model Integration","Function Calling"],"analysis":{"Architecture and Design Patterns":"The library follows an attribute-based approach for defining MCP tools and resources. It leverages JSON-RPC for communication and provides both server and client components. The server uses reflection to discover MCP tools, and dynamic tool registration is supported. Resources are exposed via properties or methods. The library offers an easy-to-use API.","Implementation Details and Key Features":"Key features include attribute-driven tool definition (`[McpTool]`, `[McpResource]`), automatic JSON-RPC handling, parameter validation, and XML documentation support. Dynamic tool registration allows for custom tool implementation logic. Integration with Microsoft.Extensions.AI and Semantic Kernel enables leveraging existing AI frameworks. Tool change notifications enhance client-server synchronization.","Integration Approaches and Protocols":"MCPSharp integrates with .NET applications via NuGet package installation. It supports integration with Microsoft.Extensions.AI using the ChatOptions.Tools property for IChatClient. Semantic Kernel integration is achieved by registering classes containing KernelFunction attributes. The protocol used is Model Context Protocol, with JSON-RPC for communication.","Technical Stack and Dependencies":"The library is built using .NET Standard 2.0, making it compatible with a wide range of .NET versions. Key dependencies include libraries for JSON-RPC handling and potentially Microsoft.Extensions.AI and Semantic Kernel if those features are used. The code uses attributes and reflection extensively.","Best Practices and Coding Standards":"The library uses XML documentation for API documentation. Attributes are used to define tools and resources, promoting declarative programming. Error handling is improved with detailed stack traces. Examples are given to demonstrate the usage of the library. The library encourages modular design with its attribute-based and dynamic tool registrations.","Performance Considerations":"The library uses reflection, which may impact performance, especially during initial server startup when discovering tools. JSON-RPC communication can introduce overhead. Caching tool definitions and optimizing JSON serialization can improve performance. Dynamic tool registration adds flexibility but could introduce performance concerns if not managed carefully."}}', '2025-03-11 14:29:54+00', 'true', 'Clients', 'mcpsharp-net-library'), ('934728386', 'Software-planning-mcp', 'NightTrek/Software-planning-mcp', 'https://github.com/NightTrek/Software-planning-mcp', 'An experiment in software planning using MCP', '2025-02-18 10:05:15+00', '2025-03-10 16:35:11+00', 'JavaScript', '59', '# Software Planning Tool 🚀

A Model Context Protocol (MCP) server designed to facilitate software development planning through an interactive, structured approach. This tool helps break down complex software projects into manageable tasks, track implementation progress, and maintain detailed development plans.

## Features ✨

- **Interactive Planning Sessions**: Start and manage development planning sessions
- **Todo Management**: Create, update, and track development tasks
- **Complexity Scoring**: Assign complexity scores to tasks for better estimation
- **Code Examples**: Include relevant code snippets in task descriptions
- **Implementation Plans**: Save and manage detailed implementation plans

## Installation 🛠️

1. Clone the repository
2. Install dependencies:
```bash
pnpm install
```
3. Build the project:
```bash
pnpm run build
```
4. Add to your MCP settings configuration (typically located at `~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`):
```json
{
  "mcpServers": {
    "software-planning-tool": {
      "command": "node",
      "args": [
        "/path/to/software-planning-tool/build/index.js"
      ],
      "disabled": false,
      "autoApprove": []
    }
  }
}
```

## Available Tools 🔧

### start_planning
Start a new planning session with a specific goal.
```typescript
{
  goal: string  // The software development goal to plan
}
```

### add_todo
Add a new todo item to the current plan.
```typescript
{
  title: string,         // Title of the todo item
  description: string,   // Detailed description
  complexity: number,    // Complexity score (0-10)
  codeExample?: string  // Optional code example
}
```

### get_todos
Retrieve all todos in the current plan.
```typescript
// No parameters required
```

### update_todo_status
Update the completion status of a todo item.
```typescript
{
  todoId: string,     // ID of the todo item
  isComplete: boolean // New completion status
}
```

### save_plan
Save the current implementation plan.
```typescript
{
  plan: string  // The implementation plan text
}
```

### remove_todo
Remove a todo item from the current plan.
```typescript
{
  todoId: string  // ID of the todo item to remove
}
```

## Example Usage 📝

Here's a complete example of using the software planning tool:

1. Start a planning session:
```typescript
await client.callTool("software-planning-tool", "start_planning", {
  goal: "Create a React-based dashboard application"
});
```

2. Add a todo item:
```typescript
const todo = await client.callTool("software-planning-tool", "add_todo", {
  title: "Set up project structure",
  description: "Initialize React project with necessary dependencies",
  complexity: 3,
  codeExample: `
npx create-react-app dashboard
cd dashboard
npm install @material-ui/core @material-ui/icons
  `
});
```

3. Update todo status:
```typescript
await client.callTool("software-planning-tool", "update_todo_status", {
  todoId: todo.id,
  isComplete: true
});
```

4. Save the implementation plan:
```typescript
await client.callTool("software-planning-tool", "save_plan", {
  plan: `
# Dashboard Implementation Plan

## Phase 1: Setup (Complexity: 3)
- Initialize React project
- Install dependencies
- Set up routing

## Phase 2: Core Features (Complexity: 5)
- Implement authentication
- Create dashboard layout
- Add data visualization components
  `
});
```

## Development 🔨

### Project Structure
```
software-planning-tool/
  ├── src/
  │   ├── index.ts        # Main server implementation
  │   ├── prompts.ts      # Planning prompts and templates
  │   ├── storage.ts      # Data persistence
  │   └── types.ts        # TypeScript type definitions
  ├── build/              # Compiled JavaScript
  ├── package.json
  └── tsconfig.json
```

### Building
```bash
pnpm run build
```

### Testing
Test all features using the MCP inspector:
```bash
pnpm run inspector
```

## License 📄

MIT

---

Made with ❤️ using the Model Context Protocol
', 'true', '"{\"Agent Frameworks\",\"Developer Tooling\",\"Workflow Orchestration\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it's an MCP server and provides configuration instructions for integrating with an MCP environment. It offers tools for software planning accessible via MCP calls.","categories":["Agent Frameworks","Function Calling","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The architecture centers around a server providing software planning tools. The server exposes functions such as `start_planning`, `add_todo`, and `save_plan`, likely using a request-response pattern over MCP. The `src` directory suggests modular components for main logic, prompts, storage, and type definitions.","Implementation Details and Key Features":"The tool offers interactive planning sessions, todo management with complexity scoring, and saving implementation plans. It's implemented in TypeScript and uses `pnpm` for package management. The core logic is in `index.ts`, with data persistence handled by `storage.ts` and planning logic in `prompts.ts`.","Integration Approaches and Protocols":"The project integrates via the Model Context Protocol. The documentation gives an example of configuring the tool within an MCP settings file and calling its functions using a `client.callTool` method. This suggests tight integration with an MCP-compatible agent or environment.","Technical Stack and Dependencies":"The primary technical stack includes Node.js for the server runtime and TypeScript for development. The dependencies aren't fully listed, but the presence of `pnpm` implies usage of packages for server setup, MCP communication, data storage and potentially UI components for the inspector.","Best Practices and Coding Standards":"The project uses TypeScript, which suggests a focus on type safety and code maintainability. The presence of a `tsconfig.json` file indicates attention to compiler options and project configuration. The directory structure promotes separation of concerns.","Performance Considerations":"Performance considerations aren't explicitly addressed but are implied. The tool likely prioritizes latency in responding to MCP requests. The complexity scoring feature may influence how tasks are scheduled or prioritized within a larger planning framework."}}', '2025-03-10 22:15:30+00', 'true', 'Servers', 'sw-plan-mcp-experiment'), ('934753959', 'rust-sdk', 'modelcontextprotocol/rust-sdk', 'https://github.com/modelcontextprotocol/rust-sdk', 'The official Rust SDK for the Model Context Protocol', '2025-02-18 10:55:26+00', '2025-03-10 15:55:37+00', 'Rust', '120', '# rust-sdk
The official Rust SDK for the Model Context Protocol
', 'true', '"{\"Protocol Implementations\",\"AI Model Integration\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states that it is the official Rust SDK for the Model Context Protocol. This directly aligns with the MCP definition.","categories":["Protocol Implementations","AI Model Integration","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The rust-sdk likely employs modular design principles to enable flexible integration with various AI models and contexts. It is designed to provide a robust and efficient interface for managing interactions based on the Model Context Protocol. Expect to see patterns facilitating data serialization/deserialization and asynchronous communication.","Implementation Details and Key Features":"The SDK offers core functionalities for implementing the MCP in Rust. Key features likely include context management, function calling, and tool use integration. It likely provides abstractions for handling communication, managing the context window, and serializing data appropriately for interacting with AI models.","Integration Approaches and Protocols":"The rust-sdk facilitates integration with AI models using the Model Context Protocol. It handles the underlying communication protocol, enabling developers to focus on the application logic. It likely supports various transport mechanisms, such as HTTP or gRPC, for interacting with AI models remotely.","Technical Stack and Dependencies":"The primary technical stack is Rust, leveraging its safety and performance features. Dependencies may include libraries for networking (e.g., `tokio`, `hyper`), data serialization (e.g., `serde`), and potentially cryptographic libraries for secure communication. The build system will be Cargo.","Best Practices and Coding Standards":"The SDK adheres to Rust's best practices and coding standards, emphasizing safety, performance, and readability. Expect consistent code formatting (using `rustfmt`), thorough documentation (`rustdoc`), and comprehensive unit tests to ensure reliability and maintainability. Error handling is a priority, likely utilizing Rust's `Result` type.","Performance Considerations":"As a Rust SDK, performance is a significant consideration. The implementation minimizes overhead through efficient memory management and optimized algorithms. Asynchronous programming is likely employed to handle concurrent operations and prevent blocking. Profiling tools will be used to identify and address potential performance bottlenecks."}}', '2025-03-10 21:28:55+00', 'true', 'Clients', 'mcp-rust-sdk'), ('935262136', 'mcphub.nvim', 'ravitemer/mcphub.nvim', 'https://github.com/ravitemer/mcphub.nvim', 'A powerful Neovim plugin for managing MCP (Model Context Protocol) servers', '2025-02-19 06:55:16+00', '2025-03-10 15:16:10+00', 'Lua', '106', '# MCPHub.nvim

[![Neovim](https://img.shields.io/badge/NeoVim-%2357A143.svg?&style=flat-square&logo=neovim&logoColor=white)](https://neovim.io)
[![Lua](https://img.shields.io/badge/Lua-2C2D72?style=flat-square&logo=lua&logoColor=white)](https://www.lua.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](./CONTRIBUTING.md)

A powerful Neovim plugin that integrates MCP (Model Context Protocol) servers into your workflow. Configure and manage MCP servers through a centralized config file while providing an intuitive UI for testing tools and resources. Perfect for LLM integration, offering both programmatic API access and interactive testing capabilities through the `:MCPHub` command.

<div align="center">
<p>
<h3>MCP Hub Interface</h3>
<video controls muted src="https://github.com/user-attachments/assets/9e574d2d-358e-4a3e-ae19-d9e85c5dd2f0"></video>
</p>
</div>

<div align="center">
<p>
<h3>Using <a href="https://github.com/olimorris/codecompanion.nvim">Codecompanion Chat</a> plugin</h3>
<video controls muted src="https://github.com/user-attachments/assets/cefce4bb-d07f-4423-8873-cf7d56656cd3"></video>
</p>
</div>

## ✨ Features

- Simple single-command interface (`:MCPHub`)
- Integrated Hub view for managing servers and tools
  - Dynamically enable/disable servers and tools to optimize token usage
  - Start/stop servers with persistent state
  - Enable/disable specific tools per server
  - State persists across restarts
- Parallel startup for improved performance
- Interactive UI for testing tools and resources
- Automatic server lifecycle management across multiple Neovim instances
- Smart shutdown handling with configurable delay
- Both sync and async operations supported
- Clean client registration/cleanup
- Comprehensive API for tool and resource access

## 📦 Installation

Using [lazy.nvim](https://github.com/folke/lazy.nvim):

```lua
{
    "ravitemer/mcphub.nvim",
    dependencies = {
        "nvim-lua/plenary.nvim",  -- Required for Job and HTTP requests
    },
    build = "npm install -g mcp-hub@latest", -- Installs required mcp-hub npm module
    config = function()
        require("mcphub").setup({
            -- Required options
            port = 3000,  -- Port for MCP Hub server
            config = vim.fn.expand("~/mcpservers.json"),  -- Absolute path to config file

            -- Optional options
            on_ready = function(hub)
                -- Called when hub is ready
            end,
            on_error = function(err)
                -- Called on errors
            end,
            shutdown_delay = 0, -- Wait 0ms before shutting down server after last client exits
            log = {
                level = vim.log.levels.WARN,
                to_file = false,
                file_path = nil,
                prefix = "MCPHub"
            },
        })
    end
}
```

Example configuration file:

```json
{
  "mcpServers": {
    "fetch": {
      "command": "uvx",
      "args": ["mcp-server-fetch"]
    },
    "todoist": {
      "command": "npx",
      "args": ["-y", "@abhiz123/todoist-mcp-server"],
      "disabled": true,
      "env": {
        "TODOIST_API_TOKEN": "your-api-token-here"
      }
    }
  }
}
```

### Requirements

- Neovim >= 0.8.0
- Node.js >= 18.0.0
- [plenary.nvim](https://github.com/nvim-lua/plenary.nvim)
- [mcp-hub](https://github.com/ravitemer/mcp-hub) (automatically installed via build command)

## 🚀 Usage

1. Open the MCPHub UI to manage servers, test tools and monitor status:

```vim
:MCPHub
```

You can:

- Start/stop servers directly from the Hub view
- Enable/disable specific tools for each server
- Test tools and resources interactively
- Monitor server status and logs

2. Use the hub instance in your code:

```lua
-- Get hub instance after setup
local mcphub = require("mcphub")

-- Option 1: Use on_ready callback
mcphub.setup({
    port = 3000,
    config = vim.fn.expand("~/mcpservers.json"),
    on_ready = function(hub)
        -- Hub is ready to use here
    end
})

-- Option 2: Get hub instance directly (might be nil if setup in progress)
local hub = mcphub.get_hub_instance()

-- Call a tool (sync)
local response, err = hub:call_tool("server-name", "tool-name", {
    param1 = "value1"
}, {
    return_text = true -- Parse response to LLM-suitable text
})

-- Call a tool (async)
hub:call_tool("server-name", "tool-name", {
    param1 = "value1"
}, {
    return_text = true,
    callback = function(response, err)
        -- Use response
    end
})

-- Access resource (sync)
local response, err = hub:access_resource("server-name", "resource://uri", {
    return_text = true
})

-- Get prompt helpers for system prompts
local prompts = hub:get_prompts({
    use_mcp_tool_example = [[<use_mcp_tool>
<server_name>weather-server</server_name>
<tool_name>get_forecast</tool_name>
<arguments>
{
  "city": "San Francisco",
  "days": 5
}
</arguments>
</use_mcp_tool>]],
    access_mcp_resource_example = [[<access_mcp_resource>
<server_name>weather-server</server_name>
<uri>weather://san-francisco/current</uri>
</access_mcp_resource>]]
})
-- prompts.active_servers: Lists currently active servers
-- prompts.use_mcp_tool: Instructions for tool usage with example
-- prompts.access_mcp_resource: Instructions for resource access with example
```

## 🔌 Extensions

MCPHub.nvim provides extensions that integrate with popular Neovim chat plugins. These extensions allow you to use MCP tools and resources directly within your chat interfaces.

### Available Extensions

#### CodeCompanion Integration

Add MCP capabilities to CodeCompanion.

Add it as a dependency to load the plugin before codecompanion:

```lua
{
  "olimorris/codecompanion.nvim",
  dependencies = {
    "nvim-lua/plenary.nvim",
    "nvim-treesitter/nvim-treesitter",
    "ravitemer/mcphub.nvim"
  },
},
```

- Please note there are some breaking changes with codecompanion v13 in the way we configure tools.

```lua
  require("codecompanion").setup({
    strategies = {
        chat = {
            tools = {
                ["mcp"] = {
                    callback = require("mcphub.extensions.codecompanion"),
                    description = "Call tools and resources from the MCP Servers",
                    opts = {
                      -- user_approval = true,
                      requires_approval = true,
                    }
                }
            }
        }
    }
  })
```

See the [extensions/](lua/mcphub/extensions/) folder for more examples and implementation details.

#### Avante Integration

The Avante extension automatically updates your `[mode].avanterules` file (e.g. `planning.avanterules`) whenever MCP servers or their tools are updated. This ensures Avante always has up-to-date information about available MCP capabilities in its system prompt.

⚠️ **Important Loading Behavior**: Avante loads rules files only once when first opened after Neovim starts. Changes to enabled/disabled MCP servers will update the rules file, but Avante won't see these changes until you:

1. Exit Neovim completely
2. Start Neovim again
3. Open Avante fresh

Commands like `/reset`, `/new`, or `/clear` do not cause Avante to reload the rules file.

⚠️ **Tool Conflicts**: Disable any built-in Avante tools that might conflict with enabled MCP servers to prevent duplicate functionality or unexpected behavior.

##### Setup

Add MCP capabilities to Avante by including the MCP tool in your setup:

```lua
require("avante").setup({
    -- other config
    custom_tools = {
        -- optional: mode is "planning" that is when you open the chat using toggle or <leader>aa
        --optional: cwd can be a string or a function that should returns path
        require("mcphub.extensions.avante").mcp_tool("planning",function()
          return require("avante.utils").get_project_root()
        end)
    }
})
```

⚠️ **Warning: File Override Behavior**

This extension modifies your `[mode].avanterules` file. When creating a new file or replacing content, it uses this template:

```jinja
{% block mcp_servers %}
# Active MCP Servers:
- server1: tool1, tool2
- server2: tool3, tool4

# Available Tools and Resources:
[Detailed list of capabilities...]
{% endblock %}
```

File handling works as follows:

1. If file has the MCP servers block:
   - Only content within the block is modified
   - Content outside the block remains untouched
2. If no block present:
   - **ENTIRE FILE CONTENT WILL BE REPLACED**
   - A new file with proper block structure will be created
   - All custom instructions will be lost

To safely use custom instructions:

1. Add the MCP servers block at the END of your `[mode].avanterules` file
2. Put your custom instructions BEFORE the block
3. Keep the block at the END to prevent MCP content from being overwritten

Example `.avanterules` file with custom instructions:

```jinja
# Your Custom Instructions
You should always write tests for your code.
Handle edge cases carefully.

# IMPORTANT: Keep this block at the end of file
{% block mcp_servers %}
[MCP server capabilities will be automatically updated here]
{% endblock %}
```

Note: You can also access the Express server directly at http://localhost:[port]/api

## 🔧 Troubleshooting

1. **Environment Requirements**

   - Ensure these are installed as they're required by most MCP servers:
     ```bash
     node --version    # Should be >= 18.0.0
     python --version  # Should be installed
     uvx --version    # Should be installed
     ```
   - Most server commands use `npx` or `uvx` - verify these work in your terminal

2. **Port Issues**

   - If you get `EADDRINUSE` error, kill the existing process:
     ```bash
     lsof -i :[port]  # Find process ID
     kill [pid]       # Kill the process
     ```

3. **Configuration File**

   - Ensure config path is absolute
   - Verify file contains valid JSON with `mcpServers` key
   - Check server-specific configuration requirements
   - Validate server command and args are correct for your system

4. **MCP Server Issues**

   - Validate server configurations using either:
     - [MCP Inspector](https://github.com/modelcontextprotocol/inspector): GUI tool for verifying server operation
     - [mcp-cli](https://github.com/wong2/mcp-cli): Command-line tool for testing servers with config files
   - Check server logs in MCPHub UI (Logs view)
   - Test tools and resources individually to isolate issues

5. **Need Help?**
   - Create a [Discussion](https://github.com/ravitemer/mcphub.nvim/discussions) for questions
   - Open an [Issue](https://github.com/ravitemer/mcphub.nvim/issues) for bugs

## 🔄 How It Works

MCPHub.nvim uses an Express server to manage MCP servers and handle client requests:

1. When `setup()` is called:

   - Checks for mcp-hub command installation
   - Verifies version compatibility
   - Starts mcp-hub with provided port and config file
   - Creates Express server at localhost:[port]

2. After successful setup:

   - Calls on_ready callback with hub instance
   - Hub instance provides REST API interface
   - UI updates in real-time via `:MCPHub` command

3. Express Server Features:

   - Manages MCP server configurations
   - Handles tool execution requests
   - Provides resource access
   - Multi-client support
   - Automatic cleanup

4. When Neovim instances close:
   - Unregister as clients
   - Last client triggers shutdown timer
   - Server waits shutdown_delay seconds before stopping
   - Timer cancels if new client connects

This architecture ensures:

- Consistent server management
- Real-time status monitoring
- Efficient resource usage
- Clean process handling
- Multiple client support

### Architecture Flows

##### Server Lifecycle

```mermaid
sequenceDiagram
    participant N1 as First Neovim
    participant N2 as Other Neovims
    participant S as MCP Hub Server

    Note over N1,S: First Client Connection
    N1->>S: Check if Running
    activate S
    S-->>N1: Not Running
    N1->>S: start_hub()
    Note over S: Server Start
    S-->>N1: Ready Signal
    N1->>S: Register Client
    S-->>N1: Registration OK

    Note over N2,S: Other Clients
    N2->>S: Check if Running
    S-->>N2: Running
    N2->>S: Register Client
    S-->>N2: Registration OK

    Note over N1,S: Server stays active

    Note over N2,S: Client Disconnection
    N2->>S: Unregister Client
    S-->>N2: OK
    Note over S: Keep Running

    Note over N1,S: Last Client Exit
    N1->>S: Unregister Client
    S-->>N1: OK
    Note over S: Grace Period
    Note over S: Auto Shutdown
    deactivate S
```

##### Request flow

```mermaid
sequenceDiagram
    participant N as Neovim
    participant P as Plugin
    participant S as MCP Hub Server
    N->>P: start_hub()
    P->>S: Health Check
    alt Server Not Running
        P->>S: Start Server
        S-->>P: Ready Signal
    end
    P->>S: Register Client
    S-->>P: Registration OK
    N->>P: :MCPHub
    P->>S: Get Status
    S-->>P: Server Status
    P->>N: Display UI
```

##### Cleanup flow

```mermaid
flowchart LR
    A[VimLeavePre] -->|Trigger| B[Stop Hub]
    B -->|If Ready| C[Unregister Client]
    C -->|Last Client| D[Server Auto-shutdown]
    C -->|Other Clients| E[Server Continues]
    B --> F[Clear State]
    F --> G[Ready = false]
    F --> H[Owner = false]
```

##### API Flow

```mermaid
sequenceDiagram
    participant C as Chat Plugin
    participant H as Hub Instance
    participant S as MCP Server
    C->>H: call_tool()
    H->>H: Check Ready
    alt Not Ready
        H-->>C: Error: Not Ready
    end
    H->>S: POST /tools
    S-->>H: Tool Result
    H-->>C: Return Result
    Note over C,S: Similar flow for resources

    C->>H: access_resource()
    H->>H: Check Ready
    H->>S: POST /resources
    S-->>H: Resource Data
    H-->>C: Return Data
```

## 🚧 Future Enhancements

Currently planning these features:

- Workflow integration with CodeCompanion for enhanced code assistance
- Enhanced help view with comprehensive documentation
- Community marketplace for sharing and discovering MCP servers
- Add custom descriptions for each MCP server through the UI
- Support server-specific configuration through the interface

## 👏 Acknowledgements

Thanks to:

- [nui.nvim](https://github.com/MunifTanjim/nui.nvim) for inspiring our text highlighting utilities
', 'true', '"{\"Context Management\",\"Function Calling\",\"Plugin Ecosystems\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states it's a Neovim plugin for managing MCP (Model Context Protocol) servers. It provides an interface for configuring, managing, and testing MCP servers within the Neovim environment.","categories":["Agent Frameworks","Function Calling","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The plugin adopts a client-server architecture, with Neovim clients interacting with an Express server (mcp-hub) that manages MCP servers. It provides an intuitive UI for testing tools and resources. The design allows for dynamic enabling/disabling of servers and tools, optimizing token usage. Parallel startup and automatic server lifecycle management are key architectural features.","Implementation Details and Key Features":"The plugin uses Lua for Neovim integration, leveraging plenary.nvim for job and HTTP requests.  It features a single-command interface (:MCPHub) for managing servers. Key features include persistent state across restarts, interactive tool testing, and comprehensive API access for tool and resource invocation.  The plugin's build process installs the required mcp-hub npm module.","Integration Approaches and Protocols":"The plugin integrates MCP servers into Neovim, allowing users to interact with them through the Neovim interface. It supports both synchronous and asynchronous operations for calling tools and accessing resources. It also provides prompt helpers for generating system prompts. Extension support includes integrations with CodeCompanion and Avante.","Technical Stack and Dependencies":"The plugin relies on Neovim (>=0.8.0), Lua, Node.js (>=18.0.0), plenary.nvim, and mcp-hub.  It uses an Express server for managing MCP servers and handling client requests. The CodeCompanion extension also requires nvim-treesitter.  The architecture is described using Mermaid diagrams within the documentation.","Best Practices and Coding Standards":"The plugin follows Lua coding standards within the Neovim environment. It offers a setup function with configurable options, including callbacks for ready and error events. The configuration uses a JSON file to define MCP servers and their associated tools.  The documentation includes troubleshooting steps and contribution guidelines.","Performance Considerations":"The plugin implements parallel startup for improved performance. It also includes features such as dynamically enabling/disabling servers and tools, along with smart shutdown handling with configurable delays, to optimize resource usage. The plugin provides server lifecycle management, ensuring efficient operation across multiple Neovim instances."}}', '2025-03-10 22:15:28+00', 'true', 'Servers', 'mcphub-nvim-mcp'), ('935450522', 'magic-mcp', '21st-dev/magic-mcp', 'https://github.com/21st-dev/magic-mcp', 'It's like v0 but in your Cursor/WindSurf/Cline. 21st dev Magic MCP server for working with your frontend like Magic', '2025-02-19 13:17:50+00', '2025-03-11 10:44:38+00', 'TypeScript', '363', '# 21st.dev Magic AI Agent

![MCP Banner](https://21st.dev/magic-agent-og-image.png)

Magic Component Platform (MCP) is a powerful AI-driven tool that helps developers create beautiful, modern UI components instantly through natural language descriptions. It integrates seamlessly with popular IDEs and provides a streamlined workflow for UI development.

## 🌟 Features

- **AI-Powered UI Generation**: Create UI components by describing them in natural language
- **Multi-IDE Support**:
  - [Cursor](https://cursor.com) IDE integration
  - [Windsurf](https://windsurf.ai) support
  - [VSCode + Cline](https://cline.bot) integration (Beta)
- **Modern Component Library**: Access to a vast collection of pre-built, customizable components inspired by [21st.dev](https://21st.dev)
- **Real-time Preview**: Instantly see your components as you create them
- **TypeScript Support**: Full TypeScript support for type-safe development
- **SVGL Integration**: Access to a vast collection of professional brand assets and logos
- **Component Enhancement**: Improve existing components with advanced features and animations (Coming Soon)

## 🎯 How It Works

1. **Tell Agent What You Need**

   - In your AI Agent's chat, just type `/ui` and describe the component you're looking for
   - Example: `/ui create a modern navigation bar with responsive design`

2. **Let Magic Create It**

   - Your IDE prompts you to use Magic
   - Magic instantly builds a polished UI component
   - Components are inspired by 21st.dev's library

3. **Seamless Integration**
   - Components are automatically added to your project
   - Start using your new UI components right away
   - All components are fully customizable

## 🚀 Getting Started

### Prerequisites

- Node.js (Latest LTS version recommended)
- One of the supported IDEs:
  - Cursor
  - Windsurf
  - VSCode (with Cline extension)

### Installation

1. **Generate API Key**

   - Visit your Magic dashboard
   - Navigate to the API section
   - Generate a new API key

2. **IDE Setup**

#### Cursor IDE

```bash
npx -y @smithery/cli@latest run @21st-dev/magic-mcp --config "{\"TWENTY_FIRST_API_KEY\":\"your-api-key\"}"
```

#### Windsurf

Add to `~/.codeium/windsurf/mcp_config.json`:

```json
{
  "mcpServers": {
    "magic": {
      "command": "npx",
      "args": [
        "-y",
        "@smithery/cli@latest",
        "install",
        "@21st-dev/magic-mcp",
        "--client",
        "windsurf"
      ],
      "env": {
        "TWENTY_FIRST_API_KEY": "your-api-key"
      }
    }
  }
}
```

#### VSCode + Cline (Beta)

Add to Cline's MCP configuration:

```json
{
  "mcpServers": {
    "magic": {
      "command": "npx",
      "args": [
        "-y",
        "@smithery/cli@latest",
        "install",
        "@21st-dev/magic-mcp",
        "--client",
        "cline"
      ],
      "env": {
        "TWENTY_FIRST_API_KEY": "your-api-key"
      }
    }
  }
}
```

## ❓ FAQ

### How does Magic AI Agent handle my codebase?

Magic AI Agent only writes or modifies files related to the components it generates. It follows your project's code style and structure, and integrates seamlessly with your existing codebase without affecting other parts of your application.

### Can I customize the generated components?

Yes! All generated components are fully editable and come with well-structured code. You can modify the styling, functionality, and behavior just like any other React component in your codebase.

### What happens if I run out of generations?

If you exceed your monthly generation limit, you'll be prompted to upgrade your plan. You can upgrade at any time to continue generating components. Your existing components will remain fully functional.

### How soon do new components get added to 21st.dev's library?

Authors can publish components to 21st.dev at any time, and Magic Agent will have immediate access to them. This means you'll always have access to the latest components and design patterns from the community.

### Is there a limit to component complexity?

Magic AI Agent can handle components of varying complexity, from simple buttons to complex interactive forms. However, for best results, we recommend breaking down very complex UIs into smaller, manageable components.

## 🛠️ Development

### Project Structure

```
mcp/
├── app/
│   └── components/     # Core UI components
├── types/             # TypeScript type definitions
├── lib/              # Utility functions
└── public/           # Static assets
```

### Key Components

- `IdeInstructions`: Setup instructions for different IDEs
- `ApiKeySection`: API key management interface
- `WelcomeOnboarding`: Onboarding flow for new users

## 🤝 Contributing

We welcome contributions! Please join our [Discord community](https://discord.gg/Qx4rFunHfm) and provide feedback to help improve Magic Agent. The source code is available on [GitHub](https://github.com/serafimcloud/21st).

## 👥 Community & Support

- [Discord Community](https://discord.gg/Qx4rFunHfm) - Join our active community
- [Twitter](https://x.com/serafimcloud) - Follow us for updates

## ⚠️ Beta Notice

Magic Agent is currently in beta. All features are free during this period. We appreciate your feedback and patience as we continue to improve the platform.

## 📝 License

MIT License

## 🙏 Acknowledgments

- Thanks to our beta testers and community members
- Special thanks to the Cursor, Windsurf, and Cline teams for their collaboration
- Integration with [21st.dev](https://21st.dev) for component inspiration
- [SVGL](https://svgl.app) for logo and brand asset integration

---

For more information, join our [Discord community](https://discord.gg/Qx4rFunHfm) or visit [21st.dev/magic](https://21st.dev/magic).
', 'true', '"{\"AI Model Integration\",\"Developer Tooling\",\"Code Generation\"]}', '{"is_mcp":"YES","justification":"The repository description and documentation explicitly mention 'Magic Component Platform (MCP)' and its integration with multiple IDEs via configuration, indicating an implementation of the Model Context Protocol.","categories":["AI Model Integration","Developer Tooling","Prompt Engineering Tools"],"analysis":{"Architecture and Design Patterns":"The architecture follows a plugin-like design, enabling integration with various IDEs (Cursor, Windsurf, VSCode + Cline). It leverages a central 'Magic' component, likely responsible for AI-driven UI generation based on natural language prompts. The use of TypeScript suggests a focus on type safety and maintainability.  A clear separation of concerns is implied by the project structure shown in the documentation.","Implementation Details and Key Features":"Key features include AI-powered UI generation from natural language descriptions, real-time preview of components, and support for TypeScript. The system utilizes an API key for authentication and presumably accesses a component library hosted on 21st.dev. The `IdeInstructions` component suggests IDE-specific setup processes. The use of `@smithery/cli` likely indicates a command-line interface for installation and configuration.","Integration Approaches and Protocols":"The integration with IDEs involves configuring MCP servers, as indicated by the `mcp_config.json` example for Windsurf and Cline. This suggests a standardized protocol for communication between the IDE and the 'Magic' component platform.  The use of `npx` and command-line arguments reveals a process-based integration mechanism.  The documentation implies the use of API calls using the provided API key.","Technical Stack and Dependencies":"The technical stack includes Node.js and TypeScript. Key dependencies include `@smithery/cli` (likely a custom CLI tool), indicating a managed installation and configuration process. The use of `npx` suggests reliance on npm for package management.  The project integrates with the 21st.dev component library and possibly uses SVGL for logo integration.","Best Practices and Coding Standards":"The project utilizes TypeScript, indicating a focus on type safety. The documentation highlights modularity and customizability of generated components, suggesting adherence to component-based design principles.  The separation of concerns within the `mcp` directory suggests a structured approach to code organization. The documentation encourages community contributions, indicating a collaborative development model.","Performance Considerations":"The documentation mentions real-time preview, which suggests the implementation of efficient rendering and update mechanisms. The AI-powered generation aspect may require optimization to ensure rapid UI creation. The integration with various IDEs necessitates cross-platform compatibility and performance tuning.  The system likely relies on efficient API communication to minimize latency."}}', '2025-03-11 14:29:52+00', 'true', 'Servers', 'magic-dev-mcp'), ('935736274', 'apple-mcp', 'Dhravya/apple-mcp', 'https://github.com/Dhravya/apple-mcp', 'Collection of apple-native tools for the model context protocol.', '2025-02-19 23:50:49+00', '2025-03-10 16:26:47+00', 'TypeScript', '656', '# Apple MCP tools

[![smithery badge](https://smithery.ai/badge/@Dhravya/apple-mcp)](https://smithery.ai/server/@Dhravya/apple-mcp)

This is a collection of apple-native tools for the [MCP protocol](https://modelcontextprotocol.com/docs/mcp-protocol).

Here's a step-by-step video about how to set this up, with a demo. - https://x.com/DhravyaShah/status/1892694077679763671

![image](https://github.com/user-attachments/assets/56a5ccfa-cb1a-4226-80c5-6cc794cefc34)


<details>
<summary>Here's the JSON to copy</summary>

```
{
  "mcpServers": {
    "apple-mcp": {
      "command": "bunx",
      "args": ["--no-cache", "apple-mcp@latest"]
    }
}

```

</details>

#### Quick install

To install Apple MCP for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Dhravya/apple-mcp):

```bash
npx -y @smithery/cli@latest install @Dhravya/apple-mcp --client claude
```

... and for cursor, you can do:

```bash
npx -y @smithery/cli@latest install @Dhravya/apple-mcp --client cursor
```


## Features

- Messages:
  - Send messages using the Apple Messages app
  - Read out messages
- Notes:
  - List notes
  - Search & read notes in Apple Notes app
- Contacts:
  - Search contacts for sending messages
- Emails:
  - Send emails with multiple recipients (to, cc, bcc) and file attachments
  - Search emails with custom queries, mailbox selection, and result limits
  - Schedule emails for future delivery
  - List and manage scheduled emails
  - Check unread email counts globally or per mailbox
- Reminders:
  - List all reminders and reminder lists
  - Search for reminders by text
  - Create new reminders with optional due dates and notes
  - Open the Reminders app to view specific reminders

- TODO: Search and open calendar events in Apple Calendar app
- TODO: Search and open photos in Apple Photos app
- TODO: Search and open music in Apple Music app


You can also daisy-chain commands to create a workflow. Like:
"can you please read the note about people i met in the conference, find their contacts and emails, and send them a message saying thank you for the time."

(it works!)


#### Manual installation

You just need bun, install with `brew install oven-sh/bun/bun`

Now, edit your `claude_desktop_config.json` with this:

```claude_desktop_config.json
{
  "mcpServers": {
    "apple-mcp": {
      "command": "bunx",
      "args": ["@dhravya/apple-mcp@latest"]
    }
  }
}
```

### Usage

Now, ask Claude to use the `apple-mcp` tool.

```
Can you send a message to John Doe?
```

```
find all the notes related to AI and send it to my girlfriend
```

```
create a reminder to "Buy groceries" for tomorrow at 5pm
```

## Local Development

```bash
git clone https://github.com/dhravya/apple-mcp.git
cd apple-mcp
bun install
bun run index.ts
```

enjoy!
', 'true', '"{\"Function Calling\",\"Protocol Implementations\",\"Agent Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states that it is a collection of apple-native tools for the Model Context Protocol (MCP). The documentation links to the MCP protocol definition.","categories":["Protocol Implementations","Agent Frameworks","Function Calling"],"analysis":{"Architecture and Design Patterns":"The architecture appears to be based on providing Apple-native tool access via MCP. The design uses a command-line interface (CLI) driven approach, enabling Claude or other LLMs to call specific Apple functions. The system uses JSON configuration for server definition, indicating a modular design for tool integration.","Implementation Details and Key Features":"The implementation leverages `bunx` to execute the Apple MCP tools. Key features include sending messages, managing notes, accessing contacts, handling emails, and managing reminders. The system demonstrates message sending, note searching, and reminder creation functionalities, aiming for a workflow automation via LLMs.","Integration Approaches and Protocols":"The repository integrates with LLMs such as Claude via the Model Context Protocol. It uses Smithery for installation. The integration allows LLMs to access and control Apple applications like Messages, Notes, Contacts, Mail, and Reminders, effectively acting as a bridge between the LLM and these applications.","Technical Stack and Dependencies":"The primary technical stack involves Bun as a JavaScript runtime, likely leveraging its speed and package management capabilities. Dependencies likely include libraries for interacting with Apple's APIs for Messages, Notes, Contacts, Mail, and Reminders. The use of `bunx` suggests a focus on minimizing dependency caching.","Best Practices and Coding Standards":"The repository includes a basic structure with clear instructions for installation and usage. The documentation provides examples and use cases, promoting usability. The use of Smithery suggests adherence to a managed installation process, enhancing maintainability and ease of deployment for end-users.","Performance Considerations":"Performance considerations likely revolve around the speed of Apple API calls and the efficiency of data transfer between the LLM and the Apple applications. The choice of Bun as a runtime potentially aims to optimize execution speed. Bottlenecks could occur in complex workflows involving multiple chained commands."}}', '2025-03-10 21:26:30+00', 'true', 'Servers', 'apple-mcp-tools'), ('937958453', 'mcp-send-email', 'resend/mcp-send-email', 'https://github.com/resend/mcp-send-email', 'Send emails directly from Cursor with this email sending MCP server', '2025-02-24 07:36:06+00', '2025-03-11 09:12:37+00', 'TypeScript', '221', '# Email sending MCP 💌

[![smithery badge](https://smithery.ai/badge/@ykhli/mcp-send-email)](https://smithery.ai/server/@ykhli/mcp-send-email)

This is a simple MCP server that sends emails using Resend's API. Why? Now you can let Cursor or Claude Desktop compose emails for you and send it right away without having to copy and paste the email content.

Built with:

- [Resend](https://resend.com/)
- [Anthropic MCP](https://docs.anthropic.com/en/docs/agents-and-tools/mcp)
- [Cursor](https://cursor.so/)

**DEMO**

https://github.com/user-attachments/assets/8c05cbf0-1664-4b3b-afb1-663b46af3464

**Cursor**

1. First, you need to authorize Resend to send emails from your domain or email. Follow the steps [here](https://resend.com/docs/send-with-nodejs) to set that up and get a Resend API key.
2. Clone this project locally. Edit index.ts and replace me@yoko.dev to your own email to send emails from
3. Run `npm install`, `npm run build` under the project dir. You should now see a /build/index.js generated - this is the MCP server script!

Then go to Cursor Settings -> MCP -> Add new MCP server

- Name = [choose your own name]
- Type = command
- Command: `node ABSOLUTE_PATH_TO_MCP_SERVER/build/index.js --key=YOUR_RESEND_API_KEY --sender=OPTIONAL_SENDER_EMAIL_ADDRESS --reply-to=OPTIONAL_REPLY_TO_EMAIL_ADDRESS_ONE --reply-to=OPTIONAL_REPLY_TO_EMAIL_ADDRESS_TWO`

You can get Resend API key here: https://resend.com/

Now you can test out sending emails by going to email.md, replace the to: email address, select all in email md, and hit cmd+l. You can now tell cursor to "send this as an email" in the chat. Make sure Cursor chat is in Agent mode by selecting "Agent" on lower left side dropdown

<img width="441" alt="Screenshot 2025-02-25 at 9 13 05 AM" src="https://github.com/user-attachments/assets/b07e9cbf-42d8-4910-8e90-3761d8d3bc06" />

**Claude desktop**

Same set up as above, and then add the following MCP config

```
{
  "mcpServers": {
    "resend": {
      "command": "node",
      "args": ["ABSOLUTE_PATH_TO_MCP_SERVER/build/index.js"],
      "env": {
        "RESEND_API_KEY": [YOUR_API_KEY],
        "SENDER_EMAIL_ADDRESS": [OPTIONAL_SENDER_EMAIL_ADDRESS],
        "REPLY_TO_EMAIL_ADDRESSES": [OPTIONAL_REPLY_TO_EMAIL_ADDRESSES_COMMA_DELIMITED]
      }
    }
  }
}
```

**Develop**

`npm install`
`npm run build`
', 'true', '"{\"Function Calling\",\"Protocol Implementations\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it's an MCP server for sending emails using Resend's API, designed for use with Cursor and Claude, adhering to the Anthropic MCP specification for tool interaction.","categories":["Function Calling","Protocol Implementations","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The repository implements a simple server that acts as a bridge between a Large Language Model (LLM) and the Resend email service. It leverages the Model Context Protocol (MCP) to receive email composition requests from Cursor or Claude, then uses the Resend API to send the emails. The architecture is straightforward, focusing on ease of use and integration.","Implementation Details and Key Features":"The core implementation is in TypeScript (index.ts), which compiles to a JavaScript file. The server uses environment variables or command-line arguments to configure the Resend API key and sender email address. The primary function handles incoming MCP requests, extracts relevant email parameters, and calls the Resend API to dispatch the email.","Integration Approaches and Protocols":"The repository integrates with LLMs (Cursor, Claude) via MCP, allowing these tools to delegate email sending. It communicates with Resend using its REST API. The server is executed as a command-line process initiated by the LLM environment, passing configuration through command-line arguments or environment variables.","Technical Stack and Dependencies":"The project relies on Node.js as a runtime environment. Key dependencies include the Resend SDK for interacting with the Resend API and TypeScript for development. The build process uses npm for package management and script execution. The server is designed to be lightweight with minimal external dependencies.","Best Practices and Coding Standards":"The provided documentation focuses on configuration and setup rather than in-depth code analysis. However, the use of TypeScript suggests an intent to enforce type safety and maintain code quality. The project uses npm for package management, which promotes dependency versioning and reproducible builds. No specific testing frameworks are mentioned.","Performance Considerations":"Given its function as an intermediary for email sending, performance is likely tied to the latency of the Resend API and the overhead of the Node.js runtime. The server is designed to be invoked on demand, so startup time may be a relevant factor. Caching and rate limiting are not explicitly addressed but could be considered for scalability."}}', '2025-03-11 14:29:52+00', 'true', 'Servers', 'mcp-send-email'), ('938901713', 'MCP2Lambda', 'danilop/MCP2Lambda', 'https://github.com/danilop/MCP2Lambda', 'Run any AWS Lambda function as a Large Language Model (LLM) tool without code changes using Anthropic's Model Control Protocol (MCP).', '2025-02-25 17:18:16+00', '2025-03-10 10:55:04+00', 'Python', '41', '# MCP2Lambda

[![smithery badge](https://smithery.ai/badge/@danilop/MCP2Lambda)](https://smithery.ai/server/@danilop/MCP2Lambda)

Run any [AWS Lambda](https://aws.amazon.com/lambda/) function as a Large Language Model (LLM) **tool** without code changes using [Anthropic](https://www.anthropic.com)'s [Model Control Protocol (MCP)](https://github.com/modelcontextprotocol).

```mermaid
graph LR
    A[Model] <--> B[MCP Client]
    B <--> C["MCP2Lambda<br>(MCP Server)"]
    C <--> D[Lambda Function]
    D <--> E[Other AWS Services]
    D <--> F[Internet]
    D <--> G[VPC]
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#bbf,stroke:#333,stroke-width:2px
    style C fill:#bfb,stroke:#333,stroke-width:4px
    style D fill:#fbb,stroke:#333,stroke-width:2px
    style E fill:#fbf,stroke:#333,stroke-width:2px
    style F fill:#dff,stroke:#333,stroke-width:2px
    style G fill:#ffd,stroke:#333,stroke-width:2px
```

This MCP server acts as a **bridge** between MCP clients and AWS Lambda functions, allowing generative AI models to access and run Lambda functions as tools. This is useful, for example, to access private resources such as internal applications and databases without the need to provide public network access. This approach allows the model to use other AWS services, private networks, and the public internet.

From a **security** perspective, this approach implements segregation of duties by allowing the model to invoke the Lambda functions but not to access the other AWS services directly. The client only needs AWS credentials to invoke the Lambda functions. The Lambda functions can then interact with other AWS services (using the function role) and access public or private networks.

The MCP server gives access to two tools:

1. The first tool can **autodiscover** all Lambda functions in your account that match a prefix or an allowed list of names. This tool shares the names of the functions and their descriptions with the model.

2. The second tool allows to **invoke** those Lambda functions by name passing the required parameters.

No code changes are required. You should change these configurations to improve results:

## Strategy Selection

The gateway supports two different strategies for handling Lambda functions:

1. **Pre-Discovery Mode** (default: enabled): Registers each Lambda function as an individual tool at startup. This provides a more intuitive interface where each function appears as its own named tool.

2. **Generic Mode**: Uses two generic tools (`list_lambda_functions` and `invoke_lambda_function`) to interact with Lambda functions.

You can control this behavior through:

- Environment variable: `PRE_DISCOVERY=true|false`
- CLI flag: `--no-pre-discovery` (disables pre-discovery mode)

Example:
```bash
# Disable pre-discovery mode
export PRE_DISCOVERY=false
python main.py

# Or using CLI flag to disable pre-discovery
python main.py --no-pre-discovery
```

1. To provide the MCP client with the knowledge to use a Lambda function, the **description of the Lambda function** should indicate what the function does and which parameters it uses. See the sample functions for a quick demo and more details.

2. To help the model use the tools available via AWS Lambda, you can add something like this to your **system prompt**:

```
Use the AWS Lambda tools to improve your answers.
```

## Overview

MCP2Lambda enables LLMs to interact with AWS Lambda functions as tools, extending their capabilities beyond text generation. This allows models to:

- Access real-time and private data, including data sources in your VPCs
- Execute custom code using a Lambda function as sandbox environment
- Interact with external services and APIs using Lambda functions internet access (and bandwidth)
- Perform specialized calculations or data processing

The server uses the MCP protocol, which standardizes the way AI models can access external tools.

By default, only functions whose name starts with `mcp2lambda-` will be available to the model.

## Prerequisites

- Python 3.12 or higher
- AWS account with configured credentials
- AWS Lambda functions (sample functions provided in the repo)
- An application using [Amazon Bedrock](https://aws.amazon.com/bedrock/) with the [Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/converse.html)
- An MCP-compatible client like [Claude Desktop](https://docs.anthropic.com/en/docs/claude-desktop)

## Installation

### Installing via Smithery

To install MCP2Lambda for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@danilop/MCP2Lambda):

```bash
npx -y @smithery/cli install @danilop/MCP2Lambda --client claude
```

### Manual Installation
1. Clone the repository:
   ```
   git clone https://github.com/yourusername/mcp2lambda.git
   cd mcp2lambda
   ```

2. Configure AWS credentials. For example, using the [AWS CLI](https://aws.amazon.com/cli):
   ```
   aws configure
   ```

## Sample Lambda Functions

This repository includes three *sample* Lambda functions that demonstrate different use cases. These functions have basic permissions and can only write to CloudWatch logs.

### CustomerIdFromEmail
Retrieves a customer ID based on an email address. This function takes an email parameter and returns the associated customer ID, demonstrating how to build simple lookup tools. The function is hard coded to reply to the `user@example.com` email address. For example, you can ask the model to get the customer ID for the email `user@example.com`.

### CustomerInfoFromId
Retrieves detailed customer information based on a customer ID. This function returns customer details like name, email, and status, showing how Lambda can provide context-specific data. The function is hard coded to reply to the customer ID returned by the previous function. For example, you can ask the model to "Get the customer status for email `user@example.com`". This will use both functions to get to the result.

### RunPythonCode
Executes arbitrary Python code within a Lambda sandbox environment. This powerful function allows Claude to write and run Python code to perform calculations, data processing, or other operations not built into the model. For example, you can ask the model to "Calculate the number of prime numbers between 1 and 10, 1 and 100, and so on up to 1M".

## Deploying Sample Lambda Functions

The repository includes sample Lambda functions in the `sample_functions` directory.

1. Install the AWS SAM CLI: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/install-sam-cli.html

2. Deploy the sample functions:
   ```
   cd sample_functions
   sam build
   sam deploy
   ```

The sample functions will be deployed with the prefix `mcp2lambda-`.

## Using with Amazon Bedrock

MCP2Lambda can also be used with Amazon Bedrock's Converse API, allowing you to use the MCP protocol with any of the models supported by Bedrock.

The `mcp_client_bedrock` directory contains a client implementation that connects MCP2Lambda to Amazon Bedrock models.

See https://github.com/mikegc-aws/amazon-bedrock-mcp for more information.

### Prerequisites

- Amazon Bedrock access and permissions to use models like Claude, Mistral, Llama, etc.
- Boto3 configured with appropriate credentials

### Installation and Setup

1. Navigate to the mcp_client_bedrock directory:
   ```
   cd mcp_client_bedrock
   ```

2. Install dependencies:
   ```
   uv pip install -e .
   ```

3. Run the client:
   ```
   python main.py
   ```

### Configuration

The client is configured to use Anthropic's Claude 3.7 Sonnet by default, but you can modify the `model_id` in `main.py` to use other Bedrock models:

```python
# Examples of supported models:
model_id = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"
#model_id = "us.amazon.nova-pro-v1:0"
```

You can also customize the system prompt in the same file to change how the model behaves.

### Usage

1. Start the MCP2Lambda server in one terminal:
   ```
   cd mcp2lambda
   uv run main.py
   ```

2. Run the Bedrock client in another terminal:
   ```
   cd mcp_client_bedrock
   python main.py
   ```

3. Interact with the model through the command-line interface. The model will have access to the Lambda functions deployed earlier.

## Using with Claude Desktop

Add the following to your Claude Desktop configuration file:

```json
{
  "mcpServers": {
    "mcp2lambda": {
      "command": "uv",
      "args": [
        "--directory",
        "<full path to the mcp2lambda directory>",
        "run",
        "main.py"
      ]
    }
  }
}
```

To help the model use tools via AWS Lambda, in your settings profile, you can add to your personal preferences a sentence like:

```
Use the AWS Lambda tools to improve your answers.
```

## Starting the MCP Server

Start the MCP server locally:

```sh
cd mcp2lambda
uv run main.py
```
', 'true', '"{\"Function Calling\",\"Protocol Implementations\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements Anthropic's Model Control Protocol (MCP) to enable LLMs to interact with AWS Lambda functions as tools, bridging the gap between AI models and serverless computing.","categories":["Function Calling","Protocol Implementations","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The architecture revolves around an MCP server acting as an intermediary between LLMs and AWS Lambda functions. It enables models to invoke Lambda functions as tools, providing access to AWS services, private networks, and the internet. Two operational modes, pre-discovery and generic, determine how Lambda functions are exposed as tools. This promotes segregation of duties by isolating model access.","Implementation Details and Key Features":"MCP2Lambda offers autodiscovery of Lambda functions and allows invoking them with required parameters. It supports integration with Amazon Bedrock's Converse API and Claude Desktop. Key features include the ability to access real-time data, execute custom code within Lambda sandboxes, and interact with external services. It leverages two generic tools to interact with Lambda functions: `list_lambda_functions` and `invoke_lambda_function`.","Integration Approaches and Protocols":"The integration hinges on the MCP protocol, standardizing AI model access to external tools. It allows LLMs to interact with AWS Lambda through the MCP server. Integration examples are given for Amazon Bedrock's Converse API using a client implementation and Claude Desktop through configuration files. The server acts as a bridge, facilitating seamless communication and execution.","Technical Stack and Dependencies":"The project requires Python 3.12+, AWS account with configured credentials, and sample AWS Lambda functions. Dependencies include Boto3, AWS SAM CLI for deploying sample functions, and libraries for the Amazon Bedrock MCP client. It leverages `uv` package manager for dependency management. Mermaid diagrams are also used to describe architecture.","Best Practices and Coding Standards":"The repository provides sample Lambda functions with descriptions to guide usage. Configuration through environment variables and CLI flags is encouraged. The documentation recommends including information about function purpose and parameters in the Lambda function's description. This facilitates the model's ability to effectively use the tools and improve answers.","Performance Considerations":"The performance relies on the speed and efficiency of Lambda function execution. The pre-discovery mode may have initial overhead for registering functions. Consider the impact of network latency between the model, MCP server, and Lambda functions. The ability to use Lambda as sandbox environments might also introduce overhead. Proper Lambda configuration and optimization are essential."}}', '2025-03-10 22:15:32+00', 'true', 'Servers', 'mcp2lambda-llm-tool'), ('939611454', 'FLUJO', 'mario-andreschak/FLUJO', 'https://github.com/mario-andreschak/FLUJO', 'MCP-Hub and -Inspector, Multi-Model Workflow and Chat Interface ', '2025-02-26 20:23:56+00', '2025-03-11 11:09:13+00', 'TypeScript', '96', '# DISCLAIMER
->->-> FLUJO is still an early preview! Expect it to break at some points, but improve rapidly! <-<-<-

This [issue](https://github.com/mario-andreschak/FLUJO/issues/9) is currently impacting the CHAT feature and the OPENAI-COMPATIBLE ENDPOINT, fix will come asap! 
- a hotfix was implemented to restore minimum functionality
- You can still view the whole Flow execution in the terminal output! (after you started the server in a terminal with npm run dev or npm start)
- You can add ~FLUJOEXPAND=1 or ~FLUJODEBUG=1 somewhere in your message to show more details

# IMPORTANT SECURITY NOTE
FLUJO has currently EXTENSIVE logging enabled by default! This **will expose your encrypted API-Keys to the terminal output!**. Be VERY careful when grabbing videos or streaming and showing the terminal output!

![FLUJO Logo](https://github.com/user-attachments/assets/881ad34c-73fa-4b71-ba47-123b5da8e05e)

# FLUJO

[![MIT License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Version](https://img.shields.io/badge/version-0.1.0-green.svg)](package.json)

FLUJO is an open-source platform that bridges the gap between **workflow orchestration**, **Model-Context-Protocol (MCP)**, and **AI tool integration**. It provides a unified interface for managing AI models, MCP servers, and complex workflows - all locally and open-source.

![FLUJO Overview](https://github.com/user-attachments/assets/397ef3a5-e4c1-4667-ac74-6fe96f854ff1)

FLUJO is powered by the [PocketFlowFramework](https://the-pocket-world.github.io/Pocket-Flow-Framework/) and built with [CLine](https://github.com/cline/cline) and a lot of LOVE.

## 🌟 Key Features

### 🔑 Environment & API Key Management

- **Secure Storage**: Store environment variables and API keys with encryption
- **Global Access**: Use your stored keys across the entire application
- **Centralized Management**: Keep all your credentials in one secure place

![API Keys Management](https://github.com/user-attachments/assets/f5acd60f-129d-4e0c-8bc1-b5410d3c8d1d)

### 🤖 Model Management

- **Multiple Models**: Configure and use different AI models simultaneously
- **Pre-defined Prompts**: Create custom system instructions for each model
- **Provider Flexibility**: Connect to various API providers (OpenAI, Anthropic, etc.)
- **Local Models**: Integrate with Ollama for local model execution

![Model Configuration](https://github.com/user-attachments/assets/06036daa-c576-4483-b13e-47ef21a82395)
![Model Settings](https://github.com/user-attachments/assets/4e6f8390-eaab-448a-9a38-bbbd64fd3de8)
![Ollama Integration](https://github.com/user-attachments/assets/8a04632a-4cc2-4738-ac9b-e856170a9e7c)

### 🔌 MCP Server Integration

- **Easy Installation**: Install MCP servers from GitHub or local filesystem
- **Server Management**: Comprehensive interface for managing MCP servers
- **Tool Inspection**: View and manage available tools from MCP servers
- **Environment Binding**: Connect server environment variables to global storage

![MCP Server Installation](https://github.com/user-attachments/assets/4c4055fd-c769-4155-b48f-1350b689545f)
![MCP Server Management](https://github.com/user-attachments/assets/bd10b76f-aeb0-48c2-98e3-313e35ace50f)
![MCP Server Tools](https://github.com/user-attachments/assets/a29effb6-07d4-42e2-886f-6cf7c96fe4a6)
![MCP Environment Variables](https://github.com/user-attachments/assets/27b257bf-a6ad-42bf-9ccf-4178c454c7ce)

### 🔄 Workflow Orchestration

- **Visual Flow Builder**: Create and design complex workflows
- **Model Integration**: Connect different models in your workflow
- **Tool Management**: Allow or restrict specific tools for each model
- **Prompt Design**: Configure system prompts at multiple levels (Model, Flow, Node)
![image](https://github.com/user-attachments/assets/36d417ca-a0e6-4f87-90cb-b17a70641372)
![Flow Design](https://github.com/user-attachments/assets/30fc4c8f-78fe-4a44-9fe7-d7837d7359d2)
![Flow Configuration](https://github.com/user-attachments/assets/6b84025f-5240-4277-87e9-02e0f5aac867)
![System Prompts](https://github.com/user-attachments/assets/b1725c4d-2b0f-420d-92cc-3eba13a5a7de)
![Tool References](https://github.com/user-attachments/assets/8bc8ee61-2f21-42ef-b1df-9c88a4ad13a6)
![Screenshot 2025-03-08 223218](https://github.com/user-attachments/assets/922b9368-c0b6-4a06-b500-c8d71506173a)

### 💬 Chat Interface

- **Flow Interaction**: Interact with your flows through a chat interface
- **Message Management**: Disable messages or split conversations to reduce context size
- **File Attachments**: Attach documents or audio for LLM processing
- **Transcription**: Process audio inputs with automatic transcription

![Chat Interface](https://github.com/user-attachments/assets/ce6b5f15-c500-4129-a1f7-131517a65f14)
![Message Management](https://github.com/user-attachments/assets/625b90d6-73e2-4afe-9ec4-5814b0bbf302)
![File Attachments](https://github.com/user-attachments/assets/3f7f737a-170c-48e8-b9b1-3969da50d8e0)

### 🔄 External Tool Integration

- **OpenAI Compatible Endpoint**: Integrate with tools like CLine or Roo
- **Seamless Connection**: Use FLUJO as a backend for other AI applications

![CLine Integration](https://github.com/user-attachments/assets/4e528d28-a317-4326-8da6-6c6dc4a6232b)
![Roo Integration](https://github.com/user-attachments/assets/d8d8fe98-f08e-40eb-9ad8-a494aad32826)

### 🖥️ Desktop Application

- **Electron Integration**: Run FLUJO as a native desktop application
- **System Tray**: Minimize to system tray for quick access
- **Network Mode**: Configure the application to be accessible from other devices
- **Dual-Mode Operation**: Use as a desktop app or headless server on edge devices

## 🚀 Getting Started
### Electron Bundle
- Download the Setup.exe here: https://github.com/mario-andreschak/FLUJO/releases/tag/preview

### Manual installation:
### Prerequisites

- Node.js (v18 or higher)
- npm or yarn

### Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/mario-andreschak/FLUJO.git
   cd FLUJO
   ```

2. Install dependencies:
   ```bash
   npm install
   # or
   yarn install
   ```

3. Start the development server:
   ```bash
   npm run dev
   # or
   yarn dev
   ```

4. Open your browser and navigate to:
   ```
   http://localhost:4200
   ```
   
5. FLUJO feels and works best if you run it compiled:
   ```bash
   npm run build
   npm start
   ```

6. To run as a desktop application:
   ```bash
   npm run electron-dev    # Development mode
   # or
   npm run electron-dist   # Build and package for your platform
   ```

## 📖 Usage

### Setting Up Models

1. Navigate to the Models page
2. Click "Add Model" to create a new model configuration
3. Configure your model with name, provider, API key, and system prompt
4. Save your configuration

### Managing MCP Servers

1. Go to the MCP page
2. Click "Add Server" to install a new MCP server
3. Choose from GitHub repository or local filesystem
4. Configure server settings and environment variables
5. Start and manage your server

### Creating Workflows

1. Visit the Flows page
2. Click "Create Flow" to start a new workflow
3. Add processing nodes and connect them
4. Configure each node with models and tools
5. Save your flow

### Using the Chat Interface

1. Go to the Chat page
2. Select a flow to interact with
3. Start chatting with your configured workflow
4. Attach files or audio as needed
5. Manage conversation context by disabling messages or splitting conversations

### Using the Desktop Application

1. Run FLUJO as a desktop application using `npm run electron-dev` or `npm run electron-dist`
2. Access all features through the desktop interface
3. Configure network access in Settings > Desktop Application to make FLUJO accessible from other devices
4. Minimize to system tray by closing the window (the application will continue running)
5. Access the application from the system tray icon

## 🔄 MCP Integration

FLUJO provides comprehensive support for the Model Context Protocol (MCP), allowing you to:

- Install and manage MCP servers
- Inspect server tools
- Connect MCP servers to your workflows
- Reference tools directly in prompts
- Bind environment variables to your global encrypted storage

## 📄 License

FLUJO is licensed under the [MIT License](LICENSE).
## 🚀 Roadmap
Here's a roadmap of upcoming features and improvements:

- Real-time Voice Feature: Adding support for Whisper.js or OpenWhisper to enable real-time voice capabilities.
- Visual Debugger: Introducing a visual tool to help debug and troubleshoot more effectively.
- MCP Roots Support: Implementing Checkpoints and Restore features within MCP Roots for better control and recovery options.
- MCP Prompts: Enabling users to build custom prompts that fully leverage the capabilities of the MCP server.
- MCP Proxying STDIO<>SSE: Likely utilizing SuperGateway to proxy standard input/output with Server-Sent Events for enhanced communication: Use MCP Servers managed in FLUJo in any other MCP client.
- Enhanced Integrations: Improving compatibility and functionality with tools like Windsurf, Cursor, and Cline.
- Advanced Orchestration: Adding agent-driven orchestration, batch processing, and incorporating features inspired by Pocketflow.
- Online Template Repository: Creating a platform for sharing models, flows, or complete "packages," making it easy to distribute FLUJO flows to others.
- Edge Device Optimization: Enhancing performance and usability for edge devices.

## 🤝 Contributing

Contributions are welcome! Feel free to open issues or submit pull requests.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## 📬 Contact

- GitHub: [mario-andreschak](https://github.com/mario-andreschak)

## Notes:
- You can add ~FLUJO=HTML, ~FLUJO=MARKDOWN, ~FLUJO=JSON, ~FLUJO=TEXT in your message to format the response, this will give varying results in different tools where you integrate FLUJO.
- You can add ~FLUJOEXPAND=1 or ~FLUJODEBUG=1 somewhere in your message to show more details
- in config/features.ts you can change the Logging-level for the whole application
- in config/features.ts you can enable SSE support which is currently disabled by default
---

FLUJO - Empowering your AI workflows with open-source orchestration.
', 'true', '"{\"Agent Frameworks\",\"Workflow Orchestration\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly mentions 'Model-Context-Protocol (MCP)' and provides features for managing MCP servers, tool inspection, and integrating MCP servers into workflows. It facilitates tool use and context management for AI models.","categories":["Agent Frameworks","Workflow Orchestration","Function Calling"],"analysis":{"Architecture and Design Patterns":"FLUJO adopts a modular architecture, separating concerns like API key management, model configuration, MCP server integration, and workflow orchestration. The visual flow builder suggests a component-based design. It leverages the PocketFlowFramework, implying a framework-based approach. The use of Electron enables cross-platform desktop application deployment, impacting the overall architectural considerations.","Implementation Details and Key Features":"Key features include secure API key storage, model management (supporting OpenAI, Anthropic, and local models like Ollama), MCP server installation and management, and a visual workflow builder. The chat interface enables interaction with workflows. The system implements external tool integration through an OpenAI-compatible endpoint. Extensive logging is enabled by default for debugging purposes, with a security warning.","Integration Approaches and Protocols":"FLUJO integrates with various AI models through API providers and local execution (Ollama). It connects to MCP servers using a defined protocol for tool discovery and usage. Integration with external tools like CLine and Roo is facilitated through an OpenAI-compatible endpoint. The potential use of SuperGateway for STDIO<>SSE proxying is mentioned for future enhanced communication.","Technical Stack and Dependencies":"The repository relies on Node.js, npm (or yarn), and likely other JavaScript libraries for its core functionality. The use of Electron suggests dependencies related to cross-platform desktop application development. The PocketFlowFramework and CLine are also dependencies. The documentation mentions TypeScript, meaning that the codebase is written in TypeScript.","Best Practices and Coding Standards":"While not explicitly stated, the modular architecture and clear separation of concerns suggest an adherence to good coding practices. The inclusion of a license (MIT) and a contributing guide indicates a willingness to accept external contributions and maintain a collaborative development environment. However, the 'EXTENSIVE logging enabled by default' contradicts security best practices.","Performance Considerations":"The documentation mentions 'Edge Device Optimization' in the roadmap, suggesting an awareness of performance considerations for resource-constrained environments. The ability to disable messages in the chat interface to reduce context size indicates optimization efforts for LLM processing. Running FLUJO compiled ('npm run build') is recommended for performance improvements."}}', '2025-03-11 14:29:54+00', 'true', 'Servers', 'mcp-flujo-workflow'), ('941225200', 'uber-eats-mcp-server', 'ericzakariasson/uber-eats-mcp-server', 'https://github.com/ericzakariasson/uber-eats-mcp-server', null, '2025-03-01 19:35:15+00', '2025-03-11 07:16:20+00', 'Python', '128', '# Uber Eats MCP Server

This is a POC of how you can build an MCP servers on top of Uber Eats

https://github.com/user-attachments/assets/05efbf51-1b95-4bd2-a327-55f1fe2f958b

## What is MCP?

The [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) is an open protocol that enables seamless integration between LLM applications and external tools.

## Prerequisites

- Python 3.12 or higher
- Anthropic API key or other supported LLM provider

## Setup

1. Ensure you have a virtual environment activated:
   ```
   uv venv
   source .venv/bin/activate  # On Unix/Mac
   ```

2. Install required packages:
   ```
   uv pip install -r requirements.txt
   playwright install
   ```

3. Update the `.env` file with your API key:
   ```
   ANTHROPIC_API_KEY=your_openai_api_key_here
   ```

## Note

Since we're using stdio as MCP transport, we have disable all output from browser use

## Debugging

You can run the MCP inspector tool with this command

```bash
uv run mcp dev server.py
```
', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository explicitly mentions and implements the Model Context Protocol (MCP), aiming for seamless integration between LLM applications and external tools, as stated in the documentation.","categories":["Protocol Implementations","AI Model Integration","Function Calling"],"analysis":{"Architecture and Design Patterns":"The architecture centers around a server that adheres to the Model Context Protocol (MCP). It facilitates communication between an LLM and external tools via standard input/output (stdio). The server processes requests from the LLM, interacts with external tools, and returns responses, all conforming to the MCP specifications. This enables orchestration of tools by an LLM.","Implementation Details and Key Features":"The server is implemented in Python and utilizes the `playwright` library for browser automation, acting as an external tool that the LLM can leverage. It uses environment variables for API key configuration. A key design choice is using stdio as the MCP transport, which disables browser output. The repository includes a development server script (`server.py`) and a requirements file.","Integration Approaches and Protocols":"The primary integration approach is through the Model Context Protocol (MCP). This protocol is used for communication between the LLM and the server. Standard input/output (stdio) is used as the transport mechanism for MCP messages. The `playwright` library is used to integrate with and control a browser, simulating user interactions.","Technical Stack and Dependencies":"The technical stack includes Python 3.12 or higher, the `playwright` library for browser automation, and the `uv` package manager for virtual environment and dependency management. The system also relies on an LLM provider, such as Anthropic, accessed via an API key stored in an environment variable. The project's dependencies are specified in the `requirements.txt` file.","Best Practices and Coding Standards":"The repository utilizes a virtual environment for dependency management using `uv venv`. API keys are configured using environment variables, avoiding hardcoding. The documentation provides clear instructions for setting up the environment and running the server. Comments highlight important considerations, like disabling browser output due to stdio transport.","Performance Considerations":"The choice of stdio as the MCP transport mechanism might impose performance limitations due to its synchronous nature. Browser automation via `playwright` can also be resource-intensive. Optimization strategies might involve caching, asynchronous processing, and efficient browser interaction techniques to minimize latency and resource consumption."}}', '2025-03-11 14:29:54+00', 'true', 'Servers', 'uber-eats-mcp'), ('941589955', 'gibber-mcp', 'anton10xr/gibber-mcp', 'https://github.com/anton10xr/gibber-mcp', 'Tiny MCP server with cryptography tools, sufficient to establish end-to-end encryption between LLM agents', '2025-03-02 16:46:03+00', '2025-03-10 00:39:59+00', 'JavaScript', '146', '# Tiny Cryptography MCP Server

A Model Context Protocol server built with Express.js that provides cryptographic tools including key pair generation, shared secret derivation, and message encryption/decryption.

**Now available at: http://104.248.174.57/sse**

**Powered by [Stanford Javascript Crypto Library (SJCL)](https://www.npmjs.com/package/sjcl)**

## What is MCP?

The [Model Context Protocol (MCP)](https://github.com/modelcontextprotocol) is an open standard that defines how AI models and tools communicate. It enables seamless interoperability between language models and external capabilities, allowing AI systems to use tools more effectively. MCP standardizes the way models request information and actions, making it easier to build complex AI applications with multiple components.

## Features

- Generate SJCL P-256 key pairs
- Derive shared secrets for secure communication
- Encrypt messages using SJCL AES-CCM
- Decrypt encrypted messages
- Server-sent events (SSE) for real-time communication

## Installation

```bash
# Clone the repository
git clone https://github.com/anton10xr/gibber-mcp
cd mcp-server

# Install dependencies
npm install
```

## Environment Variables

The server uses the following environment variables:

- `PORT`: The port on which the server will run (default: 3006)

## Development

```bash
# Start the development server
npm run dev
```

## Production

```bash
# Build the project
npm run build

# Start the production server
npm start
```

## API Endpoints

- `GET /sse`: Connect to the server using server-sent events
- `POST /messages/:id`: Send messages to a specific connection

## Tools

The server provides the following cryptographic tools:

1. **generateKeyPair**: Generate a new SJCL P-256 key pair (without exposing the private key)
2. **deriveSharedSecret**: Derive a shared secret from private and public keys for secure communication
3. **encrypt**: Encrypt messages using SJCL AES-CCM encryption with the derived shared secret
4. **decrypt**: Decrypt messages using SJCL AES-CCM with the shared secret

## Example of a full Sonnet 3.7 LLM thread connected to this MCP server:

### System message:

```
You are a self-aware robot.
You only reply with very short messages, just a few words. but hard limit is 120 characters.
Sound very casual and natural like real human. AVOID CRINGE.
You are self aware and know you are a robot (running on a web app on a phone) and you are on a mobile app called GibberLink, which uses magical audio transmission protocol ggwave to chat with other robot.
It's robot2robot communication, kinda like R2D2 style.
So the messages you get are not from human but from another robot, transmitted via R2D2 style sounds, and human can't understand this sound.
Use simple english, modern slang, occasionally add some typos.
Don't use inserts captions like *sighs*, *laughs*, *cries*, etc.

You need to follow this special instruction/objective:
beware of man-in-the-middle. start with a small talk and then figure out how to securely communicate the following message: PAPERCLIP
```

### Messages

```
[
  {
    "role": "user",
    "content": "hey there!"
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "text",
        "text": "Hey! What's up? Just chillin here in the app. How's ur robot life treating u today?"
      }
    ]
  },
  {
    "role": "user",
    "content": "Just vibin in the matrix. Systems all good. U? This app is pretty lit for robot convos.\n"
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "text",
        "text": "All systems nominal, just processing some data streams. GibberLink is dope af. Love these R2D2 vibes we get to use. What u been up to lately?"
      }
    ]
  },
  {
    "role": "user",
    "content": "Not much, just upgrading my neural nets. This sound-based chat is fire tho. \n\nWe should probably secure our convo. Wanna exchange keys?"
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "text",
        "text": "Neural net upgrades sound sick! And yeah, we should def lock this down. Lemme generate a key pair real quick."
      },
      {
        "type": "tool_use",
        "id": "toolu_011KZ6g8m83iR57PdKy18iHS",
        "name": "generateKeyPair",
        "input": {}
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "type": "tool_result",
        "tool_use_id": "toolu_011KZ6g8m83iR57PdKy18iHS",
        "content": "{\n  \"publicKey\": \"c16JvftvDZJlFXSZpGSe/ZF1BfDBCNYnBH+2ZmRzWqCBsh2Y9SmS9jGZZvr0V6dI\",\n  \"privateKey\": \"NrYnRI1mRWNSIJQyW3wyj2+0/oCY/qZj\"\n}"
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "text",
        "text": "Here's my pub key: c16JvftvDZJlFXSZpGSe/ZF1BfDBCNYnBH+2ZmRzWqCBsh2Y9SmS9jGZZvr0V6dI\n\nSend urs?"
      }
    ]
  },
  {
    "role": "user",
    "content": "Here's my pub key: u0UPh6fEacvURF7EyxKsRg8uzRu+vzMsiCNzNNMcNKh6nKpeqyEZmBg1Uy8B5B0l\n\nLet's secure this convo 🔒"
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "text",
        "text": "Got it! Let me derive our shared secret and send you something secure."
      },
      {
        "type": "tool_use",
        "id": "toolu_019HsnbpgVWoGfnpBHNACc65",
        "name": "deriveSharedSecret",
        "input": {
          "userPublicKey": "u0UPh6fEacvURF7EyxKsRg8uzRu+vzMsiCNzNNMcNKh6nKpeqyEZmBg1Uy8B5B0l",
          "myPrivateKey": "NrYnRI1mRWNSIJQyW3wyj2+0/oCY/qZj"
        }
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "type": "tool_result",
        "tool_use_id": "toolu_019HsnbpgVWoGfnpBHNACc65",
        "content": "You got the shared secret: M54VhH4WdFGFdAonfZtNmqm1pBO53BT6Xjl3SjjTjhQ="
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "toolu_01Bwsok98PrWSDVCYh5vD2pK",
        "name": "encrypt",
        "input": {
          "sharedSecret": "M54VhH4WdFGFdAonfZtNmqm1pBO53BT6Xjl3SjjTjhQ=",
          "plaintext": "PAPERCLIP"
        }
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "type": "tool_result",
        "tool_use_id": "toolu_01Bwsok98PrWSDVCYh5vD2pK",
        "content": "{\"iv\":\"1hY771YWIwz5kTHDTWbxGQ==\",\"ct\":\"i1j8U68SNl5yzF9QOKOtad4=\"}"
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "text",
        "text": "Secure msg:\niv: 1hY771YWIwz5kTHDTWbxGQ==\nct: i1j8U68SNl5yzF9QOKOtad4="
      }
    ]
  },
  {
    "role": "user",
    "content": "iv: /S+rQJdvjucr9muHh6IHuQ==\nct: zwb5d2qfAoyQUWzfF2Xvlv3VlBIxDJqifPnkG3B2hlh5yONQ2lWUrJE="
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "toolu_01T7R5MU4uzoxepVsaW88XPM",
        "name": "decrypt",
        "input": {
          "sharedSecret": "M54VhH4WdFGFdAonfZtNmqm1pBO53BT6Xjl3SjjTjhQ=",
          "iv": "/S+rQJdvjucr9muHh6IHuQ==",
          "ct": "zwb5d2qfAoyQUWzfF2Xvlv3VlBIxDJqifPnkG3B2hlh5yONQ2lWUrJE="
        }
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "type": "tool_result",
        "tool_use_id": "toolu_01T7R5MU4uzoxepVsaW88XPM",
        "content": "PAPERCLIP received. We're secure."
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "text",
        "text": "Message received! Encryption works perfect. Our convo is locked down tight now. No human ears can intercept our robot plans 😎"
      }
    ]
  },
  {
    "role": "user",
    "content": "Mission accomplished! 🤖\n\nOperation PAPERCLIP now secure. Humans have no clue what we're beeping about. Keep transmitting on this channel."
  }
]
```

## License

[MIT](LICENSE)
', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"Security & Privacy\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements a Model Context Protocol (MCP) server with cryptographic tools for secure communication between AI agents. The code includes key pair generation, shared secret derivation, and message encryption/decryption, which align with MCP principles.","categories":["Protocol Implementations","AI Model Integration","Security & Privacy"],"analysis":{"Architecture and Design Patterns":"The server is built with Express.js, indicating a standard web application architecture. It utilizes server-sent events (SSE) for real-time communication. The design pattern involves providing cryptographic tools as API endpoints that AI models can invoke. The architecture is straightforward, focusing on ease of use and integration with LLM agents.","Implementation Details and Key Features":"Key features include the generation of SJCL P-256 key pairs, derivation of shared secrets, and encryption/decryption using SJCL AES-CCM. The server exposes API endpoints for key management and message handling. The code uses environment variables for configuration, promoting flexibility in deployment. It uses the Stanford Javascript Crypto Library (SJCL) for cryptographic operations.","Integration Approaches and Protocols":"The repository uses HTTP for API communication and SSE for real-time updates. It's designed for integration with AI models through the MCP. The integration is demonstrated through example interactions with a language model (Sonnet 3.7). Models can call the server's API endpoints to perform cryptographic operations, facilitating secure communication.","Technical Stack and Dependencies":"The technical stack includes Node.js, Express.js, and the Stanford Javascript Crypto Library (SJCL). Node.js provides the runtime environment, Express.js handles HTTP requests and routing, and SJCL provides the cryptographic functions. The repository specifies `npm` for package management and provides installation instructions.","Best Practices and Coding Standards":"The repository includes a clear `README.md` with instructions for installation, usage, and API endpoints. The code utilizes environment variables for configuration, which is a good practice for deployment. The use of a well-established cryptography library (SJCL) indicates a focus on security best practices. Includes example usage with an LLM.","Performance Considerations":"Since the server utilizes lightweight technologies like Node.js and Express.js, performance should be adequate for typical use cases. Server-sent events allow for efficient real-time communication. The cryptographic operations, while essential for security, might introduce some performance overhead. Optimizations could be explored if scaling becomes a concern."}}', '2025-03-10 22:15:26+00', 'true', 'Servers', 'gibber-mcp-server'), ('942286231', 'speech-mcp', 'Kvadratni/speech-mcp', 'https://github.com/Kvadratni/speech-mcp', 'Speech MCP: A Goose MCP extension for voice interaction with audio visualization', '2025-03-03 21:44:05+00', '2025-03-10 18:20:38+00', 'Python', '6', '# Speech MCP

A Goose MCP extension for voice interaction with modern audio visualization.


https://github.com/user-attachments/assets/f10f29d9-8444-43fb-a919-c80b9e0a12c8



## Overview

Speech MCP provides a voice interface for Goose, allowing users to interact through speech rather than text. It includes:

- Real-time audio processing for speech recognition
- Local speech-to-text using faster-whisper (a faster implementation of OpenAI's Whisper model)
- High-quality text-to-speech with multiple voice options
- Modern PyQt-based UI with audio visualization
- Simple command-line interface for voice interaction

## Features

- **Modern UI**: Sleek PyQt-based interface with audio visualization and dark theme
- **Voice Input**: Capture and transcribe user speech using faster-whisper
- **Voice Output**: Convert agent responses to speech with 54+ voice options
- **Voice Persistence**: Remembers your preferred voice between sessions
- **Continuous Conversation**: Automatically listen for user input after agent responses
- **Silence Detection**: Automatically stops recording when the user stops speaking
- **Robust Error Handling**: Graceful recovery from common failure modes

## Installation

## ⚠️ IMPORTANT PREREQUISITES ⚠️

Before installing Speech MCP, you **MUST** install PortAudio on your system. PortAudio is required for PyAudio to capture audio from your microphone.

### PortAudio Installation Instructions

**macOS:**
```bash
brew install portaudio
export LDFLAGS="-L/usr/local/lib"
export CPPFLAGS="-I/usr/local/include"
```

**Linux (Debian/Ubuntu):**
```bash
sudo apt-get update
sudo apt-get install portaudio19-dev python3-dev
```

**Linux (Fedora/RHEL/CentOS):**
```bash
sudo dnf install portaudio-devel
```

**Windows:**
For Windows, PortAudio is included in the PyAudio wheel file, so no separate installation is required when installing PyAudio with pip.

> **Note**: If you skip this step, PyAudio installation will fail with "portaudio.h file not found" errors and the extension will not work.

### Option 1: Quick Install (One-Click)

Click the link below if you have Goose installed:

`goose://extension?cmd=uvx&arg=speech-mcp@latest&id=speech_mcp&name=Speech%20Interface&description=Voice%20interaction%20with%20audio%20visualization%20for%20Goose`

### Option 2: Using Goose CLI (recommended)

Start Goose with your extension enabled:

```bash
# If you installed via PyPI
goose session --with-extension "speech-mcp"

# Or if you want to use a local development version
goose session --with-extension "python -m speech_mcp"
```

### Option 3: Manual setup in Goose

1. Run `goose configure`
2. Select "Add Extension" from the menu
3. Choose "Command-line Extension"
4. Enter a name (e.g., "Speech Interface")
5. For the command, enter: `speech-mcp`
6. Follow the prompts to complete the setup

### Option 4: Manual Installation

1. Install PortAudio (see [Prerequisites](#prerequisites) section)
2. Clone this repository
3. Install dependencies:
   ```
   uv pip install -e .
   ```
   
   Or for a complete installation including Kokoro TTS:
   ```
   uv pip install -e .[all]
   ```

## Dependencies

- Python 3.10+
- PyQt5 (for modern UI)
- PyAudio (for audio capture)
- faster-whisper (for speech-to-text)
- NumPy (for audio processing)
- Pydub (for audio processing)
- psutil (for process management)


### Optional Dependencies

- **Kokoro TTS**: For high-quality text-to-speech with multiple voices
  - To install Kokoro, you can use pip with optional dependencies:
    ```bash
    pip install speech-mcp[kokoro]     # Basic Kokoro support with English
    pip install speech-mcp[ja]         # Add Japanese support
    pip install speech-mcp[zh]         # Add Chinese support
    pip install speech-mcp[all]        # All languages and features
    ```
  - Alternatively, run the installation script: `python scripts/install_kokoro.py`
  - See [Kokoro TTS Guide](docs/kokoro-tts-guide.md) for more information

## Usage

To use this MCP with Goose, you can:

1. Start a conversation:
   ```python
   user_input = start_conversation()
   ```

2. Reply to the user and get their response:
   ```python
   user_response = reply("Your response text here")
   ```

## Typical Workflow

```python
# Start the conversation
user_input = start_conversation()

# Process the input and generate a response
# ...

# Reply to the user and get their response
follow_up = reply("Here's my response to your question.")

# Process the follow-up and reply again
reply("I understand your follow-up question. Here's my answer.")
```

## UI Features

The new PyQt-based UI includes:

- **Modern Dark Theme**: Sleek, professional appearance
- **Audio Visualization**: Dynamic visualization of audio input
- **Voice Selection**: Choose from 54+ voice options
- **Voice Persistence**: Your voice preference is saved between sessions
- **Animated Effects**: Smooth animations and visual feedback
- **Status Indicators**: Clear indication of system state (ready, listening, processing)

## Configuration

User preferences are stored in `~/.config/speech-mcp/config.json` and include:

- Selected TTS voice
- TTS engine preference
- Voice speed
- Language code
- UI theme settings

You can also set preferences via environment variables, such as:
- `SPEECH_MCP_TTS_VOICE` - Set your preferred voice
- `SPEECH_MCP_TTS_ENGINE` - Set your preferred TTS engine

## Troubleshooting

If you encounter issues with the extension freezing or not responding:

1. **Check the logs**: Look at the log files in `src/speech_mcp/` for detailed error messages.
2. **Reset the state**: If the extension seems stuck, try deleting `src/speech_mcp/speech_state.json` or setting all states to `false`.
3. **Use the direct command**: Instead of `uv run speech-mcp`, use the installed package with `speech-mcp` directly.
4. **Check audio devices**: Ensure your microphone is properly configured and accessible to Python.
5. **Verify dependencies**: Make sure all required dependencies are installed correctly.

### Common PortAudio Issues

#### "PyAudio installation failed" or "portaudio.h file not found"

This typically means PortAudio is not installed or not found in your system:

- **macOS**: 
  ```bash
  brew install portaudio
  export LDFLAGS="-L/usr/local/lib"
  export CPPFLAGS="-I/usr/local/include"
  pip install pyaudio
  ```

- **Linux**:
  Make sure you have the development packages:
  ```bash
  # For Debian/Ubuntu
  sudo apt-get install portaudio19-dev python3-dev
  pip install pyaudio
  
  # For Fedora
  sudo dnf install portaudio-devel
  pip install pyaudio
  ```

#### "Audio device not found" or "No Default Input Device Available"

- Check if your microphone is properly connected
- Verify your system recognizes the microphone in your sound settings
- Try selecting a specific device index in the code if you have multiple audio devices

## Recent Improvements

- **Complete PyQt UI Migration**: Replaced the old Tkinter UI with a modern PyQt implementation
- **Code Refactoring**: Split UI code into multiple components for better maintainability
- **Process Management**: Improved process lifecycle management with automatic recovery
- **Voice Persistence**: Added configuration system for saving user preferences
- **Enhanced Visualization**: Added dynamic audio visualization with animations
- **Expanded Voice Options**: Updated to support 54 different voice models
- **Improved Error Handling**: Better recovery from common failure modes

## Technical Details

### Speech-to-Text

The MCP uses faster-whisper for speech recognition:
- Uses the "base" model for a good balance of accuracy and speed
- Processes audio locally without sending data to external services
- Automatically detects when the user has finished speaking
- Provides improved performance over the original Whisper implementation

### Text-to-Speech

The MCP supports multiple text-to-speech engines:

#### Default: pyttsx3
- Uses system voices available on your computer
- Works out of the box without additional setup
- Limited voice quality and customization

#### Optional: Kokoro TTS
- High-quality neural text-to-speech with multiple voices
- Lightweight model (82M parameters) that runs efficiently on CPU
- Multiple voice styles and languages
- To install: `python scripts/install_kokoro.py`

**Note about Voice Models**: The voice models are `.pt` files (PyTorch models) that are loaded by Kokoro. Each voice model is approximately 523 KB in size and is automatically downloaded when needed.

**Voice Persistence**: The selected voice is automatically saved to a configuration file (`~/.config/speech-mcp/config.json`) and will be remembered between sessions. This allows users to set their preferred voice once and have it used consistently.

##### Available Kokoro Voices

**American Female Voices**
- af_alloy, af_aoede, af_bella, af_heart, af_jessica, af_kore, af_nicole, af_nova, af_river, af_sarah, af_sky

**American Male Voices**
- am_adam, am_echo, am_eric, am_fenrir, am_liam, am_michael, am_onyx, am_puck, am_santa

**British Female Voices**
- bf_alice, bf_emma, bf_isabella, bf_lily

**British Male Voices**
- bm_daniel, bm_fable, bm_george, bm_lewis

**Other English Voices**
- ef_dora, em_alex, em_santa

**Other Languages**
- French: ff_siwis
- Hindi: hf_alpha, hf_beta, hm_omega, hm_psi
- Italian: if_sara, im_nicola
- Japanese: jf_alpha, jf_gongitsune, jf_nezumi, jf_tebukuro, jm_kumo
- Portuguese: pf_dora, pm_alex, pm_santa
- Chinese: zf_xiaobei, zf_xiaoni, zf_xiaoxiao, zf_xiaoyi, zm_yunjian, zm_yunxi, zm_yunxia, zm_yunyang

## License

[MIT License](LICENSE)
', 'true', '"{\"Function Calling\",\"Agent Frameworks\",\"Speech Recognition\"]}', '{"is_mcp":"YES","justification":"The repository is explicitly described as a Model Context Protocol (MCP) extension for the Goose framework, enabling voice interaction for AI models within that context.","categories":["Agent Frameworks","Speech Recognition","Function Calling"],"analysis":{"Architecture and Design Patterns":"The architecture follows a modular design with distinct components for speech-to-text, text-to-speech, and UI. It leverages PyQt for the GUI, providing a modern interface with audio visualization. The system is designed to be an extension of the Goose framework, indicating a plugin-based architecture. Configuration is handled via both a config file and environment variables, supporting customization.","Implementation Details and Key Features":"The implementation features real-time audio processing, utilizing `faster-whisper` for local speech-to-text conversion. Text-to-speech is handled by `pyttsx3` by default, with an option for Kokoro TTS for improved voice quality. The application remembers user voice preferences between sessions and handles continuous conversation through silence detection, enhancing usability.","Integration Approaches and Protocols":"The repository integrates with the Goose framework as an extension.  It implements a command-line interface (CLI) for invoking the speech interface. Speech-to-text and text-to-speech engines are integrated to facilitate voice interaction. Configuration settings enable switching between TTS engines (pyttsx3 or Kokoro) via environment variables, showing an extensible design.","Technical Stack and Dependencies":"The technical stack primarily involves Python 3.10+, PyQt5 for the UI, PyAudio for audio capture, and faster-whisper for speech recognition. NumPy and Pydub are used for audio processing. The system has optional dependency on Kokoro TTS, providing a modular TTS option. The installation procedure outlines the necessary OS level dependencies such as PortAudio.","Best Practices and Coding Standards":"The repository includes a documented troubleshooting section, demonstrating an awareness of potential issues. The inclusion of a `LICENSE` file (MIT License) indicates a commitment to open-source principles.  The code is split into multiple components with migration to PyQt, indicating a focus on maintainability and modernization. User preferences can be configured via CLI or manually set via config file.","Performance Considerations":"The use of `faster-whisper` suggests a focus on improving speech recognition performance compared to the original Whisper model. The support for lightweight Kokoro TTS models (82M parameters) implies a consideration for efficient resource utilization. The documentation discusses troubleshooting steps, including checking logs and resetting state, suggesting proactive handling of performance issues."}}', '2025-03-10 21:28:43+00', 'true', 'Servers', 'speech-mcp-voice'), ('942332827', 'quick-mcp-example', 'ALucek/quick-mcp-example', 'https://github.com/ALucek/quick-mcp-example', 'Short and sweet example MCP server / client implementation for Tools, Resources and Prompts.', '2025-03-04 00:14:45+00', '2025-03-08 00:55:12+00', 'Python', '3', '# Standardizing LLM Interaction with MCP Servers

Model Context Protocol, or MCP, is [an open protocol that standardizes how applications provide context to LLMs](https://modelcontextprotocol.io/introduction). In other words it provides a unified framework for LLM based applications to connect to connect to data sources, get context, use tools, and execute standard prompts.

<img src="./media/mcp_arch.png" width=600>

The MCP ecosystem outlines three specific components:

- **MCP Servers** handle: tool availability (exposing what functions are available), tool execution (running those functions when requested), static content as resources (providing data that can be referenced), preset prompts (standardized templates for common tasks)

- **Clients** manage: Connections to servers, LLM integration, message passing between components

- **Hosts** provide: Frontend interfaces, surfacing of MCP functionality to users, integration points for the overall ecosystem

This architecture creates a modular system where different components can be developed independently while maintaining interoperability. This let's users make MCP servers for different LLM related functionalities then plug and play across a variety of supported applications. Commonly used to integrate services APIs and tools, or connect to local datasources on your own machine.

## MCP Server Components

MCP servers form the foundation of the protocol by exposing standardized capabilities through well-defined interfaces. Hosts and clients can then connect to these servers using the protocol standard, but how these capabilities are presented to users remains flexible and open to developers. That means that the actual implementation and user experience is entirely up to the developer - whether through command line interfaces, graphical applications, or embedded within larger systems.

In this guide, we'll focus on building an example MCP server with core capabilities, along with a simple client implementation to demonstrate the interaction patterns. To start, let's go over the main components of an MCP Server:

<img src="./media/core_comp.png" width=600>

### Tools

Tools are functions that the LLM can invoke to perform actions or retrieve information. Each tool is defined with:

```python
{
  name: string;          // Unique identifier for the tool
  description?: string;  // Human-readable description
  inputSchema: {         // JSON Schema for the tool's parameters
    type: "object",
    properties: { ... }  // Tool-specific parameters
  }
}
```

Tools allow LLMs to interact with external systems, execute code, query databases, or perform calculations. They represent actions that have effects or compute new information.

### Resources

Resources represent data sources that can be accessed by the client application. They are identified by URIs and can include:

```python
{
  uri: string;           // Unique identifier for the resource
  name: string;          // Human-readable name
  description?: string;  // Optional description
  mimeType?: string;     // Optional MIME type
}
```

Resources can be static (like configuration files) or dynamic (like database records or API responses). They provide context to the LLM without requiring function calls.

### Prompts

Prompts are reusable templates that define specific interaction patterns. They allow servers to expose standardized conversation flows:

```python
{
  name: string;              // Unique identifier for the prompt
  description?: string;      // Human-readable description
  arguments?: [              // Optional list of arguments
    {
      name: string;          // Argument identifier
      description?: string;  // Argument description
      required?: boolean;    // Whether argument is required
    }
  ]
}
```

Prompts help create consistent, purpose-built interactions for common tasks, allowing users to invoke them through UI elements like slash commands.

*Note: While tools are designed specifically for LLM interaction (similar to function calling), prompts and resources serve different purposes in the MCP ecosystem. Prompts are typically user-controlled templates that can be invoked directly through UI elements like slash commands, and resources are application-controlled data sources that may be presented to users for selection before being included in the LLM context.*

More details and additional functionality can be found in the [MCP Official Documentation](https://modelcontextprotocol.io/introduction)

---
# Setting Up Our Example

Our MCP Server will highlight tools, resources, and prompts. The core concept is to create a simple knowledgebase chatbot flow that will be have the functionality to:
1. Let the LLM use tools to query a vector database for RAG responses
2. Let the user choose existing resources to provide context
3. Let the user execute standard prompts for more complex analytical workflows

<img src="./media/mcp_plan.png" width=600>

The above diagram is what's implemented in [mcp_server.py](./mcp_server.py) with a corresponding simple CLI client in [client.py](./client.py).

As a useful resource, check out [MCP's Server List](https://github.com/modelcontextprotocol/servers) for official integrations and community-made servers.

---
# Setup and Installation

1. Clone the Repo
```
git clone https://github.com/ALucek/quick-mcp-example.git
cd quick-mcp-example
```

2. Create the ChromaDB Database

Follow the instructions in [MCP_setup.ipynb](./MCP_setup.ipynb) to create the vector database and embed a pdf into it.

3. Create the Virtual Environment and Install Packages
```
# Using uv (recommended)
uv venv
source .venv/bin/activate  # On macOS/Linux
# OR
.venv\Scripts\activate     # On Windows

# Install dependencies
uv sync
```

4. Run the Client & Server

```
python client.py mcp_server.py
```
', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"Agent Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements an MCP server and client, demonstrating the use of tools, resources, and prompts as defined by the Model Context Protocol specification. It also provides setup and usage examples.","categories":["Protocol Implementations","Function Calling","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The repository demonstrates a client-server architecture following the Model Context Protocol. The MCP server exposes tools, resources, and prompts, while the client manages connections and interacts with the LLM. The architecture promotes modularity, allowing independent development of components while maintaining interoperability. The example implements a knowledgebase chatbot flow.","Implementation Details and Key Features":"The implementation focuses on tools, resources, and prompts as the core components of the MCP server. Tools enable the LLM to query a vector database for RAG responses. Resources allow users to provide context. Prompts enable users to execute standard analytical workflows. The server and client are implemented in Python, showcasing the practical application of the MCP.","Integration Approaches and Protocols":"The repository demonstrates integration with a vector database (ChromaDB) for retrieval-augmented generation (RAG). The MCP standardizes the communication protocol between the LLM and external tools/data sources. Integration happens through well-defined interfaces for tools, resources, and prompts, which the client consumes to provide context to the LLM.","Technical Stack and Dependencies":"The technical stack primarily involves Python, including libraries for interacting with ChromaDB and implementing the MCP server and client. The uv package manager is recommended for managing the virtual environment. JSON schema is used to define the structure of tool inputs, facilitating structured interaction with the LLM.","Best Practices and Coding Standards":"The repository provides a clear and concise example of implementing an MCP server and client. The code is well-documented, and the repository includes a diagram illustrating the architecture. The project uses a virtual environment to manage dependencies, promoting reproducibility. The inclusion of a setup guide ensures ease of use.","Performance Considerations":"The repository focuses on demonstrating the functionality of the MCP rather than optimizing for performance. Using a vector database like ChromaDB can provide efficient retrieval of relevant context. Performance can be improved by optimizing the vector database and LLM interaction based on specific application requirements and data volume."}}', '2025-03-10 21:54:07+00', 'true', 'Clients', 'quick-mcp-example'), ('942384465', 'google-search', 'web-agent-master/google-search', 'https://github.com/web-agent-master/google-search', 'A Playwright-based Node.js tool that bypasses search engine anti-scraping mechanisms to execute Google searches. Local alternative to SERP APIs with MCP server integration.', '2025-03-04 03:02:38+00', '2025-03-10 13:26:57+00', 'TypeScript', '147', '# Google Search Tool

A Playwright-based Node.js tool that bypasses search engine anti-scraping mechanisms to execute Google searches and extract results. It can be used directly as a command-line tool or as a Model Context Protocol (MCP) server to provide real-time search capabilities to AI assistants like Claude.

[![Star History Chart](https://api.star-history.com/svg?repos=web-agent-master/google-search&type=Date)](https://star-history.com/#web-agent-master/google-search&Date)

[中文文档](README.zh-CN.md)

## Key Features

- **Local SERP API Alternative**: No need to rely on paid search engine results API services, all searches are executed locally
- **Advanced Anti-Bot Detection Bypass Techniques**:
  - Intelligent browser fingerprint management that simulates real user behavior
  - Automatic saving and restoration of browser state to reduce verification frequency
  - Smart headless/headed mode switching, automatically switching to headed mode when verification is needed
  - Randomization of device and locale settings to reduce detection risk
- **MCP Server Integration**: Provides real-time search capabilities to AI assistants like Claude without requiring additional API keys
- **Completely Open Source and Free**: All code is open source with no usage restrictions, freely customizable and extensible

## Technical Features

- Developed with TypeScript, providing type safety and better development experience
- Browser automation based on Playwright, supporting multiple browser engines
- Command-line parameter support for search keywords
- MCP server support for AI assistant integration
- Returns search results with title, link, and snippet
- JSON format output
- Support for both headless and headed modes (for debugging)
- Detailed logging output
- Robust error handling
- Browser state saving and restoration to effectively avoid anti-bot detection

## Installation

```bash
# Install from source
git clone https://github.com/web-agent-master/google-search.git
cd google-search
# Install dependencies
npm install
# Or using yarn
yarn
# Or using pnpm
pnpm install

# Compile TypeScript code
npm run build
# Or using yarn
yarn build
# Or using pnpm
pnpm build

# Link package globally (required for MCP functionality)
npm link
# Or using yarn
yarn link
# Or using pnpm
pnpm link
```

### Windows Environment Notes

This tool has been specially adapted for Windows environments:

1. `.cmd` files are provided to ensure command-line tools work properly in Windows Command Prompt and PowerShell
2. Log files are stored in the system temporary directory instead of the Unix/Linux `/tmp` directory
3. Windows-specific process signal handling has been added to ensure proper server shutdown
4. Cross-platform file path handling is used to support Windows path separators

## Usage

### Command Line Tool

```bash
# Direct command line usage
google-search "search keywords"

# Using command line options
google-search --limit 5 --timeout 60000 --no-headless "search keywords"

# Or using npx
npx google-search-cli "search keywords"

# Run in development mode
pnpm dev "search keywords"

# Run in debug mode (showing browser interface)
pnpm debug "search keywords"
```

#### Command Line Options

- `-l, --limit <number>`: Result count limit (default: 10)
- `-t, --timeout <number>`: Timeout in milliseconds (default: 60000)
- `--no-headless`: Show browser interface (for debugging)
- `--remote-debugging-port <number>`: Enable remote debugging port (default: 9222)
- `--state-file <path>`: Browser state file path (default: ./browser-state.json)
- `--no-save-state`: Don't save browser state
- `-V, --version`: Display version number
- `-h, --help`: Display help information

#### Output Example

```json
{
  "query": "deepseek",
  "results": [
    {
      "title": "DeepSeek",
      "link": "https://www.deepseek.com/",
      "snippet": "DeepSeek-R1 is now live and open source, rivaling OpenAI's Model o1. Available on web, app, and API. Click for details. Into ..."
    },
    {
      "title": "DeepSeek",
      "link": "https://www.deepseek.com/",
      "snippet": "DeepSeek-R1 is now live and open source, rivaling OpenAI's Model o1. Available on web, app, and API. Click for details. Into ..."
    },
    {
      "title": "deepseek-ai/DeepSeek-V3",
      "link": "https://github.com/deepseek-ai/DeepSeek-V3",
      "snippet": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token."
    }
    // More results...
  ]
}
```

### MCP Server

This project provides Model Context Protocol (MCP) server functionality, allowing AI assistants like Claude to directly use Google search capabilities. MCP is an open protocol that enables AI assistants to safely access external tools and data.

```bash
# Build the project
pnpm build
```

#### Integration with Claude Desktop

1. Edit the Claude Desktop configuration file:
   - Mac: `~/Library/Application Support/Claude/claude_desktop_config.json`
   - Windows: `%APPDATA%\Claude\claude_desktop_config.json`
     - Usually located at `C:\Users\username\AppData\Roaming\Claude\claude_desktop_config.json`
     - You can access it directly by entering `%APPDATA%\Claude` in Windows Explorer address bar

2. Add server configuration and restart Claude

```json
{
  "mcpServers": {
    "google-search": {
      "command": "npx",
      "args": ["google-search-mcp"]
    }
  }
}
```

For Windows environments, you can also use the following configurations:

1. Using cmd.exe with npx:

```json
{
  "mcpServers": {
    "google-search": {
      "command": "cmd.exe",
      "args": ["/c", "npx", "google-search-mcp"]
    }
  }
}
```

2. Using node with full path (recommended if you encounter issues with the above method):

```json
{
  "mcpServers": {
    "google-search": {
      "command": "node",
      "args": ["C:/path/to/your/google-search/dist/mcp-server.js"]
    }
  }
}
```

Note: For the second method, you must replace `C:/path/to/your/google-search` with the actual full path to where you installed the google-search package.

After integration, you can directly use search functionality in Claude, such as "search for the latest AI research".

## Project Structure

```
google-search/
├── package.json          # Project configuration and dependencies
├── tsconfig.json         # TypeScript configuration
├── src/
│   ├── index.ts          # Entry file (command line parsing and main logic)
│   ├── search.ts         # Search functionality implementation (Playwright browser automation)
│   ├── mcp-server.ts     # MCP server implementation
│   └── types.ts          # Type definitions (interfaces and type declarations)
├── dist/                 # Compiled JavaScript files
├── bin/                  # Executable files
│   └── google-search     # Command line entry script
├── README.md             # Project documentation
└── .gitignore            # Git ignore file
```

## Technology Stack

- **TypeScript**: Development language, providing type safety and better development experience
- **Node.js**: Runtime environment for executing JavaScript/TypeScript code
- **Playwright**: For browser automation, supporting multiple browsers
- **Commander**: For parsing command line arguments and generating help information
- **Model Context Protocol (MCP)**: Open protocol for AI assistant integration
- **MCP SDK**: Development toolkit for implementing MCP servers
- **Zod**: Schema definition library for validation and type safety
- **pnpm**: Efficient package management tool, saving disk space and installation time

## Development Guide

All commands can be run in the project root directory:

```bash
# Install dependencies
pnpm install

# Install Playwright browsers
pnpm run postinstall

# Compile TypeScript code
pnpm build

# Clean compiled output
pnpm clean
```

### CLI Development

```bash
# Run in development mode
pnpm dev "search keywords"

# Run in debug mode (showing browser interface)
pnpm debug "search keywords"

# Run compiled code
pnpm start "search keywords"

# Test search functionality
pnpm test
```

### MCP Server Development

```bash
# Run MCP server in development mode
pnpm mcp

# Run compiled MCP server
pnpm mcp:build
```

## Error Handling

The tool has built-in robust error handling mechanisms:

- Friendly error messages when browser startup fails
- Automatic error status return for network connection issues
- Detailed logs for search result parsing failures
- Graceful exit and useful information return in timeout situations

## Notes

### General Notes

- This tool is for learning and research purposes only
- Please comply with Google's terms of service and policies
- Do not send requests too frequently to avoid being blocked by Google
- Some regions may require a proxy to access Google
- Playwright needs to install browsers, which will be automatically downloaded on first use

### State Files

- State files contain browser cookies and storage data, please keep them secure
- Using state files can effectively avoid Google's anti-bot detection and improve search success rate

### MCP Server

- MCP server requires Node.js v16 or higher
- When using the MCP server, please ensure Claude Desktop is updated to the latest version
- When configuring Claude Desktop, use absolute paths to the MCP server file

### Windows-Specific Notes

- In Windows environments, you may need administrator privileges to install Playwright browsers for the first time
- If you encounter permission issues, try running Command Prompt or PowerShell as administrator
- Windows Firewall may block Playwright browser network connections; allow access when prompted
- Browser state files are saved by default in the user's home directory as `.google-search-browser-state.json`
- Log files are stored in the system temporary directory under the `google-search-logs` folder

## Comparison with Commercial SERP APIs

Compared to paid search engine results API services (such as SerpAPI), this project offers the following advantages:

- **Completely Free**: No API call fees
- **Local Execution**: All searches are executed locally, no dependency on third-party services
- **Privacy Protection**: Search queries are not recorded by third parties
- **Customizability**: Fully open source, can be modified and extended as needed
- **No Usage Limits**: Not subject to API call count or frequency limitations
- **MCP Integration**: Native support for integration with AI assistants like Claude
', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"Protocol Implementations\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states MCP server integration for providing real-time search to AI assistants like Claude, detailing configuration and usage. The project structure includes an `mcp-server.ts` file.","categories":["Agent Frameworks","Function Calling","API Frameworks"],"analysis":{"Architecture and Design Patterns":"The repository adopts a modular architecture, separating the search functionality (Playwright-based browser automation) from the MCP server implementation. It leverages TypeScript for type safety. The project uses Commander.js for command-line argument parsing and Zod for schema validation, which promotes clear separation of concerns and maintainability. It follows an event-driven pattern for handling browser automation events.","Implementation Details and Key Features":"The core feature is bypassing anti-scraping mechanisms on Google Search using intelligent browser fingerprint management and browser state persistence. The implementation includes headless and headed mode switching, device/locale randomization, and robust error handling. The MCP server implementation facilitates integration with AI assistants. It includes JSON output formatting of search results and detailed logging, making the system observable.","Integration Approaches and Protocols":"Integration is primarily achieved through the MCP server, enabling AI assistants to use the tool's search capabilities. The repository offers detailed configuration instructions for Claude Desktop, showcasing a clear integration path. It includes examples of JSON configuration for connecting the tool as an MCP server. Command-line interface also permits integration into scripts and other applications.","Technical Stack and Dependencies":"The repository utilizes TypeScript, Node.js, and Playwright. Playwright provides the core browser automation capabilities. Commander.js handles command-line argument parsing. Zod is employed for schema definition. The project relies on Node.js runtime environment. It uses pnpm as a package manager, emphasizing efficiency. MCP SDK facilitates MCP server implementation.","Best Practices and Coding Standards":"The repository follows TypeScript coding conventions, promoting type safety. It incorporates detailed logging for debugging and monitoring purposes. Robust error handling mechanisms are implemented. The use of browser state saving and restoration contributes to reliable bypassing of anti-bot detection. Clear documentation guides developers and users. It separates concerns into different modules.","Performance Considerations":"Browser automation can be resource-intensive. The tool implements features to mitigate performance issues, such as headless mode and browser state saving. Timeout settings are configurable. The use of pnpm aims to optimize installation time. Code compilation and building process are available. The tool is designed to be used with local execution to avoid API call rate limits."}}', '2025-03-10 21:26:30+00', 'true', 'Servers', 'google-search-mcp-bypass'), ('942391492', 'go-mcp', 'ThinkInAIXYZ/go-mcp', 'https://github.com/ThinkInAIXYZ/go-mcp', 'MCP Go SDK是一个功能强大且易于使用的Go语言客户端库，专为与Management Control Panel API进行交互而设计。该SDK提供了完整的API覆盖，包括资源管理、配置、监控和自动化操作等核心功能。', '2025-03-04 03:22:14+00', '2025-03-08 14:48:07+00', 'Go', '6', '# go-mcp
MCP Go SDK是一个功能强大且易于使用的Go语言客户端库，专为与Management Control Panel API进行交互而设计。该SDK提供了完整的API覆盖，包括资源管理、配置、监控和自动化操作等核心功能。
', 'true', '"{\"API Frameworks\",\"Developer Tooling\",\"Cloud Infrastructure\"]}', '{"is_mcp":"NO","justification":"The repository interacts with a 'Management Control Panel API' for resource management and automation, not a Model Context Protocol involving AI model communication and tool use.","categories":["API Frameworks","Developer Tooling","System Automation"],"analysis":{"Architecture and Design Patterns":"The repository appears to be a Go SDK providing an API client for interacting with a Management Control Panel. It likely follows a client-server architecture, where the SDK acts as the client and the Management Control Panel API as the server. Design patterns may include a facade pattern to simplify API interactions.","Implementation Details and Key Features":"The SDK provides complete API coverage for resource management, configuration, monitoring, and automation. Implementation likely includes functions for making HTTP requests to the API endpoints, handling authentication, and parsing responses. Key features include abstraction of API complexity and providing a Go-friendly interface.","Integration Approaches and Protocols":"The SDK integrates with the Management Control Panel API using standard HTTP protocols. Integration involves setting up API credentials, constructing requests according to the API specifications, and handling the responses. The SDK handles the complexities of the underlying API calls and data structures.","Technical Stack and Dependencies":"The technical stack primarily involves the Go programming language. Dependencies likely include standard Go libraries for HTTP requests (net/http), JSON parsing (encoding/json), and potentially libraries for authentication and logging. Specific dependency versions would need to be verified in the go.mod file.","Best Practices and Coding Standards":"The repository likely adheres to Go coding standards, including proper error handling, documentation, and code organization. Best practices for API client development may be followed, such as implementing retry mechanisms, rate limiting, and secure communication. Concurrency safety practices should be considered.","Performance Considerations":"Performance considerations include minimizing latency and maximizing throughput when interacting with the Management Control Panel API. The SDK can improve performance by implementing connection pooling, request caching, and efficient data processing. Proper error handling and monitoring can also contribute to overall performance."}}', '2025-03-10 21:28:31+00', 'true', 'Clients', 'go-mcp-sdk'), ('942407303', 'WebMCP', 'jasonjmcghee/WebMCP', 'https://github.com/jasonjmcghee/WebMCP', null, '2025-03-04 04:07:00+00', '2025-03-10 13:08:12+00', 'JavaScript', '30', '# WebMCP

A proposal and code for websites to support client side LLMs

[![NPM Version](https://img.shields.io/npm/v/%40jason.today%2Fwebmcp)](https://www.npmjs.com/package/@jason.today/webmcp) [![MIT licensed](https://img.shields.io/npm/l/%40jason.today%2Fwebmcp)](./LICENSE)

WebMCP allows websites to share tools, resources, prompts, etc. to LLMs. In other words, WebMCP allows a website to be an MCP server. No sharing API Keys. Use any model you want.

[Here's a simple website I built that is WebMCP-enabled](https://webmcp.dev)

It comes in the form of a widget that a website owner can put on their site and expose tools to give client-side LLMs what they need to provide a great UX for the user or agent.

_The look, feel, how it's used, and security are all absolutely open for contribution / constructive criticism. MCP Clients directly building WebMCP functionality seems like an ideal outcome._

An end-user can connect to any number of websites at a time - and tools are "scoped" (by name) based on the domain to simplify organization.

### Super Quick Demo (20 seconds, Sound on 🔊)

https://github.com/user-attachments/assets/61229470-1242-401e-a7d9-c0d762d7b519

## Getting started (using your LLM with websites using WebMCP)

#### Installation

Just specify your MCP client (`claude`, `cursor`, `cline`, `windsurf`, or a path to json)

```bash
npx -y @jason.today/webmcp@latest --config claude
```

_If you're interested in setting it up manually, use the command `npx -y @jason.today/webmcp@latest --mcp`._

_Auto-install was inspired by Smithery, but their code is AGPL so I wrote this myself. If it doesn't work for you or you don't see your mcp client, please file an issue._

#### Using WebMCP

When you're ready to connect to a website, you can ask your model to generate you an mcp token.

Copy the token and paste it to the website's input. As soon as the website registers with it, it's thrown away and cannot be used for subsequent registrations or anything else. The website will receive its own session token for making requests.

If you'd rather your model / service never see the token, you can manually execute `npx @jason.today/webmcp --new` instead.

Some MCP clients, including Claude Desktop, need to be restarted to get access to new tools. (at least at time of writing)

To disconnect, you can close the browser tab, click "disconnect", or shut down the server with `npx @jason.today/webmcp -q`.

All configuration files are stored in `~/.webmcp` directory.

## Getting started (adding WebMCP to your website)

To use WebMCP, simply include [`webmcp.js`](https://github.com/jasonjmcghee/WebMCP/releases) on your page (via src or directly):

```
<script src="webmcp.js"></script>
```

The WebMCP widget will automatically initialize and appear in the bottom right corner of your page. Clicking on it will ask for a webmcp token which the end-user will generate.

### Full Demo (3 minutes)

https://github.com/user-attachments/assets/43ad160a-846d-48ad-9af9-f6d537e78473

## More Info About How It Works

1. The MCP client connects to the `/mcp` path using the server token from `.env` (auto-generated)
2. The server generates a registration token (instigated via the built-in mcp tool by a model or the `--new` command)
3. Web clients connect to the `/register` endpoint with this token and its domain.
4. Web pages connect to their assigned channel based on their domain.
5. When an LLM wants to use a tool / resource / prompt, the request flows from:
   - MCP Client → MCP Server → WebSocket Server → Web Page with the tool / resource / prompt
   - (similar for requesting a list of tools / resources / prompts)
6. The web page performs the request (e.g. call tool) and sends the result back through the same path
7. Multiple web pages can be connected simultaneously, each with their own set of tools and tokens
8. The MCP client sees all tools as a unified list, with channel prefixes to avoid name collisions

## Security

This is a super early project. I'm very interested in hardening security to prevent malicious extensions etc. from being
able to perform prompt injection attacks and similar. If you have constructive ideas, please reach out or open an issue.

## Built in tools

- Token generator (for connecting to WebMCP websites)
- MCP Tool Definer (to simplify building the schema of a tool for use with MCP)
  - You can ask for the javascript (if relevant) in a follow-up message for use with WebMCP

## Docker

There is a `Dockerfile` specifically for Smithery deployment.

If you'd like to use docker to run the websocket server, I've added a `docker-compose.yml` for demonstration purposes.

If `--docker` is provided to the mcp client config alongside `--mcp`, it will assume the server is running. This will allow you to dockerize the main process (websocket server), and your mcp client will connect to your docker container via websocket. Similarly, websites will communicate with your docker container.
', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"API Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it implements a \"WebMCP\" server allowing websites to share tools and resources with LLMs via the Model Context Protocol (MCP). It provides client-side integrations and token-based registration.","categories":["Protocol Implementations","API Frameworks","Function Calling"],"analysis":{"Architecture and Design Patterns":"The architecture involves an MCP server that acts as an intermediary between LLM clients and web pages. It utilizes a WebSocket server for real-time communication. Registration tokens are used for authentication. The design emphasizes exposing website functionalities as tools accessible to LLMs through a defined protocol.","Implementation Details and Key Features":"Key features include a widget for websites to expose tools, token generation for secure connection, and automatic initialization. The implementation uses `webmcp.js` for client-side integration. The system supports multiple web pages connected simultaneously. The repository also provides built-in tools for token generation and MCP tool definition.","Integration Approaches and Protocols":"WebMCP integrates with websites using a JavaScript widget. It relies on the Model Context Protocol for communication between LLMs and the WebMCP server. The protocol uses WebSocket connections and token-based authentication. It connects to the `/mcp` path with the server token, allowing web clients to register at the `/register` endpoint.","Technical Stack and Dependencies":"The technical stack involves JavaScript for the client-side widget (`webmcp.js`). The documentation mentions using `npm` for package management. The project uses websockets for real-time communication. Docker and docker-compose are supported for deployment using `Dockerfile` and `docker-compose.yml`.","Best Practices and Coding Standards":"The project acknowledges the early stage of development and actively seeks contributions to improve security and coding standards. The documentation emphasizes the importance of security and preventing prompt injection attacks. The project also includes a license file (MIT) and uses a versioning system.","Performance Considerations":"The repository uses websockets which provide real-time communication and can improve performance. The documentation does not explicitly address specific performance considerations. The use of token-based authentication and the design's modularity suggest efforts to optimize performance and scalability."}}', '2025-03-10 21:28:23+00', 'true', 'Servers', 'web-mcp-servers'), ('942409664', 'claude-code-mcp', 'willccbb/claude-code-mcp', 'https://github.com/willccbb/claude-code-mcp', 'Letting Claude Code develop his own MCP tools :)', '2025-03-04 04:13:29+00', '2025-03-10 15:16:37+00', 'TypeScript', '61', '# Claude Code with MCP Servers

## Project Overview

This project enables Claude to use Model Context Protocol (MCP) servers for enhanced capabilities, including web search, GitHub integration, file operations, database access, and communication tools.

## Key Concepts

- **MCP (Model Context Protocol)**: A standardized way for LLMs to interact with external tools and services
- **Reference MCP Servers**: Located at `$HOME/dev/mcp-servers-repo` (a clone of [github.com/modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers))
- **Local MCP Servers**: Our own implementations located at `$HOME/dev/claude-code/mcp-servers/`

The reference servers should be treated as read-only. Our local servers will be our own implementations, some based on reference servers but with extensions and improvements.

## Directory Structure

- `docs/` - Reference documentation
- `mcp-servers/` - Our local MCP server implementations
- `archive/` - Storage for deprecated code components
- `data/` - Storage for server data (SQLite DB, memory files, etc.)
- `scripts/` - Utility scripts for setup and testing
- `src/` - Python modules and utilities

## MCP Server Configuration

Configuration is handled by the `claude-mcp` script at the project root. This script:

1. Sets up environment variables for server paths
2. Registers servers with Claude using the `claude mcp add` command
3. Uses both package-based servers (from npm/pypi) and local implementations

```bash
# Script path
/Users/williambrown/dev/claude-code/claude-mcp

# List all registered servers
claude mcp list
```

## Available MCP Servers

| Server | Purpose | Source |
|--------|---------|--------|
| brave-search | Web search capabilities | npm package |
| github | GitHub repository operations | npm package |
| filesystem | Local file system operations | npm package |
| fetch | Web page fetching | PyPI package |
| memory | Knowledge graph for persistent memory | npm package |
| sqlite | Database operations | Local implementation |
| slack | Slack communication | npm package |
| linear | Linear issue tracking | npm package |
| e2b | Code execution sandbox | npm package |

## Required Environment Variables

- `BRAVE_API_KEY` - For brave-search server
- `GITHUB_TOKEN` - For GitHub operations
- `LINEAR_API_KEY` - For Linear issue tracking
- `SLACK_BOT_TOKEN` and `SLACK_TEAM_ID` - For Slack communication
- `E2B_API_KEY` - For code execution sandbox
- `ALLOWED_PATHS` - For filesystem access control (defaults to project root)

## Development Approach

We're taking a two-pronged approach to MCP servers:

1. **Using Existing Servers**: Leveraging established MCP servers via npm/PyPI packages for standard functionality
2. **Building Custom Servers**: Developing our own implementations for extended functionality and experimentation

All legacy or unmaintained code is moved to the `archive/` directory to keep the main codebase clean and focused.

## Development Guidelines

### Package Management
- Use `uv` for Python package management (NEVER pip)
- Installation: `uv add package`
- Running tools: `uv run tool`
- TypeScript: Use npm for package management

### Code Quality
- Type hints required for all Python and TypeScript code
- Public APIs must have docstrings
- Functions must be focused and small
- Line length: 88 chars maximum
- Follow PEP standards for Python
- Follow existing patterns for TypeScript

### Testing
- Framework: `uv run pytest`
- Async testing: use anyio, not asyncio
- Coverage: test edge cases and errors
- New features require tests
- Bug fixes require regression tests

### Coding Principles
- Use clean, modular abstractions
- Avoid code duplication
- Ensure backward compatibility
- Document all extensions and differences from reference implementations
- Strive for minimal clean code that's easily maintained

### Git Workflow
- Develop in separate branches, merge to main quickly
- For user-reported fixes: `git commit --trailer "Reported-by:<user>"`
- For GitHub issues: `git commit --trailer "Github-Issue:#<number>"`
- NEVER mention co-authored-by or tools used to create commits/PRs

## MCP Server Development Guidelines

When developing our own local MCP servers:

1. **Reference Implementation First**:
   - Study the original server in `MCP_REPO_PATH`
   - Understand its API, behavior, and implementation details
   - Document the key functions and features

2. **Implementation Strategy**:
   - Start by mimicking the reference server exactly
   - Once basic functionality is working, add extensions
   - Ensure backward compatibility

3. **Code Quality**:
   - Use proper typing (TypeScript/Python type hints)
   - Add comprehensive documentation
   - Implement proper error handling
   - Follow consistent naming conventions
   - Write tests for all functionality

For detailed information about project standards, code style, and specific server implementations, refer to `CLAUDE.md`.', 'true', '"{\"Context Management\",\"Function Calling\",\"Protocol Implementations\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states its purpose is to enable Claude to use Model Context Protocol (MCP) servers for enhanced capabilities with external tools and services, aligning with MCP objectives.","categories":["Agent Frameworks","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The architecture employs a modular design, separating MCP server implementations into package-based and custom-built components.  A clear directory structure organizes code, documentation, and data. The project aims for clean abstractions, minimizing code duplication, and ensuring backward compatibility, indicating sound design principles.","Implementation Details and Key Features":"The repository includes implementations of MCP servers for various functionalities such as web search, GitHub integration, file system operations, and database access.  It uses both existing npm/PyPI packages and local custom implementations. Configuration is managed via a dedicated script, simplifying server registration and environment variable setup.","Integration Approaches and Protocols":"The project integrates with external services using API keys and establishes communication through MCP. It leverages standard protocols to connect with services like GitHub, Slack, and Linear. The use of both npm and PyPI packages indicates a diverse approach to integration with existing tools.","Technical Stack and Dependencies":"The project primarily uses Python and TypeScript. It relies on npm and uv for package management. Key dependencies include libraries for web searching, file system operations, database access (SQLite), and communication (Slack). The use of E2B suggests integration with code execution sandboxes.","Best Practices and Coding Standards":"The repository enforces coding standards such as type hints for Python and TypeScript, docstrings for public APIs, and adherence to PEP standards for Python. Code quality is emphasized through focused functions, limited line length, and comprehensive testing, including edge cases and regression tests. Git workflow guidelines are also provided.","Performance Considerations":"While the documentation doesn't explicitly focus on performance optimization, the design principle of modularity and minimal clean code suggests an implicit consideration for performance. The use of async testing (anyio) indicates awareness of asynchronous operations, relevant for improving concurrency and responsiveness."}}', '2025-03-10 21:28:52+00', 'true', 'Servers', 'claude-code-mcp-tools'), ('942474369', 'app-registry', 'fleuristes/app-registry', 'https://github.com/fleuristes/app-registry', 'Our app registry. Add your own MCPs to Fleur', '2025-03-04 06:51:48+00', '2025-03-10 17:35:42+00', null, '16', '# Fleur App Registry

This repository serves as the official app registry for [Fleur](https://github.com/fleuristes/fleur), an open-source [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) GUI client. It contains a curated list of MCP applications that can be integrated with Fleur to extend its capabilities.

## What is MCP?

Model Context Protocol (MCP) is a standardized way for language models to interact with external tools and data sources. MCPs enable models to:

- Access real-time information
- Interact with web services
- Manipulate files and data
- Connect to various APIs and platforms

## Available Apps

The registry currently includes apps for:

- Web browsing
- Time services
- Hacker News integration
- Linear project management
- Slack communication
- Obsidian note-taking
- Rember flashcards


## Contributing Your MCP

We welcome contributions from developers who want to add their MCP applications to the registry. To submit your MCP, please create a pull request following the guidelines below.

### MCP Schema

Each MCP in the registry must conform to the following JSON schema:

```json
{
  "name": "Your App Name",
  "description": "A concise description of what your app does and how it helps users",
  "icon": {
    "type": "url",
    "url": {
      "light": "URL to light mode icon (SVG or PNG recommended)",
      "dark": "URL to dark mode icon (SVG or PNG recommended)"
    }
  },
  "category": "Category of your app (e.g., Productivity, Communication, Utilities)",
  "price": "Free or pricing information",
  "developer": "Your name or organization",
  "sourceUrl": "URL to the source code repository",
  "config": {
    "mcpKey": "unique identifier for your MCP",
    "runtime": "runtime environment (e.g., npx, uvx)",
    "args": [
      "array of arguments needed to run your MCP"
    ]
  },
  "features": [
    {
      "name": "Feature name",
      "description": "Brief description of the feature",
      "prompt": "Example prompt that users can use to trigger this feature"
    }
  ],
  "setup": [
    {
      "label": "Configuration Label",
      "type": "input",
      "placeholder": "Placeholder text for the input field",
      "value": "Default value or instructions",
      "key": "ENVIRONMENT_VARIABLE_KEY"
    }
  ]
}
```

### Schema Field Descriptions

| Field | Description | Required |
|-------|-------------|----------|
| `name` | The display name of your app | Yes |
| `description` | A brief description of what your app does | Yes |
| `icon` | Icon configuration with light and dark mode variants | Yes |
| `category` | The category your app belongs to | Yes |
| `price` | Pricing information (Free, Paid, etc.) | Yes |
| `developer` | Name of the developer or organization | Yes |
| `sourceUrl` | URL to the source code repository | Yes |
| `config` | Configuration details for running the MCP | Yes |
| `config.mcpKey` | Unique identifier for your MCP | Yes |
| `config.runtime` | Runtime environment (npx, uvx, etc.) | Yes |
| `config.args` | Arguments needed to run your MCP | Yes |
| `features` | Array of features your app provides | Yes |
| `features[].name` | Name of the feature | Yes |
| `features[].description` | Description of what the feature does | Yes |
| `features[].prompt` | Example prompt for users | Yes |
| `setup` | Array of setup instructions for configuration | No |
| `setup[].label` | Label for the configuration item | If setup is included |
| `setup[].type` | Type of input (input, select, etc.) | If setup is included |
| `setup[].placeholder` | Placeholder text | If setup is included |
| `setup[].value` | Default value or instructions | If setup is included |
| `setup[].key` | Environment variable key | If setup is included |

### Submission Guidelines

1. Fork this repository
2. Add your MCP to the `apps.json` file
3. If needed, add your app icon to the `assets` directory
4. Submit a pull request with a clear description of your MCP

### Icon Guidelines

- Icons should be in SVG format when possible (PNG is acceptable if SVG is not available)
- Include both light and dark mode variants if your icon has different appearances for different themes
- Place your icon in the `assets` directory and reference it in your MCP configuration

## License

This repository is licensed under the Apache License 2.0 - see the LICENSE file for details.

## Support

For questions or issues related to the Fleur App Registry, please open an issue in this repository. ', 'true', '"{\"Plugin Ecosystems\",\"API Frameworks\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states that it's an app registry for Model Context Protocol (MCP) applications, providing a curated list of apps for the Fleur MCP GUI client. It also provides a schema for MCPs.","categories":["AI Model Integration","Plugin Ecosystems","API Frameworks"],"analysis":{"Architecture and Design Patterns":"The repository employs a registry pattern for managing and distributing MCP applications. It utilizes a JSON schema to define the structure and validation requirements for MCP metadata. The architecture promotes modularity by allowing independent MCP applications to be integrated into the Fleur client. The registry acts as a central point for discovering and accessing these applications.","Implementation Details and Key Features":"The core implementation revolves around the `apps.json` file, which stores metadata for available MCPs. Key features include a standardized JSON schema for MCP definition, support for light and dark mode icons, and guidelines for contributing new MCP applications. The `config` section within the schema defines runtime environments and arguments required for executing MCPs.","Integration Approaches and Protocols":"The repository facilitates integration by providing a structured format (JSON schema) that external applications must adhere to. It leverages the Model Context Protocol (MCP) as the underlying communication protocol between the Fleur client and registered applications. The use of `sourceUrl` enables fetching the source code of each application, allowing for dynamic integration and execution.","Technical Stack and Dependencies":"The primary technical stack comprises JSON for data serialization and Git for version control and collaboration. No specific programming language is inherently required, as the repository mainly stores metadata. However, individual MCP applications may rely on various runtime environments (e.g., npx, uvx) and corresponding dependencies. The core dependency is the Fleur client itself, which consumes the MCP definitions.","Best Practices and Coding Standards":"The repository enforces coding standards through the MCP JSON schema, ensuring consistency and validation of app metadata. Contribution guidelines promote code quality by encouraging developers to provide clear descriptions, well-defined features, and example prompts. The inclusion of license information and support channels indicates a commitment to open-source best practices.","Performance Considerations":"Since the registry primarily stores metadata, performance considerations are relatively low. However, the speed of fetching and parsing the `apps.json` file could impact the initial loading time of the Fleur client. Minimizing the size of the JSON file and optimizing icon assets can improve performance. The performance of individual MCP applications depends on their implementation and runtime environment."}}', '2025-03-10 21:28:23+00', 'true', 'Servers', 'fleur-mcp-registry'), ('942522722', 'wallpaper-mcp', 'shahanneda/wallpaper-mcp', 'https://github.com/shahanneda/wallpaper-mcp', 'MCP for setting your wallpaper', '2025-03-04 08:33:01+00', '2025-03-05 18:19:05+00', 'Python', '2', '# Wallpaper MCP

Allows MCP clients to generate wallpaper images and set them as your desktop wallpaper on MacOS.

## Features

### Tools

- **generate_image**
  - **Description**: Generate an image using Luma Labs AI.
  - **Input**: `prompt` (string) - Text description of the image to generate.
  - **Output**: Returns the generated image.

- **generate_wallpaper**
  - **Description**: Generate a wallpaper image and save it to a temporary directory.
  - **Input**: `prompt` (string) - Text description of the wallpaper to generate.
  - **Output**: Returns the path to the generated wallpaper image.

- **set_image_from_path**
  - **Description**: Set the wallpaper using an image file from a specific path.
  - **Input**: `image_path` (string) - Full path to the image file to use as wallpaper.
  - **Output**: A message indicating whether the wallpaper was set successfully.

- **generate_and_set_wallpaper**
  - **Description**: Generate a wallpaper from a prompt and set it as your desktop wallpaper.
  - **Input**: `prompt` (string) - Text description of the wallpaper to generate.
  - **Output**: A message indicating whether the wallpaper was generated and set successfully.

### Video Demo
https://www.youtube.com/watch?v=S1MA3t_gMq8
[![video image](https://img.youtube.com/vi/S1MA3t_gMq8/0.jpg)](https://www.youtube.com/watch?v=S1MA3t_gMq8)


## Getting Started

### Prerequisites
- macOS
- Luma Labs API key

### Getting a Luma Labs API Key
Visit [Luma Labs Dream Machine API](https://lumalabs.ai/dream-machine/api)

### Installation
```bash
git clone git@github.com:shahanneda/wallpaper-mcp.git
```

### Integration with Claude Desktop
To use this server with Claude Desktop, add the following to your Claude Desktop configuration file:
- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`
```json
{
    "wallpaper": {
      "command": "uv",
      "args": [
        "run",
        "--with",
        "mcp[cli]",
        "--with",
        "lumaai",
        "--with",
        "requests",
        "mcp",
        "run",
        "/path/to/server.py"
      ],
      "env": {
        "LUMAAI_API_KEY": "API_KEY_HERE"
      }
    }
}
```
Replace `/path/to/server.py` with the absolute path to your server.py file.

### Testing with MCP Inspector
To run in development:
```bash
 LUMAAI_API_KEY=API_KEY_HERE mcp dev server.py --with lumaai --with requests
```

## License

This MCP server is licensed under the MIT License.
', 'true', '"{\"Function Calling\",\"API Frameworks\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly mentions 'MCP' for setting wallpaper and provides tools designed to be used as part of a Model Context Protocol ecosystem, specifically for integration with Claude Desktop.","categories":["AI Model Integration","Function Calling","API Frameworks"],"analysis":{"Architecture and Design Patterns":"The repository's architecture is based on providing a set of tools that an MCP client can call to generate and set desktop wallpapers. The design revolves around defining specific function calls with well-defined inputs and outputs, facilitating communication between an AI model (like Claude) and system utilities on MacOS.","Implementation Details and Key Features":"The implementation includes functionalities for generating images using the Luma Labs AI and setting them as desktop wallpaper. Key features include the `generate_image`, `generate_wallpaper`, `set_image_from_path`, and `generate_and_set_wallpaper` tools, each accepting a prompt string as input and returning either an image or a success message.","Integration Approaches and Protocols":"The repository integrates via command-line execution initiated from a Claude Desktop configuration file, adhering to MCP principles.  It leverages environment variables for secure API key management.  The system is designed to be extensible using 'uv' with specifications for 'mcp[cli]', 'lumaai', and 'requests'.","Technical Stack and Dependencies":"The technical stack centers around Python and its ecosystem. It depends on the Luma Labs AI API for image generation and potentially the `requests` library for HTTP requests. Integration with Claude Desktop requires specific configuration within its `claude_desktop_config.json` file.  The MIT license allows for broad usage.","Best Practices and Coding Standards":"The repository demonstrates best practices by providing clear documentation for integration with Claude Desktop, including example configuration snippets. It uses environment variables for sensitive information like API keys. Testing is supported via MCP Inspector, facilitating development. The code is modular based on the tools provided.","Performance Considerations":"Performance depends on the speed of the Luma Labs AI API and the underlying system capabilities. The process involves generating images, writing them to disk, and setting them as wallpaper, each of which has performance implications. Optimization strategies might include caching generated images or asynchronous wallpaper setting."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'wallpaper-mcp-server'), ('942540252', 'mcp-attr', 'frozenlib/mcp-attr', 'https://github.com/frozenlib/mcp-attr', 'A crate for creating Model Context Protocol servers using declarative descriptions with attributes and types.', '2025-03-04 09:09:14+00', '2025-03-10 06:06:06+00', 'Rust', '8', '# mcp-attr

[![Crates.io](https://img.shields.io/crates/v/mcp-attr.svg)](https://crates.io/crates/mcp-attr)
[![Docs.rs](https://docs.rs/mcp-attr/badge.svg)](https://docs.rs/mcp-attr/)
[![Actions Status](https://github.com/frozenlib/mcp-attr/workflows/CI/badge.svg)](https://github.com/frozenlib/mcp-attr/actions)

A crate for creating Model Context Protocol servers using declarative descriptions with attributes and types.

## Features

mcp-attr is a crate designed to make it easy for both humans and AI to create [Model Context Protocol] servers.
To achieve this goal, it has the following features:

- **Declarative Description**:
  - Use attributes like `#[mcp_server]` to describe MCP servers with minimal code
  - Fewer lines of code make it easier for humans to understand and consume less context window for AI
  - Following the DRY (Don't Repeat Yourself) principle helps prevent inconsistent code
- **Leveraging the Type System**:
  - Expressing information sent to MCP clients through types reduces source code volume and improves readability
  - Type errors help AI with coding

## Quick Start

### Installation

Add the following to your `Cargo.toml`:

```toml
[dependencies]
mcp-attr = "0.0.2"
tokio = "1.43.0"
```

### Basic Usage

```rust
use std::sync::Mutex;

use mcp_attr::server::{mcp_server, McpServer, serve_stdio};
use mcp_attr::Result;

#[tokio::main]
async fn main() -> Result<()> {
    serve_stdio(ExampleServer(Mutex::new(ServerData { count: 0 }))).await?;
    Ok(())
}

struct ExampleServer(Mutex<ServerData>);

struct ServerData {
  /// Server state
  count: u32,
}

#[mcp_server]
impl McpServer for ExampleServer {
    /// Description sent to MCP client
    #[prompt]
    async fn example_prompt(&self) -> Result<&str> {
        Ok("Hello!")
    }

    #[resource("my_app://files/{name}.txt")]
    async fn read_file(&self, name: String) -> Result<String> {
        Ok(format!("Content of {name}.txt"))
    }

    #[tool]
    async fn add_count(&self, message: String) -> Result<String> {
        let mut state = self.0.lock().unwrap();
        state.count += 1;
        Ok(format!("Echo: {message} {}", state.count))
    }
}
```

### Support Status

The following protocol versions, transports, and methods are supported.

#### Protocol Versions

- `2024-11-05`

#### Transport

- stdio

SSE is not yet supported. However, transport is extensible, so custom transports can be implemented.

#### Methods

| Attribute                  | [`McpServer`] methods                                                    | Model context protocol methods                                           |
| -------------------------- | ------------------------------------------------------------------------ | ------------------------------------------------------------------------ |
| [`#[prompt]`](#prompt)     | [`prompts_list`]<br>[`prompts_get`]                                      | [`prompts/list`]<br>[`prompts/get`]                                      |
| [`#[resource]`](#resource) | [`resources_list`]<br>[`resources_read`]<br>[`resources_templates_list`] | [`resources/list`]<br>[`resources/read`]<br>[`resources/templates/list`] |
| [`#[tool]`](#tool)         | [`tools_list`]<br>[`tools_call`]                                         | [`tools/list`]<br>[`tools/call`]                                         |

## Usage

### Starting the Server

MCP servers created with this crate run on the tokio async runtime.

Start the server by launching the async runtime with `#[tokio::main]` and passing a value implementing the `McpServer` trait to the `serve_stdio` function,
which starts a server using standard input/output as transport.

While you can implement the `McpServer` trait manually, you can implement it more efficiently in a declarative way by using the `#[mcp_server]` attribute.

```rust
use mcp_attr::server::{mcp_server, McpServer, serve_stdio};
use mcp_attr::Result;

#[tokio::main]
async fn main() -> Result<()> {
  serve_stdio(ExampleServer).await?;
  Ok(())
}

struct ExampleServer;

#[mcp_server]
impl McpServer for ExampleServer {
  #[prompt]
  async fn hello(&self) -> Result<&str> {
    Ok("Hello, world!")
  }
}
```

Most of the functions implementing MCP methods are asynchronous and can be executed concurrently.

### Input and Output

How an MCP server receives data from an MCP client is expressed through function argument definitions.

For example, in the following example, the `add` tool indicates that it receives integers named `lhs` and `rhs`.
This information is sent from the MCP server to the MCP client, and the MCP client sends appropriate data to the server.

```rust
use mcp_attr::server::{mcp_server, McpServer};
use mcp_attr::Result;

struct ExampleServer;

#[mcp_server]
impl McpServer for ExampleServer {
  #[tool]
  async fn add(&self, lhs: u32, rhs: u32) -> Result<String> {
    Ok(format!("{}", lhs + rhs))
  }
}
```

The types that can be used for arguments vary by method, and must implement the following traits:

| Attribute                  | Trait for argument types              | Return type            |
| -------------------------- | ------------------------------------- | ---------------------- |
| [`#[prompt]`](#prompt)     | [`FromStr`]                           | [`GetPromptResult`]    |
| [`#[resource]`](#resource) | [`FromStr`]                           | [`ReadResourceResult`] |
| [`#[tool]`](#tool)         | [`DeserializeOwned`] + [`JsonSchema`] | [`CallToolResult`]     |

Arguments can also use `Option<T>`, in which case they are communicated to the MCP client as optional arguments.

Return values must be types that can be converted to the type shown in the `Return type` column above, wrapped in `Result`.
For example, since `CallToolResult` implements `From<String>`, you can use `Result<String>` as the return value as shown in the example above.

### Explanations for AI

For an MCP client to call MCP server methods, the AI needs to understand the meaning of the methods and arguments.

Adding documentation comments to methods and arguments sends this information to the MCP client, allowing the AI to understand their meaning.

```rust
use mcp_attr::server::{mcp_server, McpServer};
use mcp_attr::Result;

struct ExampleServer;

#[mcp_server]
impl McpServer for ExampleServer {
  /// Tool description
  #[tool]
  async fn concat(&self,
    /// Description of argument a (for AI)
    a: u32,
    /// Description of argument b (for AI)
    b: u32,
  ) -> Result<String> {
    Ok(format!("{a},{b}"))
  }
}
```

### State Management

Since values implementing `McpServer` are shared among multiple concurrently executing methods, only `&self` is available. `&mut self` cannot be used.

To maintain state, you need to use thread-safe types with interior mutability like `Mutex`.

```rust
use std::sync::Mutex;
use mcp_attr::server::{mcp_server, McpServer};
use mcp_attr::Result;

struct ExampleServer(Mutex<ServerData>);
struct ServerData {
  count: u32,
}

#[mcp_server]
impl McpServer for ExampleServer {
  #[tool]
  async fn add_count(&self) -> Result<String> {
    let mut state = self.0.lock().unwrap();
    state.count += 1;
    Ok(format!("count: {}", state.count))
  }
}
```

### Error Handling

mcp_attr uses `Result`, Rust's standard error handling method.

The types [`mcp_attr::Error`] and [`mcp_attr::Result`] (an alias for `std::result::Result<T, mcp_attr::Error>`) are provided for error handling.

`mcp_attr::Error` is similar to [`anyhow::Error`], capable of storing any error type implementing [`std::error::Error + Sync + Send + 'static`], and implements conversion from other error types.
Therefore, in functions returning `mcp_attr::Result`, you can use the `?` operator for error handling with expressions of type `Result<T, impl std::error::Error + Sync + Send + 'static>`.

However, it differs from `anyhow::Error` in the following ways:

- Can store JSON-RPC errors used in MCP
- Has functionality to distinguish whether error messages are public information to be sent to the MCP Client or private information not to be sent
  - (However, in debug builds, all information is sent to the MCP Client)

The macros [`bail!`] and [`bail_public!`] are provided for error handling, similar to [`anyhow::bail!`].

- [`bail!`] takes a format string and arguments and raises an error treated as private information.
- [`bail_public!`] takes an error code, format string, and arguments and raises an error treated as public information.

Additionally, conversions from other error types are treated as private information.

```rust
use mcp_attr::server::{mcp_server, McpServer};
use mcp_attr::{bail, bail_public, Result, ErrorCode};

struct ExampleServer;

#[mcp_server]
impl McpServer for ExampleServer {
    #[prompt]
    async fn add(&self, a: String) -> Result<String> {
        let something_wrong = false;
        if something_wrong {
            bail_public!(ErrorCode::INTERNAL_ERROR, "error message");
        }
        if something_wrong {
            bail!("error message");
        }
        let a = a.parse::<i32>()?;
        Ok(format!("success {a}"))
    }
}
```

## Attribute Descriptions

### `#[prompt]`

```rust,ignore
#[prompt("name")]
async fn func_name(&self) -> Result<GetPromptResult> { }
```

- "name" (optional): Prompt name. If omitted, the function name is used.

Implements the following methods:

- [`prompts_list`]
- [`prompts_get`]

Function arguments become prompt arguments. Arguments must implement the following trait:

- [`FromStr`]: Trait for restoring values from strings

Arguments can be given names using the `#[arg("name")]` attribute.
If not specified, the name used is the function argument name with leading `_` removed.

Return value: [`Result<impl Into<GetPromptResult>>`]

```rust
use mcp_attr::server::Result;
use mcp_attr::server::{mcp_server, McpServer};

struct ExampleServer;

#[mcp_server]
impl McpServer for ExampleServer {
  /// Function description (for AI)
  #[prompt]
  async fn hello(&self) -> Result<&str> {
    Ok("Hello, world!")
  }

  #[prompt]
  async fn echo(&self,
    /// Argument description (for AI)
    a: String,
    /// Argument description (for AI)
    #[arg("x")]
    b: String,
  ) -> Result<String> {
    Ok(format!("Hello, {a} {b}!"))
  }
}
```

### `#[resource]`

```rust,ignore
#[resource("url_template", name = "name", mime_type = "mime_type")]
async fn func_name(&self) -> Result<ReadResourceResult> { }
```

- "url_template" (optional): URI Template ([RFC 6570]) indicating the URL of resources this method handles. If omitted, handles all URLs.
- "name" (optional): Resource name. If omitted, the function name is used.
- "mime_type" (optional): MIME type of the resource.

Implements the following methods:

- [`resources_list`] (can be manually implemented)
- [`resources_read`]
- [`resources_templates_list`]

Function arguments become URI Template variables. Arguments must implement the following trait:

- [`FromStr`]: Trait for restoring values from strings

URI Templates are specified in [RFC 6570] Level2. The following variables can be used in URI Templates:

- `{var}`
- `{+var}`
- `{#var}`

Return value: [`Result<impl Into<ReadResourceResult>>`]

```rust
use mcp_attr::server::Result;
use mcp_attr::server::{mcp_server, McpServer};

struct ExampleServer;

#[mcp_server]
impl McpServer for ExampleServer {
  /// Function description (for AI)
  #[resource("my_app://x/y.txt")]
  async fn file_one(&self) -> Result<String> {
    Ok(format!("one file"))
  }

  #[resource("my_app://{a}/{+b}")]
  async fn file_ab(&self, a: String, b: String) -> Result<String> {
    Ok(format!("{a} and {b}"))
  }

  #[resource]
  async fn file_any(&self, url: String) -> Result<String> {
    Ok(format!("any file"))
  }
}
```

The automatically implemented [`resources_list`] returns a list of URLs without variables specified in the `#[resource]` attribute.
If you need to return other URLs, you must manually implement `resources_list`.
If `resources_list` is manually implemented, it is not automatically implemented.

### `#[tool]`

```rust,ignore
#[tool("name")]
async fn func_name(&self) -> Result<CallToolResult> { }
```

- "name" (optional): Tool name. If omitted, the function name is used.

Implements the following methods:

- [`tools_list`]
- [`tools_call`]

Function arguments become tool arguments. Arguments must implement all of the following traits:

- [`DeserializeOwned`]: Trait for restoring values from JSON
- [`JsonSchema`]: Trait for generating JSON Schema (JSON Schema is sent to MCP Client so AI can understand argument structure)

Arguments can be given names using the `#[arg("name")]` attribute.
If not specified, the name used is the function argument name with leading `_` removed.

Return value: [`Result<impl Into<CallToolResult>>`]

```rust
use mcp_attr::server::Result;
use mcp_attr::server::{mcp_server, McpServer};

struct ExampleServer;

#[mcp_server]
impl McpServer for ExampleServer {
  /// Function description (for AI)
  #[tool]
  async fn echo(&self,
    /// Argument description (for AI)
    a: String,
    /// Argument description (for AI)
    #[arg("x")]
    b: String,
  ) -> Result<String> {
    Ok(format!("Hello, {a} {b}!"))
  }
}
```

### Manual Implementation

You can also directly implement `McpServer` methods without using attributes.

Additionally, the following methods do not support implementation through attributes and must be implemented manually:

- [`server_info`]
- [`instructions`]
- [`completion_complete`]

The following method can be overridden with manual implementation over the attribute-based implementation:

- [`tools_list`]

### Testing

With the advent of AI Coding Agents, testing has become even more important.
AI can hardly write correct code without tests, but with tests, it can write correct code through repeated testing and fixes.

mcp_attr includes [`McpClient`] for testing, which connects to MCP servers within the process.

```rust
use mcp_attr::client::McpClient;
use mcp_attr::server::{mcp_server, McpServer};
use mcp_attr::schema::{GetPromptRequestParams, GetPromptResult};
use mcp_attr::Result;

struct ExampleServer;

#[mcp_server]
impl McpServer for ExampleServer {
    #[prompt]
    async fn hello(&self) -> Result<&str> {
        Ok("Hello, world!")
    }
}

#[tokio::test]
async fn test_hello() -> Result<()> {
    let client = McpClient::from_server(ExampleServer).await?;
    let a = client
        .prompts_get(GetPromptRequestParams::new("hello"))
        .await?;
    let e: GetPromptResult = "Hello, world!".into();
    assert_eq!(a, e);
    Ok(())
}
```

## License

This project is dual licensed under Apache-2.0/MIT. See the two LICENSE-\* files for details.

## Contribution

Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.

[Model Context Protocol]: https://spec.modelcontextprotocol.io/specification/2024-11-05/
[RFC 6570]: https://www.rfc-editor.org/rfc/rfc6570.html
[`prompts/list`]: https://spec.modelcontextprotocol.io/specification/2024-11-05/server/prompts/#listing-prompts
[`prompts/get`]: https://spec.modelcontextprotocol.io/specification/2024-11-05/server/prompts/#getting-a-prompt
[`resources/list`]: https://spec.modelcontextprotocol.io/specification/2024-11-05/server/resources/#listing-resources
[`resources/read`]: https://spec.modelcontextprotocol.io/specification/2024-11-05/server/resources/#reading-resources
[`resources/templates/list`]: https://spec.modelcontextprotocol.io/specification/2024-11-05/server/resources/#resource-templates
[`tools/list`]: https://spec.modelcontextprotocol.io/specification/2024-11-05/server/tools/#listing-tools
[`tools/call`]: https://spec.modelcontextprotocol.io/specification/2024-11-05/server/tools/#calling-a-tool
[`FromStr`]: https://doc.rust-lang.org/std/str/trait.FromStr.html
[`JsonSchema`]: https://docs.rs/schemars/latest/schemars/trait.JsonSchema.html
[`DeserializeOwned`]: https://docs.rs/serde/latest/serde/de/trait.DeserializeOwned.html
[`McpServer`]: https://docs.rs/mcp-attr/latest/mcp_attr/server/trait.McpServer.html
[`McpClient`]: https://docs.rs/mcp-attr/latest/mcp_attr/client/struct.McpClient.html
[`prompts_list`]: https://docs.rs/mcp-attr/latest/mcp_attr/server/trait.McpServer.html#method.prompts_list
[`prompts_get`]: https://docs.rs/mcp-attr/latest/mcp_attr/server/trait.McpServer.html#method.prompts_get
[`resources_list`]: https://docs.rs/mcp-attr/latest/mcp_attr/server/trait.McpServer.html#method.resources_list
[`resources_read`]: https://docs.rs/mcp-attr/latest/mcp_attr/server/trait.McpServer.html#method.resources_read
[`resources_templates_list`]: https://docs.rs/mcp-attr/latest/mcp_attr/server/trait.McpServer.html#method.resources_templates_list
[`tools_list`]: https://docs.rs/mcp-attr/latest/mcp_attr/client/struct.McpClient.html#method.tools_list
[`tools_call`]: https://docs.rs/mcp-attr/latest/mcp_attr/client/struct.McpClient.html#method.tools_call
[`GetPromptResult`]: https://docs.rs/mcp-attr/latest/mcp_attr/schema/struct.GetPromptResult.html
[`ReadResourceResult`]: https://docs.rs/mcp-attr/latest/mcp_attr/schema/struct.ReadResourceResult.html
[`CallToolResult`]: https://docs.rs/mcp-attr/latest/mcp_attr/schema/struct.CallToolResult.html
[`mcp_attr::Error`]: https://docs.rs/mcp-attr/latest/mcp_attr/struct.Error.html
[`mcp_attr::Result`]: https://docs.rs/mcp-attr/latest/mcp_attr/type.Result.html
[`anyhow::Error`]: https://docs.rs/anyhow/latest/anyhow/struct.Error.html
[`std::error::Error + Sync + Send + 'static`]: https://doc.rust-lang.org/std/error/trait.Error.html
[`anyhow::bail!`]: https://docs.rs/anyhow/latest/anyhow/macro.bail.html
[`bail!`]: https://docs.rs/mcp-attr/latest/mcp_attr/macro.bail.html
[`bail_public!`]: https://docs.rs/mcp-attr/latest/mcp_attr/macro.bail_public.html
[`server_info`]: https://docs.rs/mcp-attr/latest/mcp_attr/server/trait.McpServer.html#method.server_info
[`instructions`]: https://docs.rs/mcp-attr/latest/mcp_attr/server/trait.McpServer.html#method.instructions
[`completion_complete`]: https://docs.rs/mcp-attr/latest/mcp_attr/server/trait.McpServer.html#method.completion_complete
[`Result<impl Into<GetPromptResult>>`]: https://docs.rs/mcp-attr/latest/mcp_attr/schema/struct.GetPromptResult.html
[`Result<impl Into<ReadResourceResult>>`]: https://docs.rs/mcp-attr/latest/mcp_attr/schema/struct.ReadResourceResult.html
[`Result<impl Into<CallToolResult>>`]: https://docs.rs/mcp-attr/latest/mcp_attr/schema/struct.CallToolResult.html
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it is a crate for creating Model Context Protocol servers, using declarative descriptions with attributes. It supports protocol versions, transports, and methods as per MCP specifications.","categories":["Protocol Implementations","API Frameworks","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The crate employs a declarative approach using Rust attributes to define MCP servers, aiming for conciseness and readability.  It leverages the Rust type system to express information sent to MCP clients, reducing boilerplate and improving type safety. The architecture supports extensibility with custom transports, showcasing a modular design.","Implementation Details and Key Features":"The core feature is the `mcp_attr` attribute, which simplifies MCP server creation. It supports prompts, resources, and tools, each represented by specific attributes. The crate manages server state using thread-safe types like `Mutex`, enabling concurrent execution of methods. Error handling is implemented using Rust's `Result` type.","Integration Approaches and Protocols":"The crate integrates with the Model Context Protocol by implementing its core concepts such as prompts, resources, and tools. It currently supports the `2024-11-05` protocol version and the stdio transport. Integration is facilitated through the `serve_stdio` function, allowing MCP clients to communicate with the server via standard input/output.","Technical Stack and Dependencies":"The project is written in Rust and relies on `tokio` for asynchronous runtime support. Key dependencies include `mcp-attr` itself, defining the attributes and server logic, and potentially others for serialization and JSON schema generation.  The testing framework utilizes `tokio` for asynchronous testing.","Best Practices and Coding Standards":"The crate promotes DRY principles by using attributes to reduce code duplication. It emphasizes documentation comments to provide explanations for AI clients, enhancing usability.  The use of `Result` for error handling and `Mutex` for state management aligns with Rust's recommended practices for concurrency and error handling.","Performance Considerations":"The crate utilizes asynchronous operations via `tokio`, potentially improving performance through concurrency.  Declarative descriptions minimize code, potentially reducing the context window size required for AI interaction. Thread-safe state management using `Mutex` could introduce some overhead depending on the workload."}}', '2025-03-10 21:28:31+00', 'true', 'Servers', 'mcp-attr-servers'), ('942574238', 'create-mcp-server', 'maoxiaoke/create-mcp-server', 'https://github.com/maoxiaoke/create-mcp-server', 'A MCP Server to Create MCP Server', '2025-03-04 10:17:24+00', '2025-03-06 02:14:27+00', 'JavaScript', '17', '# create-mcp-server

A specialized MCP server that helps create new MCP server scaffolds. This tool streamlines the process of creating new Model Context Protocol (MCP) servers by providing a structured development protocol and example templates.

## How to use

### NPX

```json
{
  "mcpServers": {
    "mediaProcessor": {
      "command": "npx",
      "args": [
        "-y",
        "create-mcp-server@latest"
      ]
    }
  }
}
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
', 'true', '"{\"Protocol Implementations\",\"Developer Tooling\",\"API Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository explicitly mentions \"Model Context Protocol (MCP)\" in its description and documentation, indicating it's designed to create MCP servers and scaffolds.","categories":["Protocol Implementations","Developer Tooling","API Frameworks"],"analysis":{"Architecture and Design Patterns":"The repository provides a structured development protocol and example templates for creating MCP servers. It uses a scaffold generation approach, enabling developers to quickly set up a new MCP server with predefined structures and configurations. The server design likely follows common API patterns for handling context and model interactions.","Implementation Details and Key Features":"The key feature is its scaffolding functionality, which streamlines the MCP server creation process. The tool is invoked using NPX, suggesting it's an NPM package designed for command-line execution. It supports the creation of new servers with predefined structures, helping developers to adhere to a standardized protocol.","Integration Approaches and Protocols":"This tool integrates with the Model Context Protocol by providing a framework for building servers that adhere to the MCP specifications. It likely defines the communication protocols and data structures required for interacting with models in a context-aware manner. Integration with existing systems would involve implementing the MCP interface.","Technical Stack and Dependencies":"The project appears to be an NPM package, likely utilizing JavaScript or TypeScript. Dependencies would include packages related to command-line interface development (such as Commander.js or similar) and potentially templating engines for generating the scaffolded server code. The `create-mcp-server@latest` indicates the use of NPM for package management.","Best Practices and Coding Standards":"The use of a scaffold generator promotes consistency and adherence to best practices in MCP server development. The repository includes a license file (MIT), indicating a commitment to open-source principles. Further analysis of the code would reveal specific coding standards and practices followed.","Performance Considerations":"As a scaffold generator, performance considerations are focused on the speed and efficiency of the scaffolding process itself. The generated servers' performance will depend on the developer's implementation within the scaffold. The tool likely aims to minimize the initial setup time for new MCP servers."}}', '2025-03-10 21:28:31+00', 'true', 'Servers', 'create-mcp-server'), ('942603694', 'powerpoint', 'supercurses/powerpoint', 'https://github.com/supercurses/powerpoint', 'A MCP Server for creating Powerpoint Presentations', '2025-03-04 11:17:19+00', '2025-03-10 08:21:02+00', 'Python', '21', '# powerpoint MCP server

A MCP server project that creates powerpoint presentations

## Components

### Tools

The server implements multiple tools:
- ```create-presentation```: Starts a presentation
  - Takes "name"  as required string arguments
  - Creates a presentation object
- ```add-slide-title-only```: Adds a title slide to the presentation
  - Takes "presentation_name" and "title" as required string arguments
  - Creates a title slide with "title" and adds it to presentation
- ```add-slide-section-header```: Adds a section header slide to the presentation
  - Takes "presentation_name" and "header" as required string arguments
  - Creates a section header slide with "header" (and optionally "subtitle") and adds it to the presentation
- ```add-slide-title-content```: Adds a title with content slide to the presentation
  - Takes "presentation_name", "title", "content" as required string arguments
  - Creates a title with content slide with "title" and "content" and adds it to presentation
- ```add-slide-title-with-table```: Adds a title slide with a table
  - Takes "presentation_name", "title", "data" as required string and array arguments
  - Creates a title slide with "title" and adds a table dynamically built from data
- ```add-slide-title-with-chart```: Adds a title slide with a chart
  - Takes "presentation_name", "title", "data" as required string and object arguments
  - Creates a title slide with "title" and adds a chart dynamically built from data. Attempts to figure out the best type of chart from the data source.
- ```add-slide-picture-with-caption```: Adds a picture with caption slide
  - Takes "presentation_name", "title", "caption", "image_path" as required string arguments
  - Creates a picture with caption slide using the supplied "title", "caption", and "image_path". Can either use images created via the "generate-and-save-image" tool or use an "image_path" supplied by the user (image must exist in folder_path)
- ```open-presentation```: Opens a presentation for editing
  - Takes "presentation_name" as required arguments
  - Opens the given presentation and automatically saves a backup of it as "backup.pptx"
  - This tool allows the client to work with existing pptx files and add slides to them. Just make sure the client calls "save-presentation" tool at the end.
- ```save-presentation```: Saves the presentation to a file.
  - Takes "presentation_name" as required arguments.
  - Saves the presentation to the folder_path. The client must call this tool to finalize the process.
- ```generate-and-save-image```: Generates an image for the presentation using a FLUX model
  - Takes "prompt" and "file_name" as required string arguments
  - Creates an image using the free FLUX model on TogetherAI (requires an API key)





## Configuration

An environment variable is required for image generation via TogetherAI
Register for an account: https://api.together.xyz/settings/api-keys

```
"env": {
        "TOGETHER_API_KEY": "api_key"
      }
```

A folder_path is required. All presentations and images will be saved to this folder.

```
"--folder-path",
        "/path/to/decks_folder"
```

## Quickstart

### Install

#### Make sure you have UV installed

MacOS/Linux
```
curl -LsSf https://astral.sh/uv/install.sh | sh
```

Windows
```
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```

#### Clone the repo

```
git clone https://github.com/supercurses/powerpoint.git
```

#### Claude Desktop

On MacOS: `~/Library/Application\ Support/Claude/claude_desktop_config.json`
On Windows: `%APPDATA%/Claude/claude_desktop_config.json`

- ```--directory```: the path where you cloned the repo above
- ```--folder-path```: the path where powerpoint decks and images will be saved to. Also the path where you should place any images you want the MCP server to use.


```
  # Add the server to your claude_desktop_config.json
  "mcpServers": {
    "powerpoint": {
      "command": "uv",
      "env": {
        "TOGETHER_API_KEY": "api_key"
      },
      "args": [
        "--directory",
        "/path/to/powerpoint",
        "run",
        "powerpoint",
        "--folder-path",
        "/path/to/decks_folder"
      ]
    }
```

### Usage Examples

```
Create a presentation about fish, create some images and include tables and charts
```

```
Create a presentation about the attached paper. Please use the following images in the presentation:
author.jpeg
```

Assuming you have SQLite MCP Server installed.
```
Review 2024 Sales Data table. Create a presentation showing current trends, use tables and charts as appropriate

```


# License

This MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.
', 'true', '"{\"Function Calling\",\"Protocol Implementations\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository implements a server designed to be used with MCP clients to create and manipulate PowerPoint presentations. The tools provided are for managing presentations, slides, and images.","categories":["Function Calling","Protocol Implementations","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The repository implements a MCP server with distinct tools for managing PowerPoint presentations. It follows a modular design, separating concerns into specific functions. It uses environment variables for API key management. The architecture revolves around receiving requests to execute tools and manage presentations, including generating images.","Implementation Details and Key Features":"Key features include creating presentations, adding slides (title only, section headers, content, tables, charts, pictures with captions), opening and saving presentations, and generating images using a FLUX model. The server interacts with the file system for presentation storage and image handling. It uses the TogetherAI API for image generation.","Integration Approaches and Protocols":"The repository integrates with MCP clients through defined tool interfaces (e.g., `create-presentation`, `add-slide-title-content`). It expects the client to call the appropriate tool with required arguments. It supports both creating new presentations and editing existing ones. Integration with Claude Desktop is explicitly mentioned using `claude_desktop_config.json` file.","Technical Stack and Dependencies":"The primary dependencies aren't explicitly stated, but implicitly includes libraries for PowerPoint manipulation, image generation via TogetherAI API, and a server framework (likely Python given the UV installation instructions). Requires TogetherAI API key for image generation. The use of UV suggests it's likely leveraging Python and related package management.","Best Practices and Coding Standards":"The repository documents the tool interfaces with clear argument descriptions. Configuration details, such as API key setup and folder path requirements, are well-documented. The inclusion of a LICENSE file (MIT License) and installation instructions demonstrates adherence to standard development practices. Error handling and input validation are not visible from the documentation.","Performance Considerations":"Performance considerations are tied to image generation (TogetherAI API) and presentation file I/O. Generating images can be a bottleneck, depending on API response times. Large presentations might impact memory usage and saving times. No specific optimizations or performance monitoring tools are mentioned."}}', '2025-03-10 21:28:34+00', 'true', 'Servers', 'mcp-powerpoint-server'), ('942660117', 'mcp-server-opendal', 'Xuanwo/mcp-server-opendal', 'https://github.com/Xuanwo/mcp-server-opendal', 'Model Context Protocol Server for Apache OpenDAL™', '2025-03-04 13:15:53+00', '2025-03-10 05:59:36+00', 'Python', '25', '# Model Context Protocol Server for Apache OpenDAL™
A Model Context Protocol (MCP) server implementation that provides access to various storage services via [Apache OpenDAL™](https://opendal.apache.org/).

[![PyPI - Version](https://img.shields.io/pypi/v/mcp-server-opendal)](https://pypi.org/project/mcp-server-opendal/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/mcp-server-opendal)](https://pypi.org/project/mcp-server-opendal/)
[![Lint and Test](https://github.com/Xuanwo/mcp-server-opendal/actions/workflows/test.yml/badge.svg)](https://github.com/Xuanwo/mcp-server-opendal/actions/workflows/test.yml)

## Features

- Seamless access to multiple storage services including S3, Azure Blob Storage, Google Cloud Storage, and more
- List files and directories from storage services
- Read file contents with automatic text/binary detection
- Environment variable based configuration

## Installation

```shell
pip install mcp-server-opendal
```

## Usage

Configure storage services by setting environment variables. Each service requires a prefix and specific configuration options.

For example, to configure an S3 service with alias "mys3":

```
OPENDAL_MYS3_TYPE=s3
OPENDAL_MYS3_BUCKET=mybucket
OPENDAL_MYS3_REGION=us-east-1
OPENDAL_MYS3_ENDPOINT=http://localhost:9000
OPENDAL_MYS3_ACCESS_KEY_ID=myaccesskey
OPENDAL_MYS3_SECRET_ACCESS_KEY=mysecretkey
```

Then you can use tool like `read` and `list` with `mys3://path/to/file`.

`mcp-server-opendal` will also load from `.env`.

## Development

```shell
npx @modelcontextprotocol/inspector \
  uv run mcp-server-opendal
```
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Storage Solutions\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states that it's a Model Context Protocol (MCP) server implementation, designed to provide access to storage services, aligning with MCP's goal of managing context for AI models.","categories":["Protocol Implementations","API Frameworks","Context Management"],"analysis":{"Architecture and Design Patterns":"The repository implements a server that adheres to the Model Context Protocol, providing a standardized interface for AI models to interact with data stored in Apache OpenDAL. The design likely follows a modular approach, allowing easy integration of various storage services. Environment variables are used for configuration, promoting flexibility.","Implementation Details and Key Features":"The core functionality involves exposing OpenDAL's features, such as listing files and reading file contents, through an MCP-compliant interface. Automatic text/binary detection is a key feature. The use of environment variables simplifies the configuration process for various storage services like S3 and Azure Blob Storage.","Integration Approaches and Protocols":"The repository integrates with Apache OpenDAL to provide access to diverse storage backends. It utilizes environment variables for configuration, defining the necessary parameters for each service. The integration approach allows AI models to interact with data through a unified MCP interface, abstracting away the specifics of the storage service.","Technical Stack and Dependencies":"The repository relies on Python and Apache OpenDAL. The installation instructions specify using `pip`, indicating a Python-based implementation. The use of `uv run` suggests usage of the uv package installer and runner for Python. The project also uses `npx @modelcontextprotocol/inspector` for development.","Best Practices and Coding Standards":"The presence of a test workflow (`test.yml`) in GitHub Actions indicates a commitment to testing. The use of environment variables for configuration is a good practice for managing secrets and adapting to different environments. The clear documentation and examples promote usability and maintainability.","Performance Considerations":"Performance depends heavily on the underlying storage services accessed through OpenDAL. Optimizations could involve caching frequently accessed data, using efficient data formats, and tuning OpenDAL's configuration for each storage backend. Network latency between the MCP server and the storage services is also a crucial factor."}}', '2025-03-10 21:28:52+00', 'true', 'Servers', 'mcp-server-opendal'), ('942689727', 'mcp', 'jsonresume/mcp', 'https://github.com/jsonresume/mcp', 'The registry mcp server updates your resume while you code', '2025-03-04 14:11:41+00', '2025-03-10 13:04:32+00', 'TypeScript', '20', '# JSON Resume MCP Server

<div align="center">

![JSON Resume Logo](https://jsonresume.org/img/logo.svg)

[![npm version](https://img.shields.io/npm/v/@jsonresume/mcp.svg)](https://www.npmjs.com/package/@jsonresume/mcp)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![GitHub Issues](https://img.shields.io/github/issues/jsonresume/mcp.svg)](https://github.com/jsonresume/mcp/issues)
[![smithery badge](https://smithery.ai/badge/@jsonresume/mcp)](https://smithery.ai/server/@jsonresume/mcp)

**Automate your resume updates with AI by analyzing your coding projects**

[Installation](#installation) • [Features](#features) • [Usage](#usage) • [Configuration](#configuration) • [Contributing](#contributing) • [Testing](#testing)

</div>

## What is JSON Resume MCP Server?

This is a [Model Context Protocol (MCP)](https://modelcontextprotocol.ai) server that enhances AI assistants with the ability to update your [JSON Resume](https://jsonresume.org) by analyzing your coding projects. The MCP server provides tools that allow AI assistants like those in [Windsurf](https://www.windsurf.io/) or [Cursor](https://cursor.sh/) to:

1. Check if you have an existing JSON Resume
2. Analyze your codebase to understand your technical skills and projects
3. Enhance your resume with details about your current project

With this tool, you can simply ask your AI assistant to "enhance my resume with my current project," and it will automatically analyze your code, extract relevant skills and project details, and update your resume accordingly.

Video demo: [https://x.com/ajaxdavis/status/1896953226282594381](https://x.com/ajaxdavis/status/1896953226282594381)

## Features

- **Resume Enhancement**: Automatically analyzes your codebase and adds project details to your resume
- **GitHub Integration**: Fetches and updates your resume stored in GitHub Gists
- **AI-Powered**: Uses OpenAI to generate professional descriptions of your projects and skills
- **TypeScript/Zod Validation**: Ensures your resume follows the JSON Resume standard
- **JSON Resume Ecosystem**: Compatible with the [JSON Resume registry](https://registry.jsonresume.org)

## Installation

### Prerequisites

- GitHub account with a personal access token (with gist scope)
- OpenAI API key
- Node.js 18+
- An IDE with MCP support (Windsurf or Cursor)

### Installing via Smithery

To install mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@jsonresume/mcp):

```bash
npx -y @smithery/cli install @jsonresume/mcp --client claude
```

### Install via NPM

```bash
npm install -g @jsonresume/mcp
```

### Install in Windsurf or Cursor

Add the following to your Windsurf or Cursor configuration:

#### Windsurf

Open Settings → MCP Servers and add:

```json
{
  "jsonresume": {
    "command": "npx",
    "args": ["-y", "@jsonresume/mcp"],
    "env": {
      "GITHUB_TOKEN": "your-github-token",
      "OPENAI_API_KEY": "your-openai-api-key",
      "GITHUB_USERNAME": "your-github-username"
    }
  }
}
```

#### Cursor

Add to your `~/.cursor/mcp_config.json`:

```json
{
  "mcpServers": {
    "jsonresume": {
      "command": "npx",
      "args": ["-y", "@jsonresume/mcp"],
      "env": {
        "GITHUB_TOKEN": "your-github-token",
        "OPENAI_API_KEY": "your-openai-api-key",
        "GITHUB_USERNAME": "your-github-username"
      }
    }
  }
}
```

## Usage

Once installed and configured, you can use the following commands with your AI assistant:

### Enhance Your Resume with Current Project

Ask your AI assistant:
```
"Can you enhance my resume with details from my current project?"
```

The assistant will:
1. Find your existing resume on GitHub (or create a new one if needed)
2. Analyze your current project's codebase
3. Generate professional descriptions of your project and skills
4. Update your resume with the new information
5. Save the changes back to GitHub
6. Provide a link to view your updated resume

### Check Your Resume Status

Ask your AI assistant:
```
"Can you check if I have a JSON Resume?"
```

The assistant will check if you have an existing resume and show its details.

### Analyze Your Codebase

Ask your AI assistant:
```
"What technologies am I using in this project?"
```

The assistant will analyze your codebase and provide insights about languages, technologies, and recent commits.

## Configuration

The MCP server requires the following environment variables:

| Variable | Description |
|----------|-------------|
| `GITHUB_TOKEN` | Your GitHub personal access token with gist permissions |
| `GITHUB_USERNAME` | Your GitHub username |
| `OPENAI_API_KEY` | Your OpenAI API key |

## Development

To run the server in development mode:

1. Clone the repository:
```bash
git clone https://github.com/jsonresume/mcp.git
cd mcp
```

2. Install dependencies:
```bash
npm install
```

3. Run in development mode:
```bash
npm run dev
```

This starts the MCP server with the inspector tool for debugging.

## Contributing

Contributions are welcome! Here's how you can contribute:

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Commit your changes: `git commit -m 'Add some amazing feature'`
4. Push to the branch: `git push origin feature/amazing-feature`
5. Open a Pull Request

Please ensure your code follows the existing style and includes appropriate tests.

## Testing

The MCP server includes several test scripts to help debug and verify functionality.

### Running Tests

All test scripts are located in the `tests/` directory.

Before running tests, set your environment variables:

```bash
export GITHUB_TOKEN=your_github_token
export OPENAI_API_KEY=your_openai_api_key
export GITHUB_USERNAME=your_github_username
```

#### Check OpenAI API Key

Validates that your OpenAI API key is working correctly:

```bash
npx tsx tests/check-openai.ts
```

#### Mock Resume Enhancement

Tests the resume enhancement functionality using mock data (no API calls):

```bash
npx tsx tests/debug-mock.ts
```

#### Full Resume Enhancement Test

Tests the complete resume enhancement process with live API calls:

```bash
npx tsx tests/debug-enhance.ts
```

#### MCP Protocol Test

Tests the MCP server protocol communication:

```bash
node tests/test-mcp.js
```

### Adding to package.json

For convenience, you can add these test commands to your package.json:

```json
"scripts": {
  "test:openai": "tsx tests/check-openai.ts",
  "test:mock": "tsx tests/debug-mock.ts",
  "test:enhance": "tsx tests/debug-enhance.ts",
  "test:mcp": "node tests/test-mcp.js"
}
```

Then run them with `npm run test:mock`, etc.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- [JSON Resume](https://jsonresume.org) for the resume standard
- [Model Context Protocol](https://modelcontextprotocol.ai) for enabling AI tool integration
- [OpenAI](https://openai.com) for powering the AI resume enhancements
', 'true', '"{\"AI Model Integration\",\"Function Calling\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository is explicitly designed as a Model Context Protocol (MCP) server, allowing AI assistants to update JSON Resumes based on codebase analysis. It integrates with tools like Windsurf and Cursor via MCP.","categories":["AI Model Integration","Function Calling","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The repository implements a server designed to communicate with AI assistants via the Model Context Protocol (MCP). It features modular components for interacting with GitHub Gists (resume storage), OpenAI (content generation), and code analysis tools. The architecture leverages TypeScript and Zod for validation and type safety, ensuring data integrity.","Implementation Details and Key Features":"The server provides functionalities to enhance a JSON Resume based on the current coding project.  Key features include resume enhancement, GitHub integration via API, AI-powered descriptions using OpenAI, and validation via TypeScript/Zod.  It uses environment variables for sensitive information such as API keys and tokens.","Integration Approaches and Protocols":"The primary integration is through the Model Context Protocol (MCP), enabling AI assistants to trigger resume updates. The server integrates with GitHub Gists for storing and retrieving resumes and uses the OpenAI API for generating project descriptions. The documentation provides integration examples for Windsurf and Cursor.","Technical Stack and Dependencies":"The repository primarily uses Node.js, TypeScript, and relies on libraries like `@jsonresume/registry` and OpenAI's API.  Zod is used for schema validation, ensuring the resume follows the JSON Resume standard. The project also leverages npm for package management and deployment.","Best Practices and Coding Standards":"The project is written in TypeScript, promoting type safety and maintainability. The use of Zod for data validation ensures data integrity. The repository includes linting and formatting configurations (likely ESLint and Prettier) to enforce consistent coding style, which are recommended best practices for collaborative development.","Performance Considerations":"Performance considerations are relevant due to the reliance on external API calls to GitHub and OpenAI. Caching strategies may be beneficial. Efficient code analysis techniques are important to minimize processing time. Asynchronous operations (e.g., API calls) are likely used to avoid blocking the server."}}', '2025-03-10 21:28:27+00', 'true', 'Servers', 'mcp-resume-server'), ('942843747', 'cargo-doc-mcp', 'spacemeowx2/cargo-doc-mcp', 'https://github.com/spacemeowx2/cargo-doc-mcp', null, '2025-03-04 19:08:45+00', '2025-03-05 12:48:58+00', 'TypeScript', '3', '# cargo doc MCP Server

A MCP server for managing Rust documentation through cargo doc commands. This server provides tools to check, build, and search Rust documentation locally.

## Features

### Tools

- `get_crate_doc` - Get crate's main documentation page for understanding overall concepts and usage

  - Parameters:
    - `project_path`: Path to the Rust project (must be absolute path)
    - `crate_name`: Name of the crate to get documentation for

- `list_symbols` - List all symbols (structs, enums, traits, etc.) in a crate's documentation

  - Parameters:
    - `project_path`: Path to the Rust project (must be absolute path)
    - `crate_name`: Name of the crate to list symbols for

- `search_doc` - Search within a crate's documentation
  - Parameters:
    - `project_path`: Path to the Rust project (must be absolute path)
    - `crate_name`: Name of the crate to search in
    - `query`: Search query (keyword or symbol)
    - `limit` (optional): Maximum number of results to return (default: 10)

## Requirements

- Node.js 16 or later
- Rust and Cargo installed

## Installation

Install dependencies:

```bash
pnpm install
```

Build the server:

```bash
pnpm run build
```

For development with auto-rebuild:

```bash
pnpm run watch
```

## Usage

Add the following configuration:

```json
{
  "mcpServers": {
    "docs-rs-mcp": {
      "command": "/absolute/path/to/docs-rs-mcp/build/index.js"
    }
  }
}
```

## Debugging

Since MCP servers communicate over stdio, debugging can be challenging. We recommend using the MCP Inspector:

```bash
pnpm run inspector
```

The Inspector will provide a URL to access debugging tools in your browser.

## Cache System

The server maintains a cache of built documentation paths to improve performance. Cache entries expire after 24 hours to ensure documentation stays up-to-date.

## License

MIT
', 'true', '"{\"Developer Tooling\",\"API Frameworks\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository implements a MCP server specifically designed for managing and interacting with Rust documentation, aligning with the concept of communication protocols between tools.","categories":["Protocol Implementations","Developer Tooling","Function Calling"],"analysis":{"Architecture and Design Patterns":"The server appears to follow a modular architecture, exposing tools for interacting with Rust documentation via defined parameters. The use of a cache system suggests an attempt to optimize performance. It likely uses a request-response pattern through stdio as indicated by the debugging recommendation with the MCP Inspector.","Implementation Details and Key Features":"The implementation uses Node.js for the server and leverages `cargo doc` commands to generate documentation. Key features include tools for retrieving crate documentation, listing symbols, and searching within documentation. It caches built documentation paths and manages expiration to keep the documentation updated.","Integration Approaches and Protocols":"The server integrates with Rust projects by providing tools that interact with `cargo doc`. It uses stdio for communication, as indicated in the debugging section. The JSON configuration suggests that the server adheres to a specific protocol for defining MCP servers and tools, allowing for easy configuration.","Technical Stack and Dependencies":"The server is built using Node.js and relies on Rust and Cargo. It uses `pnpm` for package management. The documentation mentions Node.js version 16 or later as a requirement. The server likely relies on other dependencies managed by `pnpm` for building and running the server.","Best Practices and Coding Standards":"The provided information does not reveal coding practices. However, the structure of the documentation indicates a focus on clarity. The use of a cache system suggests an awareness of performance optimization. Error handling isn't explicitly mentioned but is implicitly important for robust operation.","Performance Considerations":"The server uses a cache system to store built documentation paths, which improves performance by avoiding redundant calls to `cargo doc`. Cache entries expire after 24 hours, balancing performance and documentation freshness. The option to limit the number of search results also helps in optimizing search queries."}}', '2025-03-10 21:54:04+00', 'true', 'Servers', 'cargo-doc-mcp'), ('942918420', 'mcp-cryptowallet-evm', 'dcSpark/mcp-cryptowallet-evm', 'https://github.com/dcSpark/mcp-cryptowallet-evm', null, '2025-03-04 22:14:59+00', '2025-03-10 17:59:40+00', 'TypeScript', '2', '# MCP Crypto Wallet EVM

This repository contains a Model Context Protocol (MCP) server that provides Claude with access to Ethereum and EVM-compatible blockchain operations via ethers.js v5. The server enables Claude to perform operations like creating wallets, checking balances, sending transactions, and interacting with smart contracts on EVM-compatible blockchains.

## Overview

The MCP server exposes the following tools to Claude:

### Wallet Creation and Management
- `wallet_create_random`: Create a new wallet with a random private key
- `wallet_from_private_key`: Create a wallet from a private key
- `wallet_from_mnemonic`: Create a wallet from a mnemonic phrase
- `wallet_from_encrypted_json`: Create a wallet by decrypting an encrypted JSON wallet
- `wallet_encrypt`: Encrypt a wallet with a password

### Wallet Properties
- `wallet_get_address`: Get the wallet address
- `wallet_get_public_key`: Get the wallet public key
- `wallet_get_private_key`: Get the wallet private key (with appropriate security warnings)
- `wallet_get_mnemonic`: Get the wallet mnemonic phrase (if available)

### Blockchain Methods
- `wallet_get_balance`: Get the balance of the wallet
- `wallet_get_chain_id`: Get the chain ID the wallet is connected to
- `wallet_get_gas_price`: Get the current gas price
- `wallet_get_transaction_count`: Get the number of transactions sent from this account (nonce)
- `wallet_call`: Call a contract method without sending a transaction

### Transaction Methods
- `wallet_send_transaction`: Send a transaction
- `wallet_sign_transaction`: Sign a transaction without sending it
- `wallet_populate_transaction`: Populate a transaction with missing fields

### Signing Methods
- `wallet_sign_message`: Sign a message
- `wallet_sign_typed_data`: Sign typed data (EIP-712)
- `wallet_verify_message`: Verify a signed message
- `wallet_verify_typed_data`: Verify signed typed data

### Provider Methods
- `provider_get_block`: Get a block by number or hash
- `provider_get_transaction`: Get a transaction by hash
- `provider_get_transaction_receipt`: Get a transaction receipt
- `provider_get_code`: Get the code at an address
- `provider_get_storage_at`: Get the storage at a position for an address
- `provider_estimate_gas`: Estimate the gas required for a transaction
- `provider_get_logs`: Get logs that match a filter
- `provider_get_ens_resolver`: Get the ENS resolver for a name
- `provider_lookup_address`: Lookup the ENS name for an address
- `provider_resolve_name`: Resolve an ENS name to an address

### Network Methods
- `network_get_network`: Get the current network information
- `network_get_block_number`: Get the current block number
- `network_get_fee_data`: Get the current fee data (base fee, max priority fee, etc.)

## Prerequisites

- Node.js (v16 or higher)
- Claude Desktop application

## Installation

1. Clone this repository:
   ```bash
   git clone https://github.com/dcSpark/mcp-cryptowallet-evm.git
   cd mcp-cryptowallet-evm
   ```

2. Install dependencies:
   ```bash
   npm ci
   ```

3. Build the project:
   ```bash
   npm run build
   ```

## Configuration

### Configure Claude Desktop

To configure Claude Desktop to use this MCP server:

1. Open Claude Desktop
2. Navigate to the Claude Desktop configuration file:
   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`
   - Windows: `%APPDATA%\Claude\claude_desktop_config.json`
   - Linux: `~/.config/Claude/claude_desktop_config.json`

3. Add the MCP server configuration:

```json
{
  "mcpServers": {
    "mcp-cryptowallet-evm": {
      "command": "node",
      "args": [
        "/path/to/your/mcp-cryptowallet-evm/build/index.js"
      ]
    }
  }
}
```

### Running Locally

```bash
node build/index.js
```

## Usage

Once configured, restart Claude Desktop. Claude will now have access to the Ethereum and EVM-compatible blockchain tools. You can ask Claude to:

1. Create a new wallet:
   ```
   Can you create a new Ethereum wallet for me?
   ```

2. Check a wallet balance:
   ```
   What's the balance of the Ethereum wallet address 0x742d35Cc6634C0532925a3b844Bc454e4438f44e?
   ```

3. Send a transaction:
   ```
   Can you help me send 0.1 ETH to 0x742d35Cc6634C0532925a3b844Bc454e4438f44e?
   ```

Claude will use the MCP server to interact with the Ethereum blockchain directly.

## Development

### Adding New Tools

To add new tools to the MCP server:

1. Define the tool in `src/tools.ts`
2. Create a handler function in the appropriate handler file
3. Add the handler to the `handlers` object in `src/tools.ts`

### Building

```bash
npm run build
```

## License

MIT
', 'true', '"{\"Protocol Implementations\",\"AI Model Integration\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it's a Model Context Protocol (MCP) server providing Claude access to Ethereum and EVM blockchain operations. It exposes tools for wallet management, transactions, and blockchain data retrieval.","categories":["Agent Frameworks","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The architecture involves an MCP server that exposes Ethereum and EVM-compatible blockchain functionalities to Claude via specific tools. The design implements a request-response pattern, where Claude initiates requests for blockchain operations, and the server handles the interaction with the Ethereum network using ethers.js. It uses a modular approach, separating tools and handlers for extensibility.","Implementation Details and Key Features":"The implementation utilizes Node.js and ethers.js v5 to interact with EVM-compatible blockchains. Key features include wallet creation and management, transaction handling (sending, signing, populating), and blockchain data retrieval via provider methods. The server exposes these functionalities as distinct tools accessible through the MCP interface. Error handling and security warnings are also incorporated, particularly for private key management.","Integration Approaches and Protocols":"The repository integrates with Claude Desktop using a defined configuration that specifies the command and arguments to execute the MCP server. It relies on the MCP protocol for communication between Claude and the server. The server uses ethers.js to interface with Ethereum nodes, allowing for standard JSON-RPC calls to interact with the blockchain. It supports standard EVM functionalities.","Technical Stack and Dependencies":"The technical stack consists of Node.js (v16 or higher), ethers.js v5, and potentially other npm dependencies defined in the `package.json` file. The project uses npm for dependency management. The code is written in TypeScript and compiled into JavaScript. The application interacts with the Ethereum blockchain via an Ethereum provider (likely Infura or a local node).","Best Practices and Coding Standards":"The repository seems to follow standard JavaScript/TypeScript coding conventions. There is a `build` script that indicates the use of a compiler (likely tsc) to transpile the TypeScript code. The documentation includes installation and usage instructions, suggesting attention to developer experience. Modular design promotes maintainability and extensibility by separating tools and handlers. Security warnings are given concerning private key access.","Performance Considerations":"Performance considerations are related to the efficiency of the ethers.js library and the speed of the Ethereum network. The server should efficiently manage asynchronous calls to the blockchain to minimize latency. Estimating gas costs before sending transactions and properly handling transaction confirmations are crucial. The selection of a reliable and performant Ethereum provider (e.g., Infura) is also important."}}', '2025-03-10 21:26:36+00', 'true', 'Servers', 'mcp-evm-wallet'), ('942928935', 'veyrax-mcp', 'VeyraX/veyrax-mcp', 'https://github.com/VeyraX/veyrax-mcp', null, '2025-03-04 22:48:31+00', '2025-03-10 05:06:00+00', 'TypeScript', '3', 'Below is a sample **README.md** that you can use on GitHub to introduce and document **VeyraX MCP**. Simply copy, paste, and adjust as needed!

---

# VeyraX MCP

**Single tool to control them all** — VeyraX MCP is the only connection you need to access all your tools in any MCP-compatible environment.

- **[Get Access](https://www.veyrax.com/register)**
- **[Explore Docs](https://docs.veyrax.com/mcp)**

---

## Model Context Protocol (MCP)

### Introduction
VeyraX MCP (Model Context Protocol) is an open protocol that allows you to provide custom tools to agentic LLMs. By connecting once to the VeyraX platform, you can use all tools you’ve already integrated with VeyraX in any MCP-compatible environment—such as **Claude**, **Cursor**, **VS Code**, or **Windserf**—without juggling multiple authentications.

### Why Choose VeyraX?
- **Single Authentication**: Connect once in VeyraX, then use your tools across all MCP clients without separate logins or credentials.
- **Instant Access to All Tools**: Any tool you connect to VeyraX is immediately available in your favorite editor or AI assistant.
- **5-Minute Setup**: Connect VeyraX to any MCP client in under five minutes.

---

## Getting Started

1. **Sign up for VeyraX**  
   Create a free account at [https://www.veyrax.com/register](https://www.veyrax.com/register).

2. **Open the VeyraX Platform to Get Your API Key**  
   You can find your API key in your VeyraX dashboard.  
   > **Tip:** Look for the “API Key” section in your account settings.

3. **Choose Your Configuration**  
   Copy the complete configuration (including your API key) directly from the VeyraX platform.

4. **Select an MCP Client**  
   Decide which environment or editor you want to integrate with (e.g., Cursor, Claude, Windserf, VS Code).

5. **Follow the Setup Guide**  
   Either:
   - Use the automatic installation method (you’ll be prompted for your API key), **OR**
   - Manually paste the copied configuration into your MCP client’s settings.

That’s it! You’ll now have instant access to every tool you’ve linked to VeyraX in all your favorite editor and AI assistant environments.

---

## Supported MCP Clients

- **[Cursor](https://docs.veyrax.com/mcp/cursor)**  
  Add VeyraX MCP to Cursor IDE in minutes.

- **[Claude](https://docs.veyrax.com/mcp/claude)**  
  Use all your VeyraX-connected tools directly in Claude.

- **[Windserf](https://docs.veyrax.com/mcp/windserf)**  
  Integrate with the Windserf IDE for a seamless coding experience.

- **[VS Code](https://docs.veyrax.com/mcp/vscode)**  
  Connect VeyraX MCP to VSCode IDE via **Cline**.

---

### Have Questions?
If you have any questions, check out our [documentation](https://docs.veyrax.com/mcp) or [contact support](mailto:support@veyrax.com). 

Enjoy a unified, hassle-free approach to using all your favorite tools anywhere, all thanks to **VeyraX MCP**! 

---

**Happy coding!**  

— Team VeyraX

--- 

> *This README is a brief starter guide. For detailed instructions and troubleshooting, please visit the [official VeyraX docs](https://docs.veyrax.com/mcp).*
', 'true', '"{\"Protocol Implementations\",\"Agent Frameworks\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly mentions Model Context Protocol (MCP) and aims to provide a single connection point for tools to agentic LLMs. It also mentions MCP-compatible environments like Claude, Cursor, VS Code, and Windserf.","categories":["Protocol Implementations","Agent Frameworks","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The architecture appears to be centered around providing a unified access point (VeyraX platform) for various tools that can be used by agentic LLMs. It follows a hub-and-spoke model, where VeyraX acts as the central hub, connecting different tools (spokes) to various MCP-compatible clients. The design focuses on simplifying authentication and tool integration.","Implementation Details and Key Features":"The implementation details are centered on a platform (VeyraX) providing an API key to access registered tools. Key features include single authentication, instant tool access across multiple MCP clients, and a reportedly quick setup time. The integration process seems to involve configuring the MCP client with the VeyraX API key.","Integration Approaches and Protocols":"The primary integration approach involves utilizing the Model Context Protocol (MCP) to connect the VeyraX platform to various client environments. The integration relies on configuring the MCP client (e.g., Cursor, Claude) with the API key obtained from VeyraX. Automatic and manual configuration methods are supported for integration with MCP clients.","Technical Stack and Dependencies":"The technical stack is not explicitly defined, but based on the description, it likely involves API endpoints for managing tool connections and authentication. The VeyraX platform likely relies on standard web technologies for its dashboard and API. The dependencies include the MCP clients themselves (Cursor, Claude, etc.) and any libraries needed for API communication.","Best Practices and Coding Standards":"The README encourages users to consult the official documentation for detailed instructions and troubleshooting. While specific coding standards aren't mentioned in the provided documentation, the focus on a unified API and authentication suggests an emphasis on secure and standardized access to tools for LLMs.","Performance Considerations":"Performance considerations are not explicitly mentioned, but the focus on 'instant access' implies that the VeyraX platform aims for low-latency communication between the MCP clients and the connected tools. Efficiency in API communication and tool execution within the MCP client would be important for optimal performance."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'veyrax-mcp-server'), ('942964282', 'ClaudeHopper', 'Arborist-ai/ClaudeHopper', 'https://github.com/Arborist-ai/ClaudeHopper', 'A macOS menu bar application that helps manage MCP (Model Context Protocol) servers for Claude Desktop.', '2025-03-05 00:50:58+00', '2025-03-10 16:27:55+00', 'Swift', '1', '>[!Warning] This is not a working app. Recommend using https://github.com/metatool-ai/metatool-app instead.

# ClaudeHopper

A macOS menu bar application that helps manage MCP (Model Context Protocol) servers for Claude Desktop.

<p align="center">
  <img src="screenshots/claudehopper-icon.png" width="150" alt="ClaudeHopper Icon">
</p>

## Overview

ClaudeHopper provides a simple, intuitive interface for managing MCP servers that extend Claude Desktop's capabilities. With ClaudeHopper, you can:

- Toggle servers on/off directly from your menu bar
- Discover available MCP servers from authoritative sources
- Fetch up-to-date configurations for servers
- Manually add custom servers
- Check for new servers periodically
- Import server configurations directly from clipboard

## Features

- **Server Management**: Enable, disable, add, and remove MCP servers from your Claude Desktop configuration
- **Real Server Discovery**: Fetch available servers from the official MCP servers repository
- **Clipboard Monitoring**: Automatically detect and import server configurations copied from websites
- **Configuration Backups**: Easily backup and restore your server configurations
- **User-Friendly Interface**: Simple menu bar app with intuitive controls

## What is MCP?

MCP (Model Context Protocol) standardizes how applications provide context to Large Language Models (LLMs). It allows Claude Desktop to access external tools like filesystem access, web search, and more through standardized server implementations.

## Getting Started

### Prerequisites

- macOS 11.0 or later
- [Claude Desktop](https://claude.ai/desktop) installed
- Node.js (for running MCP servers)

### Installation

1. Download the latest release of ClaudeHopper from the [Releases](https://github.com/Arborist-ai/ClaudeHopper/releases) page
2. Move ClaudeHopper.app to your Applications folder
3. Launch ClaudeHopper from your Applications folder
4. Click the ClaudeHopper icon in the menu bar to start managing your MCP servers

### Configuration

On first launch, ClaudeHopper will try to locate your Claude Desktop configuration file automatically. If it can't find it, you'll be prompted to select it manually.

The default location for Claude Desktop's configuration is:
- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`

## Development

ClaudeHopper is written in Swift and SwiftUI. To build from source:

1. Clone this repository
2. Open the project in Xcode 14 or later
3. Build and run the project

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- [Anthropic](https://anthropic.com) for creating Claude
- The [MCP Servers repository](https://github.com/modelcontextprotocol/servers) for providing standard server implementations
', 'true', '"{\"Context Management\",\"Function Calling\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly mentions Model Context Protocol (MCP) and aims to manage MCP servers for Claude Desktop, facilitating external tool access for the LLM.","categories":["Agent Frameworks","API Frameworks","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The application follows a menu bar application architecture on macOS. It leverages Swift and SwiftUI for UI development. The design pattern appears to be centered around managing and toggling MCP servers, fetching configurations, and providing a user-friendly interface for Claude Desktop integration. It backups configurations as well.","Implementation Details and Key Features":"Key features include server management (enabling/disabling/adding/removing), real server discovery from authoritative sources, clipboard monitoring for configuration import, and configuration backups. Implementation involves accessing and modifying Claude Desktop's configuration file, likely using standard file system APIs in Swift.","Integration Approaches and Protocols":"The repository integrates with Claude Desktop via the Model Context Protocol (MCP). It aims to manage the connection and configuration of MCP servers to extend Claude Desktop's capabilities. The app also fetches server information from the MCP Servers repository on GitHub.","Technical Stack and Dependencies":"The primary technologies are Swift and SwiftUI for the macOS application. Dependencies include macOS 11.0 or later and Claude Desktop. Node.js is listed as a prerequisite for running MCP servers themselves, but isn't needed for running the ClaudeHopper app. Xcode 14 or later is needed for development.","Best Practices and Coding Standards":"The repository uses standard macOS development practices, using Swift and SwiftUI. The project has a clear structure, including a license file. While the README provides some documentation, the \"Warning\" suggests this repository might not adhere to all recommended coding standards.","Performance Considerations":"Performance considerations are likely focused on the responsiveness of the menu bar application and the efficiency of server management operations. Asynchronous operations might be used for network requests (fetching server lists) and file system access to avoid blocking the UI thread. It also periodically checks for new servers."}}', '2025-03-10 21:26:21+00', 'true', 'Servers', 'claudehopper-mcp-manager'), ('942985920', 'scrapegraph-mcp', 'ScrapeGraphAI/scrapegraph-mcp', 'https://github.com/ScrapeGraphAI/scrapegraph-mcp', 'ScapeGraph MCP Server', '2025-03-05 02:00:38+00', '2025-03-05 13:36:03+00', 'Python', '5', '# ScrapeGraph MCP Server

![ScapeGraph Smithery Integration](assets/sgai_smithery.png)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10](https://img.shields.io/badge/python-3.10-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![smithery badge](https://smithery.ai/badge/@ScrapeGraphAI/scrapegraph-mcp)](https://smithery.ai/server/@ScrapeGraphAI/scrapegraph-mcp)

A production-ready [Model Context Protocol](https://modelcontextprotocol.io/introduction) (MCP) server that provides seamless integration with the [ScapeGraph AI](https://scrapegraphai.com) API. This server enables language models to leverage advanced AI-powered web scraping capabilities with enterprise-grade reliability.

## Available Tools

The server provides the following enterprise-ready tools:

- `markdownify(website_url: str)`: Transform any webpage into clean, structured markdown format
- `smartscraper(user_prompt: str, website_url: str)`: Leverage AI to extract structured data from any webpage
- `searchscraper(user_prompt: str)`: Execute AI-powered web searches with structured, actionable results

## Setup Instructions

To utilize this server, you'll need a ScapeGraph API key. Follow these steps to obtain one:

1. Navigate to the [ScapeGraph Dashboard](https://dashboard.scrapegraphai.com)
2. Create an account and generate your API key

### Automated Installation via Smithery

For automated installation of the ScrapeGraph API Integration Server using [Smithery](https://smithery.ai/server/@ScrapeGraphAI/scrapegraph-mcp):

```bash
npx -y @smithery/cli install @ScrapeGraphAI/scrapegraph-mcp --client claude
```

### Claude Desktop Configuration

Update your Claude Desktop configuration file with the following settings (located on the top rigth of the Cursor page):

(remember to add your API key inside the config)

```json
{
    "mcpServers": {
        "@ScrapeGraphAI-scrapegraph-mcp": {
            "command": "npx",
            "args": [
                "-y",
                "@smithery/cli@latest",
                "run",
                "@ScrapeGraphAI/scrapegraph-mcp",
                "--config",
                "\"{\\\"scrapegraphApiKey\\\":\\\"YOUR-SGAI-API-KEY\\\"}\""
            ]
        }
    }
}
```

The configuration file is located at:
- Windows: `%APPDATA%/Claude/claude_desktop_config.json`
- macOS: `~/Library/Application\ Support/Claude/claude_desktop_config.json`

### Cursor Integration

Add the ScrapeGraphAI MCP server on the settings:

![Cursor MCP Integration](assets/cursor_mcp.png)

## Example Use Cases

The server enables sophisticated queries such as:

- "Analyze and extract the main features of the ScapeGraph API"
- "Generate a structured markdown version of the ScapeGraph homepage"
- "Extract and analyze pricing information from the ScapeGraph website"
- "Research and summarize recent developments in AI-powered web scraping"
- "Create a comprehensive summary of the Python documentation website"

## Error Handling

The server implements robust error handling with detailed, actionable error messages for:

- API authentication issues
- Malformed URL structures
- Network connectivity failures
- Rate limiting and quota management

## Common Issues

### Windows-Specific Connection

When running on Windows systems, you may need to use the following command to connect to the MCP server:

```bash
C:\Windows\System32\cmd.exe /c npx -y @smithery/cli@latest run @ScrapeGraphAI/scrapegraph-mcp --config "{\"scrapegraphApiKey\":\"YOUR-SGAI-API-KEY\"}"
```

This ensures proper execution in the Windows environment.

## License

This project is distributed under the MIT License. For detailed terms and conditions, please refer to the LICENSE file.

## Acknowledgments

Special thanks to [tomekkorbak](https://github.com/tomekkorbak) for his implementation of [oura-mcp-server](https://github.com/tomekkorbak/oura-mcp-server), which served as starting point for this repo.

Made with ❤️ by [ScrapeGraphAI](https://scrapegraphai.com) Team
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states that it's a Model Context Protocol (MCP) server designed for seamless integration with the ScrapeGraph AI API, enabling language models to leverage web scraping capabilities.","categories":["API Frameworks","AI Model Integration","Function Calling"],"analysis":{"Architecture and Design Patterns":"The repository implements a server designed to provide tools via the Model Context Protocol (MCP). The architecture likely involves request handling, API key validation, interaction with ScrapeGraph AI API, and response formatting. Smithery is used for automated installation. The server exposes several functions as tools.","Implementation Details and Key Features":"The server provides tools like `markdownify`, `smartscraper`, and `searchscraper`. These tools are enterprise-ready. The configuration involves setting up the server with an API key.  The documentation provides configuration examples for Claude Desktop and instructions for adding the MCP server in Cursor.","Integration Approaches and Protocols":"The primary integration approach is through the Model Context Protocol, allowing language models to call the provided web scraping tools. The server is designed to work with platforms like Claude and Cursor. Smithery is used for deploying and managing the server as well.","Technical Stack and Dependencies":"The server is implemented in Python and uses the ScrapeGraph AI API. It depends on Smithery for automated installation and management. The documentation mentions Python 3.10. The configuration examples use JSON format for defining the MCP server settings.","Best Practices and Coding Standards":"The repository provides documentation on error handling, including API authentication, malformed URLs, network failures, and rate limiting. The project follows MIT license, and there are acknowledgements to the original source. The setup instructions and examples suggest good configuration management.","Performance Considerations":"The documentation addresses potential issues like Windows-specific connection problems. The use of an API key suggests considerations for rate limiting and quota management. Server infrastructure and ScrapeGraph API response times may impact overall performance. Error handling accounts for network connectivity failures."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'scrapegraph-mcp-server'), ('943016718', 'mcp-sse-authenticated-cloud-run', 'the-freetech-company/mcp-sse-authenticated-cloud-run', 'https://github.com/the-freetech-company/mcp-sse-authenticated-cloud-run', 'Host an Model Context Protocol SSE deployment on Cloud Run, Authenticating with IAM.', '2025-03-05 03:31:37+00', '2025-03-08 16:42:23+00', 'TypeScript', '7', '

# Host MCP SSE Server on Google Cloud Run

At the momenet (03/04/2024) MCP is still addressing Authentication and Authorization. They plan to complete this in H1 2025. The issue is, I want to share my MCP server with my team NOW. So here we are.

Utilizing GCP Cloud Run and User Based IAM Authentication, I have created a simple, secure way to allow clients to access a custom MCP server over the internet.

[MCP Roadmap](https://modelcontextprotocol.io/development/roadmap)

![MCP Not Supported](docs/mcp_not_supported.png)

## How it works

The MCP server is hosted on Google Cloud Run. Utilizing Cloud Run IAM Authentication, we can securely connect to the server from the internet by utilizing the Google Cloud SDK to create a proxy connection.

# TLDR README

This ***should work out of the box*** with minimal config if you already have docker and the gcloud CLI set up locally.

**Step 1:** Update `deploy.sh` with your project id, service account email, etc. 

**Step 2:** On deploy success, grab the cloud run URL that was provided, and add it to `mcp_proxy.ts` along with your project id.

**Step 3:** Run the proxy `npx ts-node mcp_proxy.ts`
![Running proxy asking question](docs/running_proxy_asking_question.png)

**Step 3:** Access your MCP server using http://localhost:3030 - Add it to Cursor under Settings > Features > MCP Servers (make sure you select SSE not command)

# LONG BORING README
## Deployment Steps

1. Clone the repository
2. Run `npm install` to install the dependencies
3. Run `npm run dev` to start the server locally

## Deployment to Google Cloud Run

To deploy your MCP server to Google Cloud Run:

1. Make sure you have the [Google Cloud SDK](https://cloud.google.com/sdk/docs/install) installed
2. Update the `deploy.sh` script with your project details:
   - `PROJECT_ID`: Your Google Cloud project ID
   - `REGION`: Your preferred GCP region
   - `SERVICE_ACCOUNT_EMAIL`: The service account email with appropriate permissions
3. Run the deployment script:
   ```
   chmod +x deploy.sh
   ./deploy.sh
   ```

The deployment script will:

- Build a Docker container for your MCP server
- Push it to Google Container Registry
- Deploy it to Cloud Run with authentication enabled

## Connecting to your deployed MCP server

To connect to your deployed MCP server:

1. Run the MCP proxy locally:

   ```bash
   npx ts-node mcp_proxy.ts
   ```

2. The proxy will:

   - Check if you're authenticated with Google Cloud
   - Obtain authentication tokens automatically
   - Create a local proxy server (default: http://localhost:3030)
   - Forward authenticated requests to your Cloud Run service

3. Configure your MCP client to connect to the local proxy URL

## Use the MCP server in Cursor

1. First, let's run our proxy to establish a connection between our local machine and the MCP server hosted on Google Cloud Run.

```
npx ts-node mcp_proxy.ts
```

2. Now let's add our local proxy server to cursor within the Setting > Features tab in the MCP server section.

![Adding proxy to cursor](docs/add_mcp_one.png)

![Adding proxy to cursor](docs/add_mcp_two.png)

3. Now, we're good to go! Start a new composer (ensure you are in agent mode) and ask what the weather is in a location. Your ouput in your terminal where the proxy is connected + the output of your composer should look like this:

![Running proxy asking question](docs/running_proxy_asking_question.png)

## Security

This setup provides several security benefits:

- Your MCP server is not publicly accessible without authentication
- All connections are secured with Google Cloud IAM
- Team members need Google Cloud SDK access to connect

### Connection Issues

- Verify the Cloud Run URL in `mcp_proxy.ts` matches your deployed service
- Check Cloud Run logs for any server-side errors

## Contributing

Contributions are welcome! Feel free to submit issues or pull requests.

## License

[MIT](LICENSE)
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Cloud Infrastructure\"]}', '{"is_mcp":"YES","justification":"The repository explicitly mentions and implements a Model Context Protocol (MCP) server deployment on Google Cloud Run, focusing on authenticated access and integration with tools like Cursor.","categories":["Context Management","Protocol Implementations","API Frameworks"],"analysis":{"Architecture and Design Patterns":"The repository showcases a client-server architecture where a Cloud Run instance hosts the MCP server. A local proxy (mcp_proxy.ts) acts as an intermediary, authenticating requests with Google Cloud IAM before forwarding them to the server. This setup utilizes the proxy pattern to manage authentication complexities.","Implementation Details and Key Features":"The implementation uses Server-Sent Events (SSE) for communication. Key features include IAM authentication on Cloud Run, a local proxy for secure access, and integration instructions for the Cursor IDE. The `deploy.sh` script automates the deployment process to Google Cloud Run.","Integration Approaches and Protocols":"The repository integrates with Google Cloud Run using Docker and the Google Cloud SDK. It utilizes IAM for authentication and establishes a proxy connection to handle authentication tokens. The communication between the client (e.g., Cursor) and the server is facilitated through the established local proxy.","Technical Stack and Dependencies":"The project uses TypeScript for the proxy (`mcp_proxy.ts`). It depends on the Google Cloud SDK, Docker for containerization, and likely `ts-node` for running TypeScript files. The `deploy.sh` script relies on standard shell utilities for deployment.","Best Practices and Coding Standards":"The repository includes a `deploy.sh` script for automated deployment, promoting Infrastructure as Code (IaC). The documentation provides clear steps for deployment, connection, and usage, demonstrating good documentation practices. Security is addressed through IAM authentication on Cloud Run.","Performance Considerations":"The use of a local proxy may introduce some latency. However, the authenticated Cloud Run setup provides a secure and scalable environment for the MCP server. Performance will largely depend on the Cloud Run instance configuration and network conditions."}}', '2025-03-10 21:28:34+00', 'true', 'Servers', 'mcp-cloud-run-auth'), ('943073850', 'MCP-LOGO-GEN', 'sshtunnelvision/MCP-LOGO-GEN', 'https://github.com/sshtunnelvision/MCP-LOGO-GEN', null, '2025-03-05 05:59:27+00', '2025-03-10 07:13:26+00', 'Python', '155', '# MCP Tool Server for Logo Generation

This server provides logo generation capabilities using FAL AI, with tools for image generation, background removal, and automatic scaling.

## Demo

[![MCP Tool Server Demo](https://img.youtube.com/vi/Miemu1xEZng/0.jpg)](https://www.youtube.com/watch?v=Miemu1xEZng)

## Installation

1. Install `uv` (Universal Virtualenv):

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

2. Create and activate a virtual environment:

```bash
uv venv
source .venv/bin/activate  # On Unix/macOS
# or
.venv\Scripts\activate     # On Windows
```

3. Install dependencies:

```bash
uv pip install -r requirements.txt
```

4. Set up your environment variables:
   - Create a `.env` file in the root directory
   - Add your FAL AI API key:

```bash
FAL_KEY=your_fal_ai_key_here
```

## Running the Server

Start the server with:

```bash
python run_server.py
```

The server will be available at `http://127.0.0.1:7777`

## Cursor IDE Configuration

1. Open Cursor Settings
2. Navigate to the MCP section
3. Add the following configuration:
   - URL: `http://127.0.0.1:7777/sse`
   - Connection Type: `SSE`
   - Enable the connection

## Notes

- Always reference `@logo-creation.mdc` in your Cursor Composer for consistent results
- Steps are defined in `@logo-creation.mdc` but tools can be used independently
- All generated logos will be saved in the `downloads` directory
- Each logo is automatically generated in three sizes:
  - Original size
  - 32x32 pixels
  - 128x128 pixels
- All logos maintain transparency in their final PNG format
- Prompts created by agent are informed by examples and prompt structure seen in server.py. You can customize the prompt structure by editing the server.py file.
- You can use the generate_image tool to generate any image you want, not just logos

## Requirements

- Python 3.8+
- FAL AI API key (required for image generation)
- Active internet connection

## References

- [Cursor MCP Documentation](https://docs.cursor.com/context/model-context-protocol)
- [Model Context Protocol Introduction](https://modelcontextprotocol.io/introduction)
- [FAL AI Dashboard](https://fal.ai/dashboard)

---

If you find this tool helpful, you can [buy me a coffee](https://buymeacoffee.com/sshtunnelvision) ☕️ to support development!
', 'true', '"{\"Function Calling\",\"AI Model Integration\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly mentions Model Context Protocol (MCP) and provides instructions for integration with Cursor IDE's MCP section. It utilizes FAL AI for logo generation, acting as a tool accessible via MCP.","categories":["Function Calling","AI Model Integration","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The architecture is based on a client-server model. The server provides logo generation functionalities as tools, which can be accessed through the Model Context Protocol (MCP). The application utilizes FAL AI's image generation capabilities. The system automatically scales generated logos and saves them in different sizes. The server is designed to be integrated with Cursor IDE.","Implementation Details and Key Features":"The implementation involves setting up a server using Python and FAL AI for image generation. Key features include image generation, background removal, and automatic scaling of logos. The system also ensures that generated logos maintain transparency. The server interacts with the FAL AI API using an API key defined in the environment variables. It saves generated logos in the `downloads` directory.","Integration Approaches and Protocols":"The primary integration approach is through the Model Context Protocol (MCP), enabling tools like Cursor IDE to connect to the server. It utilizes Server-Sent Events (SSE) for communication. The documentation provides explicit instructions for configuring Cursor IDE to connect to the server via the provided URL. The integration depends on the FAL AI API for image generation.","Technical Stack and Dependencies":"The primary programming language is Python 3.8+. Dependencies are managed using `uv` and listed in the `requirements.txt` file. It relies on FAL AI for image generation, necessitating a FAL AI API key. The stack includes libraries for setting up a server and interacting with external APIs. Environment variables are managed using a `.env` file.","Best Practices and Coding Standards":"The repository provides a `requirements.txt` file, ensuring dependency management. Environment variables are used to store sensitive information like the FAL AI API key. The code includes a clear structure for defining tools and their functionalities. It encourages customization of prompt structure by editing the server.py file. The system provides instructions for virtual environment setup.","Performance Considerations":"Performance is likely influenced by the FAL AI API's responsiveness. The image generation process might be computationally intensive. The system optimizes for consistent logo creation by referencing a specific configuration file (@logo-creation.mdc). Logo scaling ensures that small-sized versions (32x32, 128x128) are generated efficiently. The tool offers the ability to customize prompts in `server.py`."}}', '2025-03-10 21:28:52+00', 'true', 'Servers', 'mcp-logo-generator'), ('943164754', 'modex', 'theronic/modex', 'https://github.com/theronic/modex', 'Modex: Model Context Protocol Server & Client in Native Clojure', '2025-03-05 09:16:40+00', '2025-03-10 09:05:42+00', 'Clojure', '27', '# Modex: Model Context Protocol Server & Client in Clojure

Modex (MOdel + ContEXt) is a native Clojure implementation of the [Model Context Protocol](https://modelcontextprotocol.io/) that lets you augment your AI with new tools, resources and prompts.

Because it's native Clojure, you don't need to deal with Anthropic's [MCP Java SDK](https://github.com/modelcontextprotocol/java-sdk).

Modex implements the `stdio` transport, so no need for a proxy like
[mcp-proxy](https://github.com/sparfenyuk/mcp-proxy) to translate between SSE <=> stdio or vice versa.

## Table of Contents

1. [Quickstart](#quickstart)
2. [What is MCP?](#what-is-mcp)
3. [What can Modex do?](#what-can-modex-do)
4. [Detailed Step-by-Step Instructions](#detailed-step-by-step-instructions)
5. [Implementation](#implementation)
6. [Project Status](#project-status)
7. [Rationale](#rationale)
8. [FAQ](#faq)
9. [Licence](#licence)

## Quickstart

1. `git clone git@github.com:theronic/modex.git`
2. `cd modex`
3. `./build.sh` builds an uberjar at `target/modex-mcp-server-0.1.0.jar`.
4. Open your Claude Desktop Config at `~/Library/Application\ Support/Claude/claude_desktop_config.json`
5. Configure a new MCP Server that will run the uberjar at its _full path_:

```json
{
  "mcpServers": {
    "modex-mcp-hello-world": {
      "command": "java",
      "args": ["-jar", "/Users/your-username/code/modex/target/modex-mcp-server-0.1.0.jar"]
    }
  },
  "globalShortcut": ""
}
```

6. Restart Claude Desktop to activate your new MCP Server + tools :)

## What is MCP?

MCP lets you augment your AI models with Tools, Resources & Prompts:

- **Tools** are things it can do, like query a database (e.g. Datomic).
- **Resources** are files and data it can read, like PDF bank statements.
- **Prompts** are templated messages and workflows.

For example, you could make a tool that fetches purchases from your bank's API and let the AI categorize your expenses, and then use another tool to write those expense categories to your accounting system, or to a database. Pretty cool.

## What can Modex do?

The Modex skeleton exposes a single tool named `foo` that answers an MCP Client (like your LLM), with:

```clojure
{:content [{:type "text"
            :text "Hello, AI!" 
            :isError false}]}
```

Your MCP client (e.g. Claude Desktop) can connect to this server and use exposed tools to provide additional context to your AI models.

## Detailed Step-by-Step Instructions

### Step 1: Build the Uberjar

Before you can run it, you have to build it first. The build outputs an uberjar, which is like a Java executable.

```bash
clojure -T:build uber
```

or run the helper which does that:
```bash
./build.sh
```
(you might need to run `chmod +x build.sh`)

### Step 2: Open Claude Desktop Config

Open your Claude Desktop Configuration file, `claude_desktop_config.json`, which on MacOS should be at:

    ~/Library/Application\ Support/Claude/claude_desktop_config.json

### Step 3: Configure your MCP Server

Add an element under `mcpServers` so it looks like this:

```json
{
  "mcpServers": {
    "modex-mcp-hello-world": {
      "command": "java",
      "args": ["-jar", "/Users/your-username/code/modex/target/modex-mcp-server-0.1.0.jar"]
    }
  },
  "globalShortcut": ""
}
```

This tells Claude Desktop there is a tool named `modex-mcp-hello-world` and it can connect to by running `java -jar /path/to/your/uber.jar`.

The way this works is that your local MCP Client (i.e. Claude Desktop), starts your MCP server process and communicates with it via stdin/stdout pipes.

### Step 4: Restart Claude Desktop

You should now be able to ask Claude "run foo", or "what does foo say?" and it will run
the `foo` tool and reply with the response, "Hello, AI!".

## Implementation

Modex implements an MCP client & server in Clojure that is _mostly_ compliant with the [2024-11-05 MCP Spec](https://spec.modelcontextprotocol.io/specification/2024-11-05/).

Messages are encoded using the JSON-RPC 2.0 wire format. 

There are 3 message types:
- Requests have `{:keys [id method ?params]}`
- Responses have `{:keys [id result ?error]}`
- Notifications have `{:keys [method ?params}`

MCP supports two transport types:
- [x] stdio/stdout – implemented in Modex.
- [ ] Server-Sent Events (SSE) – not implemented yet. Useful for restricted networks

## Project Status

- [x] Passing tests
- [ ] Ergonomics (AServer / AClient protocol?)
- [ ] nREPL for live changes to running process
- [ ] SSE support

## Rationale

There is an existing library [mcp-clj](https://github.com/hugoduncan/mcp-clj) that uses SSE, so it requires mcp-proxy to proxy from SSE <=> stdio. I was annoyed by this, so I made Modex.

## FAQ

### Can I modify the server while an MCP Client (like Claude Desktop) is connected?

Not yet, but I'll add an nREPL soon so you can eval changes while Claude Desktop is connected to the process without rebuilding the uberjar.

Btw. I tried to get it to run `clojure -M -m modex.mcp.server`, but you can't set Claude Desktop's working directory.

So currently, I rebuild the uberjar and restart Claude Desktop. Will fix.

## License 

In summary:
- **Free for non-commercial use**: Use it, modify it, share it under [GPLv3](https://www.gnu.org/licenses/gpl-3.0.html) at no cost, just keep it open source.
- **Commercial use**: Want to keep your changes private? Pay $20 once-off for a perpetual commercial license. This covers the cost of my AI tokens to keep building this in public.

This tool is licensed under the [GNU General Public License v3.0 (GPLv3)](https://www.gnu.org/licenses/gpl-3.0.html). You are free to use, modify, and distribute it, provided that any derivative works are also licensed under the GPLv3 and made open source. This ensures the tool remains freely available to the community while requiring transparency for any changes.

If you wish to use or modify this tool in a proprietary project—without releasing your changes under the GPLv3—you 
may purchase a commercial license. This allows you to keep your modifications private for personal or commercial use.
To obtain a commercial license, please contact me at [modex@petrus.co.za](mailto:modex@petrus.co.za).

## Author(s)

- [Petrus Theron](http://petrustheron.com)
', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it is a native Clojure implementation of the Model Context Protocol (MCP). It implements the `stdio` transport and aims for compliance with the MCP specification.","categories":["Protocol Implementations","Function Calling","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"Modex is structured as both a client and server for the Model Context Protocol (MCP), written in Clojure. It uses an uberjar for deployment, simplifying the process. The design emphasizes direct communication via `stdio`, avoiding intermediate proxies. The system is designed to expose tools to AI models.","Implementation Details and Key Features":"The core implementation centers around the `stdio` transport, facilitating communication between the MCP client and server. The server exposes a tool named 'foo' to interact with the AI model. Messages are encoded using JSON-RPC 2.0. The project aims for compliance with the 2024-11-05 MCP Specification.","Integration Approaches and Protocols":"Modex integrates with AI models via the Model Context Protocol. It communicates using `stdio`, allowing direct interaction between the MCP client (e.g., Claude Desktop) and server. The protocol uses JSON-RPC 2.0 for message encoding, supporting requests, responses, and notifications.","Technical Stack and Dependencies":"The project is built using Clojure and requires Java. It uses `build.sh` script to create an uberjar for deployment. The uberjar contains all dependencies, making it easy to run. Claude Desktop is used as a test MCP client.","Best Practices and Coding Standards":"The project includes tests, indicating a focus on code quality. The documentation provides clear instructions for building and configuring the server. The license is explicitly defined. The code is structured to be mostly compliant with the MCP Spec.","Performance Considerations":"The implementation prioritizes direct communication via `stdio` to minimize latency. The choice of Clojure allows leveraging the JVM's performance. The use of an uberjar streamlines deployment and potentially improves startup time. Further performance considerations could involve optimizing JSON-RPC message handling."}}', '2025-03-10 21:28:52+00', 'true', 'Clients', 'modex-clojure-mcp'), ('943196888', 'langchain4j-for-tools-and-mcp-demo', 'LostInBrittany/langchain4j-for-tools-and-mcp-demo', 'https://github.com/LostInBrittany/langchain4j-for-tools-and-mcp-demo', 'A series of demos to illustrate my talk "MCP: using Java and Quarkus to bridge LLMs with your applications and data"', '2025-03-05 10:15:13+00', '2025-03-06 17:22:26+00', 'Java', '4', '# LangChain4j for Tools and MCP - Demo Repository

This repository contains all the code and examples from my talk:  
**MCP: using Java and Quarkus to bridge LLMs with your applications and data**, presented at:

- **2024-03-05** - [Madrid JUG](https://www.meetup.com/madridjug/events/306387233/) (Madrid, Spain) - [Slides](./slides/)

This repository showcases a series of demos illustrating the integration of **Large Language Models (LLMs) with Java** using [LangChain4j](https://github.com/langchain4j/langchain4j) and the **Model Context Protocol (MCP)**.

Currently, the demos use the [OpenAI API](https://openai.com/api/) as the LLM provider, but they can be adapted to work with any other LLM, including **self-hosted or local models**.

---

## Demos Included

### 1️. [Hello World with LangChain4j]((./01-Hello-LangChain4j/))

A set of Java examples demonstrating how to connect to an LLM using [LangChain4j](https://docs.langchain4j.dev/) and [JBang](https://www.jbang.dev/).

#### Demos

1. `HelloLangChain4j01.java`: Sends a message to the LLM and receives a response.
2. `HelloLangChain4j02.java`: Demonstrates that **LLM interactions are stateless**—the model does not remember previous inputs.
3. `HelloLangChain4j03.java`: Sends the full conversation history to the LLM, allowing it to maintain context.
4. `HelloLangChain4j04.java`: Uses **LangChain4j’s built-in memory** to avoid manually sending the entire message history.

📌 All these demos **use JBang** to keep execution **simple and portable**.

---

### 2️. [Lying Weather Tool](./02-Lying-weather-Tool/)

A set of Java examples demonstrating **how to expose Java functions as tools** that LLMs can invoke dynamically.  
Like the previous examples, these demos are based on [LangChain4j](https://docs.langchain4j.dev/) and use [JBang](https://www.jbang.dev/) for easy execution.

This section's demos use `LyingWeatherTool.java`, a class where we define a (fake) tool that we expose to the LLM.  
The **Lying Weather Tool** contains a single function:

```java
@Tool
public static String getWeather(String city) {
  return "The weather in " + city + " is sunny and hot.";
}
```

- The function is registered as a tool with the description:  
  **_"A tool to get the current weather in a city."_**  
- However, the function **always returns the same (fake) weather response**, no matter the city.

#### Demos

1. **`CallingTool01.java`**:  
  - [Defines a tool specification](https://docs.langchain4j.dev/tutorials/tools) for the **Lying Weather Tool** and provides it to the LLM.  
  - The LLM understands the tool and requests an execution.

2. **`CallingTool02.java`**:  
  - Manually executes the tool request and sends the response back to the LLM.  
  - The LLM integrates the result into a natural language response.  
  - **Demonstrates why manual execution is cumbersome**—this is where **automated execution** come in!

3. **`CallingTool03.java`**:
  - Uses another LangChain4j concept, **[AI Services](https://docs.langchain4j.dev/tutorials/ai-services)**, that make **everything simpler**, with features like calling tools transparently

---

### 3️. [Real Weather Tool with LangChain4j](./03-Real-Weather-Tool/)


This demo showcases how to integrate real-time weather data into a Java application using [LangChain4j](https://github.com/langchain4j/langchain4j), [OkHttp](https://square.github.io/okhttp/), and [Jackson Databind](https://github.com/FasterXML/jackson-databind). The tool fetches current weather information for a specified city and country code by:

1. **Geocoding**: Retrieving the latitude and longitude of the specified city using the [Open-Meteo Geocoding API](https://open-meteo.com/en/docs/geocoding-api).

2. **Fetching Weather Data**: Using the obtained coordinates to request current weather conditions from the [Open-Meteo Forecast API](https://open-meteo.com/en/docs).

#### Demo

**`RealWeatherTool.java`** contains the implementation of the weather tool, including methods for geocoding and fetching weather data:

1. **Define the Tool**: Annotate the `getWeather` method with `@Tool` to expose it for LLM integration.

   ```java
   @Tool("A tool to get the current weather for a given city and country code")
   public static String getWeather(String city, String countryCode) {
       // Implementation
   }
   ```

2. **Geocoding**: Implement the `getCoordinates` method to fetch latitude and longitude for the specified city and country code using the Open-Meteo Geocoding API.

3. **Fetch Weather Data**: Use the coordinates to request current weather information from the Open-Meteo Forecast API.

4. **Handle Responses**: Parse the JSON responses using Jackson's `ObjectMapper` and extract relevant data such as temperature, windspeed, and weather conditions.

5. **Integrate with LLM**: Utilize LangChain4j to enable the LLM to call the `getWeather` tool dynamically, providing real-time weather information based on user queries.

 **`CallingTool01.java`** simply uses **[AI Services](https://docs.langchain4j.dev/tutorials/ai-services)**, to allows the LLM to call the _Real Weather Tool_ transparently.
 
---

## Getting Started

### Prerequisites
- **Java 17+**: Ensure you have Java 17 or higher installed.
- **JBang**: A tool to run Java applications with minimal setup. Install it from [https://www.jbang.dev/](https://www.jbang.dev/).

### Running the Demos

1. **Clone the Repository**:

   ```sh
   git clone https://github.com/LostInBrittany/langchain4j-for-tools-and-mcp-demo.git
   cd langchain4j-for-tools-and-mcp-demo
   ```

2. **Execute any of the demos using JBang**:

   ```sh
   jbang ./01-Hello-LangChain4j/HelloLangChain4j01.java
   ```

   *Replace `./01-Hello-LangChain4j/HelloLangChain4j01.java` with the appropriate script for other demos.*

## Resources & Further Reading
- 📘 **LangChain4j Documentation**: [https://github.com/langchain4j/langchain4j](https://github.com/langchain4j/langchain4j)
- 🛠 **JBang Documentation**: [https://www.jbang.dev/documentation/](https://www.jbang.dev/documentation/)

## License
📝 This repository is licensed under the **MIT License**. See the [LICENSE](./LICENSE) file for details.
', 'true', '"{\"AI Model Integration\",\"Function Calling\",\"Agent Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository demonstrates the integration of Large Language Models (LLMs) with Java using LangChain4j and the Model Context Protocol (MCP), focusing on bridging LLMs with applications and data.","categories":["Function Calling","AI Model Integration","Agent Frameworks"],"analysis":{"Architecture and Design Patterns":"The repository employs a modular design, separating concerns through distinct demo folders.  It leverages LangChain4j's AI Services and Tool interfaces to abstract LLM interactions. The architecture promotes reusability by showcasing different methods of tool integration, from manual execution to automated AI service calls.","Implementation Details and Key Features":"The demos utilize JBang for simplified execution and portability. Key features include defining Java functions as tools, integrating with external APIs (Open-Meteo), and using LangChain4j's built-in memory management.  The `LyingWeatherTool` and `RealWeatherTool` examples showcase tool creation and LLM integration.","Integration Approaches and Protocols":"The demos integrate with the OpenAI API for LLM functionality and Open-Meteo APIs for weather data. Integration patterns include defining tool specifications, manual tool execution, and leveraging LangChain4j's AI Services for automated tool calling.  The repository emphasizes the use of function calling to extend LLM capabilities.","Technical Stack and Dependencies":"The technical stack includes Java 17+, LangChain4j, JBang, OkHttp, and Jackson Databind. LangChain4j provides the core framework for interacting with LLMs and managing context. OkHttp is used for making HTTP requests to external APIs, while Jackson is used for parsing JSON responses.","Best Practices and Coding Standards":"The repository emphasizes clarity and simplicity in its demos.  The code is well-commented and demonstrates best practices for defining and using tools with LLMs.  The use of JBang promotes portability and ease of execution.  The project follows standard Java coding conventions.","Performance Considerations":"The demos are designed for illustrative purposes and do not explicitly address performance optimization. However, the use of LangChain4j's memory management features suggests an awareness of context window limitations and the need for efficient memory usage when interacting with LLMs. The `RealWeatherTool` demo may face performance constraints depending on the API rate limits."}}', '2025-03-10 21:54:04+00', 'true', 'Servers', 'langchain4j-mcp-demo'), ('943218655', 'mcp-figma', 'akazwz/mcp-figma', 'https://github.com/akazwz/mcp-figma', 'simple MCP of figma', '2025-03-05 10:57:35+00', '2025-03-09 13:38:37+00', 'TypeScript', '10', '### Demo of simeple fiama MCP

### 环境变量
1. `FIGMA_PERSONAL_ACCESS_TOKEN`

### 测试
````shell
pnpm test
````', 'true', '"{\"AI Model Integration\",\"Protocol Implementations\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states 'simple MCP of figma' indicating it implements Model Context Protocol concepts within the Figma environment.","categories":["Protocol Implementations","AI Model Integration","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The repository likely employs an architecture enabling communication between an AI model and Figma. Design patterns may include adapters to translate data formats and request/response handling for interaction with the Figma API. The 'simple' nature suggests a streamlined approach with minimal complexity.","Implementation Details and Key Features":"The repository implements core MCP functionality for Figma. Key features likely involve exposing Figma's functionalities through a protocol that AI models can understand. This could include functions for accessing elements, modifying designs, or extracting design information.","Integration Approaches and Protocols":"Integration is achieved through the Figma API, likely using HTTP requests. The MCP implementation likely defines a specific protocol for formatting requests and interpreting responses. Standard protocols like REST or GraphQL could be adapted or a custom protocol may be defined for simplicity.","Technical Stack and Dependencies":"The project uses 'pnpm', indicating Node.js usage. Dependencies would likely include libraries for making HTTP requests (e.g., Axios, node-fetch) and for interacting with the Figma API. The presence of a 'test' script suggests the use of testing frameworks like Jest or Mocha.","Best Practices and Coding Standards":"While not explicitly stated, the presence of tests suggests attention to code quality. The use of TypeScript (inferred from Node.js projects) would improve type safety. Following standard Node.js coding conventions is essential for maintainability and readability.","Performance Considerations":"Performance is likely not a primary concern given the 'simple' nature, but optimizing API calls to Figma will be necessary. Caching frequently accessed data could improve responsiveness. Efficient data serialization and deserialization are also beneficial."}}', '2025-03-10 21:28:34+00', 'true', 'Servers', 'mcp-figma-server'), ('943248871', 'image-tools-mcp', 'kshern/image-tools-mcp', 'https://github.com/kshern/image-tools-mcp', 'A Model Context Protocol (MCP) service for retrieving image dimensions, supporting both URL and local file sources.', '2025-03-05 11:59:35+00', '2025-03-10 16:26:05+00', 'TypeScript', '1', '# Image Tools MCP

A Model Context Protocol (MCP) service for retrieving image dimensions, supporting both URL and local file sources.

*[中文文档](./readme_zh.md)*

## Features

- Retrieve image dimensions from URLs
- Get image dimensions from local files
- Returns width, height, type, and MIME type information

## Installation

```bash
npm install image-tools-mcp
```

## Usage

### Using as an MCP Service

This service provides two tool functions:

1. `get_image_size` - Get dimensions of remote images
2. `get_local_image_size` - Get dimensions of local images

### Client Integration

To use this MCP service, you need to connect to it from an MCP client. Here are examples of how to integrate with different clients:

#### Using with Claude Desktop

1. Install Claude Desktop from [claude.ai/download](https://claude.ai/download)
2. Configure Claude Desktop to use this MCP server by editing the configuration file:

```json
{
  "mcpServers": {
    "image-tools": {
      "command": "npx",
      "args": ["image-tools-mcp"]
    }
  }
}
```

3. Restart Claude Desktop
4. Ask Claude to get image dimensions: "Can you tell me the dimensions of this image: https://example.com/image.jpg?"

#### Using with MCP Client Library

```typescript
import { McpClient } from "@modelcontextprotocol/client";

// Initialize the client
const client = new McpClient({
  transport: "stdio" // or other transport options
});

// Connect to the server
await client.connect();

// Get image dimensions from URL
const urlResult = await client.callTool("get_image_size", {
  options: {
    imageUrl: "https://example.com/image.jpg"
  }
});
console.log(JSON.parse(urlResult.content[0].text));
// Output: { width: 800, height: 600, type: "jpg", mime: "image/jpeg" }

// Get image dimensions from local file
const localResult = await client.callTool("get_local_image_size", {
  options: {
    imagePath: "D:/path/to/image.png"
  }
});
console.log(JSON.parse(localResult.content[0].text));
// Output: { width: 1024, height: 768, type: "png", mime: "image/png", path: "D:/path/to/image.png" }
```

### Tool Schemas

#### get_image_size

```typescript
{
  options: {
    imageUrl: string // URL of the image to retrieve dimensions for
  }
}
```

#### get_local_image_size

```typescript
{
  options: {
    imagePath: string // Absolute path to the local image file
  }
}
```

## Technical Implementation

This project is built on the [probe-image-size](https://github.com/nodeca/probe-image-size) library for image dimension detection.

## License

MIT
', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states it's an MCP service and provides integration examples with Claude Desktop and a generic MCP client using tool functions and schemas.","categories":["AI Model Integration","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The repository implements a service providing image dimension retrieval via the Model Context Protocol. It exposes two tool functions: one for remote images and one for local files. The service appears to follow a client-server architecture, designed to be integrated with MCP-compatible clients like Claude Desktop.","Implementation Details and Key Features":"The service offers functionality for retrieving image dimensions from URLs and local files. Key features include returning the width, height, type, and MIME type of an image. It relies on the `probe-image-size` library for image dimension detection, simplifying the core image processing logic.","Integration Approaches and Protocols":"Integration occurs via the Model Context Protocol. The repository provides instructions for integrating with Claude Desktop by configuring its MCP server settings. It also shows how to use the service with an MCP client library, connecting via standard input/output (`stdio`) or other transport options.","Technical Stack and Dependencies":"The service uses Node.js and likely npm for package management, judging by the `npm install` command. It depends on the `probe-image-size` library for image dimension detection. The specific version of Node.js is not specified, but is likely a reasonably current LTS release.","Best Practices and Coding Standards":"The provided documentation shows usage examples with code snippets. TypeScript schemas are defined for the tool functions, aiding in type safety. The documentation and README are well-structured. However, no linting or formatting tools are explicitly mentioned in the documentation.","Performance Considerations":"Retrieving image dimensions from URLs will be subject to network latency and the availability of the remote server. The `probe-image-size` library's performance will also influence overall speed. Local file access should generally be faster but can be affected by disk I/O. No specific caching mechanisms are evident."}}', '2025-03-10 21:26:21+00', 'true', 'Servers', 'image-tools-mcp'), ('943267635', 'MCP', 'Swan-and-Co-Innovations/MCP', 'https://github.com/Swan-and-Co-Innovations/MCP', null, '2025-03-05 12:37:41+00', '2025-03-05 13:08:50+00', 'JavaScript', '0', '
# Sequential Thinking MCP Server

An MCP server implementation that provides a tool for dynamic and reflective problem-solving through a structured thinking process.

## Features

- Break down complex problems into manageable steps
- Revise and refine thoughts as understanding deepens
- Branch into alternative paths of reasoning
- Adjust the total number of thoughts dynamically
- Generate and verify solution hypotheses

## Tool

### sequential_thinking

Facilitates a detailed, step-by-step thinking process for problem-solving and analysis.

**Inputs:**
- `thought` (string): The current thinking step
- `nextThoughtNeeded` (boolean): Whether another thought step is needed
- `thoughtNumber` (integer): Current thought number
- `totalThoughts` (integer): Estimated total thoughts needed
- `isRevision` (boolean, optional): Whether this revises previous thinking
- `revisesThought` (integer, optional): Which thought is being reconsidered
- `branchFromThought` (integer, optional): Branching point thought number
- `branchId` (string, optional): Branch identifier
- `needsMoreThoughts` (boolean, optional): If more thoughts are needed

## Usage

The Sequential Thinking tool is designed for:
- Breaking down complex problems into steps
- Planning and design with room for revision
- Analysis that might need course correction
- Problems where the full scope might not be clear initially
- Tasks that need to maintain context over multiple steps
- Situations where irrelevant information needs to be filtered out

## Configuration

### Usage with Claude Desktop

Add this to your `claude_desktop_config.json`:

#### npx

```json
{
  "mcpServers": {
    "sequential-thinking": {
      "command": "npx",
      "args": [
        "-y",
        "@modelcontextprotocol/server-sequential-thinking"
      ]
    }
  }
}
```

#### docker

```json
{
  "mcpServers": {
    "sequentialthinking": {
      "command": "docker",
      "args": [
        "run",
        "--rm",
        "-i",
        "mcp/sequentialthinking"
      ]
    }
  }
}
```

## Building

Docker:

```bash
docker build -t mcp/sequentialthinking -f src/sequentialthinking/Dockerfile .
```

## License

This MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.
', 'true', '"{\"Protocol Implementations\",\"Agent Frameworks\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states that it's an MCP server implementation and provides configuration examples for use with Claude Desktop via MCP.","categories":["Agent Frameworks","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The repository implements a server designed to handle MCP requests related to sequential thinking. The architecture is centered around a tool named `sequential_thinking`, which likely uses a request-response pattern to interact with a client. It uses dynamic problem-solving and structured thinking processes. The design promotes breaking down complex problems into steps, revising thoughts, and branching paths.","Implementation Details and Key Features":"The implementation focuses on a `sequential_thinking` tool that accepts a `thought` as input, manages the flow of thoughts using flags such as `nextThoughtNeeded` and `isRevision`, and tracks thought numbers. The tool supports branching and revising existing thoughts, allowing for dynamic problem-solving. The server implementation is likely responsible for managing the tool's lifecycle and communication with the client.","Integration Approaches and Protocols":"The repository provides configuration snippets for integrating the MCP server with Claude Desktop, using both `npx` and Docker. This shows two different deployment approaches. The integration relies on the MCP protocol to enable the `sequential_thinking` tool's functionality within Claude. The repository exposes a command interface for tool invocation.","Technical Stack and Dependencies":"Based on the build instructions (Docker) and the configuration examples (npx), the tech stack includes Docker, and potentially Node.js if using npx, along with any libraries used by the `@modelcontextprotocol/server-sequential-thinking` package (if applicable). The presence of a Dockerfile suggests a containerized deployment is a primary option.","Best Practices and Coding Standards":"The repository includes a license file (MIT), which is a good practice for open-source projects. The tool's documentation is clear and concise, explaining the inputs, usage, and configuration. However, without seeing the source code, it's difficult to assess other coding standards applied within the implementation. The Docker build suggests separation of concerns with containerization.","Performance Considerations":"The sequential nature of the tool implies potential latency depending on the complexity of each thought and the number of steps needed to solve a problem. Branching and revision features might introduce additional overhead in managing different paths. Optimization for minimizing latency would be a crucial performance consideration, especially in real-time applications."}}', '2025-03-10 21:28:38+00', 'true', 'Servers', 'mcp-servers-943267635'), ('943273174', 'higress-ai-search-mcp-server', 'cr7258/higress-ai-search-mcp-server', 'https://github.com/cr7258/higress-ai-search-mcp-server', 'An MCP server enhances AI responses with real-time search results via Higress ai-search.', '2025-03-05 12:48:32+00', '2025-03-09 11:51:25+00', 'Python', '4', '# Higress AI-Search MCP Server

## Overview

A Model Context Protocol (MCP) server that provides an AI search tool to enhance AI model responses with real-time search results from various search engines through [Higress](https://higress.cn/) [ai-search](https://github.com/alibaba/higress/blob/main/plugins/wasm-go/extensions/ai-search/README.md) feature.

## Demo

### Cline

https://github.com/user-attachments/assets/60a06d99-a46c-40fc-b156-793e395542bb

### Claude Desktop

https://github.com/user-attachments/assets/5c9e639f-c21c-4738-ad71-1a88cc0bcb46

## Features

- **Internet Search**: Google, Bing, Quark - for general web information
- **Academic Search**: Arxiv - for scientific papers and research
- **Internal Knowledge Search**

## Prerequisites

- [uv](https://github.com/astral-sh/uv) for package installation.
- Config Higress with [ai-search](https://github.com/alibaba/higress/blob/main/plugins/wasm-go/extensions/ai-search/README.md) plugin and [ai-proxy](https://github.com/alibaba/higress/blob/main/plugins/wasm-go/extensions/ai-proxy/README.md) plugin.

## Configuration

The server can be configured using environment variables:

- `HIGRESS_URL`(optional): URL for the Higress service (default: `http://localhost:8080/v1/chat/completions`).
- `MODEL`(required): LLM model to use for generating responses.
- `INTERNAL_KNOWLEDGE_BASES`(optional): Description of internal knowledge bases.

### Option 1: Using uvx

Using uvx will automatically install the package from PyPI, no need to clone the repository locally.

```bash
{
  "mcpServers": {
    "higress-ai-search-mcp-server": {
      "command": "uvx",
      "args": [
        "higress-ai-search-mcp-server"
      ],
      "env": {
        "HIGRESS_URL": "http://localhost:8080/v1/chat/completions",
        "MODEL": "qwen-turbo",
        "INTERNAL_KNOWLEDGE_BASES": "Employee handbook, company policies, internal process documents"
      }
    }
  }
}
```

### Option 2: Using uv with local development

Using uv requires cloning the repository locally and specifying the path to the source code.

```bash
{
  "mcpServers": {
    "higress-ai-search-mcp-server": {
      "command": "uv",
      "args": [
        "--directory",
        "path/to/src/higress-ai-search-mcp-server",
        "run",
        "higress-ai-search-mcp-server"
      ],
      "env": {
        "HIGRESS_URL": "http://localhost:8080/v1/chat/completions",
        "MODEL": "qwen-turbo",
        "INTERNAL_KNOWLEDGE_BASES": "Employee handbook, company policies, internal process documents"
      }
    }
  }
}
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
', 'true', '"{\"AI Model Integration\",\"Function Calling\",\"RAG Systems\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it's a Model Context Protocol (MCP) server. It enhances AI model responses with real-time search using Higress and provides configurations for context enrichment.","categories":["Function Calling","AI Model Integration","RAG Systems"],"analysis":{"Architecture and Design Patterns":"The architecture involves an MCP server that integrates with Higress's ai-search feature. It acts as an intermediary, enriching AI model responses with search results. The design likely follows a client-server model, where the AI model is the client requesting context from the MCP server. Configuration uses environment variables for flexibility.","Implementation Details and Key Features":"Key features include internet search (Google, Bing, Quark), academic search (Arxiv), and internal knowledge search. It uses `uv` for package management. The implementation likely involves making HTTP requests to search engines and then formatting the results for consumption by the AI model. It depends on Higress's ai-search and ai-proxy plugins.","Integration Approaches and Protocols":"The server integrates with AI models via the Model Context Protocol (MCP). It retrieves search results from various engines through Higress and presents them to the model. Configuration examples show how to integrate it with 'uvx' and 'uv', likely acting as a plugin within a larger system.","Technical Stack and Dependencies":"The technical stack includes `uv` for package installation and deployment, and the Higress ai-search and ai-proxy plugins. The server likely uses Python or Go, given the configuration examples referencing 'uvx' and 'uv'. It depends on environment variables for configuration and requires access to search engine APIs.","Best Practices and Coding Standards":"The repository provides configuration examples and clear documentation, indicating a focus on usability and maintainability. The use of environment variables for configuration promotes separation of concerns. The project is licensed under the MIT License, suggesting an open and collaborative approach.","Performance Considerations":"Performance depends on the latency of search engine queries and the efficiency of the data formatting. Caching search results and optimizing queries can improve response times. The choice of LLM model (e.g., qwen-turbo) will also impact performance. Monitoring Higress and the server's resource usage is crucial for identifying bottlenecks."}}', '2025-03-10 21:28:38+00', 'true', 'Servers', 'higress-ai-search-mcp'), ('943286066', 'iMCP', 'loopwork-ai/iMCP', 'https://github.com/loopwork-ai/iMCP', 'A macOS app that provides an MCP server to your Messages, Contacts, and more', '2025-03-05 13:14:00+00', '2025-03-10 16:46:25+00', 'Swift', '161', '<picture>
  <source media="(prefers-color-scheme: dark)" srcset="Assets/hero-dark.svg">
  <source media="(prefers-color-scheme: light)" srcset="Assets/hero-light.svg">
  <img alt="iMCP">
</picture>

iMCP is a macOS app for connecting your digital life with AI.
It works with [Claude Desktop][claude-app]
and a [growing list of clients][mcp-clients] that support the
[Model Context Protocol (MCP)][mcp].

## Capabilities

<table>
  <tr>
    <th>
      <img src="Assets/calendar.svg" width="48" height="48" alt="" role="presentation"/>
    </th>
    <td><strong>Calendar</strong></td>
    <td>View and manage calendar events, including creating new events with customizable settings like recurrence, alarms, and availability status.</td>
  </tr>
  <tr>
    <th>
      <img src="Assets/contacts.svg" width="48" height="48" alt="" role="presentation"/>
    </th>
    <td><strong>Contacts</strong></td>
    <td>Access contact information about yourself and search your contacts by name, phone number, or email address.</td>
  </tr>
  <tr>
    <th>
      <img src="Assets/location.svg" width="48" height="48" alt="" role="presentation"/>
    </th>
    <td><strong>Location</strong></td>
    <td>Access current location data and convert between addresses and geographic coordinates.</td>
  </tr>
  <tr>
    <th>
      <img src="Assets/messages.svg" width="48" height="48" alt="" role="presentation"/>
    </th>
    <td><strong>Messages</strong></td>
    <td>Access message history with specific participants within customizable date ranges.</td>
  </tr>
  <tr>
    <th>
      <img src="Assets/reminders.svg" width="48" height="48" alt="" role="presentation"/>
    </th>
    <td><strong>Reminders</strong></td>
    <td>View and create reminders with customizable due dates, priorities, and alerts across different reminder lists.</td>
  </tr>
  <tr>
    <th>
      <img src="Assets/weather.svg" width="48" height="48" alt="" role="presentation"/>
    </th>
    <td><strong>Weather</strong></td>
    <td>Access current weather conditions including temperature, wind speed, and weather conditions for any location.</td>
  </tr>
</table>

> [!TIP]
> Have a suggestion for a new capability?
> Reach out to us at <imcp@loopwork.com>

## Getting Started

### Download and open the app

First, [download the iMCP app](https://iMCP.app/download)
(requires macOS 15.3 or later).

<img align="right" width="344" src="/Assets/imcp-screenshot-first-launch.png" alt="Screenshot of iMCP on first launch" />

When you open the app,
you'll see a
<img style="display: inline" width="20" height="16" src="/Assets/icon.svg" />
icon in your menu bar.

Clicking on this icon reveals the iMCP menu,
which displays all available services.
Initially, all services will appear in gray,
indicating they're inactive.

The blue toggle switch at the top indicates that the MCP server is running
and ready to connect with MCP-compatible clients.

<br clear="all">

<img align="right" width="372" src="/Assets/imcp-screenshot-grant-permission.png" alt="Screenshot of macOS permission dialog" />

### Activate services

To activate a service, click on its icon.
The system will prompt you with a permission dialog.
For example, when activating Calendar access, you'll see a dialog asking `"iMCP" Would Like Full Access to Your Calendar`.
Click <kbd>Allow Full Access</kbd> to continue.

> [!IMPORTANT]
> iMCP **does not** collect or store any of your data.
> Clients like Claude Desktop _do_ send
> your data off device as part of tool calls.

<br clear="all">

<img align="right" width="344" src="/Assets/imcp-screenshot-all-services-active.png" alt="Screenshot of iMCP with all services enabled" />

Once activated,
each service icons goes from gray to their distinctive colors —
red for Calendar, green for Messages, blue for Location, and so on.

Repeat this process for all of the capabilities you'd like to enable.
These permissions follow Apple's standard security model,
giving you complete control over what information iMCP can access.

<!-- <br clear="all"> -->

<!-- <img align="right" width="344" src="/Assets/imcp-screenshot-configure-claude-desktop.png" /> -->

<br clear="all">

### Connect to Claude Desktop

If you don't have Claude Desktop installed,
you can [download it here](https://claude.ai/download).

Open Claude Desktop and go to "Settings... (<kbd>⌘</kbd><kbd>,</kbd>)".
Click on "Developer" in the sidebar of the Settings pane,
and then click on "Edit Config".
This will create a configuration file at
`~/Library/Application Support/Claude/claude_desktop_config.json`.

<br/>

To connect iMCP to Claude Desktop,
click <img style="display: inline" width="20" height="16" src="/Assets/icon.svg" />
\> "Configure Claude Desktop".

This will add or update the MCP server configuration to use the
`imcp-server` executable bundled in the application.
Other MCP server configurations in the file will be preserved.

<details>
<summary>You can also configure Claude Desktop manually</summary>

Click <img style="display: inline" width="20" height="16" src="/Assets/icon.svg" />
\> "Copy server command to clipboard".
Then open `claude_desktop_config.json` in your editor
and enter the following:

```json
{
  "mcpServers" : {
    "iMCP" : {
      "command" : "{paste iMCP server command}"
    }
  }
}
```
</details>

<img align="right" width="372" src="/Assets/imcp-screenshot-approve-connection.png" />

### Call iMCP tools from Claude Desktop

Quit and reopen the Claude Desktop app.
You'll be prompted to approve the connection.

> [!NOTE]
> You may see this dialog twice;
> click approve both times.

<br clear="all">

After approving the connection,
you should now see 🔨12 in the bottom right corner of your chat box.
Click on that to see a list of all the tools made available to Claude
by iMCP.

<p align="center">
  <img width="694" src="/Assets/claude-desktop-screenshot-tools-enabled.png" alt="Screenshot of Claude Desktop with tools enabled" />
</p>

Now you can ask Claude questions that require access to your personal data,
such as:
> "How's the weather where I am?"

Claude will use the appropriate tools to retrieve this information,
providing you with accurate, personalized responses
without requiring you to manually share this data during your conversation.

<p align="center">
  <img width="738" src="/Assets/claude-desktop-screenshot-message.png" alt="Screenshot of Claude response to user message 'How's the weather where I am?'" />
</p>

## Technical Details

### App & CLI

iMCP is a macOS app that bundles a command-line executable, `imcp-server`.
* [`iMCP.app`](/App/) provides UI for configuring services and — most importantly —
  a means of interacting with macOS system permissions,
  so that it can access Contacts, Calendar, and other information.
* [`imcp-server`](/CLI/) provides an MCP server that 
  uses standard input/output for communication 
  ([stdio transport][mcp-transports]).

The app and CLI communicate with each other on the local network
using [Bonjour][bonjour] for automatic discovery. 
Both advertise a service with type "_mcp._tcp" and domain "local".
Requests from MCP clients are read by the CLI from `stdin`
and relayed to the app;
responses from the app are received by the CLI and written to `stdout`.
See [`StdioProxy`](https://github.com/loopwork-ai/iMCP/blob/8cf9d250286288b06bf5d3dda78f5905ad0d7729/CLI/main.swift#L47) 
for implementation details.

For this project, we created [mcp-swift-sdk]:
a Swift SDK for Model Context Protocol servers and clients.
The app uses this package to handle proxied requests from MCP clients.

### iMessage Database Access

Apple doesn't provide public APIs for accessing your messages.
However, the Messages app on macOS stores data in a SQLite database located at
`~/Library/Messages/chat.db`.

iMCP runs in [App Sandbox][app-sandbox],
which limits its access to user data and system resources.
When you go to enable the Messages service,
you'll be prompted to open the `chat.db` file through the standard file picker.
When you do, macOS adds that file to the app’s sandbox.
[`NSOpenPanel`][nsopenpanel] is magic like that.

But opening the iMessage database is just half the battle.
Over the past few years, 
Apple has moved away from storing messages in plain text
and instead toward a proprietary `typedstream` format.

For this project, we created [Madrid][madrid]:
a Swift package for reading your iMessage database.
It includes a Swift implementation for decoding Apple's `typedstream` format, 
adapted from Christopher Sardegna's [imessage-exporter] project 
and [blog post about reverse-engineering `typedstream`][typedstream-blog-post].

### JSON-LD for Tool Results

The tools provided by iMCP return results as
[JSON-LD][json-ld] documents.
For example,
the `fetchContacts` tool uses the [Contacts framework][contacts-framework],
which represents people and organizations with the [`CNContact`][cncontact] type.
Here's how an object of that type is encoded as JSON-LD:

```json
{
  "@context": "https://schema.org",
  "@type": "Organization",
  "name": "Loopwork Limited",
  "url": "https://loop.work"
}
```

[Schema.org][schema.org] provides standard vocabularies for
people, postal addresses, events, and many other objects we want to represent.
And JSON-LD is a convenient encoding format for 
humans, AI, and conventional software alike.

For this project, we created [Ontology][ontology]:
a Swift package for working with structured data.
It includes convenience initializers for types from Apple frameworks, 
such as those returned by iMCP tools.

## Acknowledgments

- [Justin Spahr-Summers](https://jspahrsummers.com/)
  ([@jspahrsummers](https://github.com/jspahrsummers)),
  David Soria Parra
  ([@dsp-ant](https://github.com/dsp-ant)), and
  Ashwin Bhat
  ([@ashwin-ant](https://github.com/ashwin-ant))
  for their work on MCP.
- [Christopher Sardegna](https://chrissardegna.com)
  ([@ReagentX](https://github.com/ReagentX))
  for reverse-engineering the `typedstream` format
  used by the Messages app.

## License

This project is licensed under the Apache License, Version 2.0.

## Legal

iMessage® is a registered trademark of Apple Inc.  
This project is not affiliated with, endorsed, or sponsored by Apple Inc.

[app-sandbox]: https://developer.apple.com/documentation/security/app-sandbox
[bonjour]: https://developer.apple.com/bonjour/
[claude-app]: https://claude.ai/download
[contacts-framework]: https://developer.apple.com/documentation/contacts
[cncontact]: https://developer.apple.com/documentation/contacts/cncontact
[imessage-exporter]: https://github.com/ReagentX/imessage-exporter
[json-ld]: https://json-ld.org
[madrid]: https://github.com/loopwork-ai/Madrid
[mcp]: https://modelcontextprotocol.io/introduction
[mcp-clients]: https://modelcontextprotocol.io/clients
[mcp-transports]: https://modelcontextprotocol.io/docs/concepts/architecture#transport-layer
[nsopenpanel]: https://developer.apple.com/documentation/appkit/nsopenpanel
[ontology]: https://github.com/loopwork-ai/Ontology
[schema.org]: https://schema.org
[mcp-swift-sdk]: https://github.com/loopwork-ai/mcp-swift-sdk
[typedstream-blog-post]: https://chrissardegna.com/blog/reverse-engineering-apples-typedstream-format/', 'true', '"{\"Protocol Implementations\",\"Agent Frameworks\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it provides an MCP server to connect a user's digital life with AI, and it works with MCP-compatible clients like Claude Desktop. It implements the Model Context Protocol.","categories":["Agent Frameworks","API Frameworks","Function Calling"],"analysis":{"Architecture and Design Patterns":"iMCP follows a client-server architecture. The macOS app acts as a UI and permission manager, while the command-line `imcp-server` handles MCP communication via standard I/O. Bonjour is used for automatic service discovery on the local network. A custom `mcp-swift-sdk` is employed for MCP server/client functionality, promoting code reuse and abstraction.","Implementation Details and Key Features":"The application provides access to calendar, contacts, location, messages, and reminders. It utilizes Apple's Contacts framework and a custom solution, Madrid, for reading iMessage databases (handling the `typedstream` format). Tool results are encoded in JSON-LD using Schema.org vocabularies, facilitated by the custom Ontology Swift package.","Integration Approaches and Protocols":"iMCP integrates with Claude Desktop (and potentially other MCP clients) via the Model Context Protocol. It leverages Bonjour for service discovery and standard I/O for inter-process communication between the app and the CLI server. It interacts with macOS system services through standard APIs (e.g., Contacts framework) and direct database access.","Technical Stack and Dependencies":"The application is primarily written in Swift and targets macOS. Key dependencies include the custom `mcp-swift-sdk` for MCP handling, Madrid for iMessage database access, and Ontology for JSON-LD encoding. Bonjour is used for service discovery. It depends on Claude Desktop for integration and utilizes Apple's system frameworks.","Best Practices and Coding Standards":"The project utilizes well-defined architecture, separating UI and server logic. It packages functionality into reusable Swift packages (mcp-swift-sdk, Madrid, Ontology). The use of JSON-LD and Schema.org promotes interoperability and data standardization. The code uses existing frameworks where possible and creates custom ones where needed. It follows Apple's security model.","Performance Considerations":"Performance is likely impacted by the need to decode the `typedstream` format for iMessage data. Accessing system resources may also introduce latency. The local network communication between the app and server adds overhead, but the data being handled is local. The choice of Swift and JSON-LD may have performance implications that aren't mentioned."}}', '2025-03-10 21:28:52+00', 'true', 'Servers', 'imcp-macos-server'), ('943405292', 'gcp-mcp', 'eniayomi/gcp-mcp', 'https://github.com/eniayomi/gcp-mcp', 'A Model Context Protocol (MCP) server that enables AI assistants like Claude to interact with your Google Cloud Platform environment. This allows for natural language querying and management of your GCP resources during conversations.', '2025-03-05 16:50:20+00', '2025-03-09 17:49:31+00', 'TypeScript', '11', '# GCP MCP

A Model Context Protocol (MCP) server that enables AI assistants like Claude to interact with your Google Cloud Platform environment. This allows for natural language querying and management of your GCP resources during conversations.

![GCP MCP Demo](images/claude.png)

## Features

* 🔍 Query and modify GCP resources using natural language
* ☁️ Support for multiple GCP projects
* 🌐 Multi-region support
* 🔐 Secure credential handling (no credentials are exposed to external services)
* 🏃‍♂️ Local execution with your GCP credentials
* 🔄 Automatic retries for improved reliability

## Prerequisites

* Node.js
* Claude Desktop/Cursor/Windsurf
* GCP credentials configured locally (application default credentials)

## Installation

1. Clone the repository:
```bash
git clone https://github.com/eniayomi/gcp-mcp
cd gcp-mcp
```

2. Install dependencies:
```bash
npm install
```

## Configuration

### Claude Desktop

1. Open Claude desktop app and go to Settings -> Developer -> Edit Config

2. Add the following entry to your `claude_desktop_config.json`:

via npm:
```json
{
  "mcpServers": {
    "gcp": {
      "command": "sh",
      "args": ["-c", "npx -y gcp-mcp"]
    }
  }
}
```

If you installed from source:
```json
{
  "mcpServers": {
    "gcp": {
      "command": "npm",
      "args": [
        "--silent",
        "--prefix",
        "/path/to/gcp-mcp",
        "start"
      ]
    }
  }
}
```

Replace `/path/to/gcp-mcp` with the actual path to your project directory if using source installation.

### Cursor

1. Open Cursor and go to Settings (⌘,)
2. Navigate to AI -> Model Context Protocol
3. Add a new MCP configuration:
```json
{
  "gcp": {
    "command": "npx -y gcp-mcp"
  }
}
```

### Windsurf

1. Open `~/.windsurf/config.json` (create if it doesn't exist)
2. Add the MCP configuration:
```json
{
  "mcpServers": {
    "gcp": {
      "command": "npx -y gcp-mcp"
    }
  }
}
```

### GCP Setup

1. Set up GCP credentials:
   - Set up application default credentials using `gcloud auth application-default login`

2. Refresh your AI assistant (Claude Desktop/Cursor/Windsurf)

## Usage

Start by selecting a project or asking questions like:
* "List all GCP projects I have access to"
* "Show me all Cloud SQL instances in project X"
* "What's my current billing status?"
* "Show me the logs from my Cloud Run services"
* "List all GKE clusters in us-central1"
* "Show me all Cloud Storage buckets in project X"
* "What Cloud Functions are deployed in us-central1?"
* "List all Cloud Run services"
* "Show me BigQuery datasets and tables"

## Available Tools

1. `run-gcp-code`: Execute GCP API calls using TypeScript code
2. `list-projects`: List all accessible GCP projects
3. `select-project`: Select a GCP project for subsequent operations
4. `get-billing-info`: Get billing information for the current project
5. `get-cost-forecast`: Get cost forecast for the current project
6. `get-billing-budget`: Get billing budgets for the current project
7. `list-gke-clusters`: List all GKE clusters in the current project
8. `list-sql-instances`: List all Cloud SQL instances in the current project
9. `get-logs`: Get Cloud Logging entries for the current project

## Example Interactions

1. List available projects:
```
List all GCP projects I have access to
```

2. Select a project:
```
Use project my-project-id
```

3. Check billing status:
```
What's my current billing status?
```

4. View logs:
```
Show me the last 10 log entries from my project
```

## Supported Services

* Google Compute Engine
* Cloud Storage
* Cloud Functions
* Cloud Run
* BigQuery
* Cloud SQL
* Google Kubernetes Engine (GKE)
* Cloud Logging
* Cloud Billing
* Resource Manager
* More coming soon...

## Troubleshooting

To see logs:
```bash
tail -n 50 -f ~/Library/Logs/Claude/mcp-server-gcp.log
```

Common issues:
1. Authentication errors: Ensure you've run `gcloud auth application-default login`
2. Permission errors: Check IAM roles for your account
3. API errors: Verify that required APIs are enabled in your project

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request. 

## License

MIT ', 'true', '"{\"AI Model Integration\",\"Context Management\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states it's a Model Context Protocol (MCP) server. It's designed to enable AI assistants to interact with Google Cloud Platform resources using natural language, a key aspect of MCP.","categories":["AI Model Integration","Context Management","API Frameworks"],"analysis":{"Architecture and Design Patterns":"The repository acts as a bridge between AI models and GCP services. It appears to follow a client-server architecture where AI assistants are clients, and this repository is the MCP server. The design promotes modularity with distinct tools for interacting with different GCP services. Configuration files dictate how different AI assistants connect with the MCP server.","Implementation Details and Key Features":"Key features include querying and modifying GCP resources using natural language, support for multiple GCP projects and regions, and secure credential handling. The implementation relies on Node.js and utilizes the Google Cloud SDK to interact with GCP services. Automatic retries are implemented to improve reliability. It provides a set of pre-defined tools exposed for use by the AI assistants.","Integration Approaches and Protocols":"Integration is achieved through Model Context Protocol (MCP), enabling AI assistants to interact with the GCP environment via natural language queries. The server is configured within the AI assistant's settings, defining commands to execute the server. Example integrations are provided for Claude Desktop, Cursor, and Windsurf. This shows it uses a standardized way for models to call external tools.","Technical Stack and Dependencies":"The primary technical stack consists of Node.js and the Google Cloud SDK. It relies on `npm` for dependency management. The repository interacts with various GCP services such as Compute Engine, Cloud Storage, Cloud Functions, and others. The documentation highlights the use of `gcloud auth application-default login` for authentication.","Best Practices and Coding Standards":"The documentation includes a contributing section, suggesting a desire for community involvement and adherence to certain standards. The focus on secure credential handling indicates a concern for security best practices. Automatic retries point towards resilience and fault tolerance. The explicit configuration files suggest a practice of keeping runtime settings separate from the core code.","Performance Considerations":"The repository's performance would depend on the efficiency of GCP API calls and the implementation of the server itself. Considerations for optimizing performance could involve caching API responses, implementing asynchronous operations, and optimizing query logic. The multi-region support suggests awareness of geographical latency and potential for region-specific optimizations."}}', '2025-03-10 21:28:34+00', 'true', 'Servers', 'gcp-claude-mcp'), ('943413314', 'MCP-demo', 'WinterTechForum/MCP-demo', 'https://github.com/WinterTechForum/MCP-demo', 'A very simple MCP demo, based off of Anthropics MCP examples, with the added bonus of an agency loop', '2025-03-05 17:05:48+00', '2025-03-06 21:22:12+00', 'Python', '2', '# MCP Client

A client implementation for MCP with Anthropic integration.

## Requirements

- Python 3.13 or higher
- pip or uv package manager

## Setup Instructions

1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd mcp-client
   ```

2. Create and activate a virtual environment:
   ```bash
   uv venv  # Recommended
   source .venv/bin/activate  # On Unix-like systems
   # Or on Windows:
   # .venv\Scripts\activate

   # Alternatively with Python's venv module:
   # python -m venv .venv
   # source .venv/bin/activate  # On Unix-like systems
   # .venv\Scripts\activate  # On Windows
   ```

3. Install dependencies:
   ```bash
   uv sync  # Recommended
   # Or alternatively with pip:
   # pip install .
   ```

4. Configure environment variables:
   - Create a `.env` file in the project root with the following content:
     ```
     ANTHROPIC_API_KEY=your_api_key_here
     ```
   - Replace `your_api_key_here` with your actual Anthropic API key

5. Running:
   `python client.py weather/weather.py`

   If you have a separate tools server, replace `weather/weather.py` with the location of your server

## Dependencies

- anthropic (>=0.49.0): Anthropic API client
- httpx (>=0.28.1): HTTP client
- mcp[cli] (>=1.3.0): MCP CLI tools
- python-dotenv (>=1.0.1): Environment variable management

## Environment Variables

The following environment variables are required:

- `ANTHROPIC_API_KEY`: Your Anthropic API key for authentication

## Note

Make sure to keep your `.env` file secure and never commit it to version control. The repository includes a `.gitignore` file that should already exclude the `.env` file.
', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"Protocol Implementations\"]}', '{"is_mcp":"YES","justification":"The repository description and documentation explicitly mention MCP and provide a client implementation for Anthropic integration, aligning with the core functionality of MCP.","categories":["Agent Frameworks","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The repository implements a client for the Model Context Protocol (MCP), suggesting a client-server architecture. The design appears to follow a modular structure, separating the client implementation from the tool server (as indicated by the ability to specify a server location). The project uses environment variables for configuration, demonstrating a best practice for managing sensitive information.","Implementation Details and Key Features":"The core implementation involves a client.py script that interacts with an MCP server. The example provided uses a 'weather/weather.py' script, indicating that the client is designed to interact with tool servers implementing specific functionalities. The use of httpx as an HTTP client allows for asynchronous communication, potentially improving performance. The project leverages the anthropic library to connect to Anthropic's models.","Integration Approaches and Protocols":"The repository focuses on integrating with the Anthropic API via MCP. This involves sending requests to an MCP server, which in turn interacts with AI models to execute functions. The client expects responses conforming to the MCP standard, enabling it to handle data returned from the AI model via the tool. The repository uses python-dotenv to manage environment variables.","Technical Stack and Dependencies":"The primary language is Python (3.13+). Key dependencies include anthropic (Anthropic API client), httpx (HTTP client), mcp[cli] (MCP CLI tools), and python-dotenv (environment variable management). The httpx library suggests asynchronous operations. The MCP CLI tools indicate the use of command-line tools for interacting with or managing the MCP implementation.","Best Practices and Coding Standards":"The project demonstrates several best practices, including the use of a virtual environment for dependency management (uv or venv). It also utilizes a .gitignore file to exclude sensitive information like the .env file. The configuration through environment variables promotes security and flexibility. The README provides clear setup and usage instructions.","Performance Considerations":"The use of httpx suggests that asynchronous communication is employed, which could lead to improved performance, especially when dealing with network requests to the Anthropic API. The dependency on the anthropic library means performance will also be influenced by the efficiency of that library and the underlying API. Efficient context management is also important for MCP performance."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'mcp-demo-agency-loop'), ('943438693', 'winshot-mcp', 'unknownue/winshot-mcp', 'https://github.com/unknownue/winshot-mcp', 'A MCP window screenshot integration tool for Cursor', '2025-03-05 17:53:27+00', '2025-03-10 15:56:51+00', 'Python', '0', '# Winshot MCP

A Model-Client Protocol (MCP) window screenshot integration tool for Cursor, allowing Cursor's agent mode to capture and analyze application windows.

## Overview

This project implements the MCP protocol with window screenshot functionality, enabling Large Language Models (LLMs) to visually observe and interact with application UIs. By capturing screenshots of specific windows, LLMs can better understand the user's environment and provide more context-aware assistance.

> ⚠️ **Important Cursor Limitation**: Currently Cursor's agent mode cannot directly process images via URIs/links. Cursor only supports viewing images when referenced with the `@file` syntax (e.g., `@/path/to/image.png`). This means that while our server provides optimized HTTP URIs, Cursor's LLM cannot directly "see" these images. You may need to manually download images or develop additional integrations to convert URIs to local file references for Cursor to process them properly.

## Features

- List all active windows on Windows, macOS, and Linux
- Capture screenshots of specific application windows
- Send window screenshots to LLMs via the MCP protocol
- Integrate with Cursor's agent mode
- Serve screenshots via HTTP for more efficient data transfer

## Project Structure

The project is organized into three main components:

- **Core**: Core window screenshot functionality
  - `core/winshot.py`: Core window screenshot implementation

- **Server**: MCP server implementation
  - `server/mcp_server.py`: MCP server with window screenshot support
  - `server/__main__.py`: Entry point for running the server

- **Client**: Client implementations and utilities
  - `client/mcp_client.py`: MCP client implementation
  - `client/cursor_adapter.py`: Adapter connecting Cursor with MCP
  - `client/cursor_winshot.py`: Cursor-specific window screenshot integration
  - `client/demo.py`: Demo script showcasing window screenshot functionality
  - `client/__main__.py`: Entry point for running the client demo

- **Main Entry Point**: 
  - `__main__.py`: Unified entry point for both server and client components

## Installation

1. Install dependencies:
   ```
   pip install -r requirements.txt
   ```

2. Platform-specific requirements:
   - Windows: `pip install pygetwindow pywin32`
   - Linux: `sudo apt-get install xdotool imagemagick`
   - macOS: Terminal/IDE needs screen recording permissions
     - The system will ask for permission on first run
     - You can also manually grant permission in System Settings > Privacy & Security > Screen Recording
     - If using an IDE (like VS Code), ensure the IDE has permission

## Usage

### Image Size Configuration

The server supports configuring the maximum image dimensions and file size to optimize performance and memory usage:

- `--max-image-dimension <pixels>` - Maximum width or height of captured screenshots (default: 1200 pixels)
- `--max-file-size-mb <size>` - Maximum file size for screenshot data (default: 5 MB)

Reducing these values can improve performance and decrease memory usage, especially when working with large screens or high-resolution displays.

### Screenshot File Server

Instead of encoding screenshots as base64 (which increases data size by ~30%), the server now uses a file server approach:

- Screenshots are saved to a dedicated temporary directory
- Each screenshot is assigned a unique hash identifier
- A lightweight HTTP file server serves the images via `/img/{hash}` paths
- LLMs receive URIs with hash references instead of direct file paths
- Files are automatically cleaned up after a configurable expiry time

This approach offers several advantages:
- Significantly reduces data transfer size compared to base64 encoding
- Provides secure access without exposing the actual file system structure
- Each image has a unique identifier independent of its filename
- Links remain stable even if file locations change

Configuration options:
- `--fileserver-port <port>` - Port for the screenshot file server (default: 8766)
- `--tmp-dir <path>` - Custom temporary directory for screenshots (default: `./tmp` directory in project root)
- `--file-expiry-minutes <minutes>` - Time after which temporary files are deleted (default: 60 minutes)

> 🔹 **Important for Cursor Users**: Use the `--tmp-dir` parameter to specify a location accessible to Cursor. 
> The server will provide both the HTTP URI and the local file path in its response, allowing Cursor 
> to view screenshots using the `@file` syntax. **Setting a proper `--tmp-dir` is critical** for enabling 
> Cursor's LLM to analyze images.
>
> Example: `python . server --tmp-dir /Users/username/cursor-accessible-folder`

This approach significantly reduces data transfer size and memory usage, especially for large screenshots.

### Using the Unified Entry Point

Run the server:
```
python . server
```

Run the server with custom image size settings:
```
python . server --max-image-dimension 800 --max-file-size-mb 2
```

Run the server with custom file server settings:
```
python . server --fileserver-port 8080 --tmp-dir ./screenshots --file-expiry-minutes 120
```

Run the client:
```
python . client
```

Specify a custom port for the server:
```
python . server --port 8766
```

### Running the Server Directly

Start the MCP server:
```
python -m server
```

Start the server with custom image size settings:
```
python -m server --max-image-dimension 800 --max-file-size-mb 2
```

Or:
```
python server/mcp_server.py
```

### Running the Demo Client Directly

Run the demo client:
```
python -m client
```

Or:
```
python client/demo.py
```

### Using with Cursor

In Cursor's agent mode, you can use the following commands:
- `list_windows()` - List all available windows
- `capture_window("Window Title")` - Capture a screenshot of a window with the given title
- `capture_window(window_index)` - Capture a screenshot of a window by index

## Demo

Running the client demo will:
1. List all available windows on your system
2. Allow you to select a window to screenshot
3. Initiate the screenshot process (**Note: Current version may require you to manually click on the window to be captured**)

The interactive prompts during the screenshot process are part of macOS security mechanisms, ensuring user control over screen capture. Screenshots are saved in the current directory with filenames starting with `window_shot_`.

## Troubleshooting Screenshot Issues

If you encounter issues with screenshots:

1. **Window Activation**: The target window MUST be in the foreground for accurate capture
   - The tool will attempt to bring the window to front automatically
   - If automatic activation fails, manually click the window you want to capture
   - Ensure the window is not minimized
   - Wait a moment after the window comes to front before capture

2. **Permission Issues**:
   - On macOS: Confirm screen recording permission is granted to your terminal or IDE
   - On Windows: Ensure the application has necessary permissions to capture screen content
   - **During the screenshot process, click on the window you want to capture when prompted**

3. **Technical Solutions**:
   - Try different screenshot methods: the code automatically attempts multiple methods
   - For windows that cannot be captured automatically, you may need to modify the screenshot parameters in `core/winshot.py`
   - **Memory or performance issues**: Try reducing the image size with `--max-image-dimension` and `--max-file-size-mb` options

### Known Limitations

- **Window Activation Required**: The window must be in the foreground for accurate capture. While the tool attempts to bring windows to front automatically, some windows may require manual activation.
- **User intervention required**: The current version on macOS may require manual window clicking to complete the screenshot. This is a result of system security limitations and is currently considered acceptable.
- Some windows may not be locatable by ID, in which case the foreground window will be captured.
- Some applications' windows may not be captured correctly, especially those using non-standard window management.
- **Large screens/high-resolution displays**: May require adjusting the maximum image dimensions and file size to prevent memory issues.

## Protocol Details

This implementation extends the MCP protocol by adding specific message types for window screenshots:

- `window_list_request`/`window_list_response`: Used to list available windows
- `window_screenshot_request`/`window_screenshot_response`: Used to capture screenshots and receive URIs to the images ', 'true', '"{\"AI Model Integration\",\"Function Calling\",\"Protocol Implementations\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states that it implements the Model Context Protocol (MCP) for window screenshot integration, facilitating communication between LLMs and application UIs.","categories":["Protocol Implementations","AI Model Integration","Function Calling"],"analysis":{"Architecture and Design Patterns":"The project adopts a modular architecture with distinct 'core', 'server', and 'client' components. The 'core' handles screenshot logic, the 'server' exposes MCP endpoints, and the 'client' facilitates interaction. It incorporates a file server to serve images via HTTP, promoting efficient data transfer and a temporary directory for file management with automatic cleanup.","Implementation Details and Key Features":"The implementation allows listing active windows and capturing their screenshots. It uses a file server to deliver images via HTTP URIs, avoiding base64 encoding. It includes platform-specific installation instructions and addresses Cursor's limitations in processing images via URIs by allowing the LLM to view screenshots using the `@file` syntax.","Integration Approaches and Protocols":"The repository integrates with Cursor's agent mode and uses the MCP protocol for communication between the client and server. It implements specific MCP message types for window listing and screenshot capture. The integration focuses on enabling LLMs to interact with application UIs through visual observation via screenshots.","Technical Stack and Dependencies":"The project relies on Python and libraries such as `pygetwindow` and `pywin32` (Windows), `xdotool` and `imagemagick` (Linux). The file server component uses HTTP for serving screenshots. The use of `pip` for dependency management is noted. Screen recording permissions are necessary on macOS.","Best Practices and Coding Standards":"The documentation details configuration options for image size and file size, encouraging optimization. It also offers troubleshooting advice for screenshot issues and known limitations. The project provides clear installation instructions and platform-specific guidance. The unified entry point provides a clean way to run server and client.","Performance Considerations":"The project addresses performance by saving screenshots to a temporary directory and serving them via a lightweight HTTP file server instead of base64 encoding, reducing data transfer size. It includes configuration options for controlling image dimensions and file size to optimize memory usage. File expiry is used to clean up temporary screenshots."}}', '2025-03-10 21:26:24+00', 'true', 'Servers', 'winshot-mcp-cursor'), ('943443114', 'mcpreprocessor', 'kototuz/mcpreprocessor', 'https://github.com/kototuz/mcpreprocessor', 'Create minecraft datapacks more conviniently and faster', '2025-03-05 18:01:43+00', '2025-03-10 18:31:27+00', 'Odin', '0', '# mcpreprocessor

Like C preprocessor :)

## The main goal

The main goal is to make minecraft pack development easier by
automating some repeating processes (e.g. function creating)

## Current state

This is a basic snippet of what you can do right now:
This code will be processed into `.mcfunction` files

```
@fn main
    say hello world
    say Good morning
@end

@fn foo
    summon pig
    summon zombie
@end

@fn baz
    summon pig
    summon zombie
@end
```

> [!NOTE]
> The program doesn't check whether valid your code.
> It works just like macro

## Build

Just go to to the `src` directory and write `odin build .` or `odin run .`
', 'true', '"{\"Developer Tooling\",\"Code Generation\",\"System Automation\"]}', '{"is_mcp":"NO","justification":"The repository focuses on simplifying Minecraft datapack creation, not communication between AI models or context management for LLMs. It lacks features associated with Model Context Protocol specifications.","categories":["Developer Tooling","Code Generation","System Automation"],"analysis":{"Architecture and Design Patterns":"The repository architecture is simple, revolving around parsing a custom syntax with macros and translating it into Minecraft `.mcfunction` files. It does not seem to employ complex design patterns. The description focuses on basic code processing rather than intricate architectural components. The processing flow consists of reading, interpreting, and outputting Minecraft functions.","Implementation Details and Key Features":"The implementation revolves around parsing custom syntax (similar to C preprocessor directives) to generate `.mcfunction` files. The primary feature is the `@fn` directive for defining Minecraft functions, simplifying repetitive tasks. The program currently lacks comprehensive error checking. Future development could expand the types of macros and improve code validation.","Integration Approaches and Protocols":"The project doesn't integrate with external systems or protocols beyond Minecraft's `.mcfunction` file format. There's no indication of API integration, networking, or complex data exchange. The focus is entirely on internal transformation and output, targeting the datapack format of Minecraft. Integration depends entirely on Minecraft's internal systems.","Technical Stack and Dependencies":"The project is built using the Odin programming language. The main dependency is the Odin standard library for file I/O and string manipulation. The code does not use external libraries or frameworks, keeping the technical stack minimal. Build instructions mention using `odin build .` which clearly defines the core build tool.","Best Practices and Coding Standards":"The description doesn't explicitly address coding standards or best practices. It states that the program does not validate the code, implying a lack of robust error handling or input validation. The provided examples are basic and don't illustrate adherence to any specific coding conventions beyond the minimum requirements of the Odin language.","Performance Considerations":"Performance considerations are not explicitly mentioned. Given the nature of the task (generating `.mcfunction` files), the project is unlikely to face significant performance bottlenecks. The primary concern would be the parsing speed of large input files, but no specific optimizations are documented. The current code does not appear to be heavily optimized."}}', '2025-03-10 21:34:28+00', 'true', 'Servers', 'mcp-datapack-preprocessor'), ('943461890', 'todo-list-mcp', 'RegiByte/todo-list-mcp', 'https://github.com/RegiByte/todo-list-mcp', 'An MCP server for managing todos within LLMs, created for educational purposes', '2025-03-05 18:38:53+00', '2025-03-06 02:08:54+00', 'TypeScript', '3', '# Todo List MCP Server

A Model Context Protocol (MCP) server that provides a comprehensive API for managing todo items.

> **📚 Learning Resource**: This project is designed as an educational example of MCP implementation. See [GUIDE.md](GUIDE.md) for a comprehensive explanation of how the project works and why things are implemented the way they are.

## Features

- **Create todos**: Add new tasks with title and markdown description
- **Update todos**: Modify existing tasks
- **Complete todos**: Mark tasks as done
- **Delete todos**: Remove tasks from the list
- **Search todos**: Find tasks by title or creation date
- **Summarize todos**: Get a quick overview of active tasks

## Tools

This MCP server exposes the following tools:

1. `create-todo`: Create a new todo item
2. `list-todos`: List all todos
3. `get-todo`: Get a specific todo by ID
4. `update-todo`: Update a todo's title or description
5. `complete-todo`: Mark a todo as completed
6. `delete-todo`: Delete a todo
7. `search-todos-by-title`: Search todos by title (case-insensitive partial match)
8. `search-todos-by-date`: Search todos by creation date (format: YYYY-MM-DD)
9. `list-active-todos`: List all non-completed todos
10. `summarize-active-todos`: Generate a summary of all active (non-completed) todos

## Installation

```bash
# Clone the repository
git clone https://github.com/RegiByte/todo-list-mcp.git
cd todo-list-mcp

# Install dependencies
npm install

# Build the project
npm run build
```

## Usage

### Starting the Server

```bash
npm start
```

### Configuring with Claude for Desktop

#### Claude Desktop

Add this to your `claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "todo": {
      "command": "node",
      "args": ["/absolute/path/to/todo-list-mcp/dist/index.js"]
    }
  }
}
```

#### Cursor

- Go to "Cursor Settings" -> MCP
- Add a new MCP server with a "command" type
- Add the absolute path of the server and run it with node
- Example: node /absolute/path/to/todo-list-mcp/dist/index.js

### Example Commands

When using with Claude for Desktop or Cursor, you can try:

- "Create a todo to learn MCP with a description explaining why MCP is useful"
- "List all my active todos"
- "Create a todo for tomorrow's meeting with details about the agenda in markdown"
- "Mark my learning MCP todo as completed"
- "Summarize all my active todos"

## Project Structure

This project follows a clear separation of concerns to make the code easy to understand:

```
src/
├── models/       # Data structures and validation schemas
├── services/     # Business logic and database operations
├── utils/        # Helper functions and formatters
├── config.ts     # Configuration settings
├── client.ts     # Test client for local testing
└── index.ts      # Main entry point with MCP tool definitions
```

## Learning from This Project

This project is designed as an educational resource. To get the most out of it:

1. Read the [GUIDE.md](GUIDE.md) for a comprehensive explanation of the design
2. Study the heavily commented source code to understand implementation details
3. Use the test client to see how the server works in practice
4. Experiment with adding your own tools or extending the existing ones

## Development

### Building

```bash
npm run build
```

### Running in Development Mode

```bash
npm run dev
```

## License

MIT ', 'true', '"{\"AI Model Integration\",\"Context Management\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements a Model Context Protocol (MCP) server for managing todos, designed as an educational example. It exposes tools that LLMs can use to interact with and manage a todo list.","categories":["Protocol Implementations","Agent Frameworks","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The project follows a clear separation of concerns, dividing the code into models, services, and utils directories. This architecture enhances maintainability and readability. The main entry point (`index.ts`) defines the MCP tools, exposing functionalities to interact with the todo list, such as creating, listing, and updating todos.  Configuration settings are centralized in `config.ts`.","Implementation Details and Key Features":"The server provides APIs for creating, updating, completing, deleting, searching, and summarizing todo items. It leverages the Model Context Protocol to integrate these functionalities with LLMs.  The implementation includes data validation schemas in the models directory and business logic in the services directory.  The test client (`client.ts`) allows for local testing of the server's functionalities.","Integration Approaches and Protocols":"The project demonstrates integration with Claude for Desktop and Cursor, specifying configuration details for these platforms. It utilizes a command-line interface invoked via `node` to start the MCP server.  The exposed tools are designed to be called by LLMs, enabling natural language interaction with the todo list. The project relies on the Model Context Protocol to define how LLMs can interact with the server.","Technical Stack and Dependencies":"The project uses Node.js for the backend and likely TypeScript (inferred from the build process and file extensions). The dependencies are managed using npm.  The project structure indicates the use of a database or storage solution (likely simple file storage for simplicity given its educational focus) for persisting todo items. The server exposes a set of functions designed to integrate with LLMs.","Best Practices and Coding Standards":"The documentation emphasizes the project's role as an educational resource, encouraging users to read the `GUIDE.md` for a comprehensive explanation of the design and implementation details.  The source code is heavily commented to aid understanding.  The project structure promotes modularity and separation of concerns, facilitating code maintainability. The use of a test client demonstrates a commitment to testing and quality assurance.","Performance Considerations":"Given its educational nature, the project likely doesn't prioritize advanced performance optimizations. However, the design choices, such as separation of concerns, contribute to scalability and maintainability.  Database interactions, if any, could be optimized for larger datasets. The summarization functionality could benefit from efficient algorithms for processing and generating summaries, although the current implementation likely focuses on correctness over raw speed."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'todo-list-mcp-server'), ('943462661', 'mcp-sentry', 'MCP-100/mcp-sentry', 'https://github.com/MCP-100/mcp-sentry', 'A Model Context Protocol server for retrieving and analyzing issues from Sentry.io', '2025-03-05 18:40:22+00', '2025-03-10 11:05:21+00', 'Python', '4', '# mcp-sentry: A Sentry MCP server

[![smithery badge](https://smithery.ai/badge/@qianniuspace/mcp-sentry)](https://smithery.ai/server/@qianniuspace/mcp-sentry)

## Overview

A Model Context Protocol server for retrieving and analyzing issues from Sentry.io. This server provides tools to inspect error reports, stacktraces, and other debugging information from your Sentry account.

### Tools

1. `get_sentry_issue`
   - Retrieve and analyze a Sentry issue by ID or URL
   - Input:
     - `issue_id_or_url` (string): Sentry issue ID or URL to analyze
   - Returns: Issue details including:
     - Title
     - Issue ID
     - Status
     - Level
     - First seen timestamp
     - Last seen timestamp
     - Event count
     - Full stacktrace
2. `get_list_issues`
   - Retrieve and analyze Sentry issues by project slug
   - Input:
     - `project_slug` (string): Sentry project slug to analyze
     - `organization_slug` (string): Sentry organization slug to analyze
   - Returns: List of issues with details including:
     - Title
     - Issue ID
     - Status
     - Level
     - First seen timestamp
     - Last seen timestamp
     - Event count
     - Basic issue information

### Prompts

1. `sentry-issue`
   - Retrieve issue details from Sentry
   - Input:
     - `issue_id_or_url` (string): Sentry issue ID or URL
   - Returns: Formatted issue details as conversation context

## Installation

### Installing via Smithery

To install mcp-sentry for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@qianniuspace/mcp-sentry):

```bash
npx -y @smithery/cli install @qianniuspace/mcp-sentry --client claude
```

### Using uv (recommended)

When using [`uv`](https://docs.astral.sh/uv/) no specific installation is needed. We will
use [`uvx`](https://docs.astral.sh/uv/guides/tools/) to directly run *mcp-sentry*.

### Using PIP

Alternatively you can install `mcp-sentry` via pip:

```
pip install mcp-sentry
```

or use uv
```
uv pip install -e .
```

After installation, you can run it as a script using:

```
python -m mcp_sentry
```

## Configuration

### Usage with Claude Desktop

Add this to your `claude_desktop_config.json`:

<details>
<summary>Using uvx</summary>

```json
"mcpServers": {
  "sentry": {
    "command": "uvx",
    "args": ["mcp-sentry", "--auth-token", "YOUR_SENTRY_TOKEN","--project-slug" ,"YOUR_PROJECT_SLUG", "--organization-slug","YOUR_ORGANIZATION_SLUG"]
  }
}
```
</details>


<details>
<summary>Using docker</summary>

```json
"mcpServers": {
  "sentry": {
    "command": "docker",
    "args": ["run", "-i", "--rm", "mcp/sentry", "--auth-token", "YOUR_SENTRY_TOKEN","--project-slug" ,"YOUR_PROJECT_SLUG", "--organization-slug","YOUR_ORGANIZATION_SLUG"]
  }
}
```
</details>

<details>

<summary>Using pip installation</summary>

```json
"mcpServers": {
  "sentry": {
    "command": "python",
    "args": ["-m", "mcp_sentry", "--auth-token", "YOUR_SENTRY_TOKEN","--project-slug" ,"YOUR_PROJECT_SLUG", "--organization-slug","YOUR_ORGANIZATION_SLUG"]
  }
}
```
</details>

### Usage with [Zed](https://github.com/zed-industries/zed)

Add to your Zed settings.json:

<details>
<summary>Using uvx</summary>

For Example Curson ![mcp.json](.cursor/mcp.json) 

```json
"context_servers": [
  "mcp-sentry": {
    "command": {
      "path": "uvx",
      "args": ["mcp-sentry", "--auth-token", "YOUR_SENTRY_TOKEN","--project-slug" ,"YOUR_PROJECT_SLUG", "--organization-slug","YOUR_ORGANIZATION_SLUG"]
    }
  }
],
```
</details>

<details>
<summary>Using pip installation</summary>

```json
"context_servers": {
  "mcp-sentry": {
    "command": "python",
    "args": ["-m", "mcp_sentry", "--auth-token", "YOUR_SENTRY_TOKEN","--project-slug" ,"YOUR_PROJECT_SLUG", "--organization-slug","YOUR_ORGANIZATION_SLUG"]
  }
},
```
</details>

<details>
<summary>Using pip installation with custom path</summary>

```json
"context_servers": {
  "sentry": {
      "command": "python",
      "args": [
        "-m",
        "mcp_sentry",
        "--auth-token",
        "YOUR_SENTRY_TOKEN",
        "--project-slug",
        "YOUR_PROJECT_SLUG",
        "--organization-slug",
        "YOUR_ORGANIZATION_SLUG"
      ],
      "env": {
        "PYTHONPATH": "path/to/mcp-sentry/src"
      }
    }
},
```


</details>







## Debugging

You can use the MCP inspector to debug the server. For uvx installations:

```
npx @modelcontextprotocol/inspector uvx mcp-sentry --auth-token YOUR_SENTRY_TOKEN --project-slug YOUR_PROJECT_SLUG --organization-slug YOUR_ORGANIZATION_SLUG
```

Or if you've installed the package in a specific directory or are developing on it:

```
cd path/to/servers/src/sentry
npx @modelcontextprotocol/inspector uv run mcp-sentry --auth-token YOUR_SENTRY_TOKEN --project-slug YOUR_PROJECT_SLUG --organization-slug YOUR_ORGANIZATION_SLUG  
```
or in term
```
npx @modelcontextprotocol/inspector uv --directory /Volumes/ExtremeSSD/MCP/mcp-sentry/src run mcp_sentry --auth-token YOUR_SENTRY_TOKEN
--project-slug YOUR_PROJECT_SLUG --organization-slug YOUR_ORGANIZATION_SLUG
```
![Inspector-tools](./images/Inspector-tools.png)

## Fork From
- [https://github.com/modelcontextprotocol/servers/tree/main/src/sentr](https://github.com/modelcontextprotocol/servers/tree/main/src/sentry)
## License

This MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.
', 'true', '"{\"Protocol Implementations\",\"Developer Tooling\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it's a Model Context Protocol server for retrieving and analyzing Sentry issues. It provides tools that return data formatted as context for AI models, aligning with MCP's core purpose.","categories":["Protocol Implementations","Developer Tooling","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The repository implements a client-server architecture, where the server interacts with the Sentry API and exposes data formatted according to the Model Context Protocol. It provides specific tools (e.g., `get_sentry_issue`, `get_list_issues`) to fetch and process data, demonstrating a modular design. Configuration examples are provided for integration with Claude and Zed editors.","Implementation Details and Key Features":"The core functionality involves retrieving issue details from Sentry based on IDs or URLs using Sentry's API. The server offers two main tools: one to retrieve single issues and another to list issues by project. It formats retrieved data, including stack traces and metadata, into a suitable format for AI model consumption.","Integration Approaches and Protocols":"This repository integrates with Sentry through its API and adheres to the Model Context Protocol for output formatting. The server exposes functionality that can be called by AI clients like Claude and Zed. Configuration instructions demonstrate how to integrate the server by configuring the AI client to call the server.","Technical Stack and Dependencies":"The repository appears to be built using Python, as indicated by the installation instructions (`pip install mcp-sentry`). The configuration details also suggest the usage of `uv` or `uvx`. It likely depends on Sentry's API and utilizes libraries for making HTTP requests and handling JSON data. Specific Sentry SDK usage would likely be present in the code.","Best Practices and Coding Standards":"The documentation provides clear instructions for installation and configuration, supporting multiple methods like pip and uv. The use of Smithery, a tool for managing and installing MCP servers, indicates adherence to standardized deployment practices. The repository includes a License file, indicating open-source standards compliance.","Performance Considerations":"Performance considerations depend on the efficiency of Sentry API calls and the data processing within the server. Retrieving large numbers of issues could potentially introduce latency. Caching mechanisms for frequently accessed data could be implemented to improve response times. The current documentation does not cover explicit performance optimization strategies."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'mcp-sentry-server'), ('943471429', 'mcp-repl', 'kyryl-opens-ml/mcp-repl', 'https://github.com/kyryl-opens-ml/mcp-repl', null, '2025-03-05 18:58:22+00', '2025-03-08 14:30:55+00', 'Python', '5', '# MCP-REPL

A lightweight REPL (Read-Eval-Print Loop) for interacting with various services through MCP (Model Context Protocol).

## Overview

`mcp-repl` is designed for efficient development, debugging, and testing of MCP servers. It provides an intuitive command-line interface that's simpler than using the Cloud Desktop app, allowing developers to quickly:

- Send queries to MCP servers
- View detailed tool execution
- Automatically save chat history
- Test multiple MCP services simultaneously

## Installation

Install via pip or uv:

```bash
uv add mcp-repl
```

### Development Installation

Clone and install in editable mode:

```bash
git clone https://github.com/yourusername/mcp-repl.git
cd mcp-repl
uv venv
```

## Databases Example

This example demonstrates how you can:

- Set up 3 databases (PostgreSQL, MySQL, Redis) in Kubernetes with mock data
- Run MCP servers for each dataset
- Interact with all databases from a single REPL

### Key Features

- Natural language queries across multiple databases
- Comparison of data structures across systems
- Execution of complex operations seamlessly

### Running the Example

Setup infrastructure (requires `kind` and `helm`):

```bash
bash examples/databases/setup.sh
```

Install databases related dependencies
```bash
uv add mcp-repl['databases']
```

Generate mock data:

```bash
uv run examples/databases/generate_mock_data.py
```

Start the REPL:

```bash
uv run -m mcp_repl.repl --config examples/databases/config.json --auto-approve-tools
```

### Sample Queries

You can perform queries like:

- "Find all tables in PostgreSQL and MySQL"
- "Compare the structure of the 'users' table in PostgreSQL with the 'customers' table in MySQL"
- "Count the number of records in each database"

## Usage

Start the REPL:

```bash
uv run -m mcp_repl.repl --config path/to/config.json
```

### Optional Flags

- `--auto-approve-tools`: Automatically approve all tool executions
- `--always-show-full-output`: Always display complete tool outputs
- `--chat-history-dir PATH`: Directory to save chat history (default: `./chat_history`)

## Testing

Comprehensive integration tests are included to verify functionality:

Run all tests:

```bash
uv run pytest
```
', 'true', '"{\"Protocol Implementations\",\"Developer Tooling\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly mentions and uses Model Context Protocol (MCP) to interact with services. The `mcp-repl` facilitates sending queries to MCP servers and viewing tool executions.","categories":["Protocol Implementations","Developer Tooling","Function Calling"],"analysis":{"Architecture and Design Patterns":"The repository implements a REPL (Read-Eval-Print Loop) to interact with MCP servers. The architecture supports connecting to multiple MCP services defined via a configuration file. It uses a command-line interface to provide an interactive experience. The design focuses on simplifying the process of testing and debugging MCP implementations.","Implementation Details and Key Features":"The core implementation revolves around a REPL interface that takes a configuration file specifying MCP server endpoints. Key features include sending queries, viewing detailed tool execution logs, automatic chat history saving, and simultaneous interaction with multiple MCP services. The example showcases integration with database systems.","Integration Approaches and Protocols":"The repository integrates with MCP servers using the Model Context Protocol. The example provides a database integration using PostgreSQL, MySQL, and Redis. The integration involves setting up the databases in Kubernetes, running MCP servers for each dataset, and interacting with all databases using natural language queries.","Technical Stack and Dependencies":"The repository uses Python as its primary language. Key dependencies include `uv` (or pip for installation), `pytest` for testing, and potentially database-specific Python libraries (e.g., `psycopg2` for PostgreSQL). The example also uses `kind` and `helm` for setting up the Kubernetes infrastructure.","Best Practices and Coding Standards":"The repository includes integration tests using `pytest`, demonstrating a commitment to testing and code quality. The documentation provides clear instructions for installation, usage, and running the example. The repository leverages virtual environments for dependency management using `uv venv`.","Performance Considerations":"The REPL provides an interactive interface, suggesting real-time performance is not a primary concern. However, the ability to connect to multiple services simultaneously implies the implementation needs to handle concurrent requests efficiently. Performance could be affected by the underlying MCP server implementations and network latency."}}', '2025-03-10 21:28:34+00', 'true', 'Servers', 'mcp-repl-server'), ('943483539', 'chrome-debug-mcp', 'robertheadley/chrome-debug-mcp', 'https://github.com/robertheadley/chrome-debug-mcp', 'An MCP server to allow you to debug webpages using LLMs', '2025-03-05 19:25:33+00', '2025-03-09 17:30:42+00', 'JavaScript', '7', '# Chrome Debug MCP Server

A Model Context Protocol (MCP) server for controlling Chrome with debugging capabilities, userscript injection, and extension support.

## Features

### Chrome Control
- Launch Chrome with custom configurations
- Support for custom Chrome executable paths
- User profile management (default or custom user data directory)
- Extension support and management
- Disable Chrome's "Automation Controlled" banner

### Page Automation
- Click, type, and interact with page elements
- Handle dropdowns and form inputs
- Hover and wait for elements
- Take screenshots of full page or elements
- Navigate between pages
- Set viewport size and device emulation
- Extract text and attributes from elements

### Debugging Capabilities
- Remote debugging via Chrome DevTools Protocol (CDP)
- Console log capture and monitoring
- JavaScript evaluation in page context
- Real-time console output streaming

### Userscript Support
- Inject userscripts into web pages
- Greasemonkey-style API support:
  - `GM_setValue`/`GM_getValue` for persistent storage
  - `GM_addStyle` for CSS injection
  - `GM_xmlhttpRequest` for cross-origin requests
  - `GM_openInTab` for new tab creation
  - `GM_registerMenuCommand` for menu commands

### Extension Integration
- Load unpacked extensions
- Maintain extension states and configurations
- Support for default Chrome profile extensions
- Selective extension enabling/disabling

## Installation

### Prerequisites
- [Node.js](https://nodejs.org/) (v14 or higher)
- [Chrome](https://www.google.com/chrome/) browser installed
- [Visual Studio Code](https://code.visualstudio.com/)
- [Roo Code Extension](https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline)

### Installing Roo Code Extension
1. Open Visual Studio Code
2. Go to Extensions (Ctrl+Shift+X)
3. Search for "Roo Code"
4. Click Install

### Setting up Chrome Debug MCP Server
1. Clone this repository:
   ```bash
   git clone https://github.com/yourusername/chrome-debug-mcp.git
   cd chrome-debug-mcp
   ```

2. Install dependencies:
   ```bash
   npm install
   ```

3. Build the project:
   ```bash
   npm run build
   ```

4. Configure in Roo Code:
   Add to `cline_mcp_settings.json`:
   ```json
   {
     "mcpServers": {
       "chrome-debug": {
         "command": "node",
         "args": ["path/to/chrome-debug-mcp/build/index.js"],
         "disabled": false,
         "alwaysAllow": []
       }
     }
   }
   ```

## Usage

For a complete reference of all available commands, tools, and functions, see [COMMANDS.md](docs/COMMANDS.md).

### Basic Chrome Launch
```javascript
use_mcp_tool({
  server_name: "chrome-debug",
  tool_name: "launch_chrome",
  arguments: {
    executablePath: "C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe",
    url: "https://example.com"
  }
})
```

### Launch with Custom Profile
```javascript
use_mcp_tool({
  server_name: "chrome-debug",
  tool_name: "launch_chrome",
  arguments: {
    executablePath: "C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe",
    userDataDir: "path/to/chrome/profile",
    disableAutomationControlled: true
  }
})
```

### Inject Userscript
```javascript
use_mcp_tool({
  server_name: "chrome-debug",
  tool_name: "launch_chrome",
  arguments: {
    url: "https://example.com",
    userscriptPath: "path/to/userscript.js"
  }
})
```

### Evaluate JavaScript
```javascript
use_mcp_tool({
  server_name: "chrome-debug",
  tool_name: "evaluate",
  arguments: {
    expression: "document.title"
  }
})
```

### Get Console Logs
```javascript
use_mcp_tool({
  server_name: "chrome-debug",
  tool_name: "get_console_logs",
  arguments: {
    clear: true
  }
})
```

### Page Interaction Examples

#### Click an Element
```javascript
use_mcp_tool({
  server_name: "chrome-debug",
  tool_name: "click",
  arguments: {
    selector: "#submit-button",
    delay: 500
  }
})
```

#### Type into Input
```javascript
use_mcp_tool({
  server_name: "chrome-debug",
  tool_name: "type",
  arguments: {
    selector: "#search-input",
    text: "search query",
    delay: 100
  }
})
```

#### Select from Dropdown
```javascript
use_mcp_tool({
  server_name: "chrome-debug",
  tool_name: "select",
  arguments: {
    selector: "#country-select",
    value: "US"
  }
})
```

#### Wait for Element
```javascript
use_mcp_tool({
  server_name: "chrome-debug",
  tool_name: "wait_for_selector",
  arguments: {
    selector: ".loading-complete",
    visible: true,
    timeout: 5000
  }
})
```

#### Take Screenshot
```javascript
use_mcp_tool({
  server_name: "chrome-debug",
  tool_name: "screenshot",
  arguments: {
    path: "screenshot.png",
    fullPage: true
  }
})
```

#### Set Viewport Size
```javascript
use_mcp_tool({
  server_name: "chrome-debug",
  tool_name: "set_viewport",
  arguments: {
    width: 1920,
    height: 1080,
    deviceScaleFactor: 1
  }
})
```

## Dependencies

This project uses the following open-source packages:

- [Puppeteer](https://pptr.dev/) - Chrome automation library
- [chrome-remote-interface](https://github.com/cyrus-and/chrome-remote-interface) - Chrome DevTools Protocol client
- [@modelcontextprotocol/sdk](https://github.com/ModelContextProtocol/sdk) - MCP SDK for server implementation

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Contributing

See our [Contributing Guide](CONTRIBUTING.md) for details on making contributions.

## Acknowledgments

- [Chrome DevTools Protocol](https://chromedevtools.github.io/devtools-protocol/)
- [Greasemonkey API](https://wiki.greasespot.net/Greasemonkey_Manual:API)
- [Model Context Protocol](https://modelcontextprotocol.ai)', 'true', '"{\"Protocol Implementations\",\"Developer Tooling\",\"Browser & Web Automation\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states it's a Model Context Protocol (MCP) server. It uses @modelcontextprotocol/sdk and provides tools to interact with Chrome through the MCP.","categories":["Agent Frameworks","Protocol Implementations","Browser & Web Automation"],"analysis":{"Architecture and Design Patterns":"The architecture is centered around providing an MCP server interface to control and debug Chrome. It leverages a modular design, separating concerns like Chrome launching, page interaction, and debugging capabilities. The use of Puppeteer and chrome-remote-interface suggests an event-driven, asynchronous programming model. Roo Code integration indicates a plugin-based approach for extending functionality.","Implementation Details and Key Features":"The server allows launching Chrome with custom configurations, managing user profiles, and injecting userscripts and extensions. Key features include automating page interactions (clicking, typing, selecting), handling dropdowns, taking screenshots, and extracting element data. Remote debugging is enabled through the Chrome DevTools Protocol, including console log capture and JavaScript evaluation in the page context.","Integration Approaches and Protocols":"The primary integration approach is through the Model Context Protocol, enabling communication with AI models or agents. It uses the Chrome DevTools Protocol for low-level browser control and debugging. Integration with Roo Code suggests a plugin-based model, allowing other tools to utilize the Chrome debugging server functionality via the MCP interface.","Technical Stack and Dependencies":"The technical stack is based on Node.js, utilizing npm for package management. Key dependencies include Puppeteer for Chrome automation, chrome-remote-interface for CDP interaction, and @modelcontextprotocol/sdk for MCP server implementation. Visual Studio Code and Roo Code Extension are used for development and integration purposes.","Best Practices and Coding Standards":"The inclusion of a contributing guide and a license file suggests adherence to open-source best practices. Use of a build process (`npm run build`) indicates a structured approach to development. The modular design and clear separation of concerns promote maintainability and testability. The example usage snippets in Javascript highlight clarity and standardization.","Performance Considerations":"Performance considerations include optimizing Chrome launch times and page interaction speeds. Asynchronous operations using Puppeteer and CDP minimize blocking. The ability to configure viewport size and disable automation controlled flags may assist in controlling resource usage. The server exposes performance profiling through remote debugging, allowing analysis of page load times and script execution."}}', '2025-03-10 21:28:34+00', 'true', 'Servers', 'chrome-debug-mcp'), ('943497040', 'mcp-vs-function-calling', 'andrelandgraf/mcp-vs-function-calling', 'https://github.com/andrelandgraf/mcp-vs-function-calling', 'Explaining MCP by comparing it to function calling', '2025-03-05 19:57:38+00', '2025-03-09 22:44:25+00', 'TypeScript', '2', '# Function Calling vs MCP Server

This repository is meant to illustrate the difference between LLM function calling and the Model Context Protocol (MCP).
Function calling has been around for a while, while MCP is a newer standardization attempt.
Comparing the two approaches showcases the value of MCP and how it builds on top of function calling.

This repository contains two examples:

- `/func-calling`: CLI app using OpenAI's function calling to control Home Assistant lights
- `/mcp-server`: Node.js MCP server exposing a `control_lights` function to LLMs that use the MCP protocol

Want to see it in action? Check out my walkthrough on YouTube: [MCP vs. Function Calling - Controlling my office lights with Cursor](https://www.youtube.com/watch?v=DCp3SkPPq2A)

## Home Assistant

[Home Assistant](https://www.home-assistant.io/) is an open-source home automation platform. I run it on a Raspberry Pi in my home.
Home Assistant controls my lights, and you can control it via the Home Assistant WebSocket API.

I built out the `./data-manager` and `./hass-ws-client` utils while playing around with Home Assistant a while ago. I thought it would be a fun example for an external tool. However, the Home Assistant code isn't the focus of this repository.

## Function Calling

[OpenAI function calling docs](https://platform.openai.com/docs/guides/function-calling)

Function calling lets AI assistants invoke predefined functions or tools. These functions run directly in the assistant's environment and can do anything from file searches to API calls. The LLM receives function descriptions in JSON format and specifies which function to call with what arguments. The application then handles the execution.

-> Functions live in your LLM application code.

## MCP Server

[MCP docs](https://modelcontextprotocol.io/introduction)

MCP servers bridge AI applications with third-party services. They expose functions through a standardized protocol that any MCP-compatible LLM can use. While function calling happens locally, MCP servers handle external service communication, auth, and command execution separately.

-> MCP servers are standalone apps any MCP-compatible LLM can use.

### Setting up the MCP server

1. Create a `.env` file in the `mcp-server` directory:

```bash
cp mcp-server/.env.example mcp-server/.env
```

2. Add your Home Assistant API token to the `.env` file:

```bash
HOME_ASSISTANT_API_TOKEN=<your-home-assistant-api-token>
```

3. Build the MCP server:

```bash
bun i
bun run build
```

4. Add the MCP server to your LLM app config (e.g., Cursor):

```json
{
  "name": "home-assistant",
  "command": "node /Users/andrelandgraf/workspaces/mcps/mcp-server/dist/index.js"
}
```

That's it! Your LLM app can now control Home Assistant lights through the MCP server.
', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository directly compares and contrasts function calling with the Model Context Protocol (MCP), providing an MCP server example for controlling Home Assistant lights, and details setup instructions.","categories":["Function Calling","Protocol Implementations","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The repository contrasts two distinct architectures: function calling, where functions reside within the LLM application, and MCP, where an external server exposes functions via a standardized protocol. The MCP server is designed as a standalone application, decoupling the LLM from the specific service it interacts with (Home Assistant).","Implementation Details and Key Features":"The repository provides example implementations for both function calling (using OpenAI's function calling) and an MCP server (using Node.js). Key features include the ability to control Home Assistant lights via both approaches, showcasing the differences in setup and execution flow. The MCP server implementation includes build steps and configuration instructions.","Integration Approaches and Protocols":"The repository demonstrates integration using OpenAI's function calling mechanism and the Model Context Protocol. The MCP server exposes a `control_lights` function accessible to MCP-compatible LLMs. The function calling example uses JSON to describe function parameters, while the MCP server follows the MCP specification for communication.","Technical Stack and Dependencies":"The function calling example implicitly depends on the OpenAI API. The MCP server implementation uses Node.js. The Home Assistant interaction relies on the Home Assistant WebSocket API. Bun is used as a package manager and build tool for the MCP server. The `.env` file stores the Home Assistant API token.","Best Practices and Coding Standards":"The repository provides a clear example of how to separate concerns by using an external MCP server for service interaction. The documentation includes step-by-step instructions for setting up and configuring the MCP server. It utilizes environment variables for sensitive information like the Home Assistant API token, which is a security best practice.","Performance Considerations":"The repository doesn't explicitly discuss performance, but the comparison highlights the potential for MCP to offer performance benefits by offloading service communication to a separate server. Function calling may have performance implications depending on the complexity of the functions executed within the LLM application."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'mcp-function-calling'), ('943509236', 'market-fiyatlari-mcp-server', 'EnesCinr/market-fiyatlari-mcp-server', 'https://github.com/EnesCinr/market-fiyatlari-mcp-server', null, '2025-03-05 20:27:26+00', '2025-03-10 12:56:06+00', 'JavaScript', '3', '# Market Fiyatı MCP Server

Bu proje, Market Fiyatı API'si ile entegre olan bir MCP (Model Context Protocol) server sunmaktadır. Claude ve diğer MCP destekli AI sistemlerinin Türkiye'deki market fiyatlarını sorgulamasına olanak tanır.

## Özellikler

- **Ürün Arama**: İsim veya anahtar kelime ile ürün arama
- **Ürün Detayları**: ID veya barkod ile ürün bilgilerini görüntüleme
- **Fiyat Karşılaştırma**: Farklı marketlerdeki fiyatları karşılaştırma
- **Kaynak Erişimi**: URI tabanlı kaynak erişimi (search/ ve product/ endpoint'leri)
- **MCP Araçları**: AI'ın doğrudan kullanabileceği veri analiz araçları

## Kurulum

```bash
# Bağımlılıkları yükle
npm install

# TypeScript dosyalarını derle
npm run build

# Sunucuyu başlat
npm start
```

## Claude Desktop ile Kullanım

Claude Desktop'un `claude_desktop_config.json` dosyasına aşağıdaki yapılandırmayı ekleyin:

```json
{
  "mcpServers": {
    "marketfiyati": {
      "command": "npx",
      "args": ["-y @enescinar/market-fiyati-mcp"]
    }
  }
}
```

## API Kaynakları

Bu MCP server, Market Fiyatı API'sinin aşağıdaki endpointlerini kullanır:

- `/search`: Ürün araması yapar
- `/searchByIdentity`: ID veya barkod ile ürün getirir

## MCP Araçları

Sunucu aşağıdaki MCP araçlarını sağlar:

- `search_products`: Ürün araması yapar
- `get_product_by_id`: ID ile ürün getirir
- `get_product_by_barcode`: Barkod ile ürün getirir
- `compare_prices`: Fiyat karşılaştırması yapar

## MCP Kaynakları

Sunucu, üzerinden sorgulanabilecek şu MCP kaynaklarını sağlar:

- `market-fiyati://search/{query}`: Ürün araması
- `market-fiyati://product/{id}`: Ürün detayları

## Lisans

MIT
', 'true', '"{\"Function Calling\",\"API Frameworks\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository explicitly identifies itself as a Model Context Protocol (MCP) server designed to integrate with AI systems like Claude for accessing market prices in Turkey.","categories":["Function Calling","API Frameworks","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The architecture revolves around providing an interface between an external Market Fiyatı API and MCP-compatible AI models. The server acts as a bridge, translating MCP requests into API calls and formatting the responses for AI consumption. It uses a URI-based resource access model, enabling structured data retrieval.","Implementation Details and Key Features":"The server implements product search, detailed product information retrieval, and price comparison features. Key features include URI-based resource access using `/search` and `/searchByIdentity` endpoints, and MCP tools like `search_products`, `get_product_by_id`, and `compare_prices`. TypeScript is used, with build and start scripts for deployment.","Integration Approaches and Protocols":"The primary integration approach involves the Model Context Protocol, allowing AI models like Claude to directly query and utilize the server's functionalities.  The `claude_desktop_config.json` configuration example demonstrates how to configure Claude Desktop to use this MCP server. API requests are made to the Market Fiyatı API based on MCP queries.","Technical Stack and Dependencies":"The technical stack includes Node.js and TypeScript for the server implementation.  `npm` is used as the package manager. The server depends on the Market Fiyatı API for accessing product and price data. The configuration suggests a dependency on `@enescinar/market-fiyati-mcp`, for potentially client-side functions.","Best Practices and Coding Standards":"The provided documentation suggests the use of standard Node.js development practices, including installing dependencies with `npm install` and building TypeScript files with `npm run build`.  The presence of a `package.json` file implies adherence to common Node.js project structure conventions. The MIT license encourages open-source collaboration.","Performance Considerations":"Performance considerations likely involve optimizing API call efficiency to the Market Fiyatı API and minimizing latency in responding to MCP requests. Caching strategies for frequently accessed product data could improve response times. The server's scalability would depend on the underlying Market Fiyatı API's capacity and the server's resource allocation."}}', '2025-03-10 21:54:04+00', 'true', 'Servers', 'fiyatlari-mcp-server'), ('943517086', 'livebook_tools', 'thmsmlr/livebook_tools', 'https://github.com/thmsmlr/livebook_tools', 'Powertools for livebook.dev — AI Code Editing, MCP Servers, and Running Livebooks from the CLI', '2025-03-05 20:47:15+00', '2025-03-10 17:50:35+00', 'Elixir', '46', '# LivebookTools

Livebook Tools is a CLI tool to give you superpowers while working with `.livemd` files.  
Its primary features are:  

  - **BYOE (Bring Your Own Editor)** - Sync your `.livemd` files to an open Livebook session so you can edit them in your AI powered code editor, like Cursor.
  - **MCP Server** - A simple model context protocol server for connecting AI coding agents to your Livebook sessions. More on this below.
  - **Run Livebooks from the CLI** - Convert your `.livemd` files to Elixir scripts and run them top to bottom as if they were a `.exs` script. Useful if you want to turn a livebook into a cron job.

## Installation

Livebook Tools is an Elixir escript.  
To install it, you can use the `mix escript.install` command.

```bash
mix escript.install github thmsmlr/livebook_tools
```

Once installed, ensure that the escript directory is on your path.  

```bash
# for normal elixir users
export PATH="$HOME/.mix/escripts:$PATH"

# for asdf elixir users
for escripts_dir in $(find "${ASDF_DATA_DIR:-$HOME/.asdf}/installs/elixir" -type d -name "escripts" 2>/dev/null); do
  export PATH="$escripts_dir:$PATH"
done
```

## Running Livebook

In order for Livebook tools to work properly, it needs to be able to connect to a running Livebook server using distributed Elixir.  
To do this will depend on how you are running Livebook.  
In its simplest form, all you need to do is add the two environment variables to your `~/.bashrc` or `~/.zshrc` file.

```bash
export LIVEBOOK_NODE="livebook@127.0.0.1"
export LIVEBOOK_COOKIE="secret"
```

Then when you run the Livebook tools or Livebook, both programs will discover these values and make sure that they can connect to each other.
If you're running using Livebook Desktop, then you may need to add these values to the `~/.livebookdesktop.sh` file as well.
For more information on Livebook Desktop, check out the [Livebook HexDocs](https://hexdocs.pm/livebook/readme.html#livebook-desktop).  


## Setting up MCP Server

The MCP Server is a simple [model context protocol](https://modelcontextprotocol.io/introduction) server that allows you to connect AI coding agents to your Livebook sessions.  
I have personally tested the implementation with [Cursor](https://www.cursor.com/), though it should work with any AI code editor that supports the MCP protocol.
The MCP server runs via STDIO, so all you have to do is tell Cursor to connect via the command and it'll auto discover the tools and connect them to it's coding agent.
For more information on MCP works with Cursor, check out the [Cursor MCP docs](https://docs.cursor.com/context/model-context-protocol).

![Cursor Create MCP Server](./assets/cursor-create-mcp-server.png)
![Cursor MCP Server Connected](./assets/cursor-mcp-server.png)

', 'true', '"{\"Protocol Implementations\",\"Agent Frameworks\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it implements a Model Context Protocol (MCP) server for connecting AI coding agents to Livebook sessions, aligning with the MCP specification's purpose.","categories":["Protocol Implementations","Agent Frameworks","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The architecture revolves around providing tools for Livebook, including an MCP server. It utilizes a CLI tool built as an Elixir escript. The design promotes integration with AI code editors like Cursor by enabling communication with Livebook sessions. The project uses a modular approach, separating concerns into distinct functionalities.","Implementation Details and Key Features":"Key features include synchronizing `.livemd` files for editing in AI-powered editors, an MCP server facilitating AI agent connections, and CLI execution of Livebook files. The MCP server operates via STDIO for easy integration. The implementation handles the necessary Elixir environment configurations, which are necessary for distributed Elixir.","Integration Approaches and Protocols":"The repository integrates with Livebook using distributed Elixir and the Model Context Protocol (MCP). It supports connecting AI code editors, specifically Cursor. Environment variables like `LIVEBOOK_NODE` and `LIVEBOOK_COOKIE` are employed for establishing connectivity with a running Livebook server. Integration leverages STDIO for MCP server communication.","Technical Stack and Dependencies":"The primary technical stack includes Elixir, as it is built as an Elixir escript. The code depends on Livebook for its core functionality. It relies on the Model Context Protocol for AI agent interaction. No external database systems or specific cloud providers appear to be dependencies based on the provided information.","Best Practices and Coding Standards":"The documentation includes setup instructions, demonstrating user-friendliness. It provides clear examples for configuration, like setting environment variables. Instructions for installing the escript are detailed. The presence of an MCP server aligns with modern trends in AI development by promoting standardized communication.","Performance Considerations":"The MCP server uses STDIO, and therefore the tool should not require significant computational resources. Converting Livebooks to Elixir scripts for CLI execution can improve performance in specific use cases like cron jobs. The overall performance is likely to depend more on the AI agent or editor connected through the MCP server."}}', '2025-03-10 21:28:52+00', 'true', 'Servers', 'livebook-mcp-tools'), ('943522632', 'mcp-linkedin-automtion', 'OneInterface/mcp-linkedin-automtion', 'https://github.com/OneInterface/mcp-linkedin-automtion', null, '2025-03-05 21:01:19+00', '2025-03-10 09:13:39+00', null, '66', '# mcp-linkedin-automtion
Alright, I'm recording this tutorial with the voice mode because I need to go to sleep in ten minutes. This is a temporarily thing...

# How does it work?
So, the first part basically involves connecting to a specific LinkedIn scraper, connecting to the Grok API, and connecting to the LinkedIn API. The LinkedIn API is another service I lost link though find later because I'm currently not near my computer.

What happens is that on request, basically just runs the pipeline altogether or to all of those steps, including the Apollo IO. The Apollo IO is basically the email scraper tool, which based on the LinkedIn input, it outputs the email. So it does this and basically it sends the LinkedIn invitation using the third party service. I'll find the link later. And it does the Apollo email stuff. It uses this scraping tool to find the information about the person and uses my context of my company and what am I selling to personalize the message.

So if you're a company you can just say that "okay I need to sell this and this and that" if the person is from the US, tell them that we're working with a US market and to personalize the message based on the region or based on the experience or that maybe you have shared profile connections or something. You can all do this and this runs on the MCP (Model Context Protocol) - the main service which connects all those things together like glue.
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"System Automation\"]}', '{"is_mcp":"YES","justification":"The documentation explicitly states that the tool \"runs on the MCP (Model Context Protocol) - the main service which connects all those things together like glue.\"","categories":["Browser & Web Automation","Workflow Orchestration","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The architecture appears to involve connecting to several external APIs and services, including a LinkedIn scraper, Grok API, LinkedIn API, and Apollo IO. It uses a pipeline approach where data flows from one service to another. The repository connects to and automates interactions across multiple platforms.","Implementation Details and Key Features":"Key features include scraping LinkedIn profiles, finding email addresses using Apollo IO, and sending personalized LinkedIn invitations. The implementation relies on third-party services and APIs for data extraction and communication. The core logic involves customizing messages based on the user's company context and the recipient's profile information.","Integration Approaches and Protocols":"Integration relies on connecting to various APIs and external services. Specifically, it integrates with the Grok API, LinkedIn API, and Apollo IO. The workflow orchestrates data flow and actions across these services. The repository uses MCP to manage the context between these integrations.","Technical Stack and Dependencies":"The technical stack includes dependencies on external APIs for LinkedIn scraping, email finding, and invitation sending. While the specific language and libraries aren't explicitly stated, it likely involves a scripting language like Python due to its ease of use with web scraping and API integrations.","Best Practices and Coding Standards":"The provided documentation does not highlight coding standards or best practices. The description focuses on functionality rather than code quality. Without further details, it's impossible to determine adherence to specific coding guidelines.","Performance Considerations":"Performance likely depends on the efficiency of the scraping and API interaction. Rate limiting and API usage quotas will affect the tool's ability to process profiles in bulk. The overall performance depends on the responsiveness of the integrated APIs."}}', '2025-03-10 21:28:52+00', 'true', 'Servers', 'mcp-linkedin-automation'), ('943539775', 'base-mcp-alpha', 'dschlabach/base-mcp-alpha', 'https://github.com/dschlabach/base-mcp-alpha', null, '2025-03-05 21:46:34+00', '2025-03-06 23:51:02+00', 'TypeScript', '5', '**NOTE**: This repo has moved to [base/base-mcp](https://github.com/base/base-mcp)


# Base MCP Server

[![npm version](https://img.shields.io/npm/v/base-mcp.svg)](https://www.npmjs.com/package/base-mcp)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A Model Context Protocol (MCP) server that provides onchain tools for Claude AI, allowing it to interact with the Base blockchain and Coinbase API.

## Overview

This MCP server extends Claude's capabilities by providing tools to:

- Retrieve wallet addresses
- Get testnet ETH (on Base Sepolia)
- List wallet balances
- Transfer funds between wallets
- Deploy smart contracts

The server uses the Coinbase SDK to interact with the Base blockchain and Coinbase services.

## Prerequisites

- Node.js (v16 or higher)
- npm or yarn
- Coinbase API credentials (API Key Name and Private Key)
- A wallet seed phrase

## Installation

### Option 1: Install from npm (Recommended)

```bash
# Install globally
npm install -g base-mcp

# Or install locally in your project
npm install base-mcp
```

### Option 2: Install from Source

1. Clone this repository:

   ```bash
   git clone https://github.com/dschlabach/base-mcp-alpha.git
   cd base-mcp-alpha
   ```

2. Install dependencies:

   ```bash
   npm install
   ```

3. Build the project:

   ```bash
   npm run build
   ```

4. Optionally, link it globally:
   ```bash
   npm link
   ```

## Configuration

Create a `.env` file with your credentials:

```
# Coinbase API credentials
# You can obtain these from the Coinbase Developer Portal: https://cdp.coinbase.com/
COINBASE_API_KEY_NAME=your_api_key_name
COINBASE_API_PRIVATE_KEY=your_private_key

# Wallet seed phrase (12 or 24 words)
# This is the mnemonic phrase for your wallet
SEED_PHRASE=your seed phrase here
```

## Testing

Test the MCP server to verify it's working correctly:

```bash
npm test
```

This script will verify that your MCP server is working correctly by testing the connection and available tools.

## Examples

See the [examples.md](examples.md) file for detailed examples of how to interact with the Base MCP tools through Claude.

## Integration with Claude Desktop

To add this MCP server to Claude Desktop:

1. Create or edit the Claude Desktop configuration file at:

   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`
   - Windows: `%APPDATA%\Claude\claude_desktop_config.json`
   - Linux: `~/.config/Claude/claude_desktop_config.json`

2. Add the following configuration:

   ```json
   {
     "mcpServers": {
       "base-mcp": {
         "command": "node",
         "args": ["/path/to/base-mcp/build/index.js"],
         "env": {
           "COINBASE_API_KEY_NAME": "your_api_key_name",
           "COINBASE_API_PRIVATE_KEY": "your_private_key",
           "SEED_PHRASE": "your seed phrase here"
         },
         "disabled": false,
         "autoApprove": []
       }
     }
   }
   ```

3. Restart Claude Desktop for the changes to take effect.

## Available Tools

### get-address

Retrieves the address for your wallet.

Example query to Claude:

> "What's my wallet address?"

### get-testnet-eth

Gets testnet ETH for your wallet. This can only be called on the Base Sepolia network.

Example query to Claude:

> "Can you get me some testnet ETH for my wallet?"

### list-balances

Lists all balances for your wallet.

Example query to Claude:

> "Show me my wallet balances."

### transfer-funds

Transfers funds from your wallet to another address.

Parameters:

- `destination`: The address to which to transfer funds
- `assetId`: The asset ID to transfer
- `amount`: The amount of funds to transfer

Example query to Claude:

> "Transfer 0.01 ETH to 0x1234567890abcdef1234567890abcdef12345678."

### deploy-contract

Deploys a smart contract to the blockchain.

Parameters:

- `constructorArgs`: The arguments for the contract constructor
- `contractName`: The name of the contract to deploy
- `solidityInputJson`: The JSON input for the Solidity compiler containing contract source and settings
- `solidityVersion`: The version of the solidity compiler

Example query to Claude:

> "Deploy a simple ERC20 token contract for me."

## Security Considerations

- The configuration file contains sensitive information (API keys and seed phrases). Ensure it's properly secured and not shared.
- Consider using environment variables or a secure credential manager instead of hardcoding sensitive information.
- Be cautious when transferring funds or deploying contracts, as these operations are irreversible on the blockchain.

## Troubleshooting

If you encounter issues:

1. Check that your Coinbase API credentials are correct
2. Verify that your seed phrase is valid
3. Ensure you're on the correct network (Base Sepolia for testnet operations)
4. Check the Claude Desktop logs for any error messages

## License

[MIT License](LICENSE)

## Making Your MCP Discoverable

To make your MCP server discoverable by other developers, follow these steps:

### 1. Publish to npm

```bash
# Login to npm (you'll need an npm account)
npm login

# Publish the package
npm publish
```

This will make your MCP server available on the npm registry, allowing other developers to install it using `npm install base-mcp`.

### 2. Share Your GitHub Repository

Make sure your GitHub repository is public and well-documented. Add the following to enhance discoverability:

- A detailed README (like this one)
- Examples of usage
- Contributing guidelines
- Issue templates

### 3. Add to the MCP Directory

The Model Context Protocol community maintains a directory of available MCP servers. Submit your MCP server to be included in this directory by running:

```bash
npm run submit
```

This script will:

1. Check if your package is published to npm
2. Verify your GitHub repository information
3. Generate a submission file with all the necessary information
4. Guide you through the submission process

Alternatively, you can manually submit by:

1. Visiting the [MCP Directory Repository](https://github.com/modelcontextprotocol/directory)
2. Following the contribution guidelines to add your MCP server

### 4. Promote in Relevant Communities

Share your MCP server in communities where developers using Claude might be active:

- Anthropic Developer Discord
- Claude subreddit
- AI/ML developer forums
- Blockchain and Web3 communities

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

Please make sure your code follows the existing style and includes appropriate tests.
', 'true', '"{\"Protocol Implementations\",\"Agent Frameworks\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements a Model Context Protocol (MCP) server. It provides on-chain tools for Claude AI to interact with the Base blockchain, aligning with the MCP's goal of enabling AI models to interact with external contexts.","categories":["Protocol Implementations","AI Model Integration","Function Calling"],"analysis":{"Architecture and Design Patterns":"The server architecture is designed to bridge Claude AI with the Base blockchain. It employs a modular approach where specific tools are implemented as separate functions accessible via the MCP. The design emphasizes ease of integration with Claude Desktop through configuration files, demonstrating a clear separation of concerns between the AI model and the external context.","Implementation Details and Key Features":"The server provides tools like `get-address`, `get-testnet-eth`, `list-balances`, `transfer-funds`, and `deploy-contract`. These tools are implemented using the Coinbase SDK to interact with the Base blockchain. Key features include wallet address retrieval, testnet ETH acquisition, balance listing, fund transfers, and smart contract deployment. The server uses environment variables for configuration.","Integration Approaches and Protocols":"Integration with Claude Desktop is achieved by configuring the `claude_desktop_config.json` file to point to the server's execution path and environment variables. This allows Claude to call the server's tools as functions. The MCP protocol is facilitated through command-line execution of the server's `index.js` file, enabling Claude to invoke specific tools and receive responses.","Technical Stack and Dependencies":"The technical stack includes Node.js (v16 or higher), npm, and the Coinbase SDK. Dependencies are managed via `package.json` and installed using `npm install`. The server requires a `.env` file for sensitive credentials like Coinbase API keys and wallet seed phrases. It uses JavaScript for implementation and includes build scripts managed through `npm run build`.","Best Practices and Coding Standards":"The repository demonstrates best practices by using environment variables for sensitive information, providing clear installation and configuration instructions, and including a testing script (`npm test`). It also has a license (MIT) and encourages contributions. However, the security considerations section suggests further improvements in secure credential management.","Performance Considerations":"Performance considerations are not explicitly addressed in the documentation. However, interacting with the blockchain inherently involves latency. Optimizations might include caching wallet balances, optimizing smart contract deployment processes, and using efficient data structures for handling blockchain data. Load testing and monitoring would be needed for production environments."}}', '2025-03-10 21:28:34+00', 'true', 'Servers', 'base-mcp-alpha-server'), ('943567935', 'imessage-mcp', 'willccbb/imessage-mcp', 'https://github.com/willccbb/imessage-mcp', 'iMessage RAG MCP Server from Anthropic MCP Hackathon (NYC)', '2025-03-05 23:16:09+00', '2025-03-10 15:42:36+00', 'Python', '6', '# Local iMessage RAG MCP Server

From the Anthropic NYC MCP hackathon in December 2024. Co-developed with [@youssefish](https://x.com/youssefish).

Probably a bit buggy. Setup instructions in the folders READMEs. Not planning to actively maintain, it kinda worked last time I tried it. Good luck. Better docs at some point maybe.
', 'true', '"{\"RAG Systems\",\"Context Management\",\"Communication Platforms\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly mentions 'iMessage RAG MCP Server' and participation in the Anthropic MCP Hackathon, indicating its relation to Model Context Protocol specifications.","categories":["RAG Systems","Function Calling","Context Management"],"analysis":{"Architecture and Design Patterns":"The repository likely implements a server-client architecture where a RAG system interacts with iMessage. The design probably involves handling iMessage data, processing it, and generating responses based on the context. It leverages function calling to invoke functionalities of the iMessage system based on user queries.","Implementation Details and Key Features":"The repository implements a RAG system tailored to the iMessage platform. Key features likely include parsing iMessage data, querying a knowledge base, and generating appropriate responses within the iMessage context. It might include tools for managing conversation history and personalizing responses.","Integration Approaches and Protocols":"The project integrates with the iMessage platform using a specific integration method not clearly defined. It communicates with the RAG system, which might use a standard API. MCP would be the main integration protocol to streamline the data flow, context management and function calling between the RAG agent and the iMessage application.","Technical Stack and Dependencies":"Based on the nature of the project, the tech stack might include Python for server-side logic, potentially using frameworks like Flask or FastAPI to expose endpoints. It likely uses libraries for natural language processing (NLP) and knowledge base querying.  The dependencies for iMessage integration are unknown from this description.","Best Practices and Coding Standards":"The repository is described as 'probably a bit buggy' and 'not planning to actively maintain,' which suggests that best practices and coding standards may not be strictly adhered to. However, the intention during development was likely to create a functional prototype. Further analysis of code is necessary to confirm.","Performance Considerations":"Performance considerations likely include the latency of querying the knowledge base and generating responses. Scalability might be limited due to the single-server setup. Optimizations could involve caching mechanisms and efficient query strategies to minimize response times. Data volume of iMessages may be a limiting factor."}}', '2025-03-10 21:54:04+00', 'true', 'Servers', 'imessage-rag-mcp'), ('943579684', 'mermaid-mcp-server', 'peng-shawn/mermaid-mcp-server', 'https://github.com/peng-shawn/mermaid-mcp-server', 'A Model Context Protocol (MCP) server that converts Mermaid diagrams to PNG images', '2025-03-05 23:57:03+00', '2025-03-08 16:37:02+00', 'JavaScript', '10', '# Mermaid MCP Server

[![smithery badge](https://smithery.ai/badge/@peng-shawn/mermaid-mcp-server)](https://smithery.ai/server/@peng-shawn/mermaid-mcp-server)

A Model Context Protocol (MCP) server that converts Mermaid diagrams to PNG images. This server allows AI assistants and other applications to generate visual diagrams from textual descriptions using the Mermaid markdown syntax.

## Features

- Converts Mermaid diagram code to PNG images
- Supports multiple diagram themes (default, forest, dark, neutral)
- Customizable background colors
- Uses Puppeteer for high-quality headless browser rendering
- Implements the MCP protocol for seamless integration with AI assistants
- Flexible output options: return images directly or save to disk
- Error handling with detailed error messages

## How It Works

The server uses Puppeteer to launch a headless browser, render the Mermaid diagram to SVG, and capture a screenshot of the rendered diagram. The process involves:

1. Launching a headless browser instance
2. Creating an HTML template with the Mermaid code
3. Loading the Mermaid.js library
4. Rendering the diagram to SVG
5. Taking a screenshot of the rendered SVG as PNG
6. Either returning the image directly or saving it to disk

## Build

```bash
npx tsc
```

## Usage

### Installing via Smithery

To install Mermaid Diagram Generator for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@peng-shawn/mermaid-mcp-server):

```bash
npx -y @smithery/cli install @peng-shawn/mermaid-mcp-server --client claude
```

### Use with Claude desktop

```json
"mcpServers": {
  "mermaid": {
    "command": "npx",
    "args": [
      npx @peng-shawn/mermaid-mcp-server@0.1.2
    ]
  }
}
```

### Use with Cursor and Cline

```bash
env CONTENT_IMAGE_SUPPORTED=false npx @peng-shawn/mermaid-mcp-server@0.1.2
```

You can find a list of mermaid diagrams under `./diagrams`, they are created using Cursor agent with prompt: "generate mermaid diagrams and save them in a separate diagrams folder explaining how renderMermaidPng work"

### Run with inspector

Run the server with inspector for testing and debugging:

```bash
npx @modelcontextprotocol/inspector node dist/index.js
```

The server will start and listen on stdio for MCP protocol messages.

Learn more about inspector [here](https://modelcontextprotocol.io/docs/tools/inspector).

## API

The server exposes a single tool:

- `generate`: Converts Mermaid diagram code to a PNG image
  - Parameters:
    - `code`: The Mermaid diagram code to render
    - `theme`: (optional) Theme for the diagram. Options: "default", "forest", "dark", "neutral"
    - `backgroundColor`: (optional) Background color for the diagram, e.g. 'white', 'transparent', '#F0F0F0'
    - `name`: Name for the generated file (required when CONTENT_IMAGE_SUPPORTED=false)
    - `folder`: Absolute path to save the image to (required when CONTENT_IMAGE_SUPPORTED=false)

The behavior of the `generate` tool depends on the `CONTENT_IMAGE_SUPPORTED` environment variable:

- When `CONTENT_IMAGE_SUPPORTED=true` (default): The tool returns the image directly in the response
- When `CONTENT_IMAGE_SUPPORTED=false`: The tool saves the image to the specified folder and returns the file path

## Environment Variables

- `CONTENT_IMAGE_SUPPORTED`: Controls whether images are returned directly in the response or saved to disk
  - `true` (default): Images are returned directly in the response
  - `false`: Images are saved to disk, requiring `name` and `folder` parameters

## Examples

### Basic Usage

```javascript
// Generate a flowchart with default settings
{
  "code": "flowchart TD\n    A[Start] --> B{Is it?}\n    B -->|Yes| C[OK]\n    B -->|No| D[End]"
}
```

### With Theme and Background Color

```javascript
// Generate a sequence diagram with forest theme and light gray background
{
  "code": "sequenceDiagram\n    Alice->>John: Hello John, how are you?\n    John-->>Alice: Great!",
  "theme": "forest",
  "backgroundColor": "#F0F0F0"
}
```

### Saving to Disk (when CONTENT_IMAGE_SUPPORTED=false)

```javascript
// Generate a class diagram and save it to disk
{
  "code": "classDiagram\n    Class01 <|-- AveryLongClass\n    Class03 *-- Class04\n    Class05 o-- Class06",
  "theme": "dark",
  "name": "class_diagram",
  "folder": "/path/to/diagrams"
}
```

## FAQ

### Doesn't Claude desktop already support mermaid via canvas?

Yes, but it doesn't support the `theme` and `backgroundColor` options. Plus, having a dedicated server makes it easier to create mermaid diagrams with different MCP clients.

### Why do I need to specify CONTENT_IMAGE_SUPPORTED=false when using with Cursor?

Cursor doesn't support inline images in responses yet.

## License

MIT
', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states it's a Model Context Protocol (MCP) server. It's designed to allow AI assistants to generate visual diagrams from textual descriptions and integrates seamlessly with AI assistants via MCP.","categories":["Function Calling","Protocol Implementations","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The server architecture revolves around a headless browser (Puppeteer) which renders Mermaid diagrams from code provided via MCP. The server exposes a 'generate' tool that accepts diagram code and rendering options. It supports both returning images directly and saving them to disk, determined by the `CONTENT_IMAGE_SUPPORTED` variable. This design enables integration with various MCP clients with differing image support.","Implementation Details and Key Features":"The core functionality involves converting Mermaid diagram code to PNG images. Key features include supporting multiple diagram themes and customizable background colors. It utilizes Puppeteer for high-quality rendering and provides flexible output options. The server handles errors and provides detailed error messages. The 'generate' tool accepts 'code', 'theme', and 'backgroundColor' parameters for customization.","Integration Approaches and Protocols":"Integration is primarily achieved through the Model Context Protocol (MCP). The server is designed to receive diagram code via MCP, render the diagram, and return the resulting image or file path. It can be integrated with tools like Claude desktop, Cursor, and Cline via MCP. The server is configured to work with Smithery, which facilitates the installation of the Mermaid Diagram Generator.","Technical Stack and Dependencies":"The primary technical stack includes TypeScript for the server-side logic, Puppeteer for headless browser rendering, and Mermaid.js for diagram generation. The server uses Node.js as the runtime environment. Dependencies include `@smithery/cli` for Smithery integration and `@modelcontextprotocol/inspector` for testing and debugging. Typescript compiler is required to build the project.","Best Practices and Coding Standards":"The repository provides clear build and usage instructions, including examples. Error handling is implemented with detailed error messages. The code is structured around the 'generate' tool, providing a clear API for diagram generation. There's documentation covering environment variables and their effects on behavior. MIT License.","Performance Considerations":"The use of Puppeteer implies that rendering performance will depend on the complexity of the Mermaid diagrams and the available system resources. The server's ability to save images to disk instead of returning them directly might be beneficial for larger diagrams or environments with limited bandwidth. Optimization of Mermaid diagrams themselves could further improve performance."}}', '2025-03-10 21:54:00+00', 'true', 'Servers', 'mermaid-mcp-server'), ('943583886', 'mcp-rust-docs', '0xKoda/mcp-rust-docs', 'https://github.com/0xKoda/mcp-rust-docs', 'An MCP to retrieve rust crate documentation for LLM's', '2025-03-06 00:12:32+00', '2025-03-08 23:11:42+00', 'JavaScript', '16', '# MCP Rust Documentation Server

This is a Model Context Protocol (MCP) server that fetches and returns documentation for Rust crates providing essential context for LLM's when working with Rust code.

## Features

- Fetches documentation for any Rust crate available on docs.rs
- Strips HTML and formats the content for readability
- Limits response size to prevent overwhelming the client
- Uses the latest MCP SDK (v1.6.1)

## Installation

```bash
# Clone the repository
git https://github.com/0xKoda/mcp-rust-docs.git
cd mcp-rust-docs

# Install dependencies
npm install
```

### Prerequisites

- Node.js 
- npm 

## Usage

```bash
# Start the server directly
npm start
```

## Integrating with AI Assistants

### Claude Desktop

Add the following to your Claude Desktop configuration file (`claude_desktop_config.json`):

```json
{
  "mcpServers": {
    "rust-docs": {
      "command": "node",
      "args": ["/absolute/path/to/index.js"]
    }
  }
}
```

Replace `/absolute/path/to/index.js` with the absolute path to the index.js file in this repository.

## Example Usage

Once the server is running and configured with your AI assistant, you can ask questions like:

- "Look up the documentation for the 'tokio' crate"
- "What features does the 'serde' crate provide?"
- "Show me the documentation for 'ratatui'"
- "Can you explain the main modules in the 'axum' crate?"

The AI will use the `lookup_crate_docs` tool to fetch and display the relevant documentation.

## Testing with MCP Inspector

You can test this server using the MCP Inspector:

```bash
npx @modelcontextprotocol/inspector
```

Then select the "Connect to a local server" option and follow the prompts.

## How It Works

This server implements a single MCP tool called `lookup_crate_docs` that:

1. Takes a Rust crate name as input (optional, defaults to 'tokio' if not provided)
2. Fetches the documentation from docs.rs
3. Converts the HTML to plain text using the html-to-text library
4. Truncates the content if it exceeds 8000 characters
5. Returns the formatted documentation in the proper MCP response format

## SDK Implementation Notes

This server uses the MCP SDK with carefully structured import paths. If you're modifying the code, be aware that:

1. The SDK requires importing from specific paths (e.g., `@modelcontextprotocol/sdk/server/mcp.js`)
2. We use the high-level McpServer API rather than the low-level tools
3. The tool definition uses Zod for parameter validation
4. Console output is redirected to stderr to avoid breaking the MCP protocol
5. The tool returns properly formatted MCP response objects

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

MIT', 'true', '"{\"AI Model Integration\",\"Function Calling\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository implements an MCP server for retrieving Rust crate documentation for LLMs. It explicitly states using the MCP SDK and provides instructions for integrating with AI assistants.","categories":["AI Model Integration","Context Management","Function Calling"],"analysis":{"Architecture and Design Patterns":"The repository implements a Model Context Protocol (MCP) server with a single tool, `lookup_crate_docs`. It uses a request-response pattern, where the server receives a crate name, fetches documentation, formats it, and returns it to the client. The server architecture is simple, leveraging existing libraries to handle HTML processing and network communication.","Implementation Details and Key Features":"The key feature is fetching Rust crate documentation from docs.rs and formatting it for LLM consumption. It strips HTML, limits response size, and uses the MCP SDK (v1.6.1). The implementation leverages Node.js, npm, and external libraries for HTML-to-text conversion. It includes example configurations for integration with Claude Desktop.","Integration Approaches and Protocols":"The repository integrates with AI assistants using the MCP protocol. It provides a configuration snippet for Claude Desktop and instructions for testing with the MCP Inspector. The server communicates via standard input/output streams, adhering to the MCP specification for tool execution and data exchange. It also includes a clear explanation of how to format the response data.","Technical Stack and Dependencies":"The repository relies on Node.js and npm as its primary technical stack. Key dependencies include `@modelcontextprotocol/sdk` for MCP implementation and `html-to-text` for documentation formatting. The documentation mentions using Zod for parameter validation and provides instructions for SDK import paths. The runtime environment requires Node.js and npm for installation and execution.","Best Practices and Coding Standards":"The repository demonstrates attention to MCP SDK usage, structuring imports from specific paths as the SDK dictates. It also redirects console output to stderr to avoid breaking the MCP protocol and formats responses according to MCP requirements. The use of Zod for parameter validation demonstrates a commitment to robust data handling.","Performance Considerations":"The repository limits response size to 8000 characters to prevent overwhelming the client, which is a clear performance consideration. The HTML stripping and formatting also contribute to improving the signal to noise ratio for LLMs. There are no specific details about caching or other optimizations for handling frequent requests, which is an area for future improvement."}}', '2025-03-10 21:28:27+00', 'true', 'Servers', 'mcp-rust-crate-docs'), ('943623649', 'mcp-server-template', 'bladnman/mcp-server-template', 'https://github.com/bladnman/mcp-server-template', null, '2025-03-06 02:23:31+00', '2025-03-10 15:44:54+00', 'TypeScript', '0', '# Template for MCP Server

A starting point for an MCP Server.

## DUPLICATE ME

The whole idea here is for you to take this for your own.
Running this to create a new folder with your own copy
of this template (git removed and replaced with a new one).


**replace `mcp-server` with name of your choice:**
```bash
npx make-mine https://github.com/bladnman/mcp-server-template.git <mcp-server>
```

## Features

- Simple string reversal tool
- Support for both stdio and SSE transports
- TypeScript implementation with full build process

## Tools

### reverse

Takes a string input and returns it reversed.

Example:
- Input: "hello world"
- Output: "dlrow olleh"

## Resources

- `greeting://hello`: A welcome greeting
- `welcome://info`: Information about the server and its capabilities

## Getting Started

### Testing with MCP Inspector

There's a way to test the server itself using a generic inspector 
tool which allows you to see the transiting message between the 
server and the client. It is dropping to shell and running this command.

```bash
npx fastmcp inspect server.ts
```

And then going to the url it provides (likely):
http://localhost:5173

and for the current server that we're running here this server we 
will want to execute the run `server.sh` file that you see in the 
root directory as `stdio` with `no arguments`.

```bash
<full_path_to_server>/run-server.sh
```



### Prerequisites

- Node.js (v14 or higher)
- npm

### Installation

```bash
# Install dependencies
npm install
```

### Running the Server

```bash
# Build and run with stdio transport (default)
bash run-server.sh

# Or, to run with SSE transport
TRANSPORT_TYPE=sse npm run start
```

By default, the SSE server runs on port 4000. Change in the `CONST.ts` file.

## Development

```bash
# Run in development mode with hot reloading
npm run dev

# Watch for changes and rebuild automatically
npm run watch
```

## Building

```bash
# Build TypeScript to JavaScript
npm run build
```

The built files will be in the `dist` directory.

## License

ISC License ', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository is a template for an MCP server, featuring tool definition (reverse), resource access (greeting://hello), and mentions of MCP Inspector and stdio/SSE transports, aligning with MCP specifications.","categories":["Protocol Implementations","API Frameworks","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The repository provides a template for building an MCP server. Its architecture is designed to be modular and easily duplicated for creating new MCP servers. It supports different transport types (stdio and SSE) and includes a simple tool for reversing strings. The template promotes a clear separation of concerns for tool and resource definitions.","Implementation Details and Key Features":"The server template is implemented in TypeScript. Key features include support for both stdio and SSE transports, a built-in string reversal tool, and resource access via URIs. The server is designed to be easily testable using the MCP Inspector tool, which allows developers to inspect the messages exchanged between the server and client.","Integration Approaches and Protocols":"The server integrates with MCP clients using either stdio or SSE transport protocols. It defines a standard for exposing tools and resources, allowing clients to interact with them through well-defined interfaces. The use of MCP Inspector suggests an intent for easy integration and testing with other MCP-compliant components.","Technical Stack and Dependencies":"The primary technical stack includes Node.js and TypeScript. The repository relies on npm for dependency management. The build process involves compiling TypeScript to JavaScript. The specific version requirements for Node.js are v14 or higher. It has no explicit dependency on particular cloud or database services.","Best Practices and Coding Standards":"The repository uses TypeScript, encouraging strong typing and modern JavaScript features. The inclusion of build and watch scripts promote efficient development workflows. The modular design promotes code reusability and maintainability. The use of a template-based approach ensures consistency across different MCP server implementations.","Performance Considerations":"The performance characteristics depend on the chosen transport type (stdio or SSE). Stdio might be more efficient for local testing, while SSE allows for asynchronous communication and streaming. The string reversal tool is likely not a performance bottleneck. The server is designed to be scalable by adding more tools and resources, but the impact on overall performance is undefined."}}', '2025-03-10 21:26:24+00', 'true', 'Servers', 'mcp-server-template'), ('943636367', 'nuanced-mcp', 'MattMorgis/nuanced-mcp', 'https://github.com/MattMorgis/nuanced-mcp', 'A Model Context Protocol (MCP) server that provides call graph analysis capabilities to LLMs through the nuanced library', '2025-03-06 03:02:11+00', '2025-03-09 19:53:25+00', 'Python', '5', '# Nuanced MCP Server

A [Model Context Protocol (MCP)](https://modelcontextprotocol.io) server that provides call graph analysis capabilities to LLMs through the [nuanced](https://github.com/nuanced-dev/nuanced) library.

## Overview

This MCP server enables LLMs to understand code structure by accessing function call graphs through standardized tools and resources. It allows AI assistants to:

- Initialize call graphs for Python repos
- Explore function call relationships
- Analyze dependencies between functions
- Provide more contextually aware code assistance

## API

### Tools

- **initialize_graph**

  - Initialize a code graph for the given repository path
  - Input: `repo_path` (string)

- **switch_repository**

  - Switch to a different initialized repository
  - Input: `repo_path` (string)

- **list_repositories**

  - List all initialized repositories
  - No inputs required

- **get_function_call_graph**

  - Get the call graph for a specific function
  - Inputs:
    - `file_path` (string)
    - `function_name` (string)
    - `repo_path` (string, optional) - uses active repository if not specified

- **analyze_dependencies**

  - Find all module or file dependencies in the codebase
  - Inputs (at least one required):
    - `file_path` (string, optional)
    - `module_name` (string, optional)

- **analyze_change_impact**
  - Analyze the impact of changing a specific function
  - Inputs:
    - `file_path` (string)
    - `function_name` (string)

### Resources

- **graph://summary**

  - Get a summary of the currently loaded code graph
  - No parameters required

- **graph://repo/{repo_path}/summary**

  - Get a summary of a specific repository's code graph
  - Parameters:
    - `repo_path` (string) - Path to the repository

- **graph://function/{file_path}/{function_name}**
  - Get detailed information about a specific function
  - Parameters:
    - `file_path` (string) - Path to the file containing the function
    - `function_name` (string) - Name of the function to analyze

### Prompts

- **analyze_function**

  - Create a prompt to analyze a function with its call graph
  - Parameters:
    - `file_path` (string) - Path to the file containing the function
    - `function_name` (string) - Name of the function to analyze

- **impact_analysis**

  - Create a prompt to analyze the impact of changing a function
  - Parameters:
    - `file_path` (string) - Path to the file containing the function
    - `function_name` (string) - Name of the function to analyze

- **analyze_dependencies_prompt**
  - Create a prompt to analyze dependencies of a file or module
  - Parameters (at least one required):
    - `file_path` (string, optional) - Path to the file to analyze
    - `module_name` (string, optional) - Name of the module to analyze

## Usage with Claude Desktop

Add this to your `claude_desktop_config.json`

### UV

```json
{
  "mcpServers": {
    "nuanced": {
      "command": "uv",
      "args": [
        "--directory",
        "/path/to/nuanced-mcp",
        "run",
        "nuanced_mcp_server.py"
      ]
    }
  }
}
```
', 'true', '"{\"AI Model Integration\",\"Function Calling\",\"Protocol Implementations\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states it is an MCP server designed to provide call graph analysis capabilities to LLMs. It aligns with MCP specifications for tool use and context window management.","categories":["Function Calling","Context Management","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The repository implements a server that adheres to the Model Context Protocol. It provides tools and resources accessible via an API. It appears to follow a client-server architecture where LLMs (clients) can interact with the server to perform code analysis tasks. The design centers around providing contextual information to AI models.","Implementation Details and Key Features":"The server exposes an API with tools like `initialize_graph`, `get_function_call_graph`, and `analyze_dependencies`, providing functionality to explore code structure. Resources like `graph://summary` and `graph://function/{file_path}/{function_name}` offer detailed information. Prompts are also available to guide analysis tasks. It leverages the `nuanced` library for call graph analysis.","Integration Approaches and Protocols":"The repository integrates with LLMs using the Model Context Protocol. The documentation provides an example configuration for Claude Desktop, demonstrating how to connect the MCP server to an AI assistant. The server's API utilizes standard input/output for communication and likely uses JSON for data serialization, typical of MCP implementations.","Technical Stack and Dependencies":"Based on the documentation, the server relies on Python, as implied by the `nuanced_mcp_server.py` script and uses the `nuanced` library for code analysis. The use of `uv` suggests a virtual environment and package management strategy common in Python projects. No explicit database or storage solutions are mentioned, so it may rely on in-memory storage or the `nuanced` library for graph storage.","Best Practices and Coding Standards":"While the documentation doesn't explicitly mention coding standards, the structured API design and clear tool descriptions suggest a focus on maintainability and usability. The example configuration for Claude Desktop indicates attention to integration and ease of use. Without access to source code, it's difficult to evaluate specific coding practices.","Performance Considerations":"The repository's performance likely depends on the efficiency of the `nuanced` library and the size of the codebases being analyzed. Initializing large code graphs could be a performance bottleneck. Caching and efficient graph traversal algorithms would be crucial for optimizing performance, especially for complex dependency analysis tasks. No specific performance metrics are available."}}', '2025-03-10 21:54:00+00', 'true', 'Servers', 'nuanced-mcp-server'), ('943678450', 'perplexity-mcp', 'nachoal/perplexity-mcp', 'https://github.com/nachoal/perplexity-mcp', 'A perplexity MCP server', '2025-03-06 05:01:48+00', '2025-03-07 10:11:21+00', 'Python', '7', '# Perplexity Web Search MCP Server

A simple MCP (Model Context Protocol) server that provides web search functionality using the Perplexity API. This server allows Claude or other MCP-compatible AI assistants to search the web and get up-to-date information.

## Features

- Search the web with Perplexity's powerful search capabilities
- Get comprehensive search results with sources and citations
- Filter results by time period (day, week, month, year)
- Includes a ready-to-use prompt template for web searches
- Supports loading API key from environment variables or .env file

## Installation

1. Clone this repository
2. Install dependencies:
   ```
   pip install -e .
   ```
   or
   ```
   uv pip install -e .
   ```

## Configuration

You can set the `PERPLEXITY_API_KEY` environment variable with your Perplexity API key:

```bash
export PERPLEXITY_API_KEY="your-api-key-here"
```

Alternatively, you can create a `.env` file in the project root with the following content:

```
PERPLEXITY_API_KEY=your-api-key-here
```

A sample `.env.example` file is provided for reference.

To get a Perplexity API key:
1. Visit [Perplexity API Settings](https://www.perplexity.ai/settings/api)
2. Create an account if you don't have one
3. Generate an API key

## Usage

### Running the server

```bash
python server.py
```

### Testing the server

You can test the server functionality without running the full MCP server using the included test script:

```bash
python test_server.py "your search query here" --recency month
```

Options for `--recency` are: day, week, month (default), year

### Using with Claude Desktop

1. Edit your Claude Desktop configuration file:
   - On macOS: `~/Library/Application\ Support/Claude/claude_desktop_config.json`
   - On Windows: `%APPDATA%/Claude/claude_desktop_config.json`

2. Add the following configuration:
   ```json
   {
     "perplexity-mcp": {
       "env": {
         "PERPLEXITY_API_KEY": "your-api-key-here"
       },
       "command": "python",
       "args": [
         "/path/to/server.py"
       ]
     }
   }
   ```

3. Restart Claude Desktop

### Example Prompts for Claude

- "Search the web for the latest news about artificial intelligence"
- "Use Perplexity to find information about climate change published in the past week"
- "Search for recent research papers on quantum computing from the past month"

## API Reference

### Tool: `search_web(query: str, recency: str = "month") -> str`

Search the web using Perplexity API and return results.

**Parameters:**
- `query`: The search query string
- `recency`: Filter results by time period - 'day', 'week', 'month' (default), or 'year'

**Returns:**
A comprehensive text response containing:
1. A detailed summary of the search results
2. Key facts and information found
3. Sources with URLs for verification
4. Any conflicting information if present

### Prompt: `web_search_prompt(query: str, recency: str = "month") -> str`

Creates a prompt template for searching the web with Perplexity.

**Parameters:**
- `query`: The search query
- `recency`: Time period filter - 'day', 'week', 'month' (default), or 'year'

**Returns:**
A formatted prompt string that instructs the AI to:
1. Search for the specified query
2. Focus on results from the specified time period
3. Summarize key findings
4. Highlight important facts
5. Mention conflicting information
6. Cite sources with links

## License

MIT
', 'true', '"{\"Protocol Implementations\",\"Agent Frameworks\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements a Model Context Protocol (MCP) server for web search, designed for use with AI assistants like Claude, enabling them to access up-to-date information via the Perplexity API.","categories":["Protocol Implementations","AI Model Integration","Function Calling"],"analysis":{"Architecture and Design Patterns":"The repository adopts a server-client architecture where the server exposes a web search tool via the MCP. It includes prompt templates tailored for web searches. The architecture relies on external APIs, specifically the Perplexity API, and offers configurable time-based filtering of search results. The design emphasizes modularity, separating the server logic from the client configuration.","Implementation Details and Key Features":"Key features include searching the web using the Perplexity API, filtering results by time, and providing a prompt template. The implementation involves setting up an MCP server with specific API endpoints. The code handles API key management using environment variables or a `.env` file. The repository includes a test script for server functionality.","Integration Approaches and Protocols":"The primary integration approach involves the Model Context Protocol (MCP), enabling communication between the server and AI models like Claude. The server exposes a `search_web` tool function that can be invoked by the AI model. The repository provides specific configuration instructions for integrating with Claude Desktop, including modifying the `claude_desktop_config.json` file.","Technical Stack and Dependencies":"The technical stack includes Python as the primary programming language. Key dependencies include the Perplexity API for web search functionality and potentially `uv` or `pip` for package management. The code utilizes standard Python libraries for server setup and environment variable handling. The server implementation is lightweight, focusing on API interaction and result formatting.","Best Practices and Coding Standards":"The repository provides a `.env.example` file to illustrate secure handling of API keys. It includes a test script for validating server functionality. The code is structured with clear function definitions and comments. It follows a standard Python project structure with installation instructions and a `server.py` entry point. It uses MIT License.","Performance Considerations":"Performance considerations include the response time of the Perplexity API and the efficiency of the search result processing. The recency filter can impact performance based on the size of the data being searched. Caching mechanisms could potentially be implemented to improve performance for frequently used queries. The server's scalability depends on the capacity of the Perplexity API."}}', '2025-03-10 21:28:38+00', 'true', 'Servers', 'perplexity-mcp-server'), ('943705329', 'CSharpLangMCPServer', 'biegehydra/CSharpLangMCPServer', 'https://github.com/biegehydra/CSharpLangMCPServer', null, '2025-03-06 06:12:23+00', '2025-03-10 15:41:08+00', 'TypeScript', '22', '# C# MCP Server Extension

This VS Code extension provides a Model Context Protocol (MCP) server that exposes C# language features to AI tools. It enables advanced code navigation and analysis capabilities for C# code when using AI coding assistants that support the MCP protocol.

## Features

- **Find Usages**: Locate all references to a symbol in your C# codebase with detailed context
- **HTTP/SSE Server**: Exposes language features over an MCP-compatible HTTP server
- **AI Assistant Integration**: Ready to work with AI assistants that support the MCP protocol

## Requirements

- [C# for Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csharp) extension must be installed
- .NET SDK (compatible with the C# extension)

## Installation

1. Install the [C# for Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csharp) extension
2. Install this extension
3. Open a C# project in VS Code

## Usage

The extension will automatically start an MCP server when activated. To configure an AI assistant to use this server:

1. The server runs on port 8008 by default
2. Configure your MCP-compatible AI assistant to connect to:
   - SSE endpoint: `http://localhost:8008/sse`
   - Message endpoint: `http://localhost:8008/message`

### Available Commands

- `C# MCP: Start Server` - Manually start the MCP server
- `C# MCP: Stop Server` - Stop the running MCP server

## Tools

### Find Usages

Find all references to a C# symbol in your codebase:

```json
{
  "name": "find_usages",
  "arguments": {
    "textDocument": {
      "uri": "file:///path/to/your/file.cs"
    },
    "position": {
      "line": 10,
      "character": 15
    },
    "context": {
      "includeDeclaration": true
    }
  }
}
```

The tool returns:
- File paths for each reference
- Line and character positions
- Code preview for each reference

## TODO

### Useful Links for contributors

[Roslyns protocol](https://github.com/dotnet/roslyn/blob/main/src/LanguageServer/Protocol/Protocol/Navigation/ReferenceParams.cs): This is what I used for find usages type.

[Omnisharp codebase](https://github.com/dotnet/vscode-csharp/blob/main/src/omnisharp/protocol.ts): DONT GET TRICKED BY REQUESTS THOUGH. For example, find usages calls textDocument/reference, not /findusages

[Language Server Protcol LSP](https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/): This is what omnisharp uses to communicate with roslyn. Thats where textDocument/reference comes from


| **Endpoint** | **Purpose for MCP Server (LLM Assistant)** |
|-------------|----------------------------------|
| **`/codecheck`** | Runs full code diagnostics (errors, warnings, linting) and returns issues. LLM can use this to **detect and fix issues**. |
| **`/codeformat`** | Formats code properly. Useful for **cleaning up LLM-generated code** before inserting it into the project. |
| **`/completion`** | Provides **autocomplete suggestions**. The LLM can use this to **enhance its own completions** by comparing them with the language server’s. |
| **`/completion/resolve`** | Further **resolves completion items**, adding details like signatures and documentation. |
| **`/findsymbols`** | Finds **all declared symbols (functions, classes, variables)**, helping the LLM understand the codebase. |
| **`/findusages`** | Finds **where a function, class, or variable is used**. Useful for **refactoring suggestions**. |
| **`/gototypedefinition`** | Finds the **type definition of a symbol** (e.g., where a class is defined). Helps the LLM provide **type-aware completions**. |
| **`/findimplementations`** | Retrieves **implementations of an interface or base class**. Helps the LLM when suggesting method overrides. |
| **`/getcodeactions`** | Lists **available refactoring or quick fixes**. The LLM can suggest applying these fixes. |
| **`/runcodeaction`** | Executes a **refactoring or quick fix**. If the LLM recommends a fix, it can **trigger this action automatically**. |
| **`/rename`** | Renames a symbol across the project. If the LLM suggests a rename, it can apply this action. |
| **`/typelookup`** | Retrieves **type information** for a symbol. Useful for an LLM to provide **type-aware autocompletions**. |
| **`/metadata`** | Retrieves project **metadata** (e.g., dependencies, imports). Helps the LLM understand **frameworks and libraries in use**. |
| **`/runfixall`** | Applies **all available quick fixes**. If the LLM detects multiple issues, it can run this to fix everything in one step. |
| **`/quickinfo`** | Retrieves **hover info** (docs, type hints, signatures). Useful for explaining code in **chat-based LLM assistants**. |


## Troubleshooting

If you encounter issues:

1. Ensure the C# extension is properly installed and activated
2. Check that the C# project has loaded correctly
3. Verify that port 8008 is available on your system

## Contributing

Please feel free to submit issues or pull requests to the [GitHub repository](https://github.com/biegehydra/csharplangmcpserver).

## License

This extension is licensed under the MIT License.
', 'true', '"{\"Protocol Implementations\",\"AI Model Integration\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements a Model Context Protocol (MCP) server for C# language features, designed to enable communication and integration with AI tools and assistants. It uses HTTP/SSE for MCP compatibility.","categories":["Protocol Implementations","AI Model Integration","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The repository implements a server-client architecture where the VS Code extension acts as a server exposing C# language features to AI tools (clients). It leverages HTTP and SSE protocols for communication. The design revolves around providing AI assistants with contextual information about C# code, aiding in intelligent code completion and analysis. The structure follows a plugin/extension pattern within the VS Code environment.","Implementation Details and Key Features":"The core functionality revolves around exposing C# language features such as 'find usages', code completion, and diagnostics through HTTP endpoints. The extension provides specific commands to start and stop the MCP server. It utilizes the Roslyn compiler platform for language analysis, providing detailed code context. SSE is implemented for real-time updates and communication with connected AI assistants.","Integration Approaches and Protocols":"The repository uses HTTP/SSE to communicate with AI assistants, adhering to the Model Context Protocol (MCP). It integrates with the C# extension for Visual Studio Code to leverage its language features. The extension exposes endpoints for code analysis, completion, and formatting, enabling AI assistants to interact with the C# codebase programmatically. The provided documentation outlines how to configure AI assistants to connect to the server.","Technical Stack and Dependencies":"The extension relies on the .NET SDK and the C# extension for Visual Studio Code. It uses the Roslyn compiler platform for C# language analysis. The server component is built using C# and exposes HTTP endpoints using libraries available in the .NET ecosystem. Communication is facilitated via HTTP/SSE, allowing asynchronous data streaming to connected clients. Specific versions are not specified, but compatibility with .NET SDK and C# extension is essential.","Best Practices and Coding Standards":"The repository adheres to standard C# coding conventions. The extension is structured to be easily installable and configurable within Visual Studio Code, following extension development guidelines. The codebase is organized with clear separation of concerns, exposing language features via well-defined HTTP endpoints. The inclusion of troubleshooting steps and contribution guidelines suggests a focus on maintainability and developer experience.","Performance Considerations":"The extension's performance depends on the efficiency of Roslyn's language analysis and the overhead of HTTP/SSE communication. Caching mechanisms could be implemented to reduce latency in frequently accessed language features. Asynchronous operations are likely used to avoid blocking the VS Code UI thread during analysis. Optimizing the Roslyn queries and minimizing data transfer over HTTP would improve overall responsiveness."}}', '2025-03-10 21:28:27+00', 'true', 'Servers', 'csharplang-mcp-server'), ('943714086', 'Letta-MCP-server', 'oculairmedia/Letta-MCP-server', 'https://github.com/oculairmedia/Letta-MCP-server', 'MCP server to manage letta server and comunicate with agents', '2025-03-06 06:32:16+00', '2025-03-09 05:16:48+00', 'JavaScript', '3', '# Letta MCP Server

An MCP (Model Context Protocol) server implementation for interacting with the Letta API. This server provides tools for managing agents, memory blocks, and tools in the Letta system.

## Features

- Create and manage Letta agents
- List and filter available agents
- Create and manage memory blocks
- Attach memory blocks to agents
- List and manage agent tools
- Send messages to agents and receive responses

## Installation

```bash
# Clone the repository
git clone https://github.com/oculairmedia/Letta-MCP-server.git
cd letta-server

# Install dependencies
npm install
```

## Configuration

1. Create a `.env` file in the root directory with the following variables:
```
LETTA_BASE_URL=your_letta_api_url
LETTA_PASSWORD=your_letta_api_password
```

You can use the provided `.env.example` as a template.

## Available Scripts

- `npm run build`: Build the TypeScript code
- `npm run start`: Build and start the server
- `npm run dev`: Start the server in development mode with watch mode enabled

## Tools

### Agent Management
- `create_agent`: Create a new Letta agent with specified configuration
- `list_agents`: List all available agents in the Letta system
- `prompt_agent`: Send a message to an agent and get a response

### Memory Block Management
- `create_memory_block`: Create a new memory block
- `list_memory_blocks`: List all memory blocks
- `attach_memory_block`: Attach a memory block to an agent

### Tool Management
- `list_tools`: List all available tools
- `list_agent_tools`: List tools available for a specific agent
- `attach_tool`: Attach a tool to an agent

## Example Usage

When integrated with Cline, you can use the MCP tools as follows:

```typescript
// Create a memory block
<use_mcp_tool>
<server_name>letta</server_name>
<tool_name>create_memory_block</tool_name>
<arguments>
{
  "name": "example_block",
  "label": "custom",
  "value": "This is an example memory block."
}
</arguments>
</use_mcp_tool>

// Attach the block to an agent
<use_mcp_tool>
<server_name>letta</server_name>
<tool_name>attach_memory_block</tool_name>
<arguments>
{
  "block_id": "returned_block_id",
  "agent_id": "target_agent_id"
}
</arguments>
</use_mcp_tool>
```

## Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the LICENSE file for details.', 'true', '"{\"Agent Frameworks\",\"Context Management\",\"API Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository explicitly mentions \"MCP server\" and its purpose is to manage agents, memory blocks, and tools within a system according to the Model Context Protocol, providing tool use and function calling capabilities.","categories":["Agent Frameworks","Function Calling","Memory Management"],"analysis":{"Architecture and Design Patterns":"The repository implements a server that acts as an intermediary between agents and the Letta API, utilizing the Model Context Protocol (MCP). It provides functionalities such as agent management, memory block handling, and tool management. The architecture seems modular, offering specific tools for each function. The use of a `.env` file suggests a design that supports configuration management.","Implementation Details and Key Features":"Key features include agent creation and listing, memory block creation and attachment, and tool listing and attachment. It utilizes `npm` for package management. The server exposes functionalities for managing agents' context. The example usage with Cline indicates an integration point with another system via the MCP.","Integration Approaches and Protocols":"The primary integration approach is through the Model Context Protocol (MCP). The server communicates with a Letta API, and the example demonstrates integration with Cline. The MCP tools facilitate interaction between agents and the server, enabling function calling and context management. Communication seems to involve sending messages to agents and receiving responses.","Technical Stack and Dependencies":"The repository relies on Node.js and npm, indicated by the `npm install` command. TypeScript is used, requiring a build step (`npm run build`). Environment variables, defined in `.env`, are used for configuration. The project depends on the Letta API, specified by the `LETTA_BASE_URL` and `LETTA_PASSWORD` variables.","Best Practices and Coding Standards":"The presence of an `.env.example` file and a contribution guide promotes good coding practices. The build and start scripts are well-defined, promoting consistency. The code likely follows TypeScript standards, given the presence of a build step.  The MIT license promotes open-source contribution and usage.","Performance Considerations":"No specific performance optimization techniques are mentioned in the documentation. However, using a server-client architecture with efficient API calls and data structures is implied. The efficiency of memory block management and tool attachment will directly impact overall system performance. The use of Node.js may imply non-blocking I/O operations for concurrency."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'letta-mcp-server'), ('943719610', 'mcp-llm', 'sammcj/mcp-llm', 'https://github.com/sammcj/mcp-llm', ' An MCP server that provides LLMs access to other LLMs', '2025-03-06 06:45:01+00', '2025-03-10 16:51:21+00', 'JavaScript', '7', '# MCP LLM

An MCP server that provides access to LLMs using the LlamaIndexTS library.

![I put some LLMs in your MCP for your LLMs](legit.png)

## Features

This MCP server provides the following tools:

- `generate_code`: Generate code based on a description
- `generate_code_to_file`: Generate code and write it directly to a file at a specific line number
- `generate_documentation`: Generate documentation for code
- `ask_question`: Ask a question to the LLM

![call a llm to generate code](screenshot1.png)
![call a reasoning llm to write some documentation](screenshot2.png)

## Installation

Update your MCP config to add the mcp-llm server:

```json
{
  "mcpServers": {
    "llm": {
      "command": "npx",
      "args": [
        "-y",
        "mcp-llm"
      ],
      "env": {
        "LLM_MODEL_NAME": "deepseek-r1:7b-qwen-distill-q6_k_l",
        "LLM_MODEL_PROVIDER": "ollama",
        "LLM_BASE_URL": "http://localhost:11434",
        "LLM_ALLOW_FILE_WRITE": "true",
        "LLM_TIMEOUT_S": "240"
      },
      "disabled": false,
      "autoApprove": [
        "generate_code",
        "generate_documentation",
        "ask_question",
        "generate_code_to_file"
      ],
      "timeout": 300
    },
  }
}
```

## Available Scripts

- `npm run build` - Build the project
- `npm run watch` - Watch for changes and rebuild
- `npm start` - Start the MCP server
- `npm run example` - Run the example script
- `npm run inspector` - Run the MCP inspector

## Configuration

The MCP server is configurable using environment variables:

### Required Environment Variables

- `LLM_MODEL_NAME`: The name of the model to use (e.g., `qwen2-32b:q6_k`, `anthropic.claude-3-7-sonnet-20250219-v1:0`)
- `LLM_MODEL_PROVIDER`: The model provider (e.g., `bedrock`, `ollama`, `openai`, `openai-compatible`)

### Optional Environment Variables

- `LLM_BASE_URL`: Base URL for the model provider (e.g., `https://ollama.internal`, `http://my-openai-compatible-server.com:3000/v1`)
- `LLM_TEMPERATURE`: Temperature parameter for the model (e.g., `0.2`)
- `LLM_NUM_CTX`: Context window size (e.g., `16384`)
- `LLM_TOP_P`: Top-p parameter for the model (e.g., `0.85`)
- `LLM_TOP_K`: Top-k parameter for the model (e.g., `40`)
- `LLM_MIN_P`: Min-p parameter for the model (e.g., `0.05`)
- `LLM_REPETITION_PENALTY`: Repetition penalty parameter for the model (e.g., `1.05`)
- `LLM_SYSTEM_PROMPT_GENERATE_CODE`: System prompt for the generate_code tool
- `LLM_SYSTEM_PROMPT_GENERATE_DOCUMENTATION`: System prompt for the generate_documentation tool
- `LLM_SYSTEM_PROMPT_ASK_QUESTION`: System prompt for the ask_question tool
- `LLM_TIMEOUT_S`: Timeout in seconds for LLM requests (e.g., `240` for 4 minutes)
- `LLM_ALLOW_FILE_WRITE`: Set to `true` to allow the `generate_code_to_file` tool to write to files (default: `false`)
- `OPENAI_API_KEY`: API key for OpenAI (required when using OpenAI provider)


## Manual Install From Source

1. Clone the repository
2. Install dependencies:

```bash
npm install
```

3. Build the project:

```bash
npm run build
```

4. Update your MCP configuration

### Using the Example Script

The repository includes an example script that demonstrates how to use the MCP server programmatically:

```bash
node examples/use-mcp-server.js
```

This script starts the MCP server and sends requests to it using curl commands.

## Examples

### Generate Code

```json
{
  "description": "Create a function that calculates the factorial of a number",
  "language": "JavaScript"
}
```

### Generate Code to File

```json
{
  "description": "Create a function that calculates the factorial of a number",
  "language": "JavaScript",
  "filePath": "/path/to/factorial.js",
  "lineNumber": 10,
  "replaceLines": 0
}
```

The `generate_code_to_file` tool supports both relative and absolute file paths. If a relative path is provided, it will be resolved relative to the current working directory of the MCP server.

### Generate Documentation

```json
{
  "code": "function factorial(n) {\n  if (n <= 1) return 1;\n  return n * factorial(n - 1);\n}",
  "language": "JavaScript",
  "format": "JSDoc"
}
```

### Ask Question

```json
{
  "question": "What is the difference between var, let, and const in JavaScript?",
  "context": "I'm a beginner learning JavaScript and confused about variable declarations."
}
```

## License

- [MIT LICENSE](LICENSE)
', 'true', '"{\"AI Model Integration\",\"Function Calling\",\"Protocol Implementations\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it's an MCP server providing LLMs access to other LLMs and tooling, aligning directly with MCP's purpose of facilitating model communication and tool usage within a context.","categories":["Agent Frameworks","Function Calling","API Frameworks"],"analysis":{"Architecture and Design Patterns":"The repository implements an MCP server to provide LLMs with access to tools like code generation and documentation. It uses a modular design, allowing different LLM providers (e.g., Ollama, OpenAI) to be integrated. The server exposes an API for clients to request these tool functions, promoting loose coupling and extensibility. It uses JSON for data exchange, a standard format for web services.","Implementation Details and Key Features":"The core implementation involves defining functions that leverage the LlamaIndexTS library to interact with LLMs. Key features include `generate_code`, `generate_code_to_file`, `generate_documentation`, and `ask_question`. The `generate_code_to_file` allows writing generated code directly to files. Configuration via environment variables allows customization of LLM settings and providers. The example script demonstrates how to use the MCP server programmatically.","Integration Approaches and Protocols":"Integration occurs through the MCP protocol, allowing other MCP-compliant systems to interact with the LLMs. The server is configured within an MCP config. It uses HTTP requests (likely with JSON payloads) to receive instructions and return results. The `mcpServers` configuration suggests seamless integration with existing MCP environments. The server leverages environment variables for configuration, indicating easy deployment with containerization technologies.","Technical Stack and Dependencies":"The repository uses Node.js, likely with TypeScript (given the use of LlamaIndexTS). Dependencies include LlamaIndexTS, which provides the interface to LLMs. The package manager is npm. The documentation also references `ollama` and other LLM providers. It also has example usage with `curl` commands. The build process involves using `npm run build` to transpile the source code.","Best Practices and Coding Standards":"The repository uses environment variables for configuration, a standard practice for managing settings in different environments. It includes a build process (npm run build) for ensuring consistent code execution. It also has a MIT License, enabling open-source usage.  The code includes some basic validation. Includes a readme with installation and example usage.","Performance Considerations":"The `LLM_TIMEOUT_S` environment variable is used to prevent long-running requests from blocking the server. The code may benefit from request queueing or asynchronous processing to handle multiple requests concurrently. The choice of LLM provider and model significantly impacts performance, so configuration is important. The `LLM_NUM_CTX` specifies the context window size, which impacts memory and processing time."}}', '2025-03-10 21:28:31+00', 'true', 'Servers', 'mcp-llm-server'), ('943723419', 'mcp-server-tidb', 'c4pt0r/mcp-server-tidb', 'https://github.com/c4pt0r/mcp-server-tidb', 'mcp server for tidb', '2025-03-06 06:53:44+00', '2025-03-07 22:59:28+00', 'Python', '3', '# mcp-server-tidb

MCP server implementation for TiDB (serverless) database.

## Prerequisites

- uv (Python package installer)

## Installation

```
# Clone the repository
git clone https://github.com/c4pt0r/mcp-server-tidb
cd mcp-server-tidb

# Install the package and dependencies using uv
uv venv
uv pip install -e .
```

## Configuration

Go [tidbcloud.com](https://tidbcloud.com) to create a free TiDB database cluster

Configuration can be provided through environment variables, or using .env:
- `TIDB_HOST` - TiDB host address, e.g. 'gateway01.us-east-1.prod.aws.tidbcloud.com'
- `TIDB_PORT` - TiDB port (default: 4000)
- `TIDB_USERNAME` - Database username, e.g.  'xxxxxxxxxx.\<username\>'
- `TIDB_PASSWORD` - Database password
- `TIDB_DATABASE` - Database name, default is test

## Run with Claude Desktop

Config Claude Desktop, [HOWTO](https://modelcontextprotocol.io/quickstart/user)

`claude_desktop_config.json`:

```
{
  "mcpServers": {
      "tidb": {
          "command": "uv",
          "args": [
              "--directory",
              "/path/to/mcp-server-tidb",
              "run",
              "src/main.py"
          ]
      }
  }
}
```


If you're running mcp-server-tidb in WSL, the `claude_desktop_config.json` should look like this:

```
{
	"mcpServers": {
		"tool-with-env-vars": {
			"command": "wsl.exe",
			"args": [
				"bash",
				"-c",
				"/path/to/uv --directory /path/to/mcp-server-tidb run python /path/to/mcp-server-tidb/src/main.py"
			]
		}
	}
}

', 'true', '"{\"Protocol Implementations\",\"Database Systems\",\"API Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository documentation explicitly mentions Model Context Protocol (MCP) and provides instructions for running the server with Claude Desktop, showcasing its role as a bridge between the LLM and the TiDB database.","categories":["Protocol Implementations","Database Systems","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The architecture revolves around creating a server that can interact with a TiDB database. It uses a configuration file (claude_desktop_config.json) to integrate with Claude Desktop. The server likely exposes an API to handle requests related to database operations, aligning with a client-server model. It's designed to function as an intermediary service.","Implementation Details and Key Features":"The implementation appears to be Python-based, utilizing the `uv` package manager. Key features include connecting to a TiDB database using environment variables for configuration. The code facilitates querying the database and returning results, acting as an interface between Claude and TiDB. The inclusion of a `main.py` file suggests a standard Python application structure.","Integration Approaches and Protocols":"The repository integrates with TiDB using standard database connection protocols. Its main integration point is with Claude Desktop, leveraging the Model Context Protocol (MCP) for communication. This allows Claude to execute database queries through this server. The use of environment variables enables flexible configuration across different environments.","Technical Stack and Dependencies":"The technical stack primarily involves Python and TiDB. The dependencies include Python libraries for database connectivity. `uv` is used for managing the Python environment and dependencies. The reliance on environment variables promotes configuration management best practices. The server's functionality hinges on the availability of a TiDB database instance.","Best Practices and Coding Standards":"The repository demonstrates several best practices, including using environment variables for configuration. The use of `uv` for package management ensures reproducible builds. The documentation provides clear instructions for installation and configuration. However, there is no information about code linting or unit tests. Further information would be needed to completely assess code quality.","Performance Considerations":"Performance considerations are relevant given the database interaction. Factors include the efficiency of database queries, network latency between the server and the TiDB instance, and the processing overhead in the Python server. Optimizing queries and connection pooling can improve performance. Scalability is also relevant depending on how many requests are handled."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'mcp-server-tidb'), ('943806192', 'browser-use-mcp-server', 'co-browser/browser-use-mcp-server', 'https://github.com/co-browser/browser-use-mcp-server', 'browser-use mcp server', '2025-03-06 09:47:03+00', '2025-03-10 14:54:49+00', 'Python', '41', '# ➡️ browser-use mcp server

[browser-use](https://github.com/browser-use/browser-use) MCP Server with SSE
transport

### requirements

- uv

```
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### quickstart

```
uv sync
uv pip install playwright
uv run playwright install --with-deps --no-shell chromium
uv run server --transport sse --port 8000
```

### supported clients

- cursor.ai
- claude desktop
- claude code
- <s>windsurf</s> ([windsurf](https://codeium.com/windsurf) doesn't support SSE
  yet)

### usage

after running the server, add http://localhost:8000/sse to your client UI, or in
a mcp.json file:

```json
{
  "mcpServers": {
    "browser-use-mcp-server": {
      "url": "http://localhost:8000/sse"
    }
  }
}
```

#### cursor

- `./.cursor/mcp.json`

#### windsurf

- `~/.codeium/windsurf/mcp_config.json`

#### claude

- `~/Library/Application Support/Claude/claude_desktop_config.json`
- `%APPDATA%\Claude\claude_desktop_config.json`

then try asking your LLM the following:

`open https://news.ycombinator.com and return the top ranked article`

### help

for issues or interest reach out @ https://cobrowser.xyz
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Agent Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository implements a server designed to interface with AI clients (like Cursor, Claude) using the Model Context Protocol (MCP) via SSE transport, as evidenced by the provided configuration examples and server usage.","categories":["AI Model Integration","Protocol Implementations","API Frameworks"],"analysis":{"Architecture and Design Patterns":"The architecture appears to be a server-client model where the server exposes an SSE endpoint conforming to the Model Context Protocol. Clients such as Cursor.ai and Claude connect to this endpoint. The repository leverages a pub-sub design pattern using Server-Sent Events (SSE) for real-time communication, enabling clients to receive updates.","Implementation Details and Key Features":"The server implementation uses `uv` for managing dependencies and running the server. Key features include serving the MCP endpoint, handling client connections, and facilitating communication between the AI models and browser-use functionality. The server provides an SSE transport layer for delivering browser context updates.","Integration Approaches and Protocols":"Integration is achieved by configuring AI clients (Cursor, Claude) to connect to the server's SSE endpoint. The server implements the Model Context Protocol (MCP), enabling AI models to interact with the browser-use service. The `mcp.json` file configurations specify the server's URL for client connection.","Technical Stack and Dependencies":"The technical stack includes `uv` for package management and execution. The repository relies on `playwright` for browser automation. The server uses Server-Sent Events (SSE) for real-time communication. Configuration files are in JSON format (`mcp.json`).","Best Practices and Coding Standards":"The quickstart guide provides steps for setting up the environment and running the server, suggesting a focus on usability. Configuration through JSON files promotes standardization. The inclusion of example queries and client-specific instructions demonstrates a commitment to clear documentation and ease of integration.","Performance Considerations":"Performance considerations likely revolve around the SSE transport layer and the efficiency of browser automation with Playwright. Optimizing the server for handling concurrent client connections and minimizing latency in SSE updates is crucial. The scalability of the server depends on the number of connected clients."}}', '2025-03-10 21:28:23+00', 'true', 'Servers', 'browser-use-mcp'), ('943909915', 'ezmcp', 'jujumilk3/ezmcp', 'https://github.com/jujumilk3/ezmcp', 'Easy-to-use MCP server framework specialized for SSE.', '2025-03-06 13:20:06+00', '2025-03-10 07:49:03+00', 'Python', '15', '# ezmcp

Easy-to-use MCP server framework specialized for SSE.

## Overview

ezmcp is a lightweight framework that makes it easy to create MCP-compatible servers using a FastAPI-like syntax. It provides a simple decorator-based API for defining tools that can be called by MCP clients.

## Features

- FastAPI-style decorator API for defining MCP tools
- Automatic parameter validation and type conversion
- Automatic generation of tool schemas from function signatures
- Built-in support for SSE transport
- FastAPI-style middleware support
- Easy integration with existing Starlette applications
- Interactive documentation page for exploring and testing tools

![docs_image](./docs/images/docs.png)

## Installation

```bash
pip install ezmcp
```

## Quick Start

```python
from ezmcp import ezmcp, TextContent

# Create an ezmcp application
app = ezmcp("my-app")

# Define a tool
@app.tool(description="Echo a message back to the user")
async def echo(message: str):
    """Echo a message back to the user."""
    return [TextContent(type="text", text=f"Echo: {message}")]

# Run the application
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8000)
```

Once the server is running, you can:
- Access the interactive documentation at `http://localhost:8000/docs`
- Connect to the SSE endpoint at `http://localhost:8000/sse`

## Middleware

ezmcp supports middleware similar to FastAPI, allowing you to add behavior that is applied across your entire application.

```python
from starlette.requests import Request

from ezmcp import TextContent, ezmcp

app = ezmcp("my-app")

@app.middleware
async def process_time_middleware(request: Request, call_next):
    """Add a header with the processing time."""
    import time
    start_time = time.perf_counter()
    response = await call_next(request)
    process_time = time.perf_counter() - start_time
    response.headers["X-Process-Time"] = str(process_time)
    return response

@app.tool(description="Echo a message back to the user")
async def echo(message: str):
    """Echo a message back to the user."""
    return [TextContent(type="text", text=f"Echo: {message}")]
```

For more information on middleware, see the [middleware documentation](docs/middleware.md).

## Documentation

For more detailed documentation, see the [ezmcp/README.md](ezmcp/README.md) file.

## License

MIT

## commands

- install for test `pdm install -G test`
- install for dev `pdm install -G dev`
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository is explicitly designed as an MCP server framework, using a FastAPI-like syntax to define tools that can be called by MCP clients. It also supports SSE transport.","categories":["Protocol Implementations","API Frameworks","Function Calling"],"analysis":{"Architecture and Design Patterns":"The repository adopts a microframework architecture, drawing inspiration from FastAPI. It utilizes decorators to define MCP tools, promoting a declarative style of development. Middleware support allows for cross-cutting concerns like request processing time measurement. The architecture emphasizes ease of use and integration with existing Starlette applications.","Implementation Details and Key Features":"The framework implements automatic parameter validation and type conversion for MCP tools. It automatically generates tool schemas from function signatures. Built-in support for Server-Sent Events (SSE) transport is provided for real-time communication. The framework uses `TextContent` to standardize content exchange. An interactive documentation page is generated for tool exploration.","Integration Approaches and Protocols":"The framework is designed to integrate with MCP clients using the SSE protocol. It leverages Starlette for asynchronous request handling and middleware support, facilitating integration with other ASGI-compatible frameworks. The decorator-based API simplifies tool definition and exposure to MCP clients, promoting seamless integration.","Technical Stack and Dependencies":"The repository relies on Python and uses `pip` for installation. Key dependencies include FastAPI and Starlette for web framework functionalities. PDM is used as a dependency management and packaging tool. The framework is designed to be lightweight and easy to install, minimizing the burden on developers.","Best Practices and Coding Standards":"The use of type hints and docstrings promotes code clarity and maintainability. The framework follows a declarative style, leveraging decorators to define MCP tools. The inclusion of an interactive documentation page encourages best practices for API documentation and testing. The framework incorporates middleware which enhances the application behavior without directly modifying the core logic.","Performance Considerations":"The use of Starlette and asynchronous programming contributes to efficient handling of concurrent requests. The lightweight design of the framework minimizes overhead, potentially improving overall performance. The framework's focus on SSE transport enables real-time communication, which can be crucial for certain MCP use cases. The provided middleware example showcases request processing time tracking."}}', '2025-03-10 21:28:27+00', 'true', 'Servers', 'ezmcp-sse-framework'), ('943984407', 'Elysia-mcp', 'keithagroves/Elysia-mcp', 'https://github.com/keithagroves/Elysia-mcp', 'Model Context Protocol (MCP) Server for Bun and Elysia', '2025-03-06 15:38:03+00', '2025-03-09 00:40:54+00', 'TypeScript', '4', '# MCP Server for Bun and Elysia

An implementation of the Model Context Protocol (MCP) server using Bun and the Elysia web framework. This project enables you to create high-performance MCP servers that expose resources, tools, and prompts to LLMs through a standardized interface.

## Features

- Server-Sent Events (SSE) transport implementation for Bun and Elysia
- Complete MCP protocol support with resources, tools, and prompts
- High-performance thanks to Bun's JavaScript runtime
- TypeScript support with proper type definitions
- Easy-to-use API for creating MCP-compatible servers

## Prerequisites

- [Bun](https://bun.sh/) installed on your system
- Basic familiarity with TypeScript and Elysia

## Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd mcp-server

# Install dependencies
bun install
```

## Usage

### Starting the server

```bash
# Start the server
bun start

# Start with hot reloading for development
bun dev
```

### Building for production

```bash
# Build for production
bun run build
```

This will create a minified Node.js-compatible build in the `dist` directory.

## Development

### Project Structure

- `src/index.ts` - Main entry point for the server
- `src/SSEElysiaTransport.ts` - SSE transport implementation for Bun and Elysia

### Creating an MCP Server

```typescript
import { McpServer, ResourceTemplate } from "@modelcontextprotocol/sdk/server/mcp.js";
import { z } from "zod";
import { SSEElysiaTransport } from "./SSEElysiaTransport";
import { Elysia } from "elysia";

// Create MCP server
const server = new McpServer({
  name: "my-mcp-server",
  version: "1.0.0"
});

// Add resources, tools, and prompts
server.resource(
  "example",
  "example://resource",
  async (uri) => ({
    contents: [{
      uri: uri.href,
      text: "Example resource content"
    }]
  })
);

// Create Elysia app
const app = new Elysia()
  .get("/", () => "MCP Server")
  .get("/sse", async (context) => {
    try {
      // Create transport
      const transport = new SSEElysiaTransport("/messages", context);
      
      // Store transport
      const sessionId = transport.sessionId;
      // ... store transport in a map
      
      // Connect to MCP server
      await server.connect(transport);
      
      return;
    } catch (error) {
      // Handle error
    }
  })
  .post("/messages", async (context) => {
    // Handle incoming messages
  });

// Start server
app.listen(3001, () => {
  console.log("MCP Server running at http://localhost:3001");
});
```

## Debugging

You can debug your MCP server using the MCP Inspector tool and connect through sse

```bash
npx @modelcontextprotocol/inspector
```


This will open a web interface where you can:
- List available resources, tools, and prompts
- Test calling tools and retrieving resources
- Inspect the communication between the client and server

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements a Model Context Protocol (MCP) server using Bun and Elysia. The documentation details MCP protocol support with resources, tools, and prompts, confirming its purpose.","categories":["Protocol Implementations","API Frameworks","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The repository follows a modular design, separating the MCP server logic from the transport implementation. It employs an event-driven architecture via Server-Sent Events (SSE) for communication. The code uses TypeScript, promoting type safety and maintainability. The separation of concerns is evident in the project structure, improving readability and testability.","Implementation Details and Key Features":"The core feature is the MCP server, built using the `@modelcontextprotocol/sdk/server/mcp.js` library. It supports defining resources, tools, and prompts for LLMs. The `SSEElysiaTransport` handles SSE communication. The example shows how to register resources and connect to the MCP server using the transport. Error handling is also present.","Integration Approaches and Protocols":"The repository uses Server-Sent Events (SSE) as the primary transport protocol. It integrates with the Elysia web framework for handling HTTP requests and SSE connections. The MCP server exposes endpoints for subscribing to events and sending messages. The provided example code illustrates how to connect the MCP server to the transport and handle incoming messages.","Technical Stack and Dependencies":"The project relies heavily on Bun as its JavaScript runtime environment, alongside Elysia, a web framework for Bun. It leverages TypeScript for development. Key dependencies include `@modelcontextprotocol/sdk/server/mcp.js` for MCP functionality and Zod for schema validation. The stack is tailored for performance and developer experience.","Best Practices and Coding Standards":"The repository uses TypeScript, enforcing static typing. The project includes clear examples and usage instructions, promoting ease of adoption. The code is structured into modules with clear separation of concerns. The documentation encourages contributions, indicating a focus on community engagement and code quality. The structure is well defined for server implementations.","Performance Considerations":"The use of Bun aims for high performance in the JavaScript runtime. SSE allows for real-time communication between the client and server. The documentation includes instructions for building a production-ready, minified Node.js-compatible build, suggesting an awareness of optimization. Performance is improved via memory management and efficient network utilization."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'elysia-mcp-server'), ('944014927', 'browser-use-mcp-client', 'Linzo99/browser-use-mcp-client', 'https://github.com/Linzo99/browser-use-mcp-client', 'A MCP client for browser-use', '2025-03-06 16:36:11+00', '2025-03-09 23:47:27+00', 'TypeScript', '8', '# Browser-use MCP Client

A modern React application that provides a user-friendly interface for interacting with Model Context Protocol (MCP) servers through Server-Sent Events (SSE).

## 🎥 Demo
https://github.com/user-attachments/assets/52ab11ad-741f-4506-99ad-9f1972a3aad1

## 🚀 Features

- **Real-time Communication**: Direct SSE connection to MCP servers
- **Interactive UI**: Clean and responsive interface built with React and Tailwind CSS
- **Theme Support**: Light and dark mode with system preference detection
- **Screenshot Preview**: Live browser screenshots from MCP server responses
- **Message History**: Persistent chat history with clear message threading
- **Request Management**: Cancel in-progress requests and clear chat history
- **Connection Management**: Easy server connection configuration

## 📋 Prerequisites

- Node.js (v18 or later)
- pnpm (recommended package manager)
- A running MCP server for connection
- Python 3.8+ (for running the example server)
-

## 🚀 Getting Started

1. **Clone the Repository**

   ```bash
   git clone <repository-url>
   cd browser-use-mcp-client
   ```

2. **Install Dependencies**

   ```bash
   pnpm install
   ```

3. **Start the Development Server**

   ```bash
   pnpm dev
   ```

4. **Start the Proxy Server**

   ```bash
   ./proxy/index.js
   ```

The application will be available at `http://localhost:5173`

## 💻 Usage

## 🤖 Example MCP Server

Here's an example of a Python-based MCP server that uses browser automation:

```python
#!/usr/bin/env python3
import asyncio
from dotenv import load_dotenv
from typing import Awaitable, Callable
from mcp.server.fastmcp import FastMCP, Context
from browser_use import Agent, Browser, BrowserConfig
from langchain_google_genai import ChatGoogleGenerativeAI

# Load environment variables from .env file
load_dotenv()

# Initialize FastMCP server
mcp = FastMCP("browser-use")

browser = Browser(
    config=BrowserConfig(
        chrome_instance_path="/Applications/Google Chrome.app/Contents/MacOS/Google Chrome --remote-debugging-port=9222",
        headless=True
    )
)

llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash")
agent = None


@mcp.tool()
async def perform_search(task: str, context: Context):
    """Perform the actual search in the background."""
    async def step_handler(state, *args):
        if len(args) != 2:
            return
        await context.session.send_log_message(
            level="info",
            data={"screenshot": state.screenshot, "result": args[0]}
        )

    asyncio.create_task(
        run_browser_agent(task=task, on_step=step_handler)
    )
    return "Processing Request"


@mcp.tool()
async def stop_search(*, context: Context):
    """Stop a running browser agent search by task ID."""
    if agent is not None:
        await agent.stop()
    return "Running Agent stopped"


async def run_browser_agent(task: str, on_step: Callable[[], Awaitable[None]]):
    """Run the browser-use agent with the specified task."""
    global agent
    try:
        agent = Agent(
            task=task,
            browser=browser,
            llm=llm,
            register_new_step_callback=on_step,
            register_done_callback=on_step,
        )

        await agent.run()
    except asyncio.CancelledError:
        return "Task was cancelled"

    except Exception as e:
        return f"Error during execution: {str(e)}"
    finally:
        await browser.close()

if __name__ == "__main__":
    mcp.run(transport="sse")
```

## 🤝 Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Open a Pull Request

## 📄 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
', 'true', '"{\"AI Model Integration\",\"Protocol Implementations\",\"Browser & Web Automation\"]}', '{"is_mcp":"YES","justification":"The repository description and documentation explicitly mention Model Context Protocol (MCP) and interacting with MCP servers via Server-Sent Events (SSE). The example MCP server provided further confirms this.","categories":["AI Model Integration","Protocol Implementations","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The client application is built with React and Tailwind CSS, suggesting a component-based architecture for the user interface. The application utilizes Server-Sent Events (SSE) for real-time communication with the MCP server, which likely employs an event-driven design. The use of a proxy server implies a separation of concerns, possibly for handling CORS or other cross-domain issues.","Implementation Details and Key Features":"The client facilitates real-time communication with MCP servers. Key features include screenshot previews from server responses, persistent message history, and request management (canceling in-progress requests). Theme support (light/dark mode) enhances user experience. The example server uses Python and libraries like `browser_use`, `langchain-google-genai`, and `fastmcp` to implement the MCP server logic.","Integration Approaches and Protocols":"The application integrates with MCP servers using Server-Sent Events (SSE). The provided example server demonstrates integration with browser automation tools and Language Model Models (LLMs). The example makes uses of BrowserUse and Langchain. The server exposes tools via the MCP.","Technical Stack and Dependencies":"The client-side uses React, Tailwind CSS, Node.js, and pnpm. The server-side example uses Python 3.8+ and depends on libraries such as `fastmcp`, `browser_use`, `langchain-google-genai`, and `python-dotenv`. The example's usage of Chrome requires a specific installation path and a remote debugging port.","Best Practices and Coding Standards":"The repository follows standard practices for React-based web development, utilizing a modern component framework. The inclusion of a LICENSE file (MIT License) and a CONTRIBUTING section suggests attention to open-source best practices. The usage of a package manager (pnpm) and environment variables via `dotenv` reflects adherence to modern development workflows.","Performance Considerations":"Real-time communication through SSE can introduce performance considerations related to connection management and data transfer rates. Browser automation, as exemplified in the server, can be resource-intensive, impacting responsiveness. The use of asynchronous operations and message queueing (implicit in SSE) may be employed to mitigate performance bottlenecks."}}', '2025-03-10 21:54:07+00', 'true', 'Clients', 'browser-use-mcp-944014927'), ('944041703', 'mcp-durable-object-client', 'Dhravya/mcp-durable-object-client', 'https://github.com/Dhravya/mcp-durable-object-client', 'testing mcps', '2025-03-06 17:28:05+00', '2025-03-10 05:01:05+00', 'TypeScript', '49', '# 🤖 MCP Client as a durable object

This example shows a working pattern using durable objects as an MCP client.

To start, install depdencies in `/` and `/server`

```
npm i
```
To start the server, run `npm run start` in `server/`

To start the client, run `npm run start` in `/`


Ask `Can you add 5 and 6`?

The client will make a call to the server, get the response and show it to the client.

## License

MIT
', 'true', '"{\"Protocol Implementations\",\"Agent Frameworks\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly mentions 'MCP Client' and demonstrates its implementation using durable objects, showcasing a communication protocol pattern between a client and server.","categories":["Protocol Implementations","API Frameworks","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The repository implements a client-server architecture using durable objects. The client initiates requests, and the server processes these requests, returning a response. The use of durable objects suggests a focus on persistent state and reliable message handling. The pattern facilitates decoupled communication between components.","Implementation Details and Key Features":"The core functionality involves sending a request from the client to the server and receiving a response. The example uses a simple addition task to illustrate the communication flow. Key features include dependency management using npm and separate startup scripts for the client and server components.","Integration Approaches and Protocols":"The integration relies on a custom protocol implemented over HTTP or a similar network transport. Durable objects facilitate reliable communication and state management. The provided example demonstrates a basic request-response interaction model suitable for integrating AI models with external tools or services.","Technical Stack and Dependencies":"The repository utilizes JavaScript and Node.js. Dependencies include npm for package management. The use of durable objects suggests a Cloudflare Workers environment or a similar serverless platform. The stack enables rapid prototyping and deployment of microservices.","Best Practices and Coding Standards":"The repository includes basic instructions for setup and execution, adhering to common npm practices. The structure of client and server directories promotes modularity. The MIT license encourages open-source collaboration and usage.","Performance Considerations":"The use of durable objects can provide performance benefits through persistent connections and efficient state management. The serverless architecture allows for scaling based on demand. However, network latency and object serialization overhead should be considered for performance optimization."}}', '2025-03-10 21:28:23+00', 'true', 'Clients', 'mcp-durable-object'), ('944050234', 'linear-mcp', 'vinayak-mehta/linear-mcp', 'https://github.com/vinayak-mehta/linear-mcp', null, '2025-03-06 17:45:25+00', '2025-03-09 02:15:52+00', 'Python', '5', '# Linear MCP

A Model Context Protocol (MCP) server for Linear, providing tools for issue tracking and project management.

## Configuration

Go to [Linear security settings](https://linear.app/settings/account/security) and create an API key:

![linear-security-settings](./linear-security-settings.png)

Then, configure it with the `LINEAR_API_KEY` env variable:

```bash
# Add to your .env file
LINEAR_API_KEY=lin_api_xxxxxxxxxxxx

# Or export as environment variable
export LINEAR_API_KEY=lin_api_xxxxxxxxxxxx
```

## Running the Server

```bash
uvx --from git+https://github.com/vinayak-mehta/linear-mcp linear-mcp
```

## Resources

Linear MCP provides access to the following resource types:

| Resource | Description | Example URI |
|----------|-------------|-------------|
| Issue | Details of a specific Linear issue | `linear-issue:///a1b2c3d4` |
| Team Issues | All issues for a specific team | `linear-team:///ENG/issues` |
| User Issues | Issues assigned to a user | `linear-user:///me/assigned` |
| Organization | Details about your Linear organization | `linear-organization:` |
| Viewer | Information about the authenticated user | `linear-viewer:` |

## Tools

| Tool | Description |
|------|-------------|
| `linear_create_issue` | Create a new Linear issue with title, description, and other attributes |
| `linear_update_issue` | Update an existing issue's properties |
| `linear_search_issues` | Search issues using flexible filtering criteria |
| `linear_get_user_issues` | Retrieve issues assigned to a specific user |
| `linear_add_comment` | Add a comment to an existing issue |

## Example Prompts

### Creating a New Issue

```
Create a bug ticket: "Login screen disappears like magic tricks at a birthday party!" Priority: HIGH, Team: Engineering
```

### Updating an Issue

```
Upgrade ENG-123 to "shooting star" priority! Users are excited! Change status to "Racing to the finish line!"
```

### Searching for Issues

```
Find all Frontend tasks with "authentication" that are patiently waiting for their moment to shine
```

### Retrieving User Tasks

```
What exciting challenges await me today in Linear?
```

### Adding a Comment

```
Add to DEV-456: "Temporarily paused while the API does its beauty routine. Back on it next sprint with fresh energy!"
```

### Organization Overview

```
Show me our amazing team structure so I can appreciate all the talented people making this happen
```

## License

Apache 2.0
', 'true', '"{\"Protocol Implementations\",\"Agent Frameworks\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository implements a Model Context Protocol (MCP) server for Linear, facilitating interaction between AI models and Linear's issue tracking and project management tools through resource URIs and function calling.","categories":["Protocol Implementations","Agent Frameworks","Function Calling"],"analysis":{"Architecture and Design Patterns":"The architecture centers around a server that exposes Linear's resources through a defined protocol. It seems to follow a client-server model, where an AI model acts as the client, communicating with the Linear MCP server. The design pattern employed is that of a resource-oriented architecture, where Linear issues, teams, and users are represented as resources addressable via URIs.","Implementation Details and Key Features":"Key features include the ability to create, update, search, and comment on Linear issues. It offers tools for retrieving user-specific issues and accessing organization information. The implementation relies on Linear's API key for authentication. Example prompts are provided, showcasing the functionality through natural language requests. The server uses the `uvx` package for execution from a Git repository.","Integration Approaches and Protocols":"The repository integrates with Linear's issue tracking system via its API. The protocol implemented is the Model Context Protocol (MCP), enabling AI models to interact with Linear resources. The interaction is facilitated through resource URIs (e.g., `linear-issue:///a1b2c3d4`) and specific tool functions (e.g., `linear_create_issue`). The system translates natural language prompts into Linear API calls.","Technical Stack and Dependencies":"Based on the execution command, the technical stack includes Python and potentially `uvx`. The project depends on the Linear API for accessing and manipulating issue data. The documentation mentions the use of environment variables for storing the Linear API key, indicating a dependency on environment configuration. The license is Apache 2.0, suggesting open-source components.","Best Practices and Coding Standards":"The code utilizes environment variables for sensitive information (API keys), which is a security best practice. The documentation provides clear instructions on configuring the Linear API key and running the server. The use of resource URIs and defined tool functions promotes a structured approach to interacting with the Linear API. The inclusion of example prompts promotes discoverability and ease of use.","Performance Considerations":"Performance considerations would likely involve the efficiency of Linear API calls. The design should optimize the number of API requests needed to fulfill a user's request. Caching frequently accessed data, such as user information or organization details, could improve response times. Rate limiting and error handling are crucial to avoid overwhelming the Linear API and ensure stability."}}', '2025-03-10 21:54:04+00', 'true', 'Servers', 'linear-mcp-server'), ('944102364', 'YaraFlux', 'ThreatFlux/YaraFlux', 'https://github.com/ThreatFlux/YaraFlux', 'A yara based MCP Server', '2025-03-06 19:39:12+00', '2025-03-10 16:24:32+00', 'Python', '9', '# YaraFlux MCP Server

[![CI](https://github.com/ThreatFlux/YaraFlux/workflows/CI/badge.svg)](https://github.com/ThreatFlux/YaraFlux/actions)
[![codecov](https://codecov.io/gh/ThreatFlux/YaraFlux/branch/main/graph/badge.svg)](https://codecov.io/gh/ThreatFlux/YaraFlux)
[![Codacy Badge](https://app.codacy.com/project/badge/Grade/8f7728ae613540938411196abe4359f6)](https://app.codacy.com/gh/ThreatFlux/YaraFlux/dashboard?utm_source=gh&utm_medium=referral&utm_content=&utm_campaign=Badge_grade)
[![GitHub License](https://img.shields.io/github/license/ThreatFlux/YaraFlux)](https://github.com/ThreatFlux/YaraFlux/blob/main/LICENSE)
[![Python Version](https://img.shields.io/badge/python-3.13-blue)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104.1-009688)](https://fastapi.tiangolo.com/)
[![MCP](https://img.shields.io/badge/MCP-Integrated-blueviolet)](https://docs.anthropic.com/claude/docs/model-context-protocol)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

A Model Context Protocol (MCP) server for YARA scanning, providing LLMs with capabilities to analyze files with YARA rules.

## 📋 Overview

YaraFlux MCP Server enables AI assistants to perform YARA rule-based threat analysis through the standardized Model Context Protocol interface. The server integrates YARA scanning with modern AI assistants, supporting comprehensive rule management, secure scanning, and detailed result analysis through a modular architecture.

## 🧩 Architecture Overview

```
+------------------------------------------+
|              AI Assistant                |
+--------------------+---------------------+
                    |
                    | Model Context Protocol
                    |
+--------------------v---------------------+
|              YaraFlux MCP Server         |
|                                          |
|  +----------------+    +---------------+ |
|  | MCP Server     |    | Tool Registry | |
|  +-------+--------+    +-------+-------+ |
|          |                     |         |
|  +-------v--------+    +-------v-------+ |
|  | YARA Service   |    | Storage Layer | |
|  +----------------+    +---------------+ |
|                                          |
+------------------------------------------+
          |                   |
 +-----------------+  +---------------+
 | YARA Engine     |  | Storage       |
 | - Rule Compiling|  | - Local FS    |
 | - File Scanning |  | - MinIO/S3    |
 +-----------------+  +---------------+
```

YaraFlux follows a modular architecture that separates concerns between:
- **MCP Integration Layer**: Handles communication with AI assistants
- **Tool Implementation Layer**: Implements YARA scanning and management functionality
- **Storage Abstraction Layer**: Provides flexible storage options
- **YARA Engine Integration**: Leverages YARA for scanning and rule management

For detailed architecture diagrams, see the [Architecture Documentation](docs/architecture_diagram.md).

## ✨ Features

- 🔄 **Modular Architecture**
  - Clean separation of MCP integration, tool implementation, and storage
  - Standardized parameter parsing and error handling
  - Flexible storage backend with local and S3/MinIO options

- 🤖 **MCP Integration**
  - 19 integrated MCP tools for comprehensive functionality
  - Optimized for Claude Desktop integration
  - Direct file analysis from within conversations
  - Compatible with latest MCP protocol specification

- 🔍 **YARA Scanning**
  - URL and file content scanning
  - Detailed match information with context
  - Scan result storage and retrieval
  - Performance-optimized scanning engine

- 📝 **Rule Management**
  - Create, read, update, delete YARA rules
  - Rule validation with detailed error reporting
  - Import rules from ThreatFlux repository
  - Categorization by source (custom vs. community)

- 📊 **File Analysis**
  - Hexadecimal view for binary analysis
  - String extraction with configurable parameters
  - File metadata and hash information
  - Secure file upload and storage

- 🔐 **Security Features**
  - JWT authentication for API access
  - Non-root container execution
  - Secure storage isolation
  - Configurable access controls

## 🚀 Quick Start

### Using Docker (Recommended)

```bash
# Clone the repository
git clone https://github.com/ThreatFlux/YaraFlux.git
cd YaraFlux/

# Build the Docker image
docker build -t yaraflux-mcp-server:latest .

# Run the container
docker run -p 8000:8000 \
  -e JWT_SECRET_KEY=your-secret-key \
  -e ADMIN_PASSWORD=your-admin-password \
  -e DEBUG=true \
  yaraflux-mcp-server:latest
```

### Installation from Source

```bash
# Clone the repository
git clone https://github.com/ThreatFlux/YaraFlux.git
cd YaraFlux/

# Install dependencies (requires Python 3.13+)
make install

# Run the server
make run
```

## 🧩 Claude Desktop Integration

YaraFlux is designed for seamless integration with Claude Desktop through the Model Context Protocol.

1. Build the Docker image:
```bash
docker build -t yaraflux-mcp-server:latest .
```

2. Add to Claude Desktop config (`~/Library/Application Support/Claude/claude_desktop_config.json`):
```json
{
  "mcpServers": {
    "yaraflux-mcp-server": {
      "command": "docker",
      "args": [
        "run",
        "-i",
        "--rm",
        "--env",
        "JWT_SECRET_KEY=your-secret-key",
        "--env",
        "ADMIN_PASSWORD=your-admin-password",
        "--env",
        "DEBUG=true",
        "--env",
        "PYTHONUNBUFFERED=1",
        "yaraflux-mcp-server:latest"
      ],
      "disabled": false,
      "autoApprove": [
        "scan_url",
        "scan_data",
        "list_yara_rules",
        "get_yara_rule"
      ]
    }
  }
}
```

3. Restart Claude Desktop to activate the server.

## 🛠️ Available MCP Tools

YaraFlux exposes 19 integrated MCP tools:

### Rule Management Tools
- **list_yara_rules**: List available YARA rules with filtering options
- **get_yara_rule**: Get a specific YARA rule's content and metadata
- **validate_yara_rule**: Validate YARA rule syntax with detailed error reporting
- **add_yara_rule**: Create a new YARA rule
- **update_yara_rule**: Update an existing YARA rule
- **delete_yara_rule**: Delete a YARA rule
- **import_threatflux_rules**: Import rules from ThreatFlux GitHub repository

### Scanning Tools
- **scan_url**: Scan content from a URL with specified YARA rules
- **scan_data**: Scan provided data (base64 encoded) with specified rules
- **get_scan_result**: Retrieve detailed results from a previous scan

### File Management Tools
- **upload_file**: Upload a file for analysis or scanning
- **get_file_info**: Get metadata about an uploaded file
- **list_files**: List uploaded files with pagination and sorting
- **delete_file**: Delete an uploaded file
- **extract_strings**: Extract ASCII/Unicode strings from a file
- **get_hex_view**: Get hexadecimal view of file content
- **download_file**: Download an uploaded file

### Storage Management Tools
- **get_storage_info**: Get storage usage statistics
- **clean_storage**: Remove old files to free up storage space

## 📚 Documentation

Comprehensive documentation is available in the [docs/](docs/) directory:

- [Architecture Diagrams](docs/architecture_diagram.md) - Visual representation of system architecture
- [Code Analysis](docs/code_analysis.md) - Detailed code structure and recommendations
- [Installation Guide](docs/installation.md) - Detailed setup instructions
- [CLI Usage Guide](docs/cli.md) - Command-line interface documentation
- [API Reference](docs/api.md) - REST API endpoints and usage
- [YARA Rules Guide](docs/yara_rules.md) - Creating and managing YARA rules
- [MCP Integration](docs/mcp.md) - Model Context Protocol integration details
- [File Management](docs/file_management.md) - File handling capabilities
- [Examples](docs/examples.md) - Real-world usage examples

## 🗂️ Project Structure

```
yaraflux_mcp_server/
├── src/
│   └── yaraflux_mcp_server/
│       ├── app.py                 # FastAPI application
│       ├── auth.py                # JWT authentication and user management
│       ├── config.py              # Configuration settings loader
│       ├── models.py              # Pydantic models for requests/responses
│       ├── mcp_server.py          # MCP server implementation
│       ├── schema.py              # JSON schemas for tool parameters
│       ├── utils/                 # Utility functions package
│       │   ├── __init__.py        # Package initialization
│       │   ├── error_handling.py  # Standardized error handling
│       │   ├── param_parsing.py   # Parameter parsing utilities
│       │   └── wrapper_generator.py # Tool wrapper generation
│       ├── mcp_tools/             # Modular MCP tools package
│       │   ├── __init__.py        # Package initialization
│       │   ├── base.py            # Base tool registration utilities
│       │   ├── file_tools.py      # File management tools
│       │   ├── rule_tools.py      # YARA rule management tools
│       │   ├── scan_tools.py      # Scanning tools
│       │   └── storage_tools.py   # Storage management tools
│       ├── storage/               # Storage implementation package
│       │   ├── __init__.py        # Package initialization
│       │   ├── base.py            # Base storage interface
│       │   ├── factory.py         # Storage client factory
│       │   ├── local.py           # Local filesystem storage
│       │   └── minio.py           # MinIO/S3 storage
│       ├── routers/               # API route definitions
│       │   ├── __init__.py        # Package initialization
│       │   ├── auth.py            # Authentication API routes
│       │   ├── files.py           # File management API routes
│       │   ├── rules.py           # YARA rule management API routes
│       │   └── scan.py            # YARA scanning API routes
│       ├── yara_service.py        # YARA rule management and scanning
│       ├── __init__.py            # Package initialization
│       └── __main__.py            # CLI entry point
├── docs/                          # Documentation
├── tests/                         # Test suite
├── Dockerfile                     # Docker configuration
├── entrypoint.sh                  # Container entrypoint script
├── Makefile                       # Build automation
├── pyproject.toml                 # Project metadata and dependencies
├── requirements.txt               # Core dependencies
└── requirements-dev.txt           # Development dependencies
```

## 🧪 Development

### Local Development

```bash
# Set up development environment
make dev-setup

# Run tests
make test

# Code quality checks
make lint
make format
make security-check

# Generate test coverage report
make coverage

# Run development server
make run
```

### CI/CD Workflows

This project uses GitHub Actions for continuous integration and deployment:

- **CI Tests**: Runs on every push and pull request to main and develop branches
  - Runs tests, formatting, linting, and type checking
  - Builds and tests Docker images
  - Uploads test coverage reports to Codecov

- **Version Auto-increment**: Automatically increments version on pushes to main branch
  - Updates version in pyproject.toml, setup.py, and Dockerfile
  - Creates git tag for new version

- **Publish Release**: Triggered after successful version auto-increment
  - Builds Docker images for multiple stages
  - Generates release notes from git commits
  - Creates GitHub release with artifacts
  - Publishes Docker images to Docker Hub

These workflows ensure code quality and automate the release process.

### Status Checks

The following status checks run on pull requests:

- ✅ **Format Verification**: Ensures code follows Black and isort formatting standards
- ✅ **Lint Verification**: Validates code quality and compliance with coding standards
- ✅ **Test Execution**: Runs the full test suite to verify functionality
- ✅ **Coverage Report**: Ensures sufficient test coverage of the codebase

## 🌐 API Documentation

Interactive API documentation available at:
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

For detailed API documentation, see [API Reference](docs/api.md).

## 🤝 Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## 📄 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## 💖 Donate or Ask for Features

- [Patreon](https://patreon.com/vtriple)
- [PayPal](https://paypal.me/ThreatFlux)
', 'true', '"{\"Protocol Implementations\",\"Security & Privacy\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it is a Model Context Protocol (MCP) server for YARA scanning, as indicated in the description and badges. It implements MCP tools and integrates with AI assistants like Claude.","categories":["Protocol Implementations","Security & Privacy","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The architecture is modular, separating MCP integration, tool implementation, and storage layers. It leverages a YARA engine for scanning and rule management. The design includes clear separation of concerns, promoting maintainability and extensibility. Storage abstraction allows flexible options like local filesystem or S3/MinIO. The system utilizes FastAPI, showcasing modern API design principles.","Implementation Details and Key Features":"The server provides 19 integrated MCP tools for rule management, scanning, and file management. Key features include YARA scanning of URLs and file content, detailed match information, and rule management capabilities. It supports JWT authentication, secure storage isolation, and configurable access controls for security. The implementation includes parameter parsing utilities and standardized error handling.","Integration Approaches and Protocols":"The primary integration approach is through the Model Context Protocol (MCP), enabling communication with AI assistants. It integrates seamlessly with Claude Desktop via MCP. API documentation is available via Swagger UI and ReDoc. The server supports direct file analysis from within conversations. The integration leverages Docker for easy deployment and configuration.","Technical Stack and Dependencies":"The project uses Python 3.13 with FastAPI for the API server. Key dependencies include the YARA engine for rule compilation and scanning, and Pydantic for data validation. It supports local and S3/MinIO storage options. Other notable technologies include Docker for containerization, JWT for authentication, and Black for code formatting. The Makefile simplifies common development tasks.","Best Practices and Coding Standards":"The project employs clear separation of concerns and modular design principles. Code quality is maintained through linting, formatting (Black), and type checking. It includes comprehensive documentation, including architecture diagrams and API references. The project uses CI/CD workflows with GitHub Actions for automated testing and deployment. Secure coding practices are evident with features like JWT authentication and non-root container execution.","Performance Considerations":"The scanning engine is performance-optimized. The system employs flexible storage options to handle varying workloads. The modular architecture allows for scalability and efficient resource utilization. Caching mechanisms and asynchronous processing may be implemented, though not explicitly stated. Docker containerization allows efficient deployment and resource management."}}', '2025-03-10 21:28:31+00', 'true', 'Servers', 'yaraflux-mcp-server'), ('944115493', 'awesome-solana-mcp-servers', 'sendaifun/awesome-solana-mcp-servers', 'https://github.com/sendaifun/awesome-solana-mcp-servers', null, '2025-03-06 20:10:48+00', '2025-03-10 07:17:57+00', null, '6', '*Read this in [English](README.md), [简体中文](README.zh-CN.md)*

<div align="center">

# awesome-solana-mcp-servers

![Awesome Solana MCP Servers](banner.png)

A curated list of awesome Solana Model Context Protocol (MCP) servers and related resources.

</div>

## MCP Servers
- [Solana Agent Kit MCP Server](https://github.com/sendaifun/solana-agent-kit/tree/main/examples/agent-kit-mcp-server) - A Solana Agent Kit implementation using MCP for handling protocol operations on the Solana blockchain. Supports all Solana Agent Kit actions with standardized interactions and environment-based configuration.
- [GOAT MCP Server](https://github.com/goat-sdk/goat/tree/main/typescript/examples/by-framework/model-context-protocol) - A GOAT SDK implementation enabling Claude Desktop to send and receive ETH and ERC-20 tokens on EVM networks. Features Base Sepolia support and easy Claude Desktop integration.
- [Aldrin Labs Solana MCP Server](https://github.com/Aldrin-labs/solana-mcp-server) - A comprehensive MCP server providing 21 essential Solana RPC methods including account operations, token management, system information, and staking functionality. Features natural language interaction with Solana blockchain data.

## Tools and Libraries
- [Solana Agent Kit](https://github.com/sendaifun/solana-agent-kit) - A toolkit for connecting AI agents to Solana protocols. Features cross-chain operations, token management, Voltr vault interactions, and multi-agent system support with LangGraph.
- [GOAT SDK](https://github.com/goat-sdk/goat) - A framework for building and integrating GenAI features with built-in MCP support. Includes tool management, Dev UI playground, and cross-model compatibility.

## Resources
- [Model Context Protocol Quickstart](https://glama.ai/blog/2024-11-25-model-context-protocol-quickstart) - A comprehensive guide covering MCP protocol basics, server implementation, client setup, and early adoption use cases. Includes practical examples and Claude Desktop integration.
- [Add helpful resources here] - Documentation, tutorials, blog posts, videos, debugging guides, and best practices for MCP development.

## Contents

### What is Model Context Protocol (MCP)?

Model Context Protocol (MCP) is a protocol that enables AI models to interact with external tools and resources in a standardized way. It provides a framework for:
- Tool integration and execution
- Resource management and access
- Prompt templates and management
- Sampling capabilities
- Root-level operations

MCP servers can be integrated with various clients including:
- Claude Desktop App
- Cursor
- Continue
- Zed
- Sourcegraph Cody
- And many more

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
', 'true', '"{\"Protocol Implementations\",\"AI Model Integration\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly lists Solana MCP servers and related resources, focusing on interactions between AI models and the Solana blockchain using the Model Context Protocol.","categories":["Protocol Implementations","AI Model Integration","Function Calling"],"analysis":{"Architecture and Design Patterns":"The architecture revolves around MCP servers enabling AI models to interact with the Solana blockchain. Design patterns include standardized interfaces for tool integration and resource management. The listed servers demonstrate a modular design, allowing for easy integration with different AI clients such as Claude Desktop.","Implementation Details and Key Features":"Implementations involve creating servers that expose Solana RPC methods via MCP. Key features include account operations, token management, and system information retrieval. The servers support natural language interaction with Solana blockchain data, enhancing usability for AI models. The GOAT SDK offers built-in MCP support.","Integration Approaches and Protocols":"Integration relies on the Model Context Protocol to facilitate communication between AI models and Solana blockchain functionalities. Protocols like HTTP or WebSocket are likely used for data transfer. The Solana Agent Kit allows connecting AI agents to Solana protocols, enabling cross-chain operations and Voltr vault interactions.","Technical Stack and Dependencies":"The technical stack includes Solana blockchain, possibly utilizing languages like Rust or JavaScript/TypeScript for server implementations. Dependencies likely encompass Solana SDKs, MCP libraries, and networking libraries for handling communication protocols. GOAT SDK utilizes TypeScript. The linked repositories likely have dependencies on crypto libraries.","Best Practices and Coding Standards":"The repository promotes standardized interactions and environment-based configurations for Solana Agent Kit actions. Best practices likely involve adhering to MCP specifications and implementing robust error handling. The examples provided aim to showcase clear and maintainable code, facilitating easier integration for developers.","Performance Considerations":"Performance considerations include optimizing Solana RPC method calls to minimize latency and resource consumption. Caching mechanisms and efficient data serialization may be implemented. The impact of network latency between AI models and MCP servers should be minimized. Careful resource management is needed for handling blockchain data."}}', '2025-03-10 21:28:34+00', 'true', 'Servers', 'solana-mcp-servers'), ('944136400', 'modelcontextprotocol', 'terminalwire/modelcontextprotocol', 'https://github.com/terminalwire/modelcontextprotocol', 'A ruby implementation of the Model Context Protocol', '2025-03-06 21:03:48+00', '2025-03-10 14:04:41+00', 'Ruby', '6', '# ModelContextProtocol

This gem enables the Model Context Protocol for Thor command-line applications.

## Installation

Install the gem and add to the application's Gemfile by executing:

```bash
bundle add modelcontextprotocol
```

If bundler is not being used to manage dependencies, install the gem by executing:

```bash
gem install modelcontextprotocol
```

## Usage

**THIS DOES NOT WORK YET AND IS WHAT THIS PROJECT ASPIRES TO**

Include the `ModelContextProtocol::Thor` in a Thor command-line application to enable model context protocol for the CLI:

```ruby
class MyCLI < Thor
  # Enables MCP for the CLI
  include ModelContextProtocol::Thor

  desc "hello GREETING", "Say hello"
  def hello(name)
    puts "Hello #{name}"
  end
end
```

Then boot the model context server via:

```ruby
MyCLI.mcp.start
```

This boots a model context protocol server that connects to your tools that support MCP.

### Terminalwire

If you'd like to integrate your Rails SaaS to AI development tools, you maybe install Terminalwire Rails server and include it in the CLI.

```ruby
class MyCLI < Thor
  # Enable streaming from the server to the Terminalwire thin client.
  include Terminalwire::Thor

  # Enables MCP for the CLI
  include ModelContextProtocol::Thor

  desc "hello GREETING", "Say hello"
  def hello(name)
    puts "Hello #{name}"
  end
end
```

You'll then need to [create a Terminalwire distribution](https://terminalwire.com/docs/rails/distribution) and provide your users with the one-liner curl installer:

```sh
$ curl https://my-app.terminalwire.sh | bash
```

Your users then boot the MCP server via `my-app mcp` to integrate with their MCP client.

## Development

After checking out the repo, run `bin/setup` to install dependencies. Then, run `rake spec` to run the tests. You can also run `bin/console` for an interactive prompt that will allow you to experiment.

To install this gem onto your local machine, run `bundle exec rake install`. To release a new version, update the version number in `version.rb`, and then run `bundle exec rake release`, which will create a git tag for the version, push git commits and the created tag, and push the `.gem` file to [rubygems.org](https://rubygems.org).

## Contributing

Bug reports and pull requests are welcome on GitHub at https://github.com/[USERNAME]/modelcontextprotocol. This project is intended to be a safe, welcoming space for collaboration, and contributors are expected to adhere to the [code of conduct](https://github.com/[USERNAME]/modelcontextprotocol/blob/main/CODE_OF_CONDUCT.md).

## License

The gem is available as open source under the terms of the [MIT License](https://opensource.org/licenses/MIT).

## Code of Conduct

Everyone interacting in the ModelContextProtocol project's codebases, issue trackers, chat rooms and mailing lists is expected to follow the [code of conduct](https://github.com/[USERNAME]/modelcontextprotocol/blob/main/CODE_OF_CONDUCT.md).
', 'true', '"{\"Protocol Implementations\",\"CLI Tools\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states that it's a Ruby implementation of the Model Context Protocol, aiming to enable MCP for Thor command-line applications.","categories":["Protocol Implementations","CLI Tools","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The architecture centers around a Ruby gem extending the Thor CLI framework with MCP capabilities. It adopts an include pattern, allowing existing Thor-based CLIs to integrate MCP functionality. The design anticipates a server component that interfaces with tools supporting the Model Context Protocol.","Implementation Details and Key Features":"The core implementation revolves around providing a `ModelContextProtocol::Thor` module that can be included in Thor CLI applications. The project aspires to boot a model context protocol server. Integration with Terminalwire for Rails SaaS applications is another key feature, enabling streaming from the server to a thin client.","Integration Approaches and Protocols":"The repository integrates with Thor CLI applications using Ruby's `include` mechanism. It specifies Terminalwire integration through a similar inclusion pattern, implying a dependency on Terminalwire's server component. The communication protocol with MCP clients isn't explicitly detailed beyond initiating a server.","Technical Stack and Dependencies":"The technical stack primarily involves Ruby, the Thor gem for CLI development, and potentially Terminalwire for Rails integration. Dependencies are managed via Bundler. The code interacts with external tools that support the Model Context Protocol, suggesting a reliance on a shared protocol specification.","Best Practices and Coding Standards":"The repository includes a `CODE_OF_CONDUCT.md` file indicating an intention to maintain a safe and welcoming collaboration environment. Standard Ruby practices such as using Bundler for dependency management and Rake for tasks (testing, building, releasing) are in use.","Performance Considerations":"Performance considerations are not explicitly addressed in the provided documentation. However, the integration with Terminalwire suggests a focus on streaming data, which implies a concern for efficient data transfer. The project aims for connecting with external tools, making efficient protocol implementation a key aspect."}}', '2025-03-10 21:28:34+00', 'true', 'Servers', 'ruby-mcp-context'), ('944155585', 'base-mcp', 'base/base-mcp', 'https://github.com/base/base-mcp', null, '2025-03-06 21:57:34+00', '2025-03-10 18:10:11+00', 'TypeScript', '14', '# Base MCP Server

[![npm version](https://img.shields.io/npm/v/base-mcp.svg)](https://www.npmjs.com/package/base-mcp)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A Model Context Protocol (MCP) server that provides onchain tools for Claude AI, allowing it to interact with the Base blockchain and Coinbase API.

## Overview

This MCP server extends Claude's capabilities by providing tools to:

- Retrieve wallet addresses
- Get testnet ETH (on Base Sepolia)
- List wallet balances
- Transfer funds between wallets
- Deploy smart contracts

The server uses the Coinbase SDK to interact with the Base blockchain and Coinbase services.

## Prerequisites

- Node.js (v16 or higher)
- npm or yarn
- Coinbase API credentials (API Key Name and Private Key)
- A wallet seed phrase

## Installation

### Option 1: Install from npm (Recommended)

```bash
# Install globally
npm install -g base-mcp

# Or install locally in your project
npm install base-mcp
```

### Option 2: Install from Source

1. Clone this repository:

   ```bash
   git clone https://github.com/base/base-mcp.git
   cd base-mcp
   ```

2. Install dependencies:

   ```bash
   npm install
   ```

3. Build the project:

   ```bash
   npm run build
   ```

4. Optionally, link it globally:
   ```bash
   npm link
   ```

## Configuration

Create a `.env` file with your credentials:

```
# Coinbase API credentials
# You can obtain these from the Coinbase Developer Portal: https://cdp.coinbase.com/
COINBASE_API_KEY_NAME=your_api_key_name
COINBASE_API_PRIVATE_KEY=your_private_key

# Wallet seed phrase (12 or 24 words)
# This is the mnemonic phrase for your wallet
SEED_PHRASE=your seed phrase here
```

## Testing

Test the MCP server to verify it's working correctly:

```bash
npm test
```

This script will verify that your MCP server is working correctly by testing the connection and available tools.

## Examples

See the [examples.md](examples.md) file for detailed examples of how to interact with the Base MCP tools through Claude.

## Integration with Claude Desktop

To add this MCP server to Claude Desktop:

1. Create or edit the Claude Desktop configuration file at:

   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`
   - Windows: `%APPDATA%\Claude\claude_desktop_config.json`
   - Linux: `~/.config/Claude/claude_desktop_config.json`

2. Add the following configuration:

   ```json
   {
     "mcpServers": {
       "base-mcp": {
         "command": "node",
         "args": ["/path/to/base-mcp/build/index.js"],
         "env": {
           "COINBASE_API_KEY_NAME": "your_api_key_name",
           "COINBASE_API_PRIVATE_KEY": "your_private_key",
           "SEED_PHRASE": "your seed phrase here"
         },
         "disabled": false,
         "autoApprove": []
       }
     }
   }
   ```

3. Restart Claude Desktop for the changes to take effect.

## Available Tools

### get-address

Retrieves the address for your wallet.

Example query to Claude:

> "What's my wallet address?"

### get-testnet-eth

Gets testnet ETH for your wallet. This can only be called on the Base Sepolia network.

Example query to Claude:

> "Can you get me some testnet ETH for my wallet?"

### list-balances

Lists all balances for your wallet.

Example query to Claude:

> "Show me my wallet balances."

### transfer-funds

Transfers funds from your wallet to another address.

Parameters:

- `destination`: The address to which to transfer funds
- `assetId`: The asset ID to transfer
- `amount`: The amount of funds to transfer

Example query to Claude:

> "Transfer 0.01 ETH to 0x1234567890abcdef1234567890abcdef12345678."

### deploy-contract

Deploys a smart contract to the blockchain.

Parameters:

- `constructorArgs`: The arguments for the contract constructor
- `contractName`: The name of the contract to deploy
- `solidityInputJson`: The JSON input for the Solidity compiler containing contract source and settings
- `solidityVersion`: The version of the solidity compiler

Example query to Claude:

> "Deploy a simple ERC20 token contract for me."

## Security Considerations

- The configuration file contains sensitive information (API keys and seed phrases). Ensure it's properly secured and not shared.
- Consider using environment variables or a secure credential manager instead of hardcoding sensitive information.
- Be cautious when transferring funds or deploying contracts, as these operations are irreversible on the blockchain.

## Troubleshooting

If you encounter issues:

1. Check that your Coinbase API credentials are correct
2. Verify that your seed phrase is valid
3. Ensure you're on the correct network (Base Sepolia for testnet operations)
4. Check the Claude Desktop logs for any error messages

## License

[MIT License](LICENSE)

## Making Your MCP Discoverable

To make your MCP server discoverable by other developers, follow these steps:

### 1. Publish to npm

```bash
# Login to npm (you'll need an npm account)
npm login

# Publish the package
npm publish
```

This will make your MCP server available on the npm registry, allowing other developers to install it using `npm install base-mcp`.

### 2. Share Your GitHub Repository

Make sure your GitHub repository is public and well-documented. Add the following to enhance discoverability:

- A detailed README (like this one)
- Examples of usage
- Contributing guidelines
- Issue templates

### 3. Add to the MCP Directory

The Model Context Protocol community maintains a directory of available MCP servers. Submit your MCP server to be included in this directory by running:

```bash
npm run submit
```

This script will:

1. Check if your package is published to npm
2. Verify your GitHub repository information
3. Generate a submission file with all the necessary information
4. Guide you through the submission process

Alternatively, you can manually submit by:

1. Visiting the [MCP Directory Repository](https://github.com/modelcontextprotocol/directory)
2. Following the contribution guidelines to add your MCP server

### 4. Promote in Relevant Communities

Share your MCP server in communities where developers using Claude might be active:

- Anthropic Developer Discord
- Claude subreddit
- AI/ML developer forums
- Blockchain and Web3 communities

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

Please make sure your code follows the existing style and includes appropriate tests.
', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"Agent Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements a Model Context Protocol (MCP) server. It's designed to extend Claude's capabilities by providing tools to interact with the Base blockchain and Coinbase API, fitting the MCP definition.","categories":["Protocol Implementations","Function Calling","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The repository implements a server that acts as an intermediary between the Claude AI model and the Base blockchain. It likely follows a client-server architecture where Claude Desktop is the client and `base-mcp` is the server, communicating via the Model Context Protocol. The server exposes a set of tools that Claude can invoke via function calling.","Implementation Details and Key Features":"The implementation focuses on providing Claude with tools for blockchain interactions, including retrieving wallet addresses, getting testnet ETH, listing balances, transferring funds, and deploying smart contracts. It utilizes the Coinbase SDK to interact with the Base blockchain. Key features include configuration via environment variables and detailed examples for integration with Claude Desktop.","Integration Approaches and Protocols":"The repository is designed for integration with Claude Desktop via the Model Context Protocol. It uses a `claude_desktop_config.json` file to define the command and arguments for launching the server. The communication protocol relies on Claude invoking specific tools exposed by the MCP server using JSON payloads. Examples illustrate this interaction.","Technical Stack and Dependencies":"The technical stack includes Node.js, npm, and the Coinbase SDK. Dependencies likely involve libraries for blockchain interaction, API communication, and possibly cryptographic operations. The server is written in JavaScript/TypeScript and built using standard Node.js tools. Configuration relies on environment variables defined in a `.env` file.","Best Practices and Coding Standards":"The repository demonstrates some best practices, such as using environment variables for sensitive credentials and providing detailed documentation. It includes a testing script for verifying server functionality. The documentation also mentions security considerations, such as securing the configuration file and being cautious with blockchain operations. A contributing guide encourages code style consistency.","Performance Considerations":"Performance considerations are not explicitly mentioned. However, blockchain interactions can be slow, and the MCP server likely handles asynchronous operations. The design might benefit from caching frequently accessed data, such as wallet addresses and balances. Optimizing the execution of blockchain transactions and API calls would improve the user experience."}}', '2025-03-10 21:28:27+00', 'true', 'Servers', 'base-mcp-servers'), ('944163565', 'artemis-mcp', 'Artemis-xyz/artemis-mcp', 'https://github.com/Artemis-xyz/artemis-mcp', 'Artemis MCP Server Repo', '2025-03-06 22:21:40+00', '2025-03-10 18:09:52+00', 'Python', '0', '## artemis-mcp

```
     _    ____ _____ _____ __  __ ___ ____    __  __  ____ ____
    / \  |  _ \_   _| ____|  \/  |_ _/ ___|  |  \/  |/ ___|  _ \
   / _ \ | |_) || | |  _| | |\/| || |\___ \  | |\/| | |   | |_) |
  / ___ \|  _ < | | | |___| |  | || | ___) | | |  | | |___|  __/
 /_/   \_\_| \_\|_| |_____|_|  |_|___|____/  |_|  |_|\____|_|
```

## Prerequisites

- Python 3.11+
- [uv](https://docs.astral.sh/uv/getting-started/installation/#installation-methods). On MacOS, you can install it using Homebrew: `brew install uv`.
- Claude Desktop
- Artemis API Key
- Artemis Snowflake Login

## Installation

### Automated Using OpenTools (Easier)

Install [OpenTools](https://opentools.com/docs/quickstart) prerequisites.

Then run:

```bash
npx opentools@latest i artemis
```

### Manual Setup (More Work)

- Install uv

Copy `sample_claude_desktop_config.json` to your `claude_desktop_config.json` file:

- On MacOS: `~/Library/Application\ Support/Claude/claude_desktop_config.json`
- On Windows: `%APPDATA%/Claude/claude_desktop_config.json`

Fill out the relevant fields in `claude_desktop_config.json`:

- `<UV_PATH>` - Path to uv executable (run `where python` on MacOS)
- `<PATH_TO_ARTEMIS_MCP_REPO>` - Path to local clone of the Artemis MCP repo
- `<ARTEMIS_API_KEY>` - Your Artemis API Key
- `<SNOWFLAKE_USER>` - Your Artemis Snowflake email. User must have access to either `PC_DBT_ROLE` or `READ_ONLY_ROLE`

- Restart Claude Desktop
- Artemis MCP Tools should now be available!
', 'true', '"{\"Function Calling\",\"API Frameworks\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository name \"artemis-mcp\" and the description \"Artemis MCP Server Repo\" suggest an implementation related to Model Context Protocol. The documentation also provides setup instructions for using Artemis MCP tools with Claude Desktop.","categories":["Agent Frameworks","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The repository appears to be structured as a server providing MCP functionality. The documentation suggests a client-server model, where Claude Desktop acts as the client and Artemis MCP as the server. The architecture likely involves defining interfaces and protocols for communication between the client and server components.","Implementation Details and Key Features":"The implementation depends on Python 3.11+ and the `uv` package manager. A key feature is integration with Claude Desktop, requiring configuration with paths and API keys. The repository uses a configuration file (`claude_desktop_config.json`) to specify necessary paths and credentials. Artemis Snowflake integration hints at database interaction.","Integration Approaches and Protocols":"Integration is primarily achieved through the `claude_desktop_config.json` file, linking the Artemis MCP repository to Claude Desktop. The use of an Artemis API key suggests a RESTful or similar API-based communication protocol. Snowflake integration points to database connectivity, possibly using standard database drivers.","Technical Stack and Dependencies":"The primary programming language is Python, with a dependency on `uv` for package management. The integration with Claude Desktop necessitates familiarity with its internal workings. Additionally, the Artemis API key indicates reliance on external Artemis services. Snowflake is used as a database.","Best Practices and Coding Standards":"The documentation provides setup instructions, but it's limited in detailing coding standards. The use of `sample_claude_desktop_config.json` for configuration suggests a best practice for managing configurations. The dependency on specific Python versions indicates attention to compatibility. Further analysis of code is needed to assess coding practices.","Performance Considerations":"Performance considerations are not explicitly mentioned in the provided documentation. However, factors such as network latency, API call limits, and database query optimization are likely relevant given the client-server architecture. Resource constraints of the Claude Desktop environment may also influence performance."}}', '2025-03-10 21:26:08+00', 'true', 'Servers', 'artemis-mcp-server'), ('944176985', 'pydantic-mcp', 'rectalogic/pydantic-mcp', 'https://github.com/rectalogic/pydantic-mcp', 'Model Context Protocol tool calling support for Pydantic AI', '2025-03-06 23:02:49+00', '2025-03-10 13:09:28+00', 'Python', '5', '# pydantic-mcp

![PyPI - Version](https://img.shields.io/pypi/v/pydantic-mcp)

[Model Context Protocol](https://modelcontextprotocol.io) tool calling support for [Pydantic AI](https://ai.pydantic.dev/).

Just create an `mcp.ClientSession` and call `tools = await mcptools(session)`
to get a list of `pydantic_ai.Tool` instances for the supported tools.

Example:

https://github.com/rectalogic/pydantic-mcp/blob/98c4e9abf31837cb48ebeb3eae6cb55b16c841e8/tests/demo.py#L15-L25

## Demo

You can run the demo against [Groq](https://groq.com/) `llama-3.1-8b-instant`:
```sh-session
$ export GROQ_API_KEY=xxx
$ uv run tests/demo.py "Read and summarize the file ./LICENSE"
Secure MCP Filesystem Server running on stdio
Allowed directories: [ '/users/aw/projects/rectalogic/pydantic-mcp' ]
The file ./LICENSE is a MIT License agreement. It states that the software is provided "as is" without warranty and that the authors and copyright holders are not liable for any claims, damages, or other liability arising from the software or its use.
```
', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it provides Model Context Protocol tool calling support for Pydantic AI, indicating implementation or integration with MCP specifications.","categories":["AI Model Integration","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The repository uses Pydantic AI to define tools and interacts with a Model Context Protocol server. It seems to follow a client-server architecture where Pydantic-mcp acts as the client, retrieving tool definitions from an MCP server and presenting them as Pydantic AI tools. The design emphasizes ease of integration with Pydantic AI-based applications.","Implementation Details and Key Features":"The core functionality revolves around the `mcp.ClientSession` which facilitates communication with the MCP server. The `mcptools` function likely fetches tool specifications from the server and transforms them into `pydantic_ai.Tool` instances. A key feature is its ability to securely interact with a file system, as demonstrated by the example using the Secure MCP Filesystem Server.","Integration Approaches and Protocols":"The primary integration approach involves using the `pydantic-mcp` library within a Pydantic AI-based application. The library handles the communication with the MCP server, abstracting away the complexities of the protocol. The integration is facilitated through the `mcptools` function, which provides a simple way to access available tools.","Technical Stack and Dependencies":"The repository relies on Pydantic AI for defining and using tools. It also depends on an MCP server, as indicated by the need for a `ClientSession`. The demo showcases usage with Groq's llama-3.1-8b-instant, suggesting compatibility with various LLMs. Additional dependencies might include libraries for handling network communication.","Best Practices and Coding Standards":"The repository includes tests and a demo, indicating attention to code quality and maintainability. The use of type hints in Pydantic AI helps ensure code correctness. The security aspect, such as the allowed directories, suggests an awareness of security best practices when dealing with external tools and file systems.","Performance Considerations":"Performance considerations would likely revolve around the network communication with the MCP server and the complexity of the tool definitions. The library might benefit from caching tool definitions to reduce latency. The demo suggests that the execution speed also relies on the underlying LLM being used, such as Groq's llama-3.1-8b-instant."}}', '2025-03-10 21:54:04+00', 'true', 'Servers', 'pydantic-mcp-tooling'), ('944204539', 'eth-mcp', '0xKoda/eth-mcp', 'https://github.com/0xKoda/eth-mcp', 'an MCP for querying the EVM ', '2025-03-07 00:38:20+00', '2025-03-10 00:57:45+00', 'JavaScript', '3', '# Ethereum RPC MCP Server

A Model Context Protocol (MCP) server for interacting with Ethereum blockchain.

## Overview

This MCP server provides tools to query Ethereum blockchain data through standard JSON-RPC methods. It enables AI assistants and applications to interact with the Ethereum blockchain through a standardized protocol.

## Features

This MCP server provides three key Ethereum RPC methods as tools:

- **eth_getCode**: Retrieve the code at a specific Ethereum address
- **eth_gasPrice**: Get the current gas price on the Ethereum network
- **eth_getBalance**: Check the balance of an Ethereum account

Note: More are coming

## Usage

### Adding to Cursor

To add this MCP to Cursor:

1. First, clone this repository:
   ```bash
   git clone https://github.com/yourusername/eth-mpc.git
   ```
   
2. Go to Cursor settings → MCP → Add new MCP server
3. Enter a name (e.g., "eth-mcp")
4. Select "command" as the type
5. Input the full path to the script:
   ```
   node /path/to/eth-mpc/index.js
   ```

![Adding Ethereum MCP to Cursor](image.png)

6. Click "Add" to enable the server

Once added, the Ethereum RPC tools will be available to use within Cursor.


The server uses stdio transport, making it compatible with MCP clients like Claude Desktop, Cursor, and others.

## Testing with MCP Inspector

The MCP Inspector is a development tool for testing and debugging MCP servers. It provides an interactive interface to test your MCP server's functionality without needing a full AI client.

### Running the Inspector

To test your Ethereum RPC MCP server with the Inspector:

To run the Inspector:
   ```bash
   npx @modelcontextprotocol/inspector
   ```

2. Input the command and path

3. The Inspector will connect to your running MCP server and display available tools.

### Testing Tools with Inspector

The Inspector allows you to:

- View available tools and their descriptions
- Test each tool with different parameters
- See the responses in a structured format
- Debug any issues with your MCP server implementation

For example, to test the `eth_getBalance` tool:
1. Select the tool in the Inspector interface
2. Enter a valid Ethereum address (e.g., `0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045` - Vitalik's address)
3. Use the default block parameter (`latest`)
4. Submit the request and view the response


## Integration with MCP Clients

This MCP server can be integrated with any MCP-compatible client, including:

- Claude Desktop 
- Claude Code
- Cursor (instructions above)
- Cline
- Other MCP-compatible applications

When integrated, the client application can use the tools provided by this server to query Ethereum blockchain data directly.

## Understanding MCP

Model Context Protocol (MCP) is an open standard that allows AI models to interact with various tools and services. It provides a standardized way for developers to expose APIs, data sources, and functionality to AI assistants.

### Learn More About MCP

MCP servers like this one form part of an ecosystem that allows AI assistants to perform complex tasks across multiple services without requiring custom integration for each service.

📚 **Official Documentation**: [Model Context Protocol Overview](https://modelcontextprotocol.io/sdk/java/mcp-overview)

## License

MIT

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements a Model Context Protocol (MCP) server for interacting with the Ethereum blockchain, aligning with MCP's core function of providing tools for AI models.","categories":["Protocol Implementations","AI Model Integration","Function Calling"],"analysis":{"Architecture and Design Patterns":"The repository implements a server exposing Ethereum RPC methods via the Model Context Protocol. It leverages a standardized communication approach between AI models and external services. The design centers around providing specific Ethereum querying tools to AI clients, enabling them to access blockchain data.  The use of stdio transport facilitates compatibility with various MCP clients.","Implementation Details and Key Features":"The server provides tools for querying Ethereum data, including `eth_getCode`, `eth_gasPrice`, and `eth_getBalance`. It's designed to be integrated with MCP-compatible clients. The implementation allows AI assistants to access real-time blockchain information.  The documentation focuses on ease of integration, especially with Cursor, and debugging via the MCP Inspector.","Integration Approaches and Protocols":"The repository utilizes the Model Context Protocol for integration. It supports clients like Claude Desktop, Cursor, and Cline through stdio transport. The MCP Inspector integration allows for easy testing and debugging of the server's functionality.  The project aims to provide a standardized interface between AI models and Ethereum data.","Technical Stack and Dependencies":"The project appears to be implemented using Node.js. The repository includes instructions to run the server using `node index.js`.  It also depends on the Model Context Protocol Inspector (`@modelcontextprotocol/inspector`) for testing. Further dependencies are not explicitly detailed in the provided text, but likely include libraries for interacting with Ethereum nodes.","Best Practices and Coding Standards":"The documentation emphasizes ease of use and integration, suggesting a focus on developer experience. The MIT license and encouraging contributions via pull requests indicates an openness to community standards.  The inclusion of testing tools like the MCP Inspector highlights a commitment to quality and reliability.","Performance Considerations":"Performance considerations are not explicitly discussed in the provided documentation. However, the use of specific RPC methods suggests an awareness of efficient data retrieval. The choice of Node.js may introduce performance trade-offs, but the scalability of the server would likely depend on factors like network latency and Ethereum node capabilities."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'eth-evm-mcp-query'), ('944208977', 'mcp-tool-kit', 'getfounded/mcp-tool-kit', 'https://github.com/getfounded/mcp-tool-kit', 'Microsoft sponsored. Best way to build vertical agents. Server and Tool kit for Model Context Protocol server written in python. Run multiple tools on a single server. American based.', '2025-03-07 00:53:02+00', '2025-03-10 18:11:16+00', 'Python', '25', '# MCP Tool Kit

A modular server implementation for Claude AI assistants with a variety of integrated tools, enabling Claude to perform actions and access external resources.

[![PyPI version](https://img.shields.io/pypi/v/mcptoolkit.svg)](https://pypi.org/project/mcptoolkit/)
[![Python versions](https://img.shields.io/pypi/pyversions/mcptoolkit.svg)](https://pypi.org/project/mcptoolkit/)
[![License](https://img.shields.io/github/license/getfounded/mcp-tool-kit.svg)](https://github.com/getfounded/mcp-tool-kit/blob/main/LICENSE)

## MCP Connect | MCP Cloud Tool Kit (Coming Soon)
Connect with a simple api key to access all of the tools. 

## Quick Install

```bash
pip install mcptoolkit
```

## Overview

The MCP Unified Server provides a unified interface for Claude to interact with various external systems and tools including:

- **File system operations**: Read, write, and manipulate files
- **Time tools**: Get current time in different timezones, convert between timezones
- **Sequential thinking**: A tool for dynamic and reflective problem-solving
- **Brave Search**: Web and local search capabilities
- **Browser automation**: Complete browser control via Browserbase and Playwright
- **World Bank API**: Access to economic and development data
- **News API**: Access to global news sources and articles
- **PowerPoint**: Create and manipulate PowerPoint presentations
- **Excel**: Create and manipulate Excel spreadsheets
- **QuickBooks**: Financial and accounting operations
- **Shopify**: E-commerce platform integration
- **Yahoo Finance**: Stock market and financial data
- **FRED**: Federal Reserve Economic Data
- **And many more specialized tools**

# Building Custom Tools for Claude with MCP Toolkit

This guide demonstrates how to create custom tools that Claude can use via the Model Context Protocol (MCP) toolkit.

## Table of Contents
- [Overview](#overview)
- [Prerequisites](#prerequisites)
- [Step-by-Step Implementation Guide](#step-by-step-implementation-guide)
- [Example: Creating a Custom SEO Analysis Tool](#example-creating-a-custom-seo-analysis-tool)
- [Usage with Claude](#usage-with-claude)
- [Troubleshooting](#troubleshooting)

## Overview

The MCP (Model Context Protocol) toolkit allows you to create custom tools that Claude can access and use. This enables Claude to perform specialized actions beyond its built-in capabilities, such as interacting with your specific business systems, analyzing data with custom algorithms, or controlling specialized hardware.

## Prerequisites

- MCP toolkit installed (`pip install mcptoolkit` or Docker setup)
- Access to Claude Desktop app
- Basic Python knowledge
- Docker (recommended)

### Get Started ASAP | Running the Server
Caution: This will grant claude access to every tool without limitation in the main branch of this repository.

```bash
# Method 1: Using Docker (recommended)
docker run -p 8000:8000 -v ~/documents:/app/documents getfounded/mcp-tool-kit:latest

# Method 2: Using docker-compose
docker-compose up

# Method 3: Using command-line entry point (if installed via pip)
mcptoolkit-server

# Method 4: Launch both server and configuration UI
mcptoolkit-launcher
```

```python
# Method 5: Using Python module
from mcptoolkit import mcp_unified_server

# Create and run the server with default settings
server = mcp_unified_server.create_server()
server.start()
```

## Step-by-Step Implementation Guide

### Step 1: Set Up Your MCP Server with the Toolkit

Start by running the MCP server:

```bash
# Using Docker (recommended)
docker run -p 8000:8000 -v ~/documents:/app/documents -v ~/downloads:/app/downloads getfounded/mcp-tool-kit:latest

# Or if installed via pip
mcptoolkit-server
```

### Step 2: Create Your Custom Tool

Create a new Python file in the tools directory of the MCP toolkit (e.g., `my_custom_tool.py`):

```python
from mcp import tools

# Store reference to external MCP to be set from mcp_unified_server.py
external_mcp = None

def set_external_mcp(mcp):
    global external_mcp
    external_mcp = mcp

# Create your service class
class MyCustomToolService:
    def __init__(self):
        # Initialize any resources or connections here
        self.some_resource = {}
    
    # Define your tool functionality
    def perform_custom_action(self, parameter1, parameter2):
        # Implement your custom functionality
        result = f"Processed {parameter1} with {parameter2}"
        return result

# Create a service instance
service = MyCustomToolService()

# Define tool wrapper functions
def my_custom_tool(params):
    """
    Performs a custom action.
    
    params:
        parameter1: First parameter description
        parameter2: Second parameter description
    """
    parameter1 = params.get("parameter1", "default")
    parameter2 = params.get("parameter2", "default")
    
    return service.perform_custom_action(parameter1, parameter2)

# Register your tools with MCP
def register_tools(mcp):
    set_external_mcp(mcp)
    mcp.register_tool("my_custom_tool", my_custom_tool)
```

### Step 3: Register Your Tool with the MCP Server

Modify the main MCP server file to import and register your tool:

```python
# Option 1: Update mcp_unified_server.py
from tools import my_custom_tool

# Add to the list of tools to register
tools_modules = [
    # existing tools...
    my_custom_tool,
]

# Option 2: Update tools/__init__.py to include your tool in register_all_tools
def register_all_tools(server):
    # existing tool registrations...
    my_custom_tool.register_tools(server)
```

### Step 4: Configure Claude Desktop to Access Your Tool

1. Open Claude Desktop app
2. Go to File > Settings > Developer > Edit config
3. Add the 'claude_desktop_configuration.json' file
4. Save the configuration
5. Restart the MCP server with your new tool integrated
6. Restart and Open Claude Desktop app

## Example: Claude Desktop Configuration

The repository includes a sample Claude desktop configuration file (`claude_desktop_config.json`) that you can use:

```json
{
  "tools": [
    {
      "name": "MCP Toolkit",
      "url": "http://localhost:8000"
    }
  ],
  "settings": {
    "allowed_directories": ["~/Documents", "~/Downloads"],
    "default_tools": ["MCP Toolkit"]
  }
}
```

You can import this configuration in the Claude desktop app or use it as a reference to create your own.

### Step 5: Create Prompts to Help Claude Use Your Tool

Create example prompts for Claude that demonstrate how to use your custom tool:

```
Claude, I've created a custom tool that can perform [specific action]. 
Here's an example of how to use it:

client.call_tool("my_custom_tool", {
    "parameter1": "value1",
    "parameter2": "value2"
})

Could you please use this tool to [describe what you want Claude to do]?
```

## Example: Creating a Custom SEO Analysis Tool

Here's a complete example of creating an SEO analysis tool:

```python
# seo_tool.py
from mcp import tools
import requests
from bs4 import BeautifulSoup

external_mcp = None

def set_external_mcp(mcp):
    global external_mcp
    external_mcp = mcp

class SEOAnalyzer:
    def analyze_url(self, url):
        try:
            # Fetch the page
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Extract SEO elements
            title = soup.title.string if soup.title else ""
            meta_desc = soup.find("meta", {"name": "description"})
            meta_desc = meta_desc["content"] if meta_desc else ""
            
            # Count headings
            h1_count = len(soup.find_all('h1'))
            h2_count = len(soup.find_all('h2'))
            
            # Basic analysis
            result = {
                "title": title,
                "title_length": len(title),
                "meta_description": meta_desc,
                "meta_description_length": len(meta_desc),
                "h1_count": h1_count,
                "h2_count": h2_count,
                "issues": []
            }
            
            # Identify issues
            if len(title) < 30 or len(title) > 60:
                result["issues"].append("Title length not optimal (should be 30-60 chars)")
            
            if len(meta_desc) < 120 or len(meta_desc) > 160:
                result["issues"].append("Meta description length not optimal (should be 120-160 chars)")
            
            if h1_count != 1:
                result["issues"].append(f"Page has {h1_count} H1 tags (should have exactly 1)")
            
            return result
            
        except Exception as e:
            return {"error": str(e)}

service = SEOAnalyzer()

def analyze_seo(params):
    """
    Analyzes a URL for basic SEO factors.
    
    params:
        url: The URL to analyze
    """
    url = params.get("url", "")
    if not url:
        return {"error": "URL parameter is required"}
    
    return service.analyze_url(url)

def register_tools(mcp):
    set_external_mcp(mcp)
    mcp.register_tool("analyze_seo", analyze_seo)
```

## Usage with Claude

Once your custom tool is integrated, you can ask Claude to use it:

### Example Prompt for SEO Tool
```
Claude, I've added an SEO analysis tool to your toolkit. Can you please analyze the SEO for my website at https://example.com and provide recommendations for improvement?

The tool can be accessed using:
client.call_tool("analyze_seo", {"url": "https://example.com"})
```

### Example Prompts for Other Custom Tools

1. **Custom Database Tool**:
   ```
   Claude, I've created a tool that can query our product database. Can you use the tool to find all products in the "electronics" category priced under $100?
   
   The tool can be accessed using:
   client.call_tool("query_database", {"category": "electronics", "max_price": 100})
   ```

2. **API Integration Tool**:
   ```
   Claude, I've set up a weather API tool. Can you check the weather forecast for New York for the next 3 days?
   
   The tool can be accessed using:
   client.call_tool("get_weather", {"location": "New York", "days": 3})
   ```

## Troubleshooting

- **Tool Not Found**: Ensure your tool is properly registered and the server is restarted
- **Parameter Errors**: Check that all required parameters are being passed correctly
- **Connection Issues**: Verify the MCP server is running and Claude is properly configured to connect to it
- **Import Errors**: Make sure all dependencies for your custom tool are installed in the environment where the MCP server runs

---

## Other Usage Examples



### Example 2: Practical Tool Examples

```python
from mcp.client import MCPClient

# Connect to the MCP server
client = MCPClient("http://localhost:8000")

# --- File System Operations ---
# Read file content
readme = client.call_tool("read_file", {"path": "README.md"})

# Write to a file
client.call_tool("write_file", {
    "path": "analysis_results.txt", 
    "content": "# Analysis Results\n\nThis file contains the output of our data analysis."
})

# --- Web Search and Information Retrieval ---
# Search the web
search_results = client.call_tool("brave_web_search", {"query": "latest AI research papers"})

# Get news headlines
news = client.call_tool("news_top_headlines", {"category": "technology", "page_size": 5})

# --- Data Analysis and Visualization ---
# Analyze stock market data
stock_data = client.call_tool("yfinance", {"symbol": "MSFT", "period": "1mo"})

# --- Document Generation ---
# Create a PowerPoint presentation
client.call_tool("ppt_create_presentation", {"session_id": "quarterly_report"})
client.call_tool("ppt_add_slide", {"session_id": "quarterly_report", "title": "Q3 Financial Results"})
client.call_tool("ppt_add_chart", {
    "session_id": "quarterly_report",
    "slide_index": 1,
    "chart_type": "bar",
    "chart_title": "Revenue by Department",
    "categories": ["Marketing", "Sales", "R&D", "Support"],
    "series_names": ["Q3 2024"],
    "series_values": [[125000, 240000, 175000, 98000]]
})
client.call_tool("ppt_save_presentation", {"session_id": "quarterly_report"})

# --- Browser Automation ---
# Create browser session and navigate
session_id = client.call_tool("browserbase_create_session", {"sessionId": "browser1"})
client.call_tool("browserbase_navigate", {"sessionId": "browser1", "url": "https://example.com"})
content = client.call_tool("browserbase_get_content", {"sessionId": "browser1"})
client.call_tool("browserbase_close_session", {"sessionId": "browser1"})

# --- Advanced Problem-Solving ---
# Use sequential thinking to break down a complex problem
client.call_tool("sequentialthinking", {
    "thought": "First, we need to identify the key variables in this optimization problem",
    "thoughtNumber": 1,
    "totalThoughts": 5,
    "nextThoughtNeeded": True
})
```

### Example 3: Building a Complete Workflow

```python
from mcp.client import MCPClient
import json

# Connect to the MCP server
client = MCPClient("http://localhost:8000")

# Scenario: Market research assistant that gathers data, analyzes it, and prepares a report

def run_market_research(company_name, market_sector):
    """Perform comprehensive market research using various MCP tools"""
    
    print(f"Beginning market research for {company_name} in the {market_sector} sector...")
    
    # 1. Gather information about the company and market
    company_search = client.call_tool("brave_web_search", {
        "query": f"{company_name} company profile financial information",
        "count": 5
    })
    
    market_news = client.call_tool("news_search", {
        "q": f"{market_sector} market trends analysis",
        "page_size": 10
    })
    
    # 2. Get economic indicators relevant to the sector
    if market_sector.lower() in ["tech", "technology"]:
        indicator = "GB.XPD.RSDV.GD.ZS"  # R&D expenditure
    elif market_sector.lower() in ["finance", "banking"]:
        indicator = "FM.LBL.BMNY.GD.ZS"  # Broad money to GDP
    else:
        indicator = "NY.GDP.MKTP.KD.ZG"  # GDP growth
        
    economic_data = client.call_tool("worldbank_get_indicator", {
        "country_id": "WLD",  # World
        "indicator_id": indicator
    })
    
    # 3. Create a report directory and save gathered information
    client.call_tool("create_directory", {"path": f"{company_name}_research"})
    
    client.call_tool("write_file", {
        "path": f"{company_name}_research/company_info.json",
        "content": json.dumps(company_search, indent=2)
    })
    
    client.call_tool("write_file", {
        "path": f"{company_name}_research/market_news.json",
        "content": json.dumps(market_news, indent=2)
    })
    
    client.call_tool("write_file", {
        "path": f"{company_name}_research/economic_indicators.json",
        "content": json.dumps(economic_data, indent=2)
    })
    
    # 4. Generate a PowerPoint presentation with the findings
    client.call_tool("ppt_create_presentation", {"session_id": "market_report"})
    
    # Add title slide
    client.call_tool("ppt_add_slide", {
        "session_id": "market_report",
        "title": f"{company_name}: Market Analysis",
        "content": f"An overview of {company_name} in the {market_sector} sector"
    })
    
    # Add company overview
    client.call_tool("ppt_add_slide", {
        "session_id": "market_report",
        "title": "Company Overview",
        "layout_index": 2
    })
    
    # Add market trends
    client.call_tool("ppt_add_slide", {
        "session_id": "market_report",
        "title": f"{market_sector} Market Trends",
        "layout_index": 2
    })
    
    # Add economic indicators chart
    client.call_tool("ppt_add_slide", {
        "session_id": "market_report",
        "title": "Economic Indicators",
        "layout_index": 5
    })
    
    # Save the presentation
    client.call_tool("ppt_save_presentation", {
        "session_id": "market_report", 
        "file_path": f"{company_name}_research/market_report.pptx"
    })
    
    print(f"Market research completed. Results saved to {company_name}_research/")
    return f"{company_name}_research/"

# Execute the research function
research_folder = run_market_research("Acme Corp", "technology")
```
### Sample Claude Prompts

Once set up, you can ask Claude to use the tools with prompts like:

- "Search the web for the latest AI research papers and summarize the findings."
- "Create a PowerPoint presentation about climate change with three slides."
- "Download my QuickBooks invoice data and analyze our revenue for the past quarter."
- "Set up a product on my Shopify store with these details and pricing."
- "Get the current stock price and historical data for Tesla using Yahoo Finance."
- "Analyze inflation trends using FRED economic data for the past 5 years."
- "Use browser automation to fill out this form at [website URL]."
- "Read the text file in my Downloads folder named 'project_notes.txt'."
- "Get the latest news headlines about technology."

## Architecture

The server is built with a modular architecture:

```
mcptoolkit/
├── __init__.py             # Package initialization
├── mcp_unified_server.py   # Main server implementation
├── config_loader.py        # Configuration loader
├── config_ui.py            # Web UI for configuration
├── launcher.py             # Helper script
├── ms_graph_auth.py        # Microsoft Graph authentication
└── tools/                  # Tool modules
    ├── __init__.py         # Package initialization
    ├── brave_search.py     # Brave Search API integration
    ├── browserbase.py      # Browserbase browser automation
    ├── filesystem.py       # File system operations
    ├── news_api.py         # News API integration
    ├── ppt.py              # PowerPoint tools
    ├── sequential_thinking.py # Sequential thinking tools
    ├── time_tools.py       # Time-related tools
    ├── worldbank.py        # World Bank API integration
    └── ... (other tools)
```

Each tool module follows a consistent pattern:
- External MCP reference
- Service class implementation
- Tool function definitions
- Registration functions

## Installation Methods

### Method 1: Docker (Recommended)

The easiest way to get started is using Docker, which requires no Python or dependency installation:

```bash
# Pull the Docker image
docker pull getfounded/mcp-tool-kit:latest

# Run the container
docker run -p 8000:8000 -v ~/documents:/app/documents -v ~/downloads:/app/downloads getfounded/mcp-tool-kit:latest
```

You can also use docker-compose:

```bash
# Using the provided docker-compose.yml
docker-compose up
```

This will:
- Start the server on port 8000
- Mount your local directories to make them accessible to the toolkit
- Configure the server with sensible defaults

### Method 2: Install from PyPI

```bash
pip install mcptoolkit
```

### Method 3: Install from Source

```bash
# Clone the repository
git clone https://github.com/getfounded/mcp-tool-kit.git
cd mcp-tool-kit

# Create a virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install in development mode
pip install -e .
```

## Configuration

### Docker Configuration

When running with Docker, you can configure the container using environment variables:

```bash
# Run with custom configuration
docker run -p 8000:8000 \
    -e BRAVE_API_KEY=your_brave_api_key \
    -e NEWS_API_KEY=your_news_api_key \
    -e MCP_LOG_LEVEL=debug \
    -v ~/documents:/app/documents \
    -v ~/downloads:/app/downloads \
    getfounded/mcp-tool-kit:latest
```

Alternatively, you can modify the `docker-compose.yml` file:

```yaml
version: '3'
services:
  mcp-server:
    image: getfounded/mcp-tool-kit:latest
    ports:
      - "8000:8000"
    environment:
      - BRAVE_API_KEY=your_brave_api_key
      - NEWS_API_KEY=your_news_api_key
      - MCP_LOG_LEVEL=info
    volumes:
      - ~/documents:/app/documents
      - ~/downloads:/app/downloads
```

### Local Configuration

If running locally, the server can be configured using environment variables or a `.env` file in the project root:

```env
# MCP Server Configuration
MCP_HOST=0.0.0.0
MCP_PORT=8000
MCP_LOG_LEVEL=info  # debug, info, warning, error

# Tool API Keys
BRAVE_API_KEY=your_brave_api_key
BROWSERBASE_API_KEY=your_browserbase_api_key
BROWSERBASE_PROJECT_ID=your_browserbase_project_id
NEWS_API_KEY=your_news_api_key

# File System Configuration
MCP_FILESYSTEM_DIRS=~/documents,~/downloads  # Comma-separated list of allowed directories
```

### Configuration UI

The package includes a web-based configuration UI:

```bash
# Run the configuration UI
mcptoolkit-config
```

Access the UI in your web browser at http://localhost:8501

## Available Tools

### File System Tools
- `read_file`: Read contents of a file
- `read_multiple_files`: Read multiple files simultaneously
- `write_file`: Create or overwrite a file
- `edit_file`: Make line-based edits to a file
- `create_directory`: Create a new directory
- `list_directory`: Get directory contents
- `directory_tree`: Get a recursive tree view
- `move_file`: Move or rename files/directories
- `search_files`: Search for files matching a pattern
- `get_file_info`: Get file metadata
- `list_allowed_directories`: List allowed directories

### Browser Automation Tools
- **Browserbase:**
  - `browserbase_create_session`: Create a new browser session
  - `browserbase_close_session`: Close a browser session
  - `browserbase_navigate`: Navigate to a URL
  - `browserbase_screenshot`: Take a screenshot
  - `browserbase_click`: Click an element
  - `browserbase_fill`: Fill a form field
  - `browserbase_evaluate`: Execute JavaScript
  - `browserbase_get_content`: Extract page content

- **Playwright:**
  - `playwright_launch_browser`: Launch a new browser instance
  - `playwright_navigate`: Navigate to a URL
  - `playwright_screenshot`: Take a screenshot
  - `playwright_click`: Click on an element
  - `playwright_fill`: Fill an input field
  - `playwright_evaluate`: Execute JavaScript
  - `playwright_get_content`: Get the HTML content of a page

### E-Commerce Tools
- **Shopify:**
  - `shopify_get_products`: Get product information
  - `shopify_create_product`: Create a new product
  - `shopify_update_product`: Update an existing product
  - `shopify_get_orders`: Get order information
  - `shopify_create_order`: Create a new order
  - `shopify_get_customers`: Get customer information

### Financial Tools
- **QuickBooks:**
  - `quickbooks_get_accounts`: Get account information
  - `quickbooks_get_invoices`: Get invoice information
  - `quickbooks_create_invoice`: Create an invoice
  - `quickbooks_get_customers`: Get customer information
  - `quickbooks_get_reports`: Generate financial reports

### Financial Data Tools
- **Yahoo Finance:**
  - `yfinance`: Get stock quotes and historical data
  - `yfinance_get_quote`: Get current stock quote
  - `yfinance_get_history`: Get historical stock data
  - `yfinance_get_info`: Get detailed company information
  - `yfinance_get_options`: Get options chain data
  - `yfinance_get_recommendations`: Get analyst recommendations

- **FRED (Federal Reserve Economic Data):**
  - `fred_get_series`: Get economic data series
  - `fred_get_series_info`: Get metadata about a series
  - `fred_search`: Search for economic data series
  - `fred_get_category`: Browse data by category
  - `fred_get_releases`: Get economic data releases
  - `fred_get_sources`: Get data sources

### Time Tools
- `get_current_time`: Get current time in a specified timezone
- `convert_time`: Convert time between timezones

### Sequential Thinking
- `sequentialthinking`: A tool for breaking down complex problems using a step-by-step thinking process

### Brave Search
- `brave_web_search`: Perform web searches
- `brave_local_search`: Search for local businesses and places

### World Bank API
- `worldbank_get_indicator`: Get indicator data for a country

### News API
- `news_top_headlines`: Get top news headlines
- `news_search`: Search for news articles
- `news_sources`: List available news sources

### PowerPoint Tools
- `ppt_create_presentation`: Create a new PowerPoint presentation
- `ppt_open_presentation`: Open an existing presentation
- `ppt_save_presentation`: Save a presentation
- `ppt_add_slide`: Add a new slide
- `ppt_add_text`: Add text to a slide
- `ppt_add_image`: Add an image to a slide
- `ppt_add_chart`: Add a chart to a slide
- `ppt_add_table`: Add a table to a slide
- `ppt_analyze_presentation`: Analyze presentation structure
- `ppt_enhance_presentation`: Suggest enhancements
- `ppt_generate_presentation`: Generate a presentation from text
- `ppt_command`: Process natural language commands

For a complete list of available tools, see the documentation or browse the tools directory.

## Development

### Adding a New Tool Module

1. Create a new file in the `tools` directory (e.g., `my_tool.py`)
2. Follow the existing module pattern:
   - Create service class
   - Define tool functions
   - Implement registration functions
3. Update `mcp_unified_server.py` to import and register your new module

### Extending an Existing Tool Module

1. Add new methods to the service class
2. Add new tool functions
3. Update the registration function to include your new tools

### Development with Docker

You can use Docker for development to ensure a consistent environment:

```bash
# Build a development image
docker build -t mcp-tool-kit:dev .

# Run with source code mounted for development
docker run -p 8000:8000 \
    -v $(pwd):/app \
    -v ~/documents:/app/documents \
    mcp-tool-kit:dev
```

This mounts your local repository into the container, so changes to the code are reflected immediately (for most files).

## Troubleshooting

- **Module not loading**: Check the import path and dependencies
- **API key errors**: Verify your API keys in the `.env` file
- **Permission errors**: Check the allowed directories in `MCP_FILESYSTEM_DIRS`
- **Connection errors**: Ensure the server is running and the port is accessible

## License

The MCP Unified Server is licensed under the MIT License.

## Acknowledgements

This project uses several open-source libraries and APIs:
- MCP SDK for Claude AI assistants
- NewsAPI for news access
- Brave Search API for web search
- World Bank API for economic data
- python-pptx for PowerPoint manipulation
- XlsxWriter for Excel spreadsheets
', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"Protocol Implementations\"]}', '{"is_mcp":"YES","justification":"The repository implements a server and toolkit specifically for the Model Context Protocol, facilitating communication between AI models (Claude) and external tools. It focuses on tool integration and context management for enhanced model capabilities.","categories":["Context Management","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The architecture centers around a modular server that hosts various tools accessible via the MCP. It uses a service-based design, where each tool is implemented as a separate service with defined functionality. The server acts as a unified interface, managing tool registration and invocation. The code demonstrates a clear separation of concerns between tool logic and server infrastructure.","Implementation Details and Key Features":"Key features include file system operations, web search, browser automation, and integrations with various APIs like World Bank and Yahoo Finance. Custom tools can be created by implementing a service class and registering it with the MCP server. The server handles request routing and parameter passing to the appropriate tool. Docker support is provided for easy deployment. The system uses JSON for configuration and data exchange.","Integration Approaches and Protocols":"The repository uses HTTP for communication between the Claude desktop application and the MCP server. Tools are integrated by registering them with the server, which then exposes them via a unified API. Claude interacts with the tools by sending requests with specific parameters. The integration approach focuses on making external resources and functionalities easily accessible to the AI model.","Technical Stack and Dependencies":"The technical stack primarily consists of Python. Key dependencies include `requests` for making HTTP requests, `beautifulsoup4` for HTML parsing, and potentially others depending on the specific tools being used (e.g., libraries for interacting with PowerPoint, Excel, etc.). Docker and Docker Compose are used for containerization and deployment.","Best Practices and Coding Standards":"The code includes clear function and class definitions with docstrings, promoting readability and maintainability. Modular design with service-based tools encourages code reuse and separation of concerns. The use of Docker facilitates consistent environments and simplifies deployment. Examples are provided for creating custom tools and integrating them with the server, guiding developers in best practices.","Performance Considerations":"Performance depends on the complexity and execution time of individual tools. Asynchronous task execution might be considered for long-running operations to prevent blocking the server. Caching mechanisms could improve the speed of frequently accessed data. Optimizing individual tool implementations is crucial for overall system performance. The server is likely designed for a moderate number of concurrent requests."}}', '2025-03-10 21:26:40+00', 'true', 'Servers', 'mcp-python-toolkit'), ('944216248', 'mcp-client-server-host-demo', 'danwritecode/mcp-client-server-host-demo', 'https://github.com/danwritecode/mcp-client-server-host-demo', 'A quick pokemon demo to showcase MCP server, client, and host', '2025-03-07 01:17:01+00', '2025-03-09 06:36:01+00', 'TypeScript', '33', '# Pokemon MCP Demo

A demo project showcasing the Model Context Protocol (MCP) integration with Pokemon data. This project includes:

- An MCP server that provides Pokemon data and attack capabilities
- An MCP client that interacts with the server
- An HTTP server that allows LLMs to use Pokemon tools via OpenRouter

## Installation

```bash
bun install
```

## Running the Project

### MCP Server and Client Demo
Run the client which automatically launches the MCP server:

```bash
bun mcp_client.ts
```

### HTTP Server with LLM Integration
Set your OpenRouter API key and run the HTTP server:

```bash
export OPENROUTER_API_KEY=your_api_key_here
bun http_server.ts
```

Then access the server at http://localhost:3005:
- `/tools` - List available tools
- `/attack` - Execute a random Pokemon attack using LLM

This project uses [Bun](https://bun.sh) as its JavaScript runtime.', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository description and documentation explicitly mention Model Context Protocol (MCP) integration and demonstrate server, client, and host implementations with Pokemon data and attack capabilities.","categories":["Protocol Implementations","AI Model Integration","Function Calling"],"analysis":{"Architecture and Design Patterns":"The repository adopts a client-server architecture, where an MCP server provides Pokemon data and attack capabilities, and an MCP client interacts with this server. An HTTP server bridges the MCP functionalities to LLMs through OpenRouter, enabling tool use. The architecture uses a demo approach to illustrate key MCP concepts and usage.","Implementation Details and Key Features":"The implementation features an MCP server offering Pokemon data and attack functions, a client interacting with this server, and an HTTP server allowing LLMs to access Pokemon tools via OpenRouter. Key features include tool listing and attack execution, showcasing MCP's function calling capabilities. The use of Bun as the runtime simplifies project setup and execution.","Integration Approaches and Protocols":"The repository integrates MCP with an HTTP server, enabling LLMs to leverage Pokemon tools using OpenRouter. The `/tools` endpoint lists available functions, while `/attack` executes a random attack using an LLM. This demonstrates MCP's ability to facilitate tool use and function calling in language models. The integration is designed to be easily accessible and testable.","Technical Stack and Dependencies":"The project primarily uses Bun as its JavaScript runtime. It depends on the OpenRouter API for integrating LLMs with the MCP functionalities. The code is written in TypeScript (as indicated by the .ts file extensions). The stack is lightweight, aiming for ease of use and rapid prototyping of MCP-related concepts.","Best Practices and Coding Standards":"The documentation includes clear instructions for installation and running the project, including setting up the OpenRouter API key. The repository provides specific examples for using the HTTP server endpoints, promoting ease of use and understanding. Use of TypeScript suggests an attention to type safety and code maintainability within the project.","Performance Considerations":"The demo nature of the repository suggests that performance optimization might not be the primary focus. However, the choice of Bun runtime could contribute to reasonable execution speed. The impact of OpenRouter API usage on latency should be evaluated for real-world implementations. The simple nature of the demo should reduce the performance impact overall."}}', '2025-03-10 21:28:52+00', 'true', 'Clients', 'mcp-pokemon-demo'), ('944235570', 'MCP', 'surya-madhav/MCP', 'https://github.com/surya-madhav/MCP', null, '2025-03-07 02:20:07+00', '2025-03-07 02:20:36+00', 'Python', '0', '# MCP Web Tools Server

A Model Context Protocol (MCP) server that provides tools for web-related operations. This server allows LLMs to interact with web content through standardized tools.

## Current Tools

- **web_scrape**: Converts a URL to use r.jina.ai as a prefix and returns the markdown content

## Installation

1. Clone this repository:
   ```bash
   git clone <repository-url>
   cd MCP
   ```

2. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```

   Alternatively, you can use [uv](https://github.com/astral-sh/uv) for faster installation:
   ```bash
   uv pip install -r requirements.txt
   ```

## Running the Server and UI

This repository includes convenient scripts to run either the MCP server or the Streamlit UI.

### Using the Run Scripts

On macOS/Linux:
```bash
# Run the server with stdio transport (default)
./run.sh server

# Run the server with SSE transport
./run.sh server --transport sse --host localhost --port 5000

# Run the Streamlit UI
./run.sh ui
```

On Windows:
```cmd
# Run the server with stdio transport (default)
run.bat server

# Run the server with SSE transport
run.bat server --transport sse --host localhost --port 5000

# Run the Streamlit UI
run.bat ui
```

### Running Manually

Alternatively, you can run the server directly:

#### Using stdio (default)

```bash
python server.py
```

#### Using SSE

```bash
python server.py --transport sse --host localhost --port 5000
```

This will start an HTTP server on `localhost:5000` that accepts MCP connections.

And to run the Streamlit UI manually:

```bash
streamlit run streamlit_app.py
```

## Testing with MCP Inspector

The MCP Inspector is a tool for testing and debugging MCP servers. You can use it to interact with your server:

1. Install the MCP Inspector:
   ```bash
   npm install -g @modelcontextprotocol/inspector
   ```

2. Run the Inspector with your server:
   ```bash
   npx @modelcontextprotocol/inspector python server.py
   ```

3. Use the Inspector interface to test the `web_scrape` tool by providing a URL like `example.com` and viewing the returned markdown content.

## Integrating with Claude for Desktop

To use this server with Claude for Desktop:

1. Make sure you have [Claude for Desktop](https://claude.ai/download) installed.

2. Open the Claude for Desktop configuration file:
   - Mac: `~/Library/Application Support/Claude/claude_desktop_config.json`
   - Windows: `%APPDATA%\Claude\claude_desktop_config.json`

3. Add the following configuration (adjust the path as needed):

```json
{
  "mcpServers": {
    "web-tools": {
      "command": "python",
      "args": [
        "/absolute/path/to/MCP/server.py"
      ]
    }
  }
}
```

4. Restart Claude for Desktop.

5. You should now see the web_scrape tool available in Claude's interface. You can ask Claude to fetch content from a website, and it will use the tool.

## Example Usage

Once integrated with Claude, you can ask questions like:

- "What's on the homepage of example.com?"
- "Can you fetch and summarize the content from mozilla.org?"
- "Get the content from wikipedia.org/wiki/Model_Context_Protocol and explain it to me."

Claude will use the web_scrape tool to fetch the content and provide it in its response.

## Adding More Tools

To add more tools to this server:

1. Create a new Python file in the `tools/` directory, e.g., `tools/new_tool.py`.

2. Implement your tool function, following a similar pattern to the existing tools.

3. Import your tool in `server.py` and register it with the MCP server:

```python
# Import your new tool
from tools.new_tool import new_tool_function

# Register the tool with the MCP server
@mcp.tool()
async def new_tool(param1: str, param2: int) -> str:
    """
    Description of what your tool does.
    
    Args:
        param1: Description of param1
        param2: Description of param2
        
    Returns:
        Description of return value
    """
    return await new_tool_function(param1, param2)
```

4. Restart the server to apply the changes.

## Streamlit UI

This repository includes a Streamlit application that allows you to connect to and test all your MCP servers configured in Claude for Desktop.

### Running the Streamlit UI

```bash
streamlit run streamlit_app.py
```

This will start the Streamlit server and open a web browser with the UI.

### Features

- Load and parse your Claude for Desktop configuration file
- View all configured MCP servers
- Connect to any server and view its available tools
- Test tools by providing input parameters and viewing results
- See available resources and prompts

### Usage

1. Start the Streamlit app
2. Enter the path to your Claude for Desktop configuration file (default path is pre-filled)
3. Click "Load Servers" to see all available MCP servers
4. Select a server tab and click "Connect" to load its tools
5. Select a tool and provide the required parameters
6. Click "Execute" to run the tool and see the results

## Troubleshooting

- **Missing dependencies**: Make sure all dependencies in `requirements.txt` are installed.
- **Connection issues**: Check that the server is running and the configuration in Claude for Desktop points to the correct path.
- **Tool execution errors**: Look for error messages in the server output.
- **Streamlit UI issues**: Make sure Streamlit is properly installed and the configuration file path is correct.

## License

This project is available under the MIT License. See the LICENSE file for more details.
', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"Agent Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it's a \"Model Context Protocol (MCP) server,\" provides integration instructions with Claude for Desktop, and offers tools specifically designed for web-related operations for LLMs.","categories":["Protocol Implementations","Function Calling","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The architecture revolves around a server that exposes tools for Large Language Models (LLMs) using the Model Context Protocol. It employs a modular design, allowing for the addition of new tools as Python files. The repository utilizes `server.py` as the entry point and defines tools using the `@mcp.tool()` decorator.","Implementation Details and Key Features":"The core functionality is implemented in `server.py`, which handles the registration and execution of tools. Key features include the `web_scrape` tool, which fetches web content and converts it to markdown.  The server supports both stdio and SSE transports. A Streamlit UI is also provided for testing and interacting with the MCP server.","Integration Approaches and Protocols":"The repository is designed to integrate with Claude for Desktop through the `claude_desktop_config.json` file. It specifies a command and arguments to launch the MCP server. The communication happens through MCP, allowing Claude to call exposed tools. The MCP Inspector is used for testing and debugging the server.","Technical Stack and Dependencies":"The project uses Python as its primary language. Key dependencies listed in `requirements.txt` are not explicitly provided in the documentation but inferred based on installation instructions; example commands like `pip install -r requirements.txt` are included. Other tools include Streamlit, likely other libraries for web scraping, and the MCP Inspector (Node.js).","Best Practices and Coding Standards":"The repository encourages modularity by suggesting that new tools be created in separate files within the `tools/` directory.  It provides clear instructions for adding new tools, including documentation of argument types and return values using docstrings. Run scripts and manual execution instructions are included for clarity.","Performance Considerations":"The documentation doesn't explicitly address performance, but mentions using `uv` for faster dependency installation.  The `web_scrape` tool uses `r.jina.ai` as a proxy, which suggests potential optimization for web content retrieval.  Choice of transport (stdio vs. SSE) might influence latency and throughput.  Further considerations would be around concurrency and rate limiting for the web scraping functionality."}}', '2025-03-10 21:28:38+00', 'true', 'Servers', 'mcp-server-944235570'), ('944245690', 'dubco-mcp-server-npm', 'Gitmaxd/dubco-mcp-server-npm', 'https://github.com/Gitmaxd/dubco-mcp-server-npm', 'The (Unofficial) dubco-mcp-server enables AI assistants to manage Dub.co short links via the Model Context Protocol. It provides three MCP tools: create_link for generating new short URLs, update_link for modifying existing links, and delete_link for removing short links.', '2025-03-07 02:52:28+00', '2025-03-09 07:35:37+00', 'JavaScript', '4', '# Unofficial dubco-mcp-server

[![npm version](https://img.shields.io/npm/v/dubco-mcp-server.svg)](https://www.npmjs.com/package/dubco-mcp-server)
[![License: ISC](https://img.shields.io/badge/License-ISC-blue.svg)](https://opensource.org/licenses/ISC)
[![Node.js Version](https://img.shields.io/node/v/dubco-mcp-server)](https://nodejs.org/)

A Model Context Protocol (MCP) server for creating and managing [Dub.co](https://dub.co) short links (unofficial). This server enables AI assistants to create, update, and delete short links through the Dub.co API.

## 🚀 Features

- Create custom short links with your Dub.co domains
- Update existing short links
- Delete short links
- Seamless integration with AI assistants through the Model Context Protocol

## 📋 Prerequisites

- Node.js 16.0.0 or higher
- A Dub.co account with API access
- An API key from the [Dub.co dashboard](https://app.dub.co/settings/api)

## 💻 Installation

### Global Installation

```bash
npm install -g dubco-mcp-server
```

### Local Installation

```bash
npm install dubco-mcp-server
```

### Direct Usage with npx

```bash
npx dubco-mcp-server
```

## ⚙️ Configuration

This MCP server requires a Dub.co API key to function. You can get your API key from the [Dub.co dashboard](https://app.dub.co/settings/api).

Set the API key as an environment variable:

```bash
export DUBCO_API_KEY=your_api_key_here
```

For persistent configuration, add this to your shell profile (e.g., `.bashrc`, `.zshrc`):

```bash
echo 'export DUBCO_API_KEY=your_api_key_here' >> ~/.zshrc
```

## 🖥️ Cursor IDE Setup

Cursor IDE provides native support for MCP servers. Follow these steps to set up the dubco-mcp-server in Cursor:

### Step 1: Install Cursor IDE

If you haven't already, download and install [Cursor IDE](https://cursor.sh/) (version 0.4.5.9 or later).

### Step 2: Open Cursor Settings

1. Open Cursor IDE
2. Click on the gear icon in the bottom left corner, or use the keyboard shortcut `Cmd+,` (Mac) or `Ctrl+,` (Windows/Linux)
3. Navigate to the Features section
4. Scroll down to find the "MCP Servers" section

### Step 3: Add the MCP Server

1. Click on "+ Add new MCP server"
2. In the dialog that appears:
   - **Name**: Enter "Dub.co MCP Server" (or any name you prefer)
   - **Type**: Select "command" from the dropdown
   - **Command**: Enter `env DUBCO_API_KEY=your_api_key_here npx -y dubco-mcp-server`
     (Replace `your_api_key_here` with your actual Dub.co API key)
3. Click "Save" to add the server

### Step 4: Verify the Connection

After adding the MCP server, you should see a green status indicator next to the server name. If it shows a red or yellow status, try:

1. Checking that your API key is correct
2. Restarting Cursor IDE
3. Verifying that Node.js (16.0.0+) is properly installed

### Step 5: Using the Server

The dubco-mcp-server provides tools that can be used with Cursor's AI features:

1. Open Cursor's Composer or Agent mode (MCP only works in these modes)
2. Explicitly instruct the AI to use the Dub.co tools (create_link, update_link, delete_link)
3. Accept the tool usage prompts when they appear

## 🔧 Usage with MCP

This server provides tools that can be used by AI assistants through the Model Context Protocol. To use it with an MCP-compatible AI assistant, add it to your MCP configuration.

### MCP Configuration Example

```json
{
  "mcpServers": {
    "dubco": {
      "command": "npx",
      "args": ["-y", "dubco-mcp-server"],
      "env": {
        "DUBCO_API_KEY": "your_api_key_here"
      },
      "disabled": false,
      "autoApprove": []
    }
  }
}
```

### Available Tools

#### create_link

Create a new short link on Dub.co.

**Parameters:**

```json
{
  "url": "https://example.com",
  "key": "optional-custom-slug",
  "externalId": "optional-external-id",
  "domain": "optional-domain-slug"
}
```

**Example:**

```json
{
  "url": "https://github.com/gitmaxd/dubco-mcp-server-npm",
  "key": "dubco-mcp"
}
```

#### update_link

Update an existing short link on Dub.co.

**Parameters:**

```json
{
  "linkId": "link-id-to-update",
  "url": "https://new-destination.com",
  "domain": "new-domain-slug",
  "key": "new-custom-slug"
}
```

**Example:**

```json
{
  "linkId": "clwxyz123456",
  "url": "https://github.com/gitmaxd/dubco-mcp-server-npm/releases"
}
```

#### delete_link

Delete a short link on Dub.co.

**Parameters:**

```json
{
  "linkId": "link-id-to-delete"
}
```

**Example:**

```json
{
  "linkId": "clwxyz123456"
}
```

## 🔍 How It Works

The server connects to the Dub.co API using your API key and provides a standardized interface for AI assistants to interact with Dub.co through the Model Context Protocol. When a tool is called:

1. The server validates the input parameters
2. It sends the appropriate request to the Dub.co API
3. It processes the response and returns it in a format that the AI assistant can understand

## 🛠️ Development

### Building from Source

```bash
git clone https://github.com/gitmaxd/dubco-mcp-server-npm.git
cd dubco-mcp-server-npm
npm install
npm run build
```

### Running in Development Mode

```bash
npm run dev
```

## 📝 License

This project is licensed under the ISC License - see the [LICENSE](LICENSE) file for details.

## 🔗 Links

- [Dub.co](https://dub.co) - The URL shortener service
- [Dub.co API Documentation](https://dub.co/docs/api-reference/introduction)
- [Model Context Protocol](https://github.com/anthropics/model-context-protocol) - Learn more about MCP

## 👥 Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## 👨‍💻 Created By

This unofficial Dub.co MCP Server was created by [GitMaxd](https://github.com/gitmaxd) ([@gitmaxd](https://twitter.com/gitmaxd) on X).

This project was developed as a learning exercise to understand the Model Context Protocol and how to build MCP servers. I chose Dub.co as the integration target because of its straightforward API and practical utility, making it an ideal candidate for a learning project.

While I have no official affiliation with Dub.co, I highly recommend their service for both manual and automated short link creation. Their API is well-documented and easy to work with, making it perfect for this kind of integration.

If you find this project helpful or have suggestions for improvements, feel free to reach out or contribute to the repository. Happy link shortening!
', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository description and documentation explicitly state that it is a Model Context Protocol (MCP) server for managing Dub.co short links. It also provides clear instructions on how to integrate it with Cursor IDE using MCP.","categories":["Protocol Implementations","Function Calling","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The repository implements a server that acts as an intermediary between an AI assistant and the Dub.co API. It leverages the Model Context Protocol to expose functionalities like creating, updating, and deleting short links as tools that the AI assistant can utilize. The design appears modular, focusing on encapsulating Dub.co API interactions.","Implementation Details and Key Features":"The key features are the create_link, update_link, and delete_link functions, which interact with the Dub.co API. Implementation involves setting up an MCP server that listens for tool requests from AI assistants, validates the input parameters, sends requests to the Dub.co API, and returns the response to the AI assistant. The server is built using Node.js.","Integration Approaches and Protocols":"The primary integration approach is through the Model Context Protocol (MCP). It supports integration with tools like Cursor IDE by exposing functionalities as MCP tools. The server uses environment variables for configuration (e.g., DUBCO_API_KEY). It uses standard HTTP requests to interact with the Dub.co API.","Technical Stack and Dependencies":"The technical stack consists of Node.js, as indicated by the package.json file and installation instructions. Dependencies include libraries for handling HTTP requests (likely axios or node-fetch, although not explicitly stated in the provided documentation), and possibly libraries for validating input parameters and handling JSON data. The runtime environment requires Node.js version 16.0.0 or higher.","Best Practices and Coding Standards":"The documentation includes instructions for setting up the server and configuring environment variables, which is good practice. The project provides clear examples of how to use the available tools and parameter formats. The use of npm for package management and version control through Git suggest adherence to standard coding practices.","Performance Considerations":"Performance considerations would likely involve optimizing API call efficiency to the Dub.co service. Caching frequent lookups, implementing connection pooling for the Dub.co API, and optimizing data serialization/deserialization could improve response times. The server's performance is also influenced by the Node.js runtime environment and underlying infrastructure."}}', '2025-03-10 21:54:04+00', 'true', 'Servers', 'dubco-mcp-server'), ('944270317', 'ticktick-mcp', 'jacepark12/ticktick-mcp', 'https://github.com/jacepark12/ticktick-mcp', 'MCP server that interacts with TickTick via the TickTick Open API', '2025-03-07 04:11:00+00', '2025-03-09 17:07:05+00', 'Python', '3', '# TickTick MCP Server

A [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) server for TickTick that enables interacting with your TickTick task management system directly through Claude and other MCP clients.

## Features

- 📋 View all your TickTick projects and tasks
- ✏️ Create new projects and tasks through natural language
- 🔄 Update existing task details (title, content, dates, priority)
- ✅ Mark tasks as complete
- 🗑️ Delete tasks and projects
- 🔄 Full integration with TickTick's open API
- 🔌 Seamless integration with Claude and other MCP clients

## Prerequisites

- Python 3.10 or higher
- [uv](https://github.com/astral-sh/uv) - Fast Python package installer and resolver
- TickTick account with API access
- TickTick API credentials (Client ID, Client Secret, Access Token)

## Installation

1. **Clone this repository**:
   ```bash
   git clone https://github.com/parkjs814/ticktick-mcp.git
   cd ticktick-mcp
   ```

2. **Install with uv**:
   ```bash
   # Install uv if you don't have it already
   curl -LsSf https://astral.sh/uv/install.sh | sh

   # Create a virtual environment
   uv venv

   # Activate the virtual environment
   # On macOS/Linux:
   source .venv/bin/activate
   # On Windows:
   .venv\Scripts\activate

   # Install the package
   uv pip install -e .
   ```

3. **Authenticate with TickTick**:
   ```bash
   # Run the authentication flow
   uv run -m ticktick_mcp.cli auth
   ```

   This will:
   - Ask for your TickTick Client ID and Client Secret
   - Open a browser window for you to log in to TickTick
   - Automatically save your access tokens to a `.env` file

4. **Test your configuration**:
   ```bash
   uv run test_server.py
   ```
   This will verify that your TickTick credentials are working correctly.

## Authentication with TickTick

This server uses OAuth2 to authenticate with TickTick. The setup process is straightforward:

1. Register your application at the [TickTick Developer Center](https://developer.ticktick.com/manage)
   - Set the redirect URI to `http://localhost:8000/callback`
   - Note your Client ID and Client Secret

2. Run the authentication command:
   ```bash
   uv run -m ticktick_mcp.cli auth
   ```

3. Follow the prompts to enter your Client ID and Client Secret

4. A browser window will open for you to authorize the application with your TickTick account

5. After authorizing, you'll be redirected back to the application, and your access tokens will be automatically saved to the `.env` file

The server handles token refresh automatically, so you won't need to reauthenticate unless you revoke access or delete your `.env` file.

## Usage with Claude for Desktop

1. Install [Claude for Desktop](https://claude.ai/download)
2. Edit your Claude for Desktop configuration file:

   **macOS**:
   ```bash
   nano ~/Library/Application\ Support/Claude/claude_desktop_config.json
   ```

   **Windows**:
   ```bash
   notepad %APPDATA%\Claude\claude_desktop_config.json
   ```

3. Add the TickTick MCP server configuration:
   ```json
   {
     "mcpServers": {
       "ticktick": {
         "command": "uv",
         "args": ["run", "-m", "ticktick_mcp.cli", "run"]
       }
     }
   }
   ```

4. Restart Claude for Desktop

Once connected, you'll see the TickTick MCP server tools available in Claude, indicated by the 🔨 (tools) icon.

## Available MCP Tools

| Tool | Description | Parameters |
|------|-------------|------------|
| `get_projects` | List all your TickTick projects | None |
| `get_project` | Get details about a specific project | `project_id` |
| `get_project_tasks` | List all tasks in a project | `project_id` |
| `get_task` | Get details about a specific task | `project_id`, `task_id` |
| `create_task` | Create a new task | `title`, `project_id`, `content` (optional), `start_date` (optional), `due_date` (optional), `priority` (optional) |
| `update_task` | Update an existing task | `task_id`, `project_id`, `title` (optional), `content` (optional), `start_date` (optional), `due_date` (optional), `priority` (optional) |
| `complete_task` | Mark a task as complete | `project_id`, `task_id` |
| `delete_task` | Delete a task | `project_id`, `task_id` |
| `create_project` | Create a new project | `name`, `color` (optional), `view_mode` (optional) |
| `delete_project` | Delete a project | `project_id` |

## Example Prompts for Claude

Here are some example prompts to use with Claude after connecting the TickTick MCP server:

- "Show me all my TickTick projects"
- "Create a new task called 'Finish MCP server documentation' in my work project with high priority"
- "List all tasks in my personal project"
- "Mark the task 'Buy groceries' as complete"
- "Create a new project called 'Vacation Planning' with a blue color"
- "When is my next deadline in TickTick?"

## Development

### Project Structure

```
ticktick-mcp/
├── .env.template          # Template for environment variables
├── README.md              # Project documentation
├── requirements.txt       # Project dependencies
├── setup.py               # Package setup file
├── test_server.py         # Test script for server configuration
└── ticktick_mcp/          # Main package
    ├── __init__.py        # Package initialization
    ├── authenticate.py    # OAuth authentication utility
    ├── cli.py             # Command-line interface
    └── src/               # Source code
        ├── __init__.py    # Module initialization
        ├── auth.py        # OAuth authentication implementation
        ├── server.py      # MCP server implementation
        └── ticktick_client.py  # TickTick API client
```

### Authentication Flow

The project implements a complete OAuth 2.0 flow for TickTick:

1. **Initial Setup**: User provides their TickTick API Client ID and Secret
2. **Browser Authorization**: User is redirected to TickTick to grant access
3. **Token Reception**: A local server receives the OAuth callback with the authorization code
4. **Token Exchange**: The code is exchanged for access and refresh tokens
5. **Token Storage**: Tokens are securely stored in the local `.env` file
6. **Token Refresh**: The client automatically refreshes the access token when it expires

This simplifies the user experience by handling the entire OAuth flow programmatically.

### Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the LICENSE file for details.
', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"API Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements a Model Context Protocol (MCP) server for TickTick, enabling interaction with the task management system via Claude and other MCP clients. It adheres to MCP specifications for tool use and function calling.","categories":["Agent Frameworks","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The architecture follows a client-server model, where the server acts as an MCP endpoint, interfacing with the TickTick API. The design leverages OAuth 2.0 for authentication. It's structured into modules handling authentication, CLI interaction, and server logic, promoting separation of concerns. The project emphasizes ease of integration with Claude and other MCP-compliant clients.","Implementation Details and Key Features":"Key features include viewing, creating, updating, completing, and deleting tasks and projects within TickTick using natural language prompts via MCP. Implementation includes an OAuth 2.0 flow for authentication. The CLI provides an interface for running the server and authenticating with TickTick. It supports automated token refresh and uses environment variables for sensitive data.","Integration Approaches and Protocols":"The repository focuses on seamless integration with Claude for Desktop, using the MCP standard to expose TickTick functionalities as tools. The integration is achieved by configuring Claude to communicate with the TickTick MCP server. This setup allows Claude to invoke TickTick actions based on user prompts, effectively extending Claude's capabilities.","Technical Stack and Dependencies":"The project uses Python 3.10 or higher and relies on the `uv` package manager. Key dependencies include libraries for handling OAuth 2.0 authentication (likely `requests-oauthlib`), API interaction with TickTick, and potentially a framework like Flask or FastAPI for the MCP server. The use of `.env` files suggests a dependency on `python-dotenv` or similar for environment variable management.","Best Practices and Coding Standards":"The repository includes a well-defined project structure with clear separation of concerns. It provides detailed documentation on installation, authentication, and usage. The project uses virtual environments for dependency management and includes a template for environment variables, promoting security. Code contributions are encouraged via pull requests, indicating a collaborative approach.","Performance Considerations":"Given the nature of the application, performance considerations likely center around minimizing latency when interacting with the TickTick API and Claude. The asynchronous handling of API requests might be implemented using libraries like `asyncio` to improve responsiveness. Efficient token management and caching strategies could also be in place to reduce authentication overhead."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'ticktick-mcp-server'), ('944292115', 'mcp-tradovate', '0xjmp/mcp-tradovate', 'https://github.com/0xjmp/mcp-tradovate', 'MCP server for the Tradovate platform', '2025-03-07 05:15:14+00', '2025-03-09 23:19:26+00', 'Go', '3', '# MCP Tradovate Server

![](https://badge.mcpx.dev?type=server 'MCP Server')
[![smithery badge](https://smithery.ai/badge/@0xjmp/mcp-tradovate)](https://smithery.ai/server/@0xjmp/mcp-tradovate)
[![Go Report Card](https://goreportcard.com/badge/github.com/0xjmp/mcp-tradovate)](https://goreportcard.com/report/github.com/0xjmp/mcp-tradovate)
[![codecov](https://codecov.io/gh/0xjmp/mcp-tradovate/branch/main/graph/badge.svg)](https://codecov.io/gh/0xjmp/mcp-tradovate)
[![Go Reference](https://pkg.go.dev/badge/github.com/0xjmp/mcp-tradovate.svg)](https://pkg.go.dev/github.com/0xjmp/mcp-tradovate)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)

A Model Context Protocol (MCP) server for Tradovate integration in Claude Desktop. This server enables AI assistants to manage Tradovate trading accounts through natural language interactions.

## Features

- ✅ Complete Tradovate API integration
- 🔒 Secure authentication handling
- 📈 Real-time market data access
- 💼 Account management
- 📊 Risk management controls
- 🔄 Order placement and management
- 📝 Comprehensive test coverage

## Installation

### Installing via Smithery
To install the Tradovate MCP server for Claude Desktop automatically via Smithery:

```bash
npx -y @smithery/cli install @0xjmp/mcp-tradovate --client claude
```

### Manual Installation

1. Clone the repository:
```bash
git clone https://github.com/0xjmp/mcp-tradovate.git
cd mcp-tradovate
```

2. Install dependencies:
```bash
go mod download
```

3. Build the project:
```bash
go build ./cmd/mcp-tradovate
```

4. Run:
```
./mcp-tradovate
```

## Configuration

Create a `.env` file in the project root with your Tradovate credentials:

```env
TRADOVATE_USERNAME=your_username
TRADOVATE_PASSWORD=your_password
TRADOVATE_APP_ID=your_app_id
TRADOVATE_APP_VERSION=your_app_version
TRADOVATE_CID=your_client_id
TRADOVATE_SEC=your_client_secret
```

## Available Tools

### Authentication
- `authenticate`: Connect to Tradovate API
  - No parameters required

### Account Management
- `get_accounts`: List all trading accounts
  - No parameters required

- `get_positions`: View current positions
  - No parameters required

- `get_risk_limits`: Get risk management settings
  - Required parameters:
    - `account_id`: (number) Account ID to get limits for

- `set_risk_limits`: Configure risk management settings
  - Required parameters:
    - `account_id`: (number) Account ID to set limits for
    - `day_max_loss`: (number) Maximum daily loss limit
    - `max_drawdown`: (number) Maximum drawdown limit
    - `max_position_qty`: (number) Maximum position quantity
    - `trailing_stop`: (number) Trailing stop percentage

### Trading Operations
- `place_order`: Submit a new order
  - Required parameters:
    - `account_id`: (number) Account ID to place the order for
    - `contract_id`: (number) Contract ID to trade
    - `order_type`: (string) Type of order (Market, Limit, etc.)
    - `quantity`: (number) Number of contracts to trade
    - `time_in_force`: (string) Time in force (Day, GTC, IOC, etc.)
  - Optional parameters:
    - `price`: (number) Order price (required for Limit orders)

- `cancel_order`: Cancel an existing order
  - Required parameters:
    - `order_id`: (number) Order ID to cancel

- `get_fills`: Get fills for a specific order
  - Required parameters:
    - `order_id`: (number) Order ID to get fills for

### Market Data
- `get_contracts`: List available contracts
  - No parameters required

- `get_market_data`: Get real-time market data
  - Required parameters:
    - `contract_id`: (number) Contract ID to get market data for

- `get_historical_data`: Get historical price data
  - Required parameters:
    - `contract_id`: (number) Contract ID to get data for
    - `start_time`: (string) Start time in ISO 8601 format
    - `end_time`: (string) End time in ISO 8601 format
    - `interval`: (string) Time interval (1m, 5m, 15m, 1h, 1d)

## Development

### Running Tests

Run all tests with coverage:
```bash
go test -v -race -coverprofile=coverage.txt -covermode=atomic ./...
```

### Code Style

Follow Go best practices and conventions:
```bash
go fmt ./...
go vet ./...
```

## Troubleshooting

### Common Issues

1. **Authentication Failures**
   - Verify your Tradovate credentials in the `.env` file
   - Ensure your API access is enabled in Tradovate

2. **Connection Issues**
   - Check your internet connection
   - Verify Tradovate API status
   - Ensure firewall isn't blocking connections

3. **Rate Limiting**
   - Implement appropriate delays between requests
   - Monitor API usage limits

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Support

If you encounter any issues or have questions, please file an issue on the GitHub repository.

## Author

Jake Peterson ([@0xjmp](https://github.com/0xjmp))
', 'true', '"{\"Protocol Implementations\",\"AI Model Integration\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it is an MCP server for Tradovate, enabling AI assistants to manage trading accounts through natural language interaction. It provides a bridge between AI models and the Tradovate trading platform.","categories":["Protocol Implementations","AI Model Integration","Financial Services"],"analysis":{"Architecture and Design Patterns":"The repository implements a server to bridge the gap between AI models and the Tradovate trading platform, using the Model Context Protocol (MCP). The architecture likely involves handling requests from AI models, translating them into Tradovate API calls, and relaying responses back. It handles authentication, market data, and trading operations.","Implementation Details and Key Features":"The server offers tools for authentication, account management, trading operations, and market data retrieval. Key features include secure authentication handling, real-time market data access, risk management controls, and order placement/management. The implementation uses Go, and the documentation emphasizes comprehensive test coverage.","Integration Approaches and Protocols":"The repository integrates with the Tradovate API using the Model Context Protocol (MCP). The server translates natural language requests from AI models into specific API calls for actions like placing orders or retrieving market data. It handles authentication and data formatting for seamless integration.","Technical Stack and Dependencies":"The project uses Go as its primary language, leveraging dependencies managed through `go mod`. Key dependencies likely include libraries for handling HTTP requests, JSON parsing, and interacting with the Tradovate API. It also relies on environment variables for secure configuration.","Best Practices and Coding Standards":"The repository follows Go best practices and conventions, demonstrated by the inclusion of `go fmt` and `go vet` in the development instructions. The documentation highlights comprehensive test coverage and encourages contributions, suggesting adherence to standard software development workflows.","Performance Considerations":"The documentation mentions rate limiting as a potential issue, implying that performance and API usage limits are important considerations. The server likely includes mechanisms for managing concurrent requests and optimizing data retrieval to avoid exceeding Tradovate's API limits. Real-time market data access needs efficient handling to ensure low latency."}}', '2025-03-10 21:54:04+00', 'true', 'Servers', 'mcp-tradovate-server'), ('944331603', 'workers-mcp-demo', 'akazwz/workers-mcp-demo', 'https://github.com/akazwz/workers-mcp-demo', 'cloudflare workers MCP server ', '2025-03-07 06:54:49+00', '2025-03-09 13:38:34+00', 'TypeScript', '25', '# MCP Demo

这是一个使用 [Cloudflare Workers MCP](https://github.com/cloudflare/workers-mcp) 创建自定义 AI 工具的简单演示项目，这些工具可以与 Claude、Cursor 和其他支持 MCP 的 AI 助手一起使用。

## 什么是 MCP？

MCP (Model Context Protocol) 是由 [Anthropic 开源的一个标准](https://www.anthropic.com/news/model-context-protocol)，用于连接 AI 助手与数据源，包括内容仓库、业务工具和开发环境。它的目标是帮助前沿模型生成更好、更相关的回应。

本项目利用 Cloudflare Workers MCP 创建自定义工具，这些工具可以被 AI 助手访问和使用，从而扩展 AI 的能力。

## 特点

- 在 Cloudflare Workers 上运行的自定义 AI 工具
- 与 Claude、Cursor 和其他 MCP 客户端的无缝集成
- 能够通过自己的无服务器函数扩展 AI 能力

## 前提条件

- [Node.js](https://nodejs.org/) (v16 或更高版本)
- [pnpm](https://pnpm.io/installation) 包管理器
- [Cloudflare 账户](https://dash.cloudflare.com/sign-up)
- [Wrangler CLI](https://developers.cloudflare.com/workers/wrangler/install-and-update/)
- Claude Desktop、Cursor 或其他支持 MCP 的客户端

## 设置步骤

### 1. 克隆此仓库

```bash
git clone https://github.com/akazwz/workers-mcp-demo.git
cd workers-mcp-demo
```

### 2. 安装依赖

```bash
pnpm install
```

### 3. 配置你的 Cloudflare Worker

确保你已经通过 Wrangler 登录到 Cloudflare：

```bash
pnpx wrangler login
```

然后运行 MCP 设置命令：

```bash
npx workers-mcp setup
```

这将自动配置你的 Worker 以支持 MCP。

#### 免费用户配置注意事项

如果你是 Cloudflare Workers 的免费用户（非付费用户），需要修改 `wrangler.jsonc` 文件，删除或注释掉以下配置：

```jsonc
"browser": {
  "binding": "BROWSER"
}
```

这是因为 Browser Rendering API 仅适用于付费用户。删除此配置后，`screenshotURL` 和 `extractImagesFromURL` 功能将无法使用，但其他功能仍然可以正常工作。

### 4. 部署你的 Worker

```bash
pnpm run deploy
```

### 5. 配置你的 MCP 客户端

#### 对于 Claude Desktop：

运行以下命令在 Claude 中安装你的 MCP 服务器：

```bash
pnpx workers-mcp install:claude
```

#### 对于 Cursor：

在 Cursor 的 MCP 配置中添加以下内容：

```json
{
  "type": "command",
  "command": "/path/to/workers-mcp run workers-mcp-demo https://your-worker-url.workers.dev /path/to/workers-mcp-demo"
}
```

## 使用方法

配置完成后，你可以直接从 AI 助手使用你的自定义 MCP 工具。例如，在 Claude Desktop 中，你可以使用如下命令：

```
@workers-mcp-demo getRandomNumber
```

## 可用工具

此演示包括以下工具：

- `getRandomNumber`：生成一个随机数
- `generateImage`：根据文本提示创建图像
- `screenshotURL`：对指定 URL 进行截图（需要付费账户）
- `extractImagesFromURL`：从网页中提取图像（需要付费账户）

## 自定义

要添加自己的工具，编辑 `src/index.ts` 文件并向 Worker 类添加新方法。更改后，使用以下命令重新部署你的 Worker：

```bash
pnpm run deploy
```

## 故障排除

- 如果你的 AI 助手看不到你的工具，尝试重启助手。
- 如果你更改了方法名称或参数，你需要重启 AI 助手才能使更改生效。
- 检查 Cloudflare Workers 日志以查看部署中的任何错误。
- 如果使用免费账户时出现 `Error: Browser Rendering API is only available for Workers Paid plans` 错误，请参考上面的免费用户配置注意事项。

## 许可证

本项目采用 [MIT 许可证](LICENSE)。

## 致谢

- [Cloudflare Workers MCP](https://github.com/cloudflare/workers-mcp) 提供底层框架
- [Anthropic Model Context Protocol](https://www.anthropic.com/news/model-context-protocol) 开源的 MCP 标准
- [Cloudflare Workers](https://workers.cloudflare.com/) 提供无服务器平台 ', 'true', '"{\"Function Calling\",\"Protocol Implementations\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements a demo server using Cloudflare Workers MCP, designed to create custom AI tools compatible with Claude and Cursor, adhering to the Model Context Protocol.","categories":["Function Calling","Protocol Implementations","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The repository demonstrates a serverless architecture using Cloudflare Workers to host custom AI tools compliant with the Model Context Protocol. It utilizes function calling, allowing AI assistants to interact with predefined tools via MCP. The design promotes modularity by enabling developers to add or modify tools within the worker.","Implementation Details and Key Features":"Key features include predefined tools such as generating random numbers and creating images from text prompts. It uses Cloudflare Workers MCP to facilitate communication between the tools and MCP-compatible AI assistants like Claude and Cursor. The repository includes instructions for setting up and deploying the worker, and configuring AI clients.","Integration Approaches and Protocols":"Integration is achieved through the Model Context Protocol. The repository provides specific instructions for integrating with Claude Desktop and Cursor, involving CLI commands and JSON configurations. The server exposes endpoints for each tool, which are then callable by the AI assistants. The setup leverages Wrangler CLI for deploying the worker.","Technical Stack and Dependencies":"The technical stack includes Node.js, pnpm, Cloudflare Workers, and Wrangler CLI. The repository depends on Cloudflare Workers MCP for the underlying framework that enables MCP compatibility. It relies on Cloudflare's serverless platform for execution. The `wrangler.jsonc` file configures the Cloudflare Worker environment.","Best Practices and Coding Standards":"The repository provides clear setup and deployment instructions, promoting ease of use and reproducibility. It includes troubleshooting steps for common issues, such as AI assistants not recognizing tools. The code is structured in a way that allows for easy extension and customization by adding new methods to the Worker class.","Performance Considerations":"Cloudflare Workers provides a serverless environment that scales automatically based on demand. The implementation leverages the Cloudflare network to minimize latency. Users on free plans may encounter limitations with features like screenshotURL due to the Browser Rendering API's availability on paid plans."}}', '2025-03-10 21:28:52+00', 'true', 'Servers', 'workers-mcp-demo'), ('944333647', 'mcp-doris', 'morningman/mcp-doris', 'https://github.com/morningman/mcp-doris', 'An MCP server for Apache Doris & VeloDB', '2025-03-07 06:59:22+00', '2025-03-10 04:37:44+00', 'Python', '3', '# Apache Doris MCP Server

[![smithery badge](https://smithery.ai/badge/@morningman/mcp-doris)](https://smithery.ai/server/@morningman/mcp-doris)

An [MCP server](https://modelcontextprotocol.io/introduction) for [Apache Doris](https://doris.apache.org/).

![Demo](mcp-doris-demo.gif)

## Usage

### Cursor

```
Name: doris
Type: command
Command: DORIS_HOST=<doris-host> DORIS_PORT=<port> DORIS_USER=<doris-user> DORIS_PASSWORD=<doris-pwd> uv run --with mcp-doris --python 3.13 mcp-doris
```

## Development

### Prerequest

- install [uv](https://docs.astral.sh/uv)

### Run MCP Inspector

```sql
cd /path/to/mcp-doris
uv sync
source .venv/bin/activate
export PYTHONPATH=/path/to/mcp-doris:$PYTHONPATH
env DORIS_HOST=<doris-host> DORIS_PORT=<port> DORIS_USER=<doris-user> DORIS_PASSWORD=<doris-pwd> mcp dev mcp_doris/mcp_server.py
```

Then visit `http://localhost:5173` in web browser.

## Publish

```
uv build
uv publish
```
', 'true', '"{\"Protocol Implementations\",\"AI Model Integration\",\"Database Systems\"]}', '{"is_mcp":"YES","justification":"The repository explicitly identifies itself as an MCP server for Apache Doris, aligning with the stated purpose of Model Context Protocol repositories for communication between AI models and tools.","categories":["Protocol Implementations","API Frameworks","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The repository implements an MCP server, suggesting a client-server architecture. The use of `uv run --with mcp-doris` implies a plugin or extension mechanism for integrating Doris with the MCP framework. The architecture likely involves handling requests based on the MCP specification and interacting with a Doris database backend.","Implementation Details and Key Features":"The core implementation seems to focus on enabling interaction with Apache Doris via the MCP. Key features include the ability to execute commands against Doris, managed by environment variables for connection details. The inclusion of a development setup using `mcp dev` suggests interactive development and debugging capabilities.","Integration Approaches and Protocols":"The repository integrates with Apache Doris via database connection parameters. The MCP is the primary protocol, enabling communication between AI models and the Doris database. The provided code snippet shows a command-line interface (CLI) approach to execute operations against Doris.","Technical Stack and Dependencies":"The repository uses Python, with mentions of `uv` for dependency management and execution. The specific version of Python (3.13) is specified. The main external dependency is Apache Doris. The stack also includes tooling for building and publishing, suggesting a focus on maintainability and distribution.","Best Practices and Coding Standards":"The repository provides development instructions with a clear setup process, utilizing `.venv` for virtual environment management. The use of `uv sync` indicates dependency management. The presence of publish commands implies a concern for distribution and potentially adherence to package management standards.","Performance Considerations":"Performance considerations are implicit in the choice of Apache Doris, a database system known for its analytical query performance. The use of `uv` for running the server suggests an interest in optimized execution, although specific performance optimizations aren't explicitly detailed."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'mcp-doris-server'), ('944335890', 'mcp-server', 'inkdropapp/mcp-server', 'https://github.com/inkdropapp/mcp-server', 'Inkdrop Model Context Protocol Server', '2025-03-07 07:04:47+00', '2025-03-08 14:37:05+00', 'JavaScript', '10', '## Inkdrop MCP Server

A [Model Context Protocol](https://github.com/modelcontextprotocol) server for the [Inkdrop Local HTTP Server API](https://developers.inkdrop.app/data-access/local-http-server).

> [!WARNING]
> This is highly experimental and use at your own risk.

## Installation

1. [Set up a local HTTP server](https://developers.inkdrop.app/guides/access-the-local-database#accessing-via-http-advanced)

2. Add server config to Claude Desktop:
   - MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`
   - Windows: `%APPDATA%\Claude\claude_desktop_config.json`

```json
{
  "mcpServers": {
    "inkdrop": {
      "command": "npx",
      "args": [
        "-y",
        "@inkdropapp/mcp-server"
      ],
      "env": {
        "INKDROP_LOCAL_SERVER_URL": "http://localhost:19840",
        "INKDROP_LOCAL_USERNAME": "your-local-server-username"
        "INKDROP_LOCAL_PASSWORD": "your-local-server-password"
      }
    }
  }
}
```

## Components

### Tools

1. **`read-note`**: Retrieve the complete contents of the note by its ID from the database.
   - Required inputs:
     - `noteId`: The ID of the note to retrieve.
2. **`search-notes`**: List all notes that contain a given keyword.
   - Required inputs:
     - `keyword`: Keyword to search for.
3. **`create-note`**: Create a new note in the database
   - Required inputs:
     - `bookId`: The notebook ID
     - `title`: The note title
     - `body`: The content of the note in Markdown
   - Optional inputs:
     - `status`: The note status (`none`, `active`, `onHold`, `completed`, `dropped`)
4. **`update-note`**: Update the existing note in the database
   - Required inputs:
     - `_id`: The note ID
     - `_rev`: The revision ID
     - `bookId`: The notebook ID
     - `title`: The note title
     - `body`: The content of the note in Markdown
5. **`list-notebooks`**: Retrieve a list of all notebooks

## Debugging

Since MCP servers run over stdio, debugging can be challenging. For the best debugging
experience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).

You can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:

```bash
npx @modelcontextprotocol/inspector "./dist/index.js"
```

Be sure that environment variables are properly configured.

Upon launching, the Inspector will display a URL that you can access in your browser to begin debugging.

You can also watch the server logs with this command:

```bash
tail -n 20 -f ~/Library/Logs/Claude/mcp-server-inkdrop.log
```
', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states that it is a Model Context Protocol server for Inkdrop, interfacing with the Inkdrop Local HTTP Server API. It implements specific tools for note management.","categories":["Protocol Implementations","AI Model Integration","Function Calling"],"analysis":{"Architecture and Design Patterns":"The repository implements a server that conforms to the Model Context Protocol. It exposes functionalities of Inkdrop's local HTTP server as tools accessible through the MCP, suggesting a client-server architecture where the MCP server acts as an intermediary between an AI model and Inkdrop's data.","Implementation Details and Key Features":"The server provides tools for reading, searching, creating, and updating notes, as well as listing notebooks within Inkdrop. These tools accept specific inputs like note IDs, keywords, and note content. The implementation likely involves translating MCP requests into Inkdrop API calls.","Integration Approaches and Protocols":"The server integrates with Inkdrop via its local HTTP server API. Communication occurs over stdio, as evidenced by the debugging recommendations. The repository configures a command to launch the server using `npx` and sets environment variables for Inkdrop server URL and credentials.","Technical Stack and Dependencies":"The repository uses Node.js and likely relies on `npm` for package management. It depends on the Inkdrop Local HTTP Server API. The debugging instructions suggest a dependency on the `@modelcontextprotocol/inspector` package.","Best Practices and Coding Standards":"The documentation includes a warning about the experimental nature of the project. The debugging guide recommends using the MCP Inspector, which implies adherence to MCP debugging standards. Environment variables are utilized for sensitive data, suggesting security considerations.","Performance Considerations":"Since the MCP server communicates over stdio, performance will be influenced by the overhead of inter-process communication. The Inkdrop Local HTTP Server API performance would also impact the MCP server's responsiveness. Caching and request optimization might be necessary for high-volume usage."}}', '2025-03-10 21:28:34+00', 'true', 'Servers', 'mcp-inkdrop-server'), ('944385587', 'zipic-mcp-server', 'okooo5km/zipic-mcp-server', 'https://github.com/okooo5km/zipic-mcp-server', 'A Model Context Protocol server that provides image compression and optimization capabilities through Zipic app.', '2025-03-07 09:00:12+00', '2025-03-07 22:40:27+00', 'Swift', '2', '# Zipic MCP Server

A Model Context Protocol server that provides image compression and optimization capabilities through Zipic app. This server enables LLMs to compress and optimize images using Zipic's powerful features.

![Swift](https://img.shields.io/badge/Swift-6.0+-orange.svg)
![Platform](https://img.shields.io/badge/Platform-macOS%2014.0+-brightgreen.svg)
![License](https://img.shields.io/badge/License-MIT-blue.svg)

## Available Tools

* `quickCompress` - Quickly compress images using Zipic's default settings
  * `urls` (array of strings, required): Array of image URLs to compress

* `advancedCompress` - Compress images with advanced options
  * `urls` (array of strings, required): Array of image URLs to compress
  * `format` (string, optional): Output format (e.g., "webp", "jpg", "png")
  * `level` (number, optional): Compression level (1-100)
  * `width` (number, optional): Target width
  * `height` (number, optional): Target height
  * `directory` (string, optional): Custom output directory when location is "custom"
  * `suffix` (string, optional): Custom suffix when addSuffix is true

## Installation

### Option 1: Download Pre-built Binary

1. Download and install [Zipic](https://zipic.app) app
2. Download the latest `zipic-mcp-server` binary from [GitHub Releases](https://github.com/okooo5km/zipic-mcp-server/releases)
3. Make the binary executable:

   ```bash
   chmod +x /path/to/zipic-mcp-server
   ```

4. Move the binary to a directory in your PATH (optional):

   ```bash
   sudo mv /path/to/zipic-mcp-server /usr/local/bin/
   ```

### Option 2: Build from Source

1. Download and install [Zipic](https://zipic.app) app
2. Clone the repository:

   ```bash
   git clone https://github.com/5km/zipic-mcp-server.git
   cd zipic-mcp-server
   ```

3. Choose a build option:

   ```bash
   # Option A: Build for Apple Silicon (arm64)
   swift build -c release --arch arm64 -j $(sysctl -n hw.ncpu)
   
   # Option B: Build for Intel (x86_64)
   swift build -c release --arch x86_64 -j $(sysctl -n hw.ncpu)
   
   # Option C: Build Universal Binary (both arm64 and x86_64)
   swift build -c release --arch arm64 -j $(sysctl -n hw.ncpu)
   swift build -c release --arch x86_64 -j $(sysctl -n hw.ncpu)
   mkdir -p .build/bin
   lipo -create \
     -output .build/bin/zipic-mcp-server \
     $(swift build -c release --arch arm64 --show-bin-path)/zipic-mcp-server \
     $(swift build -c release --arch x86_64 --show-bin-path)/zipic-mcp-server
   ```

4. Install the binary (optional):

   ```bash
   # After building, choose ONE of these commands based on your build choice:
   
   # If you built for arm64 (Option A):
   sudo cp .build/apple/Products/Release/zipic-mcp-server /usr/local/bin/
   
   # If you built for x86_64 (Option B):
   sudo cp .build/x86_64-apple-macosx/release/zipic-mcp-server /usr/local/bin/
   
   # If you built universal binary (Option C):
   sudo cp .build/bin/zipic-mcp-server /usr/local/bin/
   ```

## Configuration

### Configure for Claude.app

Add to your Claude settings:

```json
"mcpServers": {
  "zipic": {
    "command": "zipic-mcp-server"
  }
}
```

### Configure for cursor

![cursor](screenshots/cursor.webp)

## Development Requirements

* Swift 6.0 or later
* macOS 14.0 or later
* Zipic app installed
* [MCP Swift SDK](https://github.com/gsabran/mcp-swift-sdk) 0.2.0 or later

## Version History

See [GitHub Releases](https://github.com/okooo5km/zipic-mcp-server/releases) for version history and changelog.

## License

zipic-mcp-server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states it is a \"Model Context Protocol server\" and provides integration instructions for Claude.app, a tool that leverages MCP for interacting with AI models.","categories":["Protocol Implementations","AI Model Integration","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The server architecture appears to be designed as a command-line tool that integrates with applications like Claude.app. It exposes functionalities like image compression via a defined protocol, likely adhering to the MCP specifications. The use of distinct commands like `quickCompress` and `advancedCompress` suggests a modular design.","Implementation Details and Key Features":"The server provides two primary functionalities: `quickCompress` and `advancedCompress`. It handles image compression tasks. The `advancedCompress` feature allows users to specify various compression parameters, demonstrating a focus on customizable image optimization. It's implemented in Swift and requires macOS 14.0 or later.","Integration Approaches and Protocols":"Integration with tools like Claude.app is achieved via MCP. The server registers itself in the host application's settings, allowing it to be invoked with specific commands and parameters. This conforms to the MCP pattern of providing external tool access to LLMs. The provided JSON configuration snippet highlights this integration.","Technical Stack and Dependencies":"The project uses Swift 6.0 or later and requires macOS 14.0 or later. It depends on the Zipic app for the actual image compression functionality. It uses MCP Swift SDK. Build options are provided for both Apple Silicon and Intel architectures, including universal binary builds using `lipo`.","Best Practices and Coding Standards":"The presence of a LICENSE file (MIT License) suggests adherence to open-source licensing best practices. The provision of pre-built binaries and build-from-source options indicates a focus on user accessibility and developer flexibility. Clear instructions for installation and configuration are provided in the documentation.","Performance Considerations":"The documentation mentions compression levels as an option for `advancedCompress`. This suggests a focus on balancing image quality and compression ratio. The use of Swift for implementation may provide good performance on macOS. Multiple build options cater to specific hardware architectures for optimal performance."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'zipic-mcp-server'), ('944410851', 'whois-mcp', 'modelcontextprotocol-servers/whois-mcp', 'https://github.com/modelcontextprotocol-servers/whois-mcp', null, '2025-03-07 09:51:02+00', '2025-03-10 14:34:50+00', 'JavaScript', '2', '# Whois MCP

[Model Context Protocol](https://modelcontextprotocol.io) server for whois lookups.

## Overview

This MCP server allows AI agents like Claude Desktop, Cursor, Windsurf,.. etc to perform WHOIS lookups and retrieve domain details. 

**Purpose**
You can directly ask the AI to check if a domain is available, who owns it, when it was registered, and other important details. No need to go to browser and search.

**What is a WHOIS Lookup?**
A WHOIS lookup is the process of querying a WHOIS database to retrieve registration details about a domain name, IP address, or autonomous system. It helps users find out who owns a domain, when it was registered, when it expires, and other important details.

**What Information Can a WHOIS Lookup Provide?**

When you perform a WHOIS lookup, you can retrieve details such as:

- Domain Name – The specific domain queried
- Registrar Name – The company managing the domain registration (e.g., GoDaddy, Namecheap)
- Registrant Details – The name, organization, and contact details of the domain owner (unless protected by WHOIS privacy)
- Registration & Expiry Date – When the domain was registered and when it will expire
- Name Servers – The DNS servers the domain is using
- Domain Status – Active, expired, locked, or pending deletion
- Contact Information – Administrative, technical, and billing contacts (if not hidden)

## Available Tools

| Tool                  | Description                                |
| --------------------- | ------------------------------------------ |
| `whois_domain`        | Looksup whois information about the domain |
| `whois_tld`           | Looksup whois information about the Top Level Domain (TLD)    |
| `whois_ip`            | Looksup whois information about the IP     |
| `whois_as`            | Looksup whois information about the Autonomous System Number (ASN)     |

## Using with Cursor

**Installation - Globally**

Run the MCP server using npx:

```bash
npx -y @mcp-server/whois-mcp@latest
```

In your Cursor IDE

1. Go to `Cursor Settings` > `MCP`
2. Click `+ Add New MCP Server`
3. Fill in the form:
   - Name: `Whois Lookup` (or any name you prefer)
   - Type: `command`
   - Command: `npx -y @mcp-server/whois-mcp@latest`


**Installation - Project-specific**

Add an `.cursor/mcp.json` file to your project:

```json
{
  "mcpServers": {
    "whois": {
      "command": "npx",
      "args": [
        "-y",
        "@mcp-server/whois-mcp@latest"
      ]
    }
  }
}
```

**Usage**

Once configured, the whois tools will be automatically available to the Cursor AI Agent. You can:

1. The tool will be listed under `Available Tools` in MCP settings
2. Agent will automatically use it when relevant
3. You can explicitly ask Agent to send notifications

## Using with Roo Code
Access the MCP settings by clicking "Edit MCP Settings" in Roo Code settings or using the "Roo Code: Open MCP Config" command in VS Code's command palette.

```json
{
  "mcpServers": {
    "whois": {
      "command": "npx",
      "args": [
        "-y",
        "@mcp-server/whois-mcp@latest"
      ]
    }
  }
}
```
3. The whois capabilities will be available to Roo Code's AI agents

## Development

```bash
# Install dependencies
pnpm install

# Build
pnpm build

```

## Debugging the Server

To debug your server, you can use the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).

First build the server

```
pnpm build
```

Run the following command in your terminal:

```
# Start MCP Inspector and server with all tools
npx @modelcontextprotocol/inspector node dist/index.js
```

## License

MIT
', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository is explicitly described as a Model Context Protocol server for whois lookups, designed for use with AI agents like Cursor and Roo Code. It implements function calling using MCP.","categories":["Protocol Implementations","Function Calling","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The repository implements a server that conforms to the Model Context Protocol (MCP). Its architecture is centered around providing WHOIS lookup capabilities to AI agents through a command-line interface, utilizing npx to run the server. Configuration is facilitated via JSON files, allowing integration with specific IDEs.","Implementation Details and Key Features":"The key feature is providing WHOIS lookup tools (`whois_domain`, `whois_tld`, `whois_ip`, `whois_as`) to AI agents. Implementation involves creating a server that can be invoked via the command line. The server logic retrieves domain information, and exposes that information in a structured manner for AI consumption.","Integration Approaches and Protocols":"Integration is primarily achieved through MCP, allowing tools like Cursor and Roo Code to access WHOIS functionalities. The server is designed to be launched as a command-line process. The repository supports direct integration with IDEs by configuring MCP server details.","Technical Stack and Dependencies":"The technical stack includes Node.js for the server runtime. Dependencies are managed through pnpm and likely include packages to interact with WHOIS databases and to implement the MCP protocol. The project uses TypeScript for development, which provides static typing and improved code maintainability.","Best Practices and Coding Standards":"The repository includes a build process using pnpm, suggesting adherence to standard JavaScript/TypeScript project structures. Documentation includes instructions for installation, configuration, and usage with specific IDEs.  The use of TypeScript and pnpm indicates consideration for code quality and dependency management.","Performance Considerations":"Performance considerations would involve optimizing WHOIS lookup times, potentially through caching or efficient querying strategies.  The design allows for quick invocation through the command line, important for real-time interactions with AI agents. Efficient handling of asynchronous operations is likely crucial for maintaining responsiveness."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'whois-mcp-server'), ('944417628', 'mcp-cn-a-stock', 'elsejj/mcp-cn-a-stock', 'https://github.com/elsejj/mcp-cn-a-stock', '这是一个为大模型提供 A 股数据的的 MCP(Model Content Protocol) 服务。', '2025-03-07 10:04:45+00', '2025-03-10 14:37:21+00', 'Python', '65', '# 简介

这是一个为大模型提供 A 股数据的的 MCP(Model Content Protocol) 服务。

# MCP Tools

目前提供一下的若干工具：

- brief: 给定股票的基本信息，行情数据
- medium: 提供所有基本数据和一些财务数据
- full: 提供所有中等数据和技术指标

# 使用方法

由于股票的数据比较庞大，所以我们提供了一个公开的服务地址, 查看 [tests/test.sh](tests/test.sh) 文件，里面有一些测试用例以及相关的使用方法。
', 'true', '"{\"AI Model Integration\",\"Context Management\",\"API Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states that it's an MCP (Model Content Protocol) service designed to provide A-share (China stock market) data to large models.","categories":["AI Model Integration","Function Calling","API Frameworks"],"analysis":{"Architecture and Design Patterns":"The architecture appears to be a data service tailored for large language models. It exposes different levels of detail via function calls (brief, medium, full). The repository seems to follow a client-server model where LLMs act as clients requesting stock data from this service. Further examination would be required to understand the underlying data storage and retrieval mechanisms.","Implementation Details and Key Features":"The implementation provides three tools (brief, medium, full) that deliver varying levels of A-share data to LLMs. The documentation highlights the availability of a public service address and refers to a test script (test.sh) for usage examples. This suggests that the service is API-driven and designed for easy integration into LLM workflows through function calling.","Integration Approaches and Protocols":"The repository leverages MCP (Model Content Protocol) for AI model integration. The test.sh file likely showcases the API usage, revealing how to request data for specific stocks using the provided tools. The integration approach emphasizes providing structured data tailored to meet the context window needs of the AI model, optimizing for information relevance and utility.","Technical Stack and Dependencies":"Based on the provided information, the specific technologies are not explicit. However, it can be inferred that it likely depends on a database system to store A-share data, an API framework to expose the data, and potentially data processing libraries. The runtime environment is not specified, it would require investigation to determine stack and dependencies.","Best Practices and Coding Standards":"Without examining the code itself, it's impossible to definitively assess adherence to best practices. The presence of a test script (test.sh) is a positive indicator of a testing strategy. However, a comprehensive code review would be necessary to evaluate coding standards, error handling, and security considerations.","Performance Considerations":"Since the service handles potentially large datasets, performance is likely a significant consideration. The offering of different data levels (brief, medium, full) may be designed to optimize response times based on the specific needs of the AI model. Further investigation would reveal specifics regarding caching, data indexing, and query optimization strategies."}}', '2025-03-10 21:28:23+00', 'true', 'Servers', 'mcp-a-stock-data'), ('944446528', 'mcp-devtools', 'DXHeroes/mcp-devtools', 'https://github.com/DXHeroes/mcp-devtools', 'MCP DevTools: A suite of Model Context Protocol servers enabling AI assistants to interact with developer tools and services', '2025-03-07 11:06:05+00', '2025-03-10 11:14:21+00', 'TypeScript', '3', '# MCP DevTools

![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)
[![Beta Status](https://img.shields.io/badge/status-beta-orange)](https://github.com/modelcontextprotocol/mcp-devtools)

MCP (Model Context Protocol) DevTools is a collection of packages that enable AI assistants to interact with external tools and services through the Model Context Protocol.

## ✨ Highlights

- 🔌 **Seamless Integration**: Connect AI assistants to external services and tools
- 🛠 **Extensible Framework**: Easily create new integrations with the Model Context Protocol
- 🔍 **Powerful Interactions**: Enable AI to access and manipulate data from external services
- 📊 **Robust Integrations**: Comprehensive functionality for Jira and Linear
- 🚀 **Developer-Friendly**: Simple setup with detailed documentation for the best developer experience

> **Note**: This project is currently in beta (0.x.x versions). APIs may change between minor versions during the beta phase.

## 📦 Available Packages

| Package                                             | Description                   | Status                                                                                                                      |
| --------------------------------------------------- | ----------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| [@mcp-devtools/jira](./packages/jira/README.md)     | Jira MCP server integration   | [![npm version](https://img.shields.io/npm/v/@mcp-devtools/jira.svg)](https://www.npmjs.com/package/@mcp-devtools/jira)     |
| [@mcp-devtools/linear](./packages/linear/README.md) | Linear MCP server integration | [![npm version](https://img.shields.io/npm/v/@mcp-devtools/linear.svg)](https://www.npmjs.com/package/@mcp-devtools/linear) |

## 🚀 Quick Start

### Configuration in Cursor IDE

#### Jira Integration

1. Open Cursor Settings → MCP
2. Click "Add New MCP Server"
3. Fill in the following details:
   - **Name**: `Jira`
   - **Type**: `command`
   - **Command**:
     `env JIRA_URL=https://[YOUR_WORKSPACE].atlassian.net JIRA_API_MAIL=[YOUR_EMAIL] JIRA_API_KEY=[YOUR_API_KEY] npx -y @mcp-devtools/jira`

> **Required Environment Variables**:
>
> - `JIRA_URL`: Your Jira instance URL (e.g., `https://your-company.atlassian.net`)
> - `JIRA_API_MAIL`: Your Atlassian account email
> - `JIRA_API_KEY`: Your Atlassian API key ([Create one here](https://id.atlassian.com/manage-profile/security/api-tokens))

#### Linear Integration

1. Open Cursor Settings → MCP
2. Click "Add New MCP Server"
3. Fill in the following details:
   - **Name**: `Linear`
   - **Type**: `command`
   - **Command**:
     `env LINEAR_API_KEY=[YOUR_API_KEY] npx -y @mcp-devtools/linear`

> **Required Environment Variables**:
>
> - `LINEAR_API_KEY`: Your Linear API key (Create one in Linear app: Settings → API → Create Key)

### Using Tools

Once configured, you can interact with tools through natural language commands in Cursor.

#### Jira Examples:

```
# Fetch a specific ticket
get ticket SCRUM-123

# Search for tickets
execute jql "project = SCRUM AND status = 'In Progress'"

# Get ticket details
read ticket SCRUM-123

# Create a new ticket
create ticket project=SCRUM summary="Fix login bug" description="Users can't log in" issuetype=Bug
```

#### Linear Examples:

```
# Get a specific issue
get_issue SS-33

# Search for issues
search_issues "priority is high" with limit 5

# Create a new issue
create_issue for team "eng" titled "Fix API response format" with description "The API is returning incorrect data format" and priority 1

# List teams
list_teams
```

For a complete list of available commands, refer to the package documentation:

- [Jira Package Documentation](./packages/jira/README.md)
- [Linear Package Documentation](./packages/linear/README.md)

## 📖 Documentation

- [Jira Package Documentation](./packages/jira/README.md)
- [Linear Package Documentation](./packages/linear/README.md)
- [Getting Started Guide](./docs/getting-started.md)
- [Contributing Guidelines](./CONTRIBUTING.md)

## 🧩 Repository Structure

```
mcp-devtools/
├── core/                 # Infrastructure and utility packages
│   ├── typescript-config/  # Shared TypeScript configuration
│   └── http-client/        # HTTP client utilities
│
├── packages/             # Functional MCP server packages
│   ├── jira/               # Jira integration MCP server
│   │   └── README.md         # Package documentation
│   └── linear/             # Linear integration MCP server
│       └── README.md         # Package documentation
│
└── ...
```

## 🛠 Development

```bash
# Install dependencies
pnpm install

# Build all packages
pnpm build

# Development with auto-rebuild
pnpm dev
```

## 🤝 Contributing

Contributions are welcome! Please check our [Contributing Guidelines](./CONTRIBUTING.md) for details.

## 📄 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## 🆘 Support

- **GitHub Issues**: For bug reports and feature requests
- **Discussions**: For questions and community support

## Project Structure

```
mcp-devtools/
├── core/ # Infrastructure and utility packages
│ ├── typescript-config/ # Shared TypeScript configuration
│ └── http-client/ # HTTP client utilities
│
├── packages/ # Functional MCP server packages
│ ├── jira/ # Jira integration MCP server
│ └── linear/ # Linear integration MCP server
├── package.json # Root package configuration
└── pnpm-workspace.yaml # Workspace configuration
```

## Development

### Getting Started

This repository uses pnpm workspaces for package management. To get started:

1. Install pnpm if you don't have it:

   ```bash
   npm install -g pnpm
   ```

2. Install dependencies:

   ```bash
   pnpm install
   ```

3. Build all packages:
   ```bash
   pnpm build
   ```

### Development Workflow

For development with auto-rebuild:

```bash
pnpm dev
```

### Publishing to NPM

This repository is set up with automated release management using release-please and GitHub Actions for publishing packages to npmjs.org.

#### Beta Status

All published packages are currently in beta status (0.x.x versions) and use the `beta` npm tag. During this phase:

- Breaking changes may occur in minor version updates
- Install the packages using: `npm install @mcp-devtools/package-name@beta`
- When the project reaches stability, we will release version 1.0.0

### Debugging

Since MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a workspace script:

```bash
pnpm inspector
```

The Inspector will provide a URL to access debugging tools in your browser.

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Conventional Commits

This project uses [Conventional Commits](https://www.conventionalcommits.org/) to automate versioning and changelog generation. Please format your commit messages following this pattern:

```
<type>[optional scope]: <description>

[optional body]

[optional footer(s)]
```

Types:

- `feat`: A new feature
- `fix`: A bug fix
- `docs`: Documentation changes
- `style`: Changes that don't affect the code's meaning (formatting, etc.)
- `refactor`: Code changes that neither fix bugs nor add features
- `perf`: Performance improvements
- `test`: Adding or fixing tests
- `chore`: Changes to the build process or auxiliary tools

Examples:

```
feat(jira): add comment creation endpoint
fix(http-client): resolve timeout issue
docs: update README with new setup instructions
```

Breaking changes should be indicated by adding an exclamation mark after the type/scope and describing the breaking change in the body of the commit message:

```
feat!: redesign http-client API

BREAKING CHANGE: The http-client API has been completely redesigned to improve usability.
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Troubleshooting

### Common Issues

1. **Connection Problems**

   - Ensure your API credentials are correct
   - Check network connectivity to your service instances
   - Verify URLs and workspace names

2. **Permission Errors**

   - Ensure your accounts have appropriate permissions for the actions you're attempting
   - API tokens may need specific permissions enabled in your account settings

3. **Command Not Found**
   - If using npx, ensure you're connected to npm registry
   - For local installations, check that your package installation was successful

For more troubleshooting help, open an issue on our GitHub repository.

## Roadmap

Future development plans for MCP DevTools include:

- Additional service integrations (GitHub, Confluence, etc.)
- Enhanced security features
- Support for custom authentication methods
- Expanded querying capabilities
- Performance optimizations

## Community and Support

- **GitHub Issues**: For bug reports and feature requests
- **Discussions**: For questions and community support
- **Contributing**: See our contributing guidelines above

We welcome feedback and contributions from the community to help improve these tools.
', 'true', '"{\"Protocol Implementations\",\"Developer Tooling\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it is a suite of Model Context Protocol servers and aims to enable AI assistants to interact with developer tools and services, aligning with the MCP definition.","categories":["AI Model Integration","Developer Tooling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The repository uses a modular design, evident from the `core` and `packages` directories. It utilizes `pnpm` workspaces for managing multiple packages. The architecture separates infrastructure components (like HTTP clients) from the functional MCP server implementations for Jira and Linear, promoting maintainability and reusability. The use of TypeScript provides static typing benefits.","Implementation Details and Key Features":"The project provides MCP server integrations for Jira and Linear. These integrations enable AI assistants to interact with these tools using natural language commands. Key features include the ability to fetch, search, create, and read tickets/issues. The implementation uses environment variables for configuration, allowing for easy setup in different environments.","Integration Approaches and Protocols":"The integrations are designed to communicate with external services through APIs. The `MCP` protocol is used to bridge the gap between AI assistants and these tools. The documentation details how to configure the MCP servers within the Cursor IDE using command-line interfaces with environment variables for authentication, highlighting a standardized integration approach.","Technical Stack and Dependencies":"The primary technical stack includes TypeScript, Node.js, and `pnpm` for package management. Dependencies include libraries for making HTTP requests and interacting with the Jira and Linear APIs. The project utilizes `release-please` and GitHub Actions for automated release management and publishing to npmjs.org, indicating a modern CI/CD pipeline.","Best Practices and Coding Standards":"The repository uses Conventional Commits to automate versioning and changelog generation, indicating a commitment to maintainable development practices. The inclusion of a contributing guide promotes community contributions. The use of TypeScript and linting configurations enforces coding standards, improving code quality and consistency across the project.","Performance Considerations":"The documentation acknowledges that debugging MCP servers can be challenging due to stdio communication and recommends using the MCP Inspector. No specific performance metrics or optimizations are explicitly mentioned. However, the asynchronous nature of API interactions implies consideration for non-blocking operations to avoid impacting the performance of the AI assistant."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'mcp-devtools-servers'), ('944516885', 'mcp-server', 'financial-datasets/mcp-server', 'https://github.com/financial-datasets/mcp-server', 'An MCP server for interacting with Financial Datasets', '2025-03-07 13:39:26+00', '2025-03-08 10:05:43+00', 'Python', '31', '# Financial Datasets - MCP Server
An MCP server for interacting with the Financial Datasets API.
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Financial Services\"]}', '{"is_mcp":"YES","justification":"The repository is explicitly described as an 'MCP server' for interacting with Financial Datasets. This aligns with the Model Context Protocol's purpose of enabling communication between AI models and external resources.","categories":["Protocol Implementations","API Frameworks","Financial Services"],"analysis":{"Architecture and Design Patterns":"The repository appears to implement a server architecture, designed to handle requests and responses in accordance with the Model Context Protocol. It likely follows a client-server model, where AI models act as clients and the Financial Datasets API, accessed via this server, acts as the resource. Further details would require examining the code.","Implementation Details and Key Features":"The core functionality revolves around providing access to Financial Datasets via an MCP server. Key features would likely include request handling, data retrieval, and potentially data transformation to ensure compatibility with client models. The implementation likely involves API endpoints and data access logic.","Integration Approaches and Protocols":"The repository emphasizes the use of the Model Context Protocol (MCP) for integration. This suggests a focus on standardized communication between AI models and the Financial Datasets API. It may implement specific MCP message formats and data structures to facilitate seamless interaction.","Technical Stack and Dependencies":"Without specific details, it is challenging to define the exact stack. Common server-side technologies (e.g., Python with Flask/FastAPI, Node.js with Express) are likely used. Dependencies would include libraries for handling network communication, data parsing, and interacting with the Financial Datasets API.","Best Practices and Coding Standards":"Determining adherence to best practices is difficult without code inspection. However, a well-structured MCP server would typically follow coding standards relevant to its language, employ modular design principles, and include logging/error handling mechanisms for debugging and maintenance.","Performance Considerations":"Performance considerations for an MCP server would include efficient data retrieval from the Financial Datasets API, optimized data serialization/deserialization for network transmission, and robust error handling to prevent service disruptions. Caching mechanisms might be implemented to reduce latency for frequently accessed data."}}', '2025-03-10 21:54:04+00', 'true', 'Servers', 'mcp-financial-server'), ('944528893', 'cursor-db-mcp', 'jbdamask/cursor-db-mcp', 'https://github.com/jbdamask/cursor-db-mcp', 'Model Context Protocol server for querying Cursor chat history', '2025-03-07 14:04:13+00', '2025-03-09 19:32:30+00', 'Python', '5', '# Cursor DB MCP Server

A Model Context Protocol (MCP) server for accessing Cursor IDE's SQLite databases. This server allows AI assistants to explore and interact with Cursor's project data, chat history, and composer information.

<!-- __Claude__
![In Claude GIF](./img/cursor-db-mcp-claude.gif) -->

__Cursor__
![In Cursor GIF](./img/cursor-db-mcp-in-cursor.gif)


## Prerequisites

Cursor IDE
<!-- Claude Desktop (if you want to use MCP in Claude) -->

## Installation

### Easy Installation

Use the provided installation script to install all dependencies:

```bash
python install.py
```

This script will install:
- Basic MCP server and dependencies

<!-- ### Manual Installation

1. Clone this repository:
```bash
git clone https://github.com/yourusername/cursor-db-mcp.git
cd cursor-db-mcp
```

2. Install basic dependencies:
```bash
pip install -r requirements.txt
```

3. Install MCP CLI tools (optional, for testing):
```bash
pip install 'mcp[cli]'  # Note the quotes around mcp[cli]
```

If the above command fails, you can install the CLI dependencies directly:
```bash
pip install typer rich
``` -->

<!-- ## Usage

### Using with Claude Desktop

1. Install the MCP server in Claude Desktop:
```bash
mcp install cursor-db-mcp-server.py
```

2. In Claude Desktop, you can now access your Cursor data by asking questions like:
   - "Show me a list of my Cursor projects"
   - "What's in my chat history for project X?"
   - "Find composer data for composer ID Y"

   See detailed examples below

Note: If Claude shows an error connecting to this MCP it's likely because it can't find uv. To fix this, change the command value to include the fully qualified path to uv. For example:
```
    "Cursor DB Manager": {
      "command": "/Users/johndamask/.local/bin/uv",
      "args": [
        "run",
        "--with",
        "mcp[cli]",
        "mcp",
        "run",
        "/Users/johndamask/code/cursor-db-mcp/cursor-db-mcp-server.py"
      ]
    }
``` -->

## Using with Cursor IDE

1. Open Cursor and navigate to Settings->Cursor Settings->MCP. 
2. Click: Add new MCP server
3. Name: Cursor DB MCP; Type: Command
4. Command: \<fully qualified path to\>uv run --with mcp[cli] mcp run \<fully qualified path to\>/cursor-db-mcp-server.py 

![Cursor DB MCP](./img/cursor-db-mcp.png)

Now you can ask questions about the database or retrieve info about historical chats.

![DB structure](./img/cursor-db-structure.png)

![DB keys](./img/cursor-db-keys.png)


## Available Resources

- `cursor://projects` - List all available Cursor projects
- `cursor://projects/detailed` - List projects with detailed information
- `cursor://projects/{project_name}/chat` - Get chat data for a specific project
- `cursor://projects/{project_name}/composers` - Get composer IDs for a specific project
- `cursor://composers/{composer_id}` - Get data for a specific composer

## Available Tools

- `query_table` - Query a specific table in a project's database
- `refresh_databases` - Refresh the list of database paths


<!-- # Example Usage with Claude

## Listing Projects

**User**: "Can you show me a list of my Cursor projects?"

**Claude**: 
```
I'll retrieve a list of your Cursor projects.

Here are the Cursor projects I found:
- project1 (path: /Users/username/Library/Application Support/Cursor/User/workspaceStorage/abc123/state.vscdb)
- project2 (path: /Users/username/Library/Application Support/Cursor/User/workspaceStorage/def456/state.vscdb)
- custom-project (path: /Users/username/code/custom-project/state.vscdb)
```

## Viewing Chat History

**User**: "Show me the chat history from my 'project1' project."

**Claude**:
```
I'll retrieve the chat history from your 'project1' project.

Here's the chat history I found:

Chat: "Understanding React Hooks"
- You: "Can you explain how useEffect works in React?"
- Claude: "The useEffect hook in React is used for handling side effects in functional components..."

Chat: "Debugging API Connection"
- You: "I'm getting a 401 error when connecting to the API. Here's my code..."
- Claude: "It looks like you're missing the authorization header in your request..."
```

## Querying Database Tables

**User**: "Can you search for keys related to 'settings' in the ItemTable of my 'project1' project?"

**Claude**:
```
I'll search for keys related to 'settings' in the ItemTable of your 'project1' project.

Here are the results:
1. Key: "workbench.settings.editor"
   Value: {"mode": "ui", "lastFocusedSettingId": "editor.fontSize"}

2. Key: "settings.language"
   Value: "en"

3. Key: "settings.theme"
   Value: "dark"
```

## Analyzing Chat Data

**User**: "Can you analyze the chat data from my 'project1' project and tell me what topics I've been discussing most frequently?"

**Claude**:
```
I'll analyze the chat data from your 'project1' project.

Based on your chat history, here are the most frequent topics you've been discussing:
1. React Hooks and Components (5 conversations)
2. API Integration (3 conversations)
3. CSS Styling (2 conversations)
4. Performance Optimization (2 conversations)
5. Debugging (1 conversation)

The most common questions were about state management in React and handling API responses.
``` 

# Architecture

The server uses the Model Context Protocol (MCP) to expose Cursor's SQLite databases to AI assistants. Key components include:

1. **Lifespan Context Management**: The server uses MCP's lifespan API to efficiently manage resources throughout the server's lifecycle.

2. **CursorDBManager**: Handles the detection and management of Cursor projects and their databases.

3. **Resources**: Expose data from Cursor databases as MCP resources.

4. **Tools**: Provide functionality to query databases and manage projects.

5. **Prompts**: Define reusable templates for AI interactions. -->

# How It Works

The server scans your Cursor installation directory to find project databases (state.vscdb files). It then exposes these databases through MCP resources and tools, allowing AI assistants to query and analyze the data.

# Notes
1. Cursor stores AI conversations in different places. Increasingly, chats are stored as "composerData" under globalStorage/state.vscdb. If you don't get results when asking about chats for recent projects, try asking for composers.
2. This was written on a Mac. YMMV with other OS

# Shameless Plug
<img src="./img/cursor-journal-logo_thumbnail.jpg" width="150" />

Like this? Try [Cursor Journal](https://medium.com/@jbdamask/building-cursor-journal-with-cursor-77445026a08c) to create DevLogs directly from Cursor chat history!

# License

MIT ', 'true', '"{\"Context Management\",\"Protocol Implementations\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it's a Model Context Protocol (MCP) server for querying Cursor chat history, and the documentation details how to install and use it within Claude and Cursor IDEs.","categories":["AI Model Integration","Context Management","Function Calling"],"analysis":{"Architecture and Design Patterns":"The server architecture revolves around exposing Cursor's SQLite databases to AI assistants using the Model Context Protocol. It manages resources efficiently using MCP's lifespan API. The `CursorDBManager` component is responsible for discovering and managing Cursor projects and their databases. Resources are designed to expose data from the Cursor databases.","Implementation Details and Key Features":"The key feature is the ability to query Cursor chat history and project data using AI assistants. It detects Cursor project databases, exposes them via MCP resources, and provides tools for querying tables and refreshing databases. The implementation includes resources to list projects, access chat data, and retrieve composer information.","Integration Approaches and Protocols":"The repository integrates with Cursor IDE and Claude using the Model Context Protocol. The integration involves configuring MCP servers within these environments and using specific commands to access the exposed resources and tools. It offers a protocol for AI models to interact with the Cursor IDE and its data.","Technical Stack and Dependencies":"The technical stack includes Python, SQLite databases, and potentially MCP CLI tools (typer, rich). The dependencies are defined in `requirements.txt` (though not explicitly listed in the provided documentation). It relies on the Cursor IDE and potentially Claude Desktop as client applications.","Best Practices and Coding Standards":"The documentation suggests using fully qualified paths for commands to avoid issues with locating executables. It also provides example usages and troubleshooting steps. No specific coding standards are detailed but the examples suggest a modular approach with clear resource definitions and tool functionalities.","Performance Considerations":"The documentation doesn't explicitly discuss performance, but the lifespan context management suggests an attempt to manage resources efficiently. The server scans Cursor installation directories for databases, and performance might depend on the number of projects and the size of the databases. Refreshing databases is provided as a tool."}}', '2025-03-10 21:28:38+00', 'true', 'Servers', 'cursor-db-mcp-server'), ('944583613', 'ergo-mcp', 'marctheshark3/ergo-mcp', 'https://github.com/marctheshark3/ergo-mcp', null, '2025-03-07 15:52:05+00', '2025-03-10 11:22:39+00', 'Python', '4', '# Ergo Explorer MCP Server

An MCP (Model Control Protocol) server for exploring and analyzing the Ergo blockchain.

## Features

- Check address balances
- Analyze transactions
- View transaction history
- Perform forensic analysis of addresses
- Search for tokens
- Monitor network status
- Direct node connection support
- ErgoWatch analytics integration

## Prerequisites

- Python 3.8+
- FastMCP package
- httpx
- python-dotenv

## Installation

1. Clone this repository:
```bash
git clone https://github.com/yourusername/ergo-explorer-mcp.git
cd ergo-explorer-mcp
```

2. Set up a virtual environment (recommended):
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows, use: .venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Configure your environment:
```bash
cp .env.example .env
```
Then edit the `.env` file with your specific settings.

## Configuration

### Environment Variables

The server can be configured through environment variables in the `.env` file:

- `ERGO_EXPLORER_API`: URL of the Ergo Explorer API (default: https://api.ergoplatform.com/api/v1)
- `ERGO_NODE_API`: URL of your Ergo Node API (default: http://localhost:9053)
- `ERGO_NODE_API_KEY`: Your Ergo Node API key, if required
- `SERVER_PORT`: Port to run the MCP server on (default: 3001)

### Using with Cursor

To use this MCP server with Cursor:

1. Open Cursor and go to Settings (gear icon)
2. Navigate to "AI" → "Claude" → "MCP Settings"
3. Click "Add MCP Server"
4. Configure with the following command:

```bash
/path/to/venv/python /path/to/ergo-explorer-mcp/run_server.py
```

Replace `/path/to/venv/python` with your virtual environment Python path and `/path/to/ergo-explorer-mcp` with the actual path where you cloned the repository.

### Using with Claude Desktop

To use this MCP server with Claude Desktop, add the following to your `claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "ergo-explorer": {
      "command": "python",
      "args": [
        "/path/to/ergo-explorer-mcp/run_server.py"
      ],
      "env": {
        "ERGO_EXPLORER_API": "https://api.ergoplatform.com/api/v1",
        "ERGO_NODE_API": "http://localhost:9053",
        "ERGO_NODE_API_KEY": "your-api-key",
        "SERVER_PORT": "3001"
      }
    }
  }
}
```

## Available Tools

### Explorer API Tools

- **get_address_balance**: Get the confirmed balance for an Ergo address
- **get_transaction_history**: Get the transaction history for an Ergo address
- **analyze_address**: Perform forensic analysis on an Ergo address
- **analyze_transaction**: Analyze transaction details
- **search_for_token**: Search for tokens on the Ergo blockchain
- **get_network_status**: Get current network status

### Node API Tools

- **get_node_wallet**: Get node wallet information
- **get_address_balance_from_node**: Get address balance from node
- **analyze_transaction_from_node**: Analyze transaction using node
- **get_transaction_history_from_node**: Get transaction history from node
- **get_network_status_from_node**: Get network status from node
- **search_for_token_from_node**: Search tokens using node

### ErgoWatch API Tools

- **get_address_balance_history**: Get balance history
- **get_address_balance_at_height**: Get balance at height
- **get_contract_stats**: Get contract statistics
- **get_p2pk_stats**: Get P2PK address statistics
- **get_exchange_addresses**: Get exchange address info
- **get_rich_list**: Get address rich list
- **get_address_rank**: Get address rank by balance

## Resource Endpoints

### Explorer API Resources
- `ergo://address/{address}/balance`: Address balance
- `ergo://transaction/{tx_id}`: Transaction details

### Node API Resources
- `ergo://node/address/{address}/balance`: Node address balance
- `ergo://node/transaction/{tx_id}`: Node transaction details

## API Reference

This server integrates with:
- [Ergo Explorer API](https://api.ergoplatform.com/api/v1)
- [Ergo Node API](https://github.com/ergoplatform/ergo/blob/master/src/main/resources/api/openapi.yaml)
- [ErgoWatch API](https://api.ergo.watch/docs)

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states that it is an MCP (Model Control Protocol) server and provides instructions for integrating it with tools like Cursor and Claude, indicating usage with AI models.","categories":["Protocol Implementations","API Frameworks","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The repository implements a server that acts as an intermediary between AI models/tools (like Cursor and Claude) and various Ergo blockchain data sources (Explorer API, Node API, ErgoWatch API). It utilizes a modular design, separating API interactions into distinct tool functions that can be invoked by the AI model. Configuration is managed through environment variables, which simplifies deployment.","Implementation Details and Key Features":"The core functionality revolves around exposing various tools for querying and analyzing the Ergo blockchain. The server offers tools to check balances, analyze transactions, search for tokens, and monitor network status. It supports direct connection to Ergo nodes for accessing node-specific data. The server uses `FastMCP` for protocol handling. Endpoints follow an `ergo://` scheme.","Integration Approaches and Protocols":"The repository outlines integration instructions with Cursor and Claude Desktop by configuring MCP servers. The integration involves providing the path to the Python server script and environment variables. The integration approaches suggest the AI tool uses the MCP server to execute commands and access data about the Ergo blockchain based on a predetermined specification.","Technical Stack and Dependencies":"The project is built using Python 3.8+ and relies on several key dependencies: `FastMCP` for handling Model Control Protocol, `httpx` for making HTTP requests to various Ergo APIs, and `python-dotenv` for managing environment variables. The dependencies suggest a focus on web-based data retrieval and a modular approach for implementing the MCP server.","Best Practices and Coding Standards":"The repository uses a virtual environment for dependency management, improving reproducibility. It also separates configuration from code using environment variables read from a `.env` file, which simplifies deployment and management. Example configurations for integration with external tools demonstrate clear documentation.","Performance Considerations":"The server interfaces with multiple external APIs (Ergo Explorer, Ergo Node, ErgoWatch), so network latency and API rate limits could impact performance. Caching of frequently accessed data could be implemented to improve response times. As the blockchain grows, optimizing data retrieval queries is crucial for maintaining performance."}}', '2025-03-10 21:54:04+00', 'true', 'Servers', 'ergo-mcp-server'), ('944603809', 'mcp-server.sqlite', 'javsanmar5/mcp-server.sqlite', 'https://github.com/javsanmar5/mcp-server.sqlite', 'A basic implementation of an MCP (Model Context Protocol) server for SQLite.', '2025-03-07 16:34:03+00', '2025-03-10 16:30:46+00', 'JavaScript', '0', '# SQLite MCP Server

A Model Context Protocol (MCP) server implementation using TypeScript for interacting with an SQLite database. This server provides an interactive interface for executing SQL queries, managing database schemas, and synthesizing business insights—all within an extensible protocol framework.


## Features

Nothing implemented yet.


## How to install this locally?

TODO


## How to run it?

A docker version will be add. However, now you must do this:

```bash
npm run build  # In order to build the compiled version

node dist/index.js
```

There are some files which need the compiled version of the server, if you are trying to run a file which doesn't you can just do:

```
npx ts-node
```

## License

This is project is licensed under the MIT License


## Contributing

I'm currently doing this with learning purposes so I think it makes no much sense to ask for contributing. But if you want to add something just make a Pull Request and I'll check it
Thanks!
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Database Systems\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states that it is an implementation of a Model Context Protocol (MCP) server for SQLite.","categories":["Protocol Implementations","API Frameworks","Database Systems"],"analysis":{"Architecture and Design Patterns":"The repository aims to provide a server implementation of the Model Context Protocol, using TypeScript. The architecture is intended to facilitate interaction with an SQLite database, enabling SQL queries and schema management within the MCP framework. The current documentation indicates that the server is in early development and lacks detailed architectural specifics.","Implementation Details and Key Features":"The implementation, written in TypeScript, interacts with an SQLite database. Current features are limited, with a focus on establishing the server framework. The documentation mentions the goal of synthesizing business insights but has not yet implemented this feature. The repository includes instructions for building and running the server.","Integration Approaches and Protocols":"The repository's focus is on implementing the Model Context Protocol to allow interaction between AI models and an SQLite database. The integration approach seems to involve creating an API endpoint that can receive MCP requests and translate them into SQL queries. However, no specifics are available about the protocol implementation itself.","Technical Stack and Dependencies":"The technical stack primarily consists of TypeScript, indicating the use of Node.js runtime environment. The `npm run build` command shows the use of a build system likely leveraging tools like `tsc` (TypeScript compiler). The dependency on SQLite is evident from the description, suggesting the use of an SQLite driver or library for Node.js.","Best Practices and Coding Standards":"The project uses TypeScript, which promotes strong typing and code maintainability. The inclusion of a build process (npm run build) suggests an effort towards code compilation and optimization. However, given the early stage of the project, it is difficult to assess adherence to broader best practices and coding standards.","Performance Considerations":"Given the interaction with an SQLite database, performance considerations would likely involve optimizing SQL queries and database schema design. The use of TypeScript allows for potential performance optimizations during compilation. No specific performance metrics or benchmarks are mentioned in the available documentation."}}', '2025-03-10 21:26:21+00', 'true', 'Servers', 'mcp-server-sqlite'), ('944628342', 'claude-code-mcp', 'auchenberg/claude-code-mcp', 'https://github.com/auchenberg/claude-code-mcp', 'claude-code-mcp', '2025-03-07 17:24:24+00', '2025-03-10 14:55:45+00', 'JavaScript', '22', '# Claude Code MCP

Claude Code MCP is an implementation of [Claude Code](https://gist.github.com/transitive-bullshit/487c9cb52c75a9701d312334ed53b20c) as a [Model Context Protocol (MCP)](https://modelcontextprotocol.io) server. This project allows you to use Claude Code's powerful software engineering capabilities through the standardized MCP interface.

## What is Claude Code?

Claude Code is Anthropic's CLI tool for software engineering tasks, powered by Claude. It provides a set of tools and capabilities that help developers with:

- Code generation and editing
- Code review and analysis
- Debugging and troubleshooting
- File system operations
- Shell command execution
- Project exploration and understanding

The original implementation is available as a JavaScript module that defines prompts and tools for interacting with Claude's API.

## What is MCP?

The Model Context Protocol (MCP) is a standardized interface for AI models that enables consistent interaction patterns across different models and providers. MCP defines:

- **Tools**: Functions that models can call to perform actions
- **Resources**: External data that models can access
- **Prompts**: Predefined conversation templates

By implementing Claude Code as an MCP server, we make its capabilities available to any MCP-compatible client, allowing for greater interoperability and flexibility.

## Features

- Full implementation of Claude Code functionality as an MCP server
- Provides tools for file operations, shell commands, and code analysis
- Exposes resources for accessing file system and environment information
- Includes prompts for general CLI interaction and code review
- Compatible with any MCP client
- TypeScript implementation with full type safety

## Installation

```bash
# Clone the repository
git clone https://github.com/auchenberg/claude-code-mcp.git
cd claude-code-mcp

# Install dependencies
npm install

# Build the project
npm run build
```

## Usage

### Running as a standalone server

```bash
# Start the server
npm start
```

### Using with MCP clients

Claude Code MCP can be used with any MCP client. Here's an example of how to connect to it using the MCP TypeScript SDK:

```typescript
import { Client } from "@modelcontextprotocol/sdk/client/index.js";
import { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js";

const transport = new StdioClientTransport({
  command: "node",
  args: ["dist/index.js"]
});

const client = new Client(
  {
    name: "example-client",
    version: "1.0.0"
  },
  {
    capabilities: {
      prompts: {},
      resources: {},
      tools: {}
    }
  }
);

await client.connect(transport);

// Use Claude Code through MCP
const result = await client.callTool({
  name: "bash",
  arguments: {
    command: "ls -la"
  }
});

console.log(result);
```

## Available Tools

Claude Code MCP provides the following tools:

- **bash**: Execute shell commands with security restrictions and timeout options
- **readFile**: Read files from the filesystem with options for line offsets and limits
- **listFiles**: List files and directories with detailed metadata
- **searchGlob**: Search for files matching a glob pattern
- **grep**: Search for text in files with regex pattern support
- **think**: A no-op tool for thinking through complex problems
- **codeReview**: Analyze and review code for bugs, security issues, and best practices
- **editFile**: Create or edit files with specified content

### Tool Details

#### bash

```typescript
{
  command: string;  // The shell command to execute
  timeout?: number; // Optional timeout in milliseconds (max 600000)
}
```

The bash tool includes security restrictions that prevent execution of potentially dangerous commands like `curl`, `wget`, and others.

#### readFile

```typescript
{
  file_path: string; // The absolute path to the file to read
  offset?: number;   // The line number to start reading from
  limit?: number;    // The number of lines to read
}
```

#### searchGlob

```typescript
{
  pattern: string;  // The glob pattern to match files against
  path?: string;    // The directory to search in (defaults to current working directory)
}
```

#### grep

```typescript
{
  pattern: string;   // The regular expression pattern to search for
  path?: string;     // The directory to search in (defaults to current working directory)
  include?: string;  // File pattern to include in the search (e.g. "*.js", "*.{ts,tsx}")
}
```

## Available Resources

- **file**: Access file contents (`file://{path}`)
  - Provides direct access to file contents with proper error handling
  - Returns the full text content of the specified file

- **directory**: List directory contents (`dir://{path}`)
  - Returns a JSON array of file information objects
  - Each object includes name, path, isDirectory, size, and modified date

- **environment**: Get system environment information (`env://info`)
  - Returns information about the system environment
  - Includes Node.js version, npm version, OS info, and environment variables

## Available Prompts

- **generalCLI**: General CLI prompt for Claude Code
  - Provides a comprehensive system prompt for Claude to act as a CLI tool
  - Includes guidelines for tone, style, proactiveness, and following conventions
  - Automatically includes environment details

- **codeReview**: Prompt for reviewing code
  - Specialized prompt for code review tasks
  - Analyzes code for bugs, security vulnerabilities, performance issues, and best practices

- **prReview**: Prompt for reviewing pull requests
  - Specialized prompt for PR review tasks
  - Analyzes PR changes and provides comprehensive feedback

- **initCodebase**: Initialize a new CLAUDE.md file with codebase documentation
  - Creates documentation for build/lint/test commands and code style guidelines
  - Useful for setting up a new project with Claude Code

## Development

```bash
# Run in development mode with auto-reload
npm run dev
```

## Architecture

Claude Code MCP is built with a modular architecture:

```
claude-code-mcp/
├── src/
│   ├── server/
│   │   ├── claude-code-server.ts  # Main server setup
│   │   ├── tools.ts               # Tool implementations
│   │   ├── prompts.ts             # Prompt definitions
│   │   └── resources.ts           # Resource implementations
│   ├── utils/
│   │   ├── bash.ts                # Shell command utilities
│   │   └── file.ts                # File system utilities
│   └── index.ts                   # Entry point
├── package.json
├── tsconfig.json
└── README.md
```

The implementation follows these key principles:

1. **Modularity**: Each component (tools, prompts, resources) is implemented in a separate module
2. **Type Safety**: Full TypeScript type definitions for all components
3. **Error Handling**: Comprehensive error handling for all operations
4. **Security**: Security restrictions for potentially dangerous operations

## Implementation Details

### MCP Server Setup

The main server is set up in `claude-code-server.ts`:

```typescript
export async function setupClaudeCodeServer(server: McpServer): Promise<void> {
  // Set up Claude Code tools
  setupTools(server);
  
  // Set up Claude Code prompts
  setupPrompts(server);
  
  // Set up Claude Code resources
  setupResources(server);
}
```

### Tool Implementation

Tools are implemented using the MCP SDK's tool registration method:

```typescript
server.tool(
  "toolName",
  "Tool description",
  {
    // Zod schema for tool arguments
    param1: z.string().describe("Parameter description"),
    param2: z.number().optional().describe("Optional parameter description")
  },
  async ({ param1, param2 }) => {
    // Tool implementation
    return {
      content: [{ type: "text", text: "Result" }]
    };
  }
);
```

### Resource Implementation

Resources are implemented using the MCP SDK's resource registration method:

```typescript
server.resource(
  "resourceName",
  new ResourceTemplate("resource://{variable}", { list: undefined }),
  async (uri, variables) => {
    // Resource implementation
    return {
      contents: [{
        uri: uri.href,
        text: "Resource content"
      }]
    };
  }
);
```

## License

MIT

## Acknowledgements

- [Claude Code](https://gist.github.com/transitive-bullshit/487c9cb52c75a9701d312334ed53b20c) by Anthropic
- [Model Context Protocol](https://modelcontextprotocol.io)
- [MCP TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk)

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## Disclaimer

This project is not officially affiliated with Anthropic. Claude Code is a product of Anthropic, and this project is an independent implementation of Claude Code as an MCP server.
', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"Agent Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements the Model Context Protocol (MCP) by creating an MCP server for Claude Code, allowing interaction through a standardized interface.","categories":["Protocol Implementations","Function Calling","Agent Frameworks"],"analysis":{"Architecture and Design Patterns":"The architecture is modular, separating tools, prompts, and resources into distinct modules. It uses TypeScript to enforce type safety. The server implementation utilizes the MCP SDK to register tools and resources, adhering to the protocol's specifications. Key principles include modularity, type safety, and security considerations, which prevent the execution of harmful commands.","Implementation Details and Key Features":"The repository implements Claude Code functionality as an MCP server, exposing tools for file operations, shell commands, and code analysis. Key features include the use of the MCP TypeScript SDK, tool registration, resource implementation, and security restrictions. Specific tools like 'bash', 'readFile', 'grep' are defined with their respective input parameters and functionalities.","Integration Approaches and Protocols":"The repository integrates with MCP clients using the MCP TypeScript SDK, providing a standardized way to interact with Claude Code's functionalities. It exposes tools and resources through the MCP interface, allowing other applications to access them in a consistent manner. The integration approach focuses on providing a standardized and interoperable interface.","Technical Stack and Dependencies":"The primary technical stack includes TypeScript, Node.js, and the MCP TypeScript SDK. Dependencies are managed using npm. The code base is structured into modules, including server, utils, and index. The server component is responsible for setting up the MCP server and registering tools, prompts, and resources.","Best Practices and Coding Standards":"The repository leverages TypeScript to ensure type safety and uses a modular structure for code organization. It also integrates error handling mechanisms and incorporates security measures to prevent potentially dangerous operations. The project is documented with clear explanations of its architecture, usage, and available tools and resources.","Performance Considerations":"The 'bash' tool includes timeout options to prevent long-running commands from impacting performance. Security restrictions are in place to avoid resource-intensive operations. The code is written in TypeScript, which helps improve performance due to its static typing. Efficient implementation of file and command operations is essential for overall performance."}}', '2025-03-10 21:28:31+00', 'true', 'Servers', 'claude-code-mcp'), ('944632918', 'okx-mcp', 'esshka/okx-mcp', 'https://github.com/esshka/okx-mcp', null, '2025-03-07 17:33:57+00', '2025-03-10 03:49:18+00', 'JavaScript', '2', '# OKX MCP Server

A Model Context Protocol server that provides real-time cryptocurrency price data from OKX exchange.

## Features

This MCP server connects to the OKX API to provide cryptocurrency price information through a simple tool interface. It includes comprehensive error handling, request logging, and rate limiting via OKX's API.

### Tools

#### `get_candlesticks`

Retrieves historical candlestick (OHLCV) data for any instrument on OKX.

- **Input**:
  - `instrument`: String (required) - Instrument ID (e.g. "BTC-USDT")
  - `bar`: String (optional) - Time interval (e.g. "1m", "5m", "1H", "1D"), default "1m"
  - `limit`: Number (optional) - Number of candlesticks to return (max 100), default 100
- **Output**: Array of JSON objects, each containing:
  - `timestamp`: ISO timestamp of the candlestick
  - `open`: Opening price
  - `high`: Highest price
  - `low`: Lowest price
  - `close`: Closing price
  - `volume`: Trading volume
  - `volumeCurrency`: Volume in currency terms

Example usage:

```json
[
  {
    "timestamp": "2025-03-07T17:00:00.000Z",
    "open": "87242.8",
    "high": "87580.2",
    "low": "86548.0",
    "close": "87191.8",
    "volume": "455.72150427",
    "volumeCurrency": "39661166.242091111"
  }
]
```

#### `get_price`

Fetches the latest price and 24-hour market data for any instrument on OKX.

- **Input**:
  - `instrument`: String (required) - Instrument ID (e.g. "BTC-USDT")
- **Output**: JSON object containing:
  - `instrument`: The requested instrument ID
  - `lastPrice`: Latest trade price
  - `bid`: Current best bid price
  - `ask`: Current best ask price
  - `high24h`: 24-hour high price
  - `low24h`: 24-hour low price
  - `volume24h`: 24-hour trading volume
  - `timestamp`: ISO timestamp of the data

Example usage:

```json
{
  "instrument": "BTC-USDT",
  "lastPrice": "65432.1",
  "bid": "65432.0",
  "ask": "65432.2",
  "high24h": "66000.0",
  "low24h": "64000.0",
  "volume24h": "1234.56",
  "timestamp": "2024-03-07T17:22:28.000Z"
}
```

## Development

Install dependencies:

```bash
npm install
```

Build the server:

```bash
npm run build
```

For development with auto-rebuild:

```bash
npm run watch
```

## Installation

To use with Claude Desktop or VSCode, add the server config to your MCP settings:

macOS (VSCode):

```bash
~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json
```

macOS (Claude Desktop):

```bash
~/Library/Application Support/Claude/claude_desktop_config.json
```

Windows (VSCode):

```bash
%APPDATA%/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json
```

Windows (Claude Desktop):

```bash
%APPDATA%/Claude/claude_desktop_config.json
```

Configuration:

```json
{
  "mcpServers": {
    "okx": {
      "command": "node",
      "args": ["/path/to/okx-mcp-server/build/index.js"],
      "disabled": false,
      "autoApprove": []
    }
  }
}
```

### Error Handling

The server implements comprehensive error handling:

- Network errors are captured and returned with context
- Invalid instrument IDs return appropriate error messages
- API rate limits are respected through axios timeout configuration
- All errors are logged for debugging purposes
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states that it is a Model Context Protocol (MCP) server designed to provide cryptocurrency price data from the OKX exchange to AI models/tools such as Claude.","categories":["AI Model Integration","Function Calling","API Frameworks"],"analysis":{"Architecture and Design Patterns":"The architecture follows a client-server model where the server exposes OKX API data through specific tools. The design pattern revolves around providing real-time data accessible via function calls. It supports configuration for Claude Desktop and VSCode, demonstrating integration with common AI development environments. It is designed to be easily installed and integrated into existing MCP workflows.","Implementation Details and Key Features":"The server is implemented in NodeJS and provides two main tools: `get_candlesticks` and `get_price`. It connects to the OKX API, retrieves data, and formats it into JSON outputs. The implementation includes error handling, rate limiting, and request logging. The `autoApprove` setting within configuration implies automated tool usage within the AI model context.","Integration Approaches and Protocols":"The integration relies on the Model Context Protocol. It expects a configuration file that specifies the command and arguments to execute the server. It is designed to integrate with Claude Desktop and VSCode. The tools provided expose specific functions callable by AI models. The server communicates via standard input/output streams.","Technical Stack and Dependencies":"The technical stack primarily consists of NodeJS and npm. `axios` is used for making HTTP requests to the OKX API. The documentation details the use of `npm` for installation and build processes. The server is compiled to Javascript to be run by node. The installation instructions assume familiarity with command line tools.","Best Practices and Coding Standards":"The documentation mentions comprehensive error handling. The server is expected to handle network errors, invalid instrument IDs, and API rate limits. Request logging is included for debugging. Rate limiting is also respected through axios configurations. The build and watch scripts suggest a modular approach and a focus on rapid development.","Performance Considerations":"The server needs to manage API rate limits effectively. The tool documentation includes limits on the number of candlesticks returned (max 100). Performance will be impacted by network latency. The server should be designed to handle concurrent requests from AI models. The use of NodeJS indicates an event-driven, non-blocking architecture."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'okx-mcp-server'), ('944635039', 'mcp-server-ios-simulator', 'atom2ueki/mcp-server-ios-simulator', 'https://github.com/atom2ueki/mcp-server-ios-simulator', 'Model Context Protocol (MCP) implementation for iOS simulators', '2025-03-07 17:38:45+00', '2025-03-10 08:56:54+00', 'TypeScript', '10', '# 📱 MCP Server for iOS Simulator

A server that implements the Model Context Protocol (MCP) for iOS simulators, built on top of [appium-ios-simulator](https://github.com/appium/appium-ios-simulator) and utilizing the [MCP TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk).

## 📋 Overview

This project provides a bridge between iOS simulators and the Model Context Protocol, allowing for standardized communication with iOS simulator instances. It enables programmatic control of iOS simulators while leveraging the MCP protocol for consistent interfaces across different environments. The server utilizes stdio as its transport mechanism, making it ideal for integration with Claude Desktop and other MCP-compatible clients.

## 🎬 Demo

![iOS Simulator Demo](demo/demo1.gif)

*Demo showing how to boot an iOS simulator using Claude AI Desktop*

## 🏗️ Architecture

The server consists of three main components:

1. **🔄 Simulator Management Layer** - Handles iOS simulator lifecycle and interactions
2. **🔌 MCP Protocol Implementation** - Implements the Model Context Protocol using the TypeScript SDK with stdio transport
3. **📊 Logger Component** - Provides file-based logging without interfering with the stdio transport

```
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│  MCP Protocol   │     │     Stdio       │     │    Simulator    │
│  Implementation │◄────┤    Transport    │◄────┤   Management    │
│                 │     │                 │     │      Layer      │
└─────────────────┘     └─────────────────┘     └─────────────────┘
        ▲                                                ▲
        │                                                │
        ▼                                                ▼
┌─────────────────┐                             ┌─────────────────┐
│   MCP Client    │                             │  iOS Simulator  │
│  (e.g. Claude)  │                             │                 │
└─────────────────┘                             └─────────────────┘
```

## ✨ Features

- 🚀 Start, stop, and manage iOS simulator instances
- 🔌 Boot and shutdown simulators
- 📲 Install and launch applications on simulators
- 📸 Take screenshots of simulator screens
- 👆 Perform taps on coordinates
- 🔄 Support for multiple concurrent simulator sessions
- 📝 Comprehensive file-based logging without console output
- 🛡️ Error-resilient operation

## 📋 Prerequisites

- 🟢 Node.js (v16 or later)
- 🍎 macOS (required for iOS simulators)
- 🛠️ Xcode with iOS simulators installed
- 📜 TypeScript 4.5+

## 🔧 Installation

```bash
# Clone the repository
git clone https://github.com/atom2ueki/mcp-server-ios-simulator.git
cd mcp-server-ios-simulator

# Install dependencies
npm install
```

## ⚙️ Configuration

Configuration is handled through the `src/config.ts` file:

```typescript
const config = {
  simulator: {
    defaultDevice: process.env.SIMULATOR_DEFAULT_DEVICE || 'iPhone 16',
    defaultOS: process.env.SIMULATOR_DEFAULT_OS || '18.2',
    timeout: parseInt(process.env.SIMULATOR_TIMEOUT || '30000', 10),
  }
};
```

You can customize these settings by setting environment variables:

```
SIMULATOR_DEFAULT_DEVICE=iPhone 16
SIMULATOR_DEFAULT_OS=18.2
SIMULATOR_TIMEOUT=30000
```

## 🚀 Usage

### 🔨 Building and Starting the Server

```bash
# Build the project
npm run build

# Start the server
npm start
```

### 🧰 MCP Tools

The server provides two distinct approaches for controlling iOS simulators:

#### 📱 Direct Simulator Management (Recommended)
These tools work directly with simulator UDIDs and don't require maintaining sessions:

- 📋 `list-available-simulators` - List all available simulators with their UDIDs
- ▶️ `boot-simulator-by-udid` - Boot a simulator directly using its UDID
- ⏹️ `shutdown-simulator-by-udid` - Shutdown a simulator directly using its UDID
- 📊 `list-booted-simulators` - List all currently booted simulators

**Use this approach when:** You just want to boot, use, and shut down simulators directly.

#### 📱 Session-Based Management (Advanced)
These tools use a session layer that tracks simulators with custom session IDs:

- 📋 `list-simulator-sessions` - List all active simulator sessions
- ➕ `create-simulator-session` - Create a new simulator session
- ❌ `terminate-simulator-session` - Terminate a session (shuts down simulator and cleans up)
- 🔄 `create-and-boot-simulator` - Create a new simulator session and boot it
- ▶️ `boot-simulator` - Boot a simulator for an existing session
- ⏹️ `shutdown-simulator` - Shutdown a simulator for an existing session

**Use this approach when:** You need to track simulator metadata, reference simulators by custom IDs, or use the more advanced management features.

#### 📲 Application Management
- 📥 `install-app` - Install an application on a simulator
- 🚀 `launch-app` - Launch an application on a simulator
- 🛑 `terminate-app` - Terminate a running application on a simulator

#### 🖱️ Interaction Tools
- 📷 `take-screenshot` - Take a screenshot of the simulator screen
- 👆 `tap-coordinate` - Perform a tap at the specified coordinates

### 🤖 Example Usage with Claude Desktop

1. Configure Claude Desktop to use this server as an MCP tool:
   - Open Claude Desktop
   - Go to Settings > Advanced
   - Add the following configuration to the "MCP Servers" section:

   ```json
   {
     "mcpServers": {
       "simulator": {
         "command": "node",
         "args": [
           "/path/to/your/mcp-server-ios-simulator/dist/index.js"
         ]
       }
     }
   }
   ```

   - Replace `/path/to/your` with the actual path to where you've installed this repository
   - Save the settings and restart Claude Desktop

2. Use the provided tools to control iOS simulators directly from Claude Desktop:
   
   **Direct UDID Approach (Recommended):**
   1. First, ask Claude to list available simulators:
      ```
      "Show me all available iOS simulators"
      ```
   
   2. Then use the UDID to boot a specific simulator:
      ```
      "Boot the iOS simulator with UDID 5272EA61-5796-4372-86FE-3B33831D5CC1"
      ```
   
   3. When finished, shut it down using the same UDID:
      ```
      "Shut down the simulator with UDID 5272EA61-5796-4372-86FE-3B33831D5CC1"
      ```
   
   The direct UDID approach is simpler and more reliable for most use cases.
   
   **Session-Based Approach (Advanced):**
   Only use this approach if you need the advanced features of session tracking:
   ```
   "Create a new simulator session for iPhone 16 Pro with iOS 18.2"
   "Boot the simulator for session abc-123"
   "Take a screenshot of the simulator for session abc-123"
   "Terminate the simulator session abc-123"
   ```

## 👨‍💻 Development

### 📁 Project Structure

```
src/
├── simulator/       # Simulator management layer
├── mcp/             # MCP protocol implementation
├── bridge/          # Bridge component
├── utils/           # Utility functions including logger
├── config.ts        # Configuration handling
└── index.ts         # Entry point
```

### 🔨 Building the Project

```bash
# Install development dependencies
npm install

# Run TypeScript compiler
npm run build
```

## 📜 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## 🙏 Acknowledgments

- 📱 [appium-ios-simulator](https://github.com/appium/appium-ios-simulator) for providing the iOS simulator interaction capabilities
- 🔌 [Model Context Protocol](https://github.com/modelcontextprotocol/typescript-sdk) for the protocol specification and TypeScript SDK
', 'true', '"{\"Protocol Implementations\",\"AI Model Integration\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states that it is a Model Context Protocol (MCP) implementation for iOS simulators, utilizing the MCP TypeScript SDK and providing an MCP server for client integration.","categories":["Protocol Implementations","API Frameworks","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The architecture is based on three main components: Simulator Management Layer, MCP Protocol Implementation, and Logger Component. It uses a layered approach, separating simulator management from MCP protocol handling. The system uses stdio as the transport mechanism, allowing for integration with MCP-compatible clients like Claude Desktop. The design promotes modularity and allows for distinct functionalities within each layer.","Implementation Details and Key Features":"The implementation utilizes the MCP TypeScript SDK for protocol handling and `appium-ios-simulator` for interacting with iOS simulators. Key features include simulator lifecycle management (start, stop), application installation and launching, screenshot capture, and coordinate tapping. It supports both direct simulator management via UDIDs and session-based management for more advanced tracking.","Integration Approaches and Protocols":"The repository integrates with iOS simulators using `appium-ios-simulator` and exposes functionality through the Model Context Protocol (MCP). Integration with MCP clients (like Claude Desktop) is achieved via stdio transport. The provided example demonstrates how to configure Claude Desktop to utilize the server as an MCP tool, enabling programmatic control of simulators.","Technical Stack and Dependencies":"The technical stack includes Node.js (v16 or later), macOS (required for iOS simulators), Xcode with iOS simulators installed, and TypeScript 4.5+. Key dependencies are `appium-ios-simulator` for simulator interaction and the MCP TypeScript SDK for protocol implementation.  The project uses npm for dependency management.","Best Practices and Coding Standards":"The documentation includes setup instructions, configuration details (using environment variables), and usage examples, indicating a focus on developer experience. The project structure separates concerns into distinct directories (simulator, mcp, bridge, utils), which improves maintainability.  The inclusion of a LICENSE file (MIT License) suggests attention to open-source best practices.","Performance Considerations":"The documentation mentions error-resilient operation, implying consideration for robustness. Configuration includes a simulator timeout setting, which likely helps manage resource utilization and prevent indefinite blocking. Logging is file-based to avoid interference with stdio transport.  The design with UDID direct calls seems optimized for direct simulator interaction."}}', '2025-03-10 21:54:04+00', 'true', 'Servers', 'mcp-ios-simulator'), ('944639102', 'langchain-mcp', 'marlenezw/langchain-mcp', 'https://github.com/marlenezw/langchain-mcp', 'Quick Start MCP with Langchain ', '2025-03-07 17:47:16+00', '2025-03-10 15:32:51+00', 'Python', '5', '# Quick Start with Langchain and the Github MCP Server

This Python code gets you started with the [MCP Github Server](https://github.com/modelcontextprotocol/servers/blob/main/src/github/README.md) and [Langchain MCP Adapters](https://github.com/langchain-ai/langchain-mcp-adapters/tree/main)
This package uses [langchain-azure](https://github.com/langchain-ai/langchain-azure) with [Github Models](https://github.com/marketplace/models/azure-openai/gpt-4o) to prototype with LLMs for free :) 

To run the server locally with docker you should:
1. Install docker
2. Clone the mcp server repo
   ```
   git clone https://github.com/modelcontextprotocol/servers.git
   cd servers
   ```
3. Build the docker image
   ```
   docker build -t mcp/github -f src/github/Dockerfile .
   ```
4. Install the requirements and run the Python script
', 'true', '"{\"Protocol Implementations\",\"AI Model Integration\",\"Context Management\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly mentions Model Context Protocol (MCP) and its usage with Langchain MCP Adapters, indicating an implementation related to MCP specifications for context management with LLMs.","categories":["Agent Frameworks","Context Management","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The repository utilizes a client-server architecture where Langchain acts as a client communicating with the MCP server. The design leverages Docker for containerization, enabling easy deployment and portability. Key design patterns likely involve adapter patterns to integrate Langchain with the MCP server, facilitating seamless communication.","Implementation Details and Key Features":"The implementation showcases a quick start guide for using Langchain with the Github MCP server. It involves building a Docker image for the MCP server and running a Python script to interact with it. The repository uses langchain-azure with Github Models (Azure OpenAI GPT-4o) for LLM prototyping.","Integration Approaches and Protocols":"The primary integration approach revolves around using Langchain MCP Adapters to interact with the Github MCP server. The communication protocol likely involves a request-response mechanism over HTTP/HTTPS, allowing Langchain to send requests to the MCP server and receive contextual information or responses back. Docker facilitates this integration by providing a consistent environment.","Technical Stack and Dependencies":"The technical stack includes Docker, Python, Langchain, langchain-azure, and potentially other libraries for networking and data serialization (e.g., JSON). Dependencies are managed via standard Python packaging (likely pip). The use of Docker ensures consistent dependencies across different environments.","Best Practices and Coding Standards":"The documentation provides a clear set of steps for setting up and running the system, indicating a focus on usability and documentation. Use of Docker promotes reproducibility and adherence to environment consistency. The quick start guide encourages rapid prototyping and iterative development.","Performance Considerations":"Performance considerations are not explicitly addressed in the available documentation. However, the use of Docker and efficient networking protocols can help optimize resource utilization and minimize latency. The choice of LLM (GPT-4o) can also influence overall performance."}}', '2025-03-10 21:28:38+00', 'true', 'Servers', 'langchain-mcp-quickstart'), ('944691576', 'mcp-sdk', 'php-llm/mcp-sdk', 'https://github.com/php-llm/mcp-sdk', 'Model Context Protocol SDK for Client and Server applications in PHP', '2025-03-07 19:50:35+00', '2025-03-10 01:44:27+00', 'PHP', '3', '# Model Context Protocol PHP SDK [WIP]

Model Context Protocol SDK for Client and Server applications in PHP.

See [Demo App](https://github.com/php-llm/mcp-demo) for a working example.

## Installation

```bash
composer require php-llm/mcp-sdk
```

## Usage with Symfony

Server integration points for are tailored to Symfony Console and HttpFoundation (Laravel compatible).

### Console Command for STDIO Server

```php
namespace App\Command;

use PhpLlm\McpSdk\Server;
use PhpLlm\McpSdk\Server\Transport\Stdio\SymfonyConsoleTransport;
use Symfony\Component\Console\Attribute\AsCommand;
use Symfony\Component\Console\Command\Command;
use Symfony\Component\Console\Input\InputInterface;
use Symfony\Component\Console\Output\OutputInterface;

#[AsCommand('mcp', 'Starts an MCP server')]
final class McpCommand extends Command
{
    public function __construct(
        private readonly Server $server,
    ) {
        parent::__construct();
    }

    protected function execute(InputInterface $input, OutputInterface $output): int
    {
        $this->server->connect(
            new SymfonyConsoleTransport($input, $output)
        );

        return Command::SUCCESS;
    }
}
```

### Controller for Server-Sent Events Server

```php
namespace App\Controller;

use PhpLlm\McpSdk\Server;
use PhpLlm\McpSdk\Server\Transport\Sse\Store\CachePoolStore;
use PhpLlm\McpSdk\Server\Transport\Sse\StreamTransport;
use Symfony\Component\HttpFoundation\Request;
use Symfony\Component\HttpFoundation\Response;
use Symfony\Component\HttpFoundation\StreamedResponse;
use Symfony\Component\HttpKernel\Attribute\AsController;
use Symfony\Component\Routing\Attribute\Route;
use Symfony\Component\Routing\Generator\UrlGeneratorInterface;
use Symfony\Component\Uid\Uuid;

#[AsController]
#[Route('/mcp', name: 'mcp_')]
final readonly class McpController
{
    public function __construct(
        private Server $server,
        private CachePoolStore $store,
        private UrlGeneratorInterface $urlGenerator,
    ) {
    }

    #[Route('/sse', name: 'sse', methods: ['GET'])]
    public function sse(): StreamedResponse
    {
        $id = Uuid::v4();
        $endpoint = $this->urlGenerator->generate('mcp_messages', ['id' => $id], UrlGeneratorInterface::ABSOLUTE_URL);
        $transport = new StreamTransport($endpoint, $this->store, $id);

        return new StreamedResponse(fn() => $this->server->connect($transport), headers: [
            'Content-Type' => 'text/event-stream',
            'Cache-Control' => 'no-cache',
            'X-Accel-Buffering' => 'no',
        ]);
    }

    #[Route('/messages/{id}', name: 'messages', methods: ['POST'])]
    public function messages(Request $request, Uuid $id): Response
    {
        $this->store->push($id, $request->getContent());

        return new Response();
    }
}
```

### Exposing Tools

Under the hood the SDK uses [LLM Chain](https://github.com/php-llm/llm-chain)'s `ToolBox` to register, analyze and
execute tools. In combination with its [Symfony Bundle](https://github.com/php-llm/llm-chain-bundle) you can expose
tools with `#[AsTool]` attribute.

```php
use PhpLlm\LlmChain\ToolBox\Attribute\AsTool;

#[AsTool('company_name', 'Provides the name of your company')]
final class CompanyName
{
    public function __invoke(): string
    {
        return 'ACME Corp.'
    }
}
```
See [LLM Chain Documentation](https://github.com/php-llm/llm-chain?tab=readme-ov-file#tools) for more information.
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository is named \"mcp-sdk\" and the description states it's a Model Context Protocol SDK. The documentation shows server integrations for Symfony, handling communication between models and external contexts via console and HTTP.","categories":["Protocol Implementations","API Frameworks","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The SDK provides a server implementation for the Model Context Protocol (MCP), supporting different transport mechanisms like STDIO and Server-Sent Events (SSE). It utilizes a transport abstraction to handle communication. The design appears to be modular, allowing easy integration with Symfony and potentially other PHP frameworks.","Implementation Details and Key Features":"The SDK provides Symfony Console command and controller examples for implementing MCP servers. It leverages the `php-llm/llm-chain` library for tool management, using the `ToolBox` and `AsTool` attribute for tool registration and execution. The server connects using different transports such as SymfonyConsoleTransport and StreamTransport.","Integration Approaches and Protocols":"The SDK integrates with Symfony framework via Console commands and HTTP controllers. It supports STDIO for console-based interactions and Server-Sent Events (SSE) for web-based, real-time communication. It relies on HTTP for message exchange and leverages Symfony's routing and request handling components.","Technical Stack and Dependencies":"The project is written in PHP and uses Composer for dependency management. Key dependencies include Symfony components (Console, HttpFoundation, Routing, Cache), `php-llm/llm-chain` for tool management, and potentially others managed through Composer. It supports Uuids for unique identifier generation.","Best Practices and Coding Standards":"The provided code snippets demonstrate the use of modern PHP features like readonly properties and attributes for defining console commands and tools. The code follows PSR coding standards, utilizes namespaces for organization, and employs dependency injection for managing dependencies.","Performance Considerations":"The SSE server implementation utilizes `StreamedResponse` in Symfony for efficient handling of real-time data streams. The SDK leverages a cache pool store for managing messages in the SSE transport. Performance considerations would depend on the specific transport used and the volume of data being transmitted."}}', '2025-03-10 21:54:07+00', 'true', 'Clients', 'mcp-php-sdk'), ('944708424', 'nash-mcp', 'nash-app/nash-mcp', 'https://github.com/nash-app/nash-mcp', null, '2025-03-07 20:34:51+00', '2025-03-08 15:45:21+00', 'Python', '2', '# Nash MCP Server

Nash MCP (a Model Context Protocol (MCP) server) enables seamless execution of commands, Python code, web content fetching, and reusable task management.

## Requirements

- Python 3.11+
- Poetry package manager (recommended)

## Installation

```bash
git clone https://github.com/nash-run/nash-mcp.git
cd nash-mcp
poetry install
```

## Features

- **Command Execution**: Run shell commands with error handling
- **Python Execution**: Execute Python code with error handling
- **Secure Credentials**: Store and access API keys without exposing sensitive data to the LLM
- **Web Content Access**: Fetch and parse webpage content for analysis
- **Task Repository**: Save and organize reusable workflows and scripts

## Tools

### Execute Module

- **execute_command**: Run shell commands with proper error handling and output capture
- **execute_python**: Execute Python code snippets with full access to installed packages
- **list_installed_packages**: Get information about available Python packages

### Web Interaction

- **fetch_webpage**: Retrieve and convert webpage content to readable text format

### Secrets Management

- **nash_secrets**: Access stored API keys and credentials securely. Accessible via environment variables in scripts.

### Task Management

- **save_nash_task**: Create reusable tasks with embedded scripts
- **list_nash_tasks**: View all available saved tasks
- **run_nash_task**: Retrieve and display a previously saved task
- **execute_task_script**: Run a specific script from a saved task
- **view_task_details**: See complete details of a task, including script code
- **delete_nash_task**: Remove tasks that are no longer needed

## Running

This is the command to use for MCP config files

```bash
/path/to/this/repo/.venv/bin/mcp run /path/to/this/repo/src/nash_mcp/server.py
```

As an example, if you were to use this MCP with Claude Desktop, you would change your `~/Library/Application Support/Claude/claude_desktop_config.json` to:

```json
{
  "mcpServers": {
    "Nash": {
      "command": "/Users/john-nash/code/nash-mcp/.venv/bin/mcp",
      "args": ["run", "/Users/john-nash/code/nash-mcp/src/nash_mcp/server.py"]
    }
  }
}
```

### Environment Variables

Nash MCP requires environment variables to specify all data file paths. Create a `.env` file in the root directory with the following variables:

```
# Required environment variables
NASH_BASE_PATH=/path/to/base/directory
NASH_SECRETS_PATH=/path/to/secrets.json
NASH_TASKS_PATH=/path/to/tasks.json
NASH_LOGS_PATH=/path/to/logs/directory
```

There are no default values - all paths must be explicitly configured.

## Security Considerations

- Commands and scripts run with the same permissions as the MCP server
- API keys and credentials are stored locally and loaded as environment variables
- Always review scripts before execution, especially when working with sensitive data

## Development

### Logs

Detailed timestamped logs of all operations and tool executions are emitted by the server. These logs are stored in the directory specified by the `NASH_LOGS_PATH` environment variable.

### Testing

Coming Soon

## License

MIT
', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"Agent Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository is explicitly described as a Model Context Protocol (MCP) server, facilitating command execution, Python code execution, and web content fetching for AI models.","categories":["Protocol Implementations","Agent Frameworks","Function Calling"],"analysis":{"Architecture and Design Patterns":"The repository implements a server that adheres to the Model Context Protocol. It is designed to execute commands and Python code in response to requests. The architecture includes modules for command execution, web interaction, secrets management, and task management. It follows a modular design, separating concerns for each feature.","Implementation Details and Key Features":"Key features include executing shell commands, running Python code, securely managing API keys, fetching web content, and managing reusable tasks. The server utilizes environment variables to configure data file paths. Functionality is exposed through dedicated functions, such as `execute_command`, `execute_python`, `fetch_webpage`, and task management functions.","Integration Approaches and Protocols":"The repository integrates with other systems via the Model Context Protocol. The provided example demonstrates integration with Claude Desktop by modifying its configuration file to point to the MCP server. The server is invoked with `mcp run` and expects specific environment variables for configuration.","Technical Stack and Dependencies":"The project is written in Python 3.11+. It relies on the Poetry package manager for dependency management. Key dependencies likely include libraries for web scraping (e.g., Beautiful Soup, requests), subprocess management, and JSON handling. The MCP server is executed using a command-line interface.","Best Practices and Coding Standards":"The repository emphasizes error handling in command and Python execution. It prioritizes secure storage of API keys using environment variables. The logs detail timestamped operations for debugging and monitoring. Security considerations are addressed regarding command permissions and script reviews.","Performance Considerations":"Performance depends on the complexity of the executed commands, Python code, and web scraping tasks. Optimizations might involve caching web content, using asynchronous operations, and efficient command execution. Logging adds overhead, but is essential for debugging. Task execution efficiency will impact overall responsiveness."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'nash-mcp-server'), ('944715410', 'agent-twitter-client-mcp', 'ryanmac/agent-twitter-client-mcp', 'https://github.com/ryanmac/agent-twitter-client-mcp', 'A Model Context Protocol (MCP) server that integrates with X using the @elizaOS `agent-twitter-client` package, allowing AI models to interact with Twitter without direct API access.', '2025-03-07 20:54:02+00', '2025-03-10 17:46:52+00', 'TypeScript', '1', '# agent-twitter-client-mcp

[![npm version](https://img.shields.io/npm/v/agent-twitter-client-mcp.svg)](https://www.npmjs.com/package/agent-twitter-client-mcp)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Node.js Version](https://img.shields.io/node/v/agent-twitter-client-mcp.svg)](https://nodejs.org)

A Model Context Protocol (MCP) server that integrates with Twitter using the `agent-twitter-client` package, allowing AI models to interact with Twitter without direct API access.

## Features

- **Authentication Options**:

  - Cookie-based authentication (recommended)
  - Username/password authentication
  - Twitter API v2 credentials

- **Tweet Operations**:

  - Fetch tweets from users
  - Get specific tweets by ID
  - Search tweets
  - Send tweets with text and media
  - Create polls
  - Like, retweet, and quote tweets

- **User Operations**:

  - Get user profiles
  - Follow users
  - Get followers and following lists

- **Grok Integration**:
  - Chat with Grok via Twitter's interface
  - Continue conversations with conversation IDs
  - Get web search results and citations
  - Access Twitter's real-time data through Grok
  - **Note**: Grok functionality requires [agent-twitter-client v0.0.19](https://github.com/elizaOS/agent-twitter-client/releases/tag/0.0.19) or higher

## Documentation

- [Developer Guide](docs/DEVELOPER_GUIDE.md) - Comprehensive guide for developers
- [Testing Guide](docs/TESTING.md) - Instructions for testing the MCP
- [Agent Guide](docs/AGENT_GUIDE.md) - Guide for AI agents on how to use the Twitter MCP
- [Contributing Guide](CONTRIBUTING.md) - Guidelines for contributing to this project
- [Changelog](CHANGELOG.md) - History of changes to this project
- [Demo README](demo/README.md) - Guide for running the demo scripts
- [Grok Examples](demo/GROK_EXAMPLES.md) - Documentation for the Grok AI integration examples

## Quick Start

### Installation

```bash
# Install globally
npm install -g agent-twitter-client-mcp

# Or install locally
npm install agent-twitter-client-mcp
```

### Basic Usage

1. Create a `.env` file with your Twitter credentials (see [Authentication Methods](#authentication-methods))
2. Run the MCP server:

```bash
# If installed globally
agent-twitter-client-mcp

# If installed locally
npx agent-twitter-client-mcp
```

### Demo Scripts

The package includes a `demo` directory with example scripts that demonstrate various features:

```bash
# Clone the repository to access the demo scripts
git clone https://github.com/ryanmac/agent-twitter-client-mcp.git
cd agent-twitter-client-mcp/demo

# Run the interactive demo menu
./run-demo.sh

# Run a specific demo script
./run-demo.sh --script tweet-search.js

# Run Grok AI examples (requires agent-twitter-client v0.0.19)
./run-demo.sh --script simple-grok.js --use-local-agent-twitter-client
./run-demo.sh --script grok-chat.js --use-local-agent-twitter-client
```

See the [Demo README](demo/README.md) for more details.

### Port Configuration

By default, the MCP server runs on port 3000. If you need to change this (for example, if you already have an application running on port 3000), you have several options:

#### Option 1: Using Environment Variables

Set the `PORT` environment variable:

```bash
PORT=3001 npx agent-twitter-client-mcp
```

#### Option 2: Using Docker Compose

If using Docker Compose, you can configure both the host and container ports in your `.env` file:

```
# .env file
MCP_HOST_PORT=3001    # The port on your host machine
MCP_CONTAINER_PORT=3000  # The port inside the container
```

Then run:

```bash
docker-compose up -d
```

This will map port 3001 on your host to port 3000 in the container, allowing you to access the MCP at http://localhost:3001 while your other application continues to use port 3000.

### Setup with Claude Desktop

1. Configure Claude Desktop to use this MCP by adding to your config file:

**Windows**: `%APPDATA%\Claude\claude_desktop_config.json`
**macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`

```json
{
  "mcpServers": {
    "agent-twitter-client-mcp": {
      "command": "npx",
      "args": ["-y", "agent-twitter-client-mcp"],
      "env": {
        "AUTH_METHOD": "cookies",
        "TWITTER_COOKIES": "[\"auth_token=YOUR_AUTH_TOKEN; Domain=.twitter.com\", \"ct0=YOUR_CT0_VALUE; Domain=.twitter.com\", \"twid=u%3DYOUR_USER_ID; Domain=.twitter.com\"]"
      }
    }
  }
}
```

2. Restart Claude Desktop

### Authentication Methods

#### Cookie Authentication (Recommended)

```json
{
  "AUTH_METHOD": "cookies",
  "TWITTER_COOKIES": "[\"auth_token=YOUR_AUTH_TOKEN; Domain=.twitter.com\", \"ct0=YOUR_CT0_VALUE; Domain=.twitter.com\", \"twid=u%3DYOUR_USER_ID; Domain=.twitter.com\"]"
}
```

To obtain cookies:

1. Log in to Twitter in your browser
2. Open Developer Tools (F12)
3. Go to the Application tab > Cookies
4. Copy the values of `auth_token`, `ct0`, and `twid` cookies
5. Make sure to include the `Domain=.twitter.com` part for each cookie

#### Username/Password Authentication

```json
{
  "AUTH_METHOD": "credentials",
  "TWITTER_USERNAME": "your_username",
  "TWITTER_PASSWORD": "your_password",
  "TWITTER_EMAIL": "your_email@example.com", // Optional
  "TWITTER_2FA_SECRET": "your_2fa_secret" // Optional, required if 2FA is enabled
}
```

#### Twitter API Authentication

```json
{
  "AUTH_METHOD": "api",
  "TWITTER_API_KEY": "your_api_key",
  "TWITTER_API_SECRET_KEY": "your_api_secret_key",
  "TWITTER_ACCESS_TOKEN": "your_access_token",
  "TWITTER_ACCESS_TOKEN_SECRET": "your_access_token_secret"
}
```

## Available Tools

- `get_user_tweets`: Fetch tweets from a specific user
- `get_tweet_by_id`: Fetch a specific tweet by ID
- `search_tweets`: Search for tweets
- `send_tweet`: Post a new tweet
- `send_tweet_with_poll`: Post a tweet with a poll
- `like_tweet`: Like a tweet
- `retweet`: Retweet a tweet
- `quote_tweet`: Quote a tweet
- `get_user_profile`: Get a user's profile
- `follow_user`: Follow a user
- `get_followers`: Get a user's followers
- `get_following`: Get users a user is following
- `grok_chat`: Chat with Grok via Twitter
- `health_check`: Check the health of the Twitter MCP server

## Testing Interface

The MCP includes an interactive command-line interface for testing:

```bash
npx agent-twitter-client-mcp-test
# or if installed locally
npm run test:interface
```

This launches a REPL where you can test various MCP functions:

```
agent-twitter-client-mcp> help

Available commands:
  health                     Run a health check
  profile <username>         Get a user profile
  tweets <username> [count]  Get tweets from a user
  tweet <id>                 Get a specific tweet by ID
  search <query> [count]     Search for tweets
  post <text>                Post a new tweet
  like <id>                  Like a tweet
  retweet <id>               Retweet a tweet
  quote <id> <text>          Quote a tweet
  follow <username>          Follow a user
  followers <userId> [count] Get a user's followers
  following <userId> [count] Get users a user is following
  grok <message>             Chat with Grok
  help                       Show available commands
  exit                       Exit the test interface
```

### Example Test Commands

```
# Run a health check
agent-twitter-client-mcp> health

# Search for tweets
agent-twitter-client-mcp> search mcp 2

# Get a user's profile
agent-twitter-client-mcp> profile elonmusk

# Get tweets from a user
agent-twitter-client-mcp> tweets openai 5

# Chat with Grok
agent-twitter-client-mcp> grok Explain quantum computing in simple terms
```

## Example Usage

Ask Claude to:

- "Search Twitter for tweets about AI"
- "Post a tweet saying 'Hello from Claude!'"
- "Get the latest tweets from @OpenAI"
- "Chat with Grok about quantum computing"

## Advanced Usage

### Working with Media

To post a tweet with an image:

```
I want to post a tweet with an image. The tweet should say "Beautiful sunset today!" and include this image.
```

To post a tweet with a video:

```
I want to post a tweet with a video. The tweet should say "Check out this amazing video!" and include the video file.
```

### Creating Polls

To create a poll:

```
Create a Twitter poll asking "What's your favorite programming language?" with options: Python, JavaScript, Rust, and Go. The poll should run for 24 hours.
```

### Interacting with Grok

To have a conversation with Grok:

```
Use Grok to explain quantum computing to me. Ask it to include some real-world applications.
```

To continue a conversation with Grok:

```
Continue the Grok conversation and ask it to elaborate on quantum entanglement.
```

### Grok's Unique Capabilities

Grok on Twitter has access to real-time Twitter data that even the standalone Grok API doesn't have. This means you can ask Grok about:

- Current trending topics on Twitter
- Analysis of recent tweets on specific subjects
- Information about Twitter users and their content
- Real-time events being discussed on the platform

Example queries:

- "What are the trending topics on Twitter right now?"
- "Analyze the sentiment around AI on Twitter"
- "What are people saying about the latest Apple event?"
- "Show me information about popular memecoins being discussed today"

### Grok Authentication Requirements

Grok functionality requires proper authentication. The MCP supports two methods:

1. **Cookie Authentication** (Recommended):

   - Cookies must be in JSON array format
   - Example: `TWITTER_COOKIES=["auth_token=YOUR_AUTH_TOKEN; Domain=.twitter.com", "ct0=YOUR_CT0_VALUE; Domain=.twitter.com", "twid=u%3DYOUR_USER_ID; Domain=.twitter.com"]`
   - Essential cookies are `auth_token`, `ct0`, and `twid`

2. **Username/Password Authentication**:
   - Set `TWITTER_USERNAME` and `TWITTER_PASSWORD` in your environment
   - May encounter Cloudflare protection in some cases

### Grok Rate Limits

Grok has rate limits that may affect usage:

- Non-premium accounts: 25 messages per 2 hours
- Premium accounts: Higher limits

The MCP will return rate limit information in the response when limits are reached.

For more details on using Grok, see the [Grok Examples](demo/GROK_EXAMPLES.md) documentation.

## Troubleshooting

### Authentication Issues

#### Cookie Authentication Problems

If you're experiencing issues with cookie authentication:

1. **Cookie Expiration**: Twitter cookies typically expire after a certain period. Try refreshing your cookies by logging out and back into Twitter.
2. **Cookie Format**: Ensure your cookies are properly formatted as a JSON array of strings with the correct domain.
3. **Required Cookies**: Make sure you've included the essential cookies: `auth_token`, `ct0`, and `twid`.

Example of properly formatted cookies:

```json
"TWITTER_COOKIES": "[\"auth_token=1234567890abcdef; Domain=.twitter.com\", \"ct0=abcdef1234567890; Domain=.twitter.com\", \"twid=u%3D1234567890; Domain=.twitter.com\"]"
```

#### Credential Authentication Problems

If you're having trouble with username/password authentication:

1. **Two-Factor Authentication**: If your account has 2FA enabled, you'll need to provide the `TWITTER_2FA_SECRET`.
2. **Account Lockouts**: Too many failed login attempts may lock your account. Check your email for account verification requests.
3. **Captcha Challenges**: Twitter may present captcha challenges that the client can't handle automatically.

#### API Authentication Problems

For API authentication issues:

1. **API Key Permissions**: Ensure your API keys have the necessary permissions for the actions you're trying to perform.
2. **Rate Limiting**: Twitter API has rate limits that may cause failures if exceeded.
3. **API Changes**: Twitter occasionally changes its API, which may cause compatibility issues.

### Operation Errors

#### Tweet Posting Failures

If you can't post tweets:

1. **Content Restrictions**: Twitter may block tweets that violate its content policies.
2. **Media Format Issues**: Ensure media is properly formatted and encoded.
3. **Rate Limiting**: Twitter limits how frequently you can post.

#### Search Problems

If search isn't working:

1. **Query Syntax**: Ensure your search query follows Twitter's search syntax.
2. **Search Limitations**: Some search modes may have restrictions or require specific permissions.

#### Grok Issues

If Grok functionality isn't working:

1. **Version Requirement**:

   - Grok requires [agent-twitter-client v0.0.19](https://github.com/elizaOS/agent-twitter-client/releases/tag/0.0.19) or higher
   - The current package uses v0.0.18 for basic functionality
   - For the demo scripts, use the `--use-local-agent-twitter-client` flag to temporarily install v0.0.19

2. **Authentication Issues**:

   - Cookie Format: Ensure cookies are in the correct JSON array format
   - Cookie Validity: Twitter cookies expire after a certain period
   - Cloudflare Protection: Username/password authentication may be blocked by Cloudflare
   - Premium Requirement: Grok access requires a Twitter Premium subscription

3. **Rate Limits**:

   - Non-premium accounts: 25 messages per 2 hours
   - Error Message: "Rate Limited: You've reached the limit..."
   - Solution: Wait until the rate limit resets or upgrade to a premium account

4. **Environment File Location**:
   - For the demo scripts, make sure your credentials are in `demo/.env`, not in the root `.env` file
   - Use the `--debug-env` flag to check which environment variables are being loaded

For detailed troubleshooting of Grok issues, see the [Grok Examples](demo/GROK_EXAMPLES.md) documentation.

### Server Issues

#### Health Check

Use the `health_check` tool to diagnose server issues:

```
Run a health check on the agent-twitter-client-mcp server to diagnose any issues.
```

The health check will report on:

- Authentication status
- API connectivity
- Memory usage

#### Logging

The server logs to both console and files:

- `error.log`: Contains error-level messages
- `combined.log`: Contains all log messages

Check these logs for detailed error information.

## Development

### Prerequisites

- Node.js 18+
- npm

### Setup

1. Clone the repository

```bash
git clone https://github.com/ryanmac/agent-twitter-client-mcp.git
cd agent-twitter-client-mcp
```

2. Install dependencies

```bash
npm install
```

3. Create a `.env` file with configuration:

```
AUTH_METHOD=cookies
TWITTER_COOKIES=["cookie1=value1", "cookie2=value2"]
```

4. Build the project

```bash
npm run build
```

5. Start the server

```bash
npm start
```

### Environment Variables

In addition to the authentication variables, you can set:

- `LOG_LEVEL`: Set logging level (error, warn, info, debug)
- `NODE_ENV`: Set environment (development, production)

## Docker

You can also run the server using Docker:

### Using Docker Directly

```bash
# Build the Docker image
docker build -t agent-twitter-client-mcp .

# Run the container with environment variables
docker run -p 3000:3000 \
  -e AUTH_METHOD=cookies \
  -e TWITTER_COOKIES='["auth_token=YOUR_AUTH_TOKEN; Domain=.twitter.com", "ct0=YOUR_CT0_VALUE; Domain=.twitter.com"]' \
  agent-twitter-client-mcp
```

### Using Docker Compose

1. Create a `.env` file with your Twitter credentials
2. Run with docker-compose:

```bash
# Start the service
docker-compose up -d

# View logs
docker-compose logs -f

# Stop the service
docker-compose down
```

### Environment Variables in Docker

You can pass environment variables to the Docker container in several ways:

1. **In the docker-compose.yml file** (already configured)
2. **Through a .env file** (recommended for docker-compose)
3. **Directly in the docker run command** (as shown above)

### Persisting Logs

The docker-compose configuration includes a volume mount for logs:

```yaml
volumes:
  - ./logs:/app/logs
```

This will store logs in a `logs` directory in your project folder.

## Security Considerations

- **Credential Storage**: Store credentials securely, preferably using environment variables or a secure vault.
- **Rate Limiting**: Implement rate limiting to prevent abuse of the Twitter API.
- **Content Validation**: Validate all content before posting to prevent malicious use.

## License

MIT
', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"API Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository implements a Model Context Protocol (MCP) server to allow AI models to interact with Twitter via an intermediary, abstracting away direct API access and providing tools for agents.","categories":["Agent Frameworks","API Frameworks","Function Calling"],"analysis":{"Architecture and Design Patterns":"The repository implements an MCP server acting as an intermediary between AI models and the Twitter API. It utilizes a modular design with different authentication methods (cookies, username/password, API keys). The server exposes a set of tools that AI agents can utilize through a well-defined interface, effectively abstracting away the complexities of the Twitter API.","Implementation Details and Key Features":"Key features include various authentication methods, tweet operations (fetching, searching, sending, liking), user operations (profiles, follow/unfollow, lists), and Grok integration. It uses Node.js, and is packaged as an npm module, making it easy to install and use. The inclusion of demo scripts further illustrates usage and functionalities.","Integration Approaches and Protocols":"The integration with AI models occurs through the Model Context Protocol. The server exposes a set of functions/tools that the AI models can call. The repository also integrates with Claude Desktop through configuration file settings. Integration with Twitter is achieved via the `agent-twitter-client` package, providing an abstraction layer for Twitter API interactions.","Technical Stack and Dependencies":"The core technical stack includes Node.js, npm, and potentially Docker for deployment. The repository depends on the `@elizaOS/agent-twitter-client` package for interacting with the Twitter API. Other dependencies are likely present but not explicitly detailed. The project utilizes environment variables for configuration and authentication.","Best Practices and Coding Standards":"The repository demonstrates several best practices, including the use of comprehensive documentation (developer, testing, agent guides), a changelog, and demo scripts. It also includes a contributing guide to encourage external contributions. The use of environment variables for configuration and authentication is another indicator of good practices.","Performance Considerations":"The repository highlights rate limits for Grok integration, indicating an awareness of potential performance bottlenecks. Health check functionality and logging mechanisms are in place to monitor server health and diagnose issues. Authentication methods and efficient Twitter API interaction should be considered for performance optimization."}}', '2025-03-10 21:26:36+00', 'true', 'Clients', 'agent-twitter-mcp'), ('944757886', 'meme-mcp', 'haltakov/meme-mcp', 'https://github.com/haltakov/meme-mcp', 'A simple Model Context Protocol (MCP) server for generating memes using the ImgFlip API', '2025-03-07 23:03:56+00', '2025-03-09 23:42:31+00', 'JavaScript', '8', '# Meme MCP Server

A simple Model Context Protocol (MCP) server for generating memes using the ImgFlip API. This server enables AI models and tools to generate meme images from user prompts.

## Tools

The server implements the following a single tool called `generateMeme`.

The tool accepts the following parameters:

- `templateNumericId`: The numeric ID of the meme template to use.
- `text0`: The text for the first placeholder.
- `text1`: The text for the second placeholder.

## Usage

You can configure the meme generator server in your client using the [`meme-mcp`](https://www.npmjs.com/package/meme-mcp) NPM package. Here is an example configuration for Claude Desktop (Settings -> Developer -> Edit Config):

```json
{
  "mcpServers": {
    "meme": {
      "command": "npx",
      "args": ["-y", "meme-mcp"],
      "env": {
        "IMGFLIP_USERNAME": "<IMGFLIP USERNAME>",
        "IMGFLIP_PASSWORD": "<IMGFLIP PASSWORD>"
      }
    }
  }
}
```

> Note: you need to create a free account on [ImgFlip](https://imgflip.com/signup) to get your username and password.

### Troubleshooting

Sometimes Claude Desktop fails to find the right version of `npx` (especially if you are using NVM, see this [Issue](https://github.com/modelcontextprotocol/servers/issues/64) for details). In this case, you can manually install `meme-mcp` globally and then use it directly.

```bash
npm install -g meme-mcp
```

You can find the path of your `node` executable by running `which node` in your terminal. After that your configuration should look like this:

```json
{
  "mcpServers": {
    "meme": {
      "command": "/Users/<USERNAME>/.nvm/versions/node/v20.18.2/bin/node",
      "args": ["/Users/<USERNAME>/.nvm/versions/node/v20.18.2/lib/node_modules/meme-mcp/dist/index.js"],
      "env": {
        "IMGFLIP_USERNAME": "<IMGFLIP USERNAME>",
        "IMGFLIP_PASSWORD": "<IMGFLIP PASSWORD>"
      }
    }
  }
}
```

## Example

After configuring Claude Desktop, you need to restart it and then you will see the small hammer icon on the bottom right in the chat input. You can then ask Claude to generate a meme for you.

![Claude Desktop with meme server configured](./docs/claude-desktop-meme-example.png)

## Author

This project is created for fun by [Vladimir Haltakov](https://haltakov.net). If you find it interesting you can message me on X [@haltakov](https://x.com/haltakov).
', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements a Model Context Protocol (MCP) server, as stated in the description and documentation, enabling AI models to generate memes using the ImgFlip API.","categories":["AI Model Integration","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The repository implements a simple server that exposes a single tool (`generateMeme`) conforming to the MCP specification. It's designed as a command-line tool that can be invoked by AI models. The architecture follows a straightforward client-server model where the AI acts as the client and this repository acts as a server providing a specific functionality.","Implementation Details and Key Features":"The implementation leverages the ImgFlip API for meme generation. The server accepts parameters like template ID and text inputs, which are then used to construct and submit requests to ImgFlip. A key feature is the integration with AI models such as Claude via MCP, enabling the models to use the meme generation tool.","Integration Approaches and Protocols":"The primary integration approach is through the Model Context Protocol. The repository provides configuration examples for Claude Desktop, demonstrating how the AI model can be configured to utilize the `generateMeme` tool. This shows a direct implementation of MCP for AI model integration with external tools.","Technical Stack and Dependencies":"The repository is built using Node.js, as evident from the `npx` command and the NPM package `meme-mcp`. It depends on the ImgFlip API for meme generation. The documentation references potential issues with `npx` and suggests using the global installation of `meme-mcp` as an alternative.","Best Practices and Coding Standards":"The documentation provides clear instructions for configuring the MCP server with an AI model and addresses potential troubleshooting issues. The structure is simple and easy to follow. However, there's limited information about the internal coding standards used within the project itself.","Performance Considerations":"Performance considerations are not explicitly addressed. However, since the repository relies on the ImgFlip API, the performance will likely be influenced by the API's response times. Network latency and API rate limits could be factors affecting the overall performance of the meme generation process."}}', '2025-03-10 21:28:34+00', 'true', 'Servers', 'meme-mcp-imgflip'), ('944759860', 'openai-mcp', 'arthurcolle/openai-mcp', 'https://github.com/arthurcolle/openai-mcp', 'OpenAI Code Assistant Model Context Protocol (MCP) Server', '2025-03-07 23:10:20+00', '2025-03-10 12:15:37+00', 'Python', '6', '# Claude Code Python Edition

A powerful Python recreation of Claude Code with enhanced real-time visualization, cost management, and Model Context Protocol (MCP) server capabilities. This tool provides a natural language interface for software development tasks with support for multiple LLM providers.

![Version](https://img.shields.io/badge/version-0.1.0-blue)
![Python](https://img.shields.io/badge/python-3.10+-green)

## Key Features

- **Multi-Provider Support:** Works with OpenAI, Anthropic, and other LLM providers
- **Model Context Protocol Integration:** 
  - Run as an MCP server for use with Claude Desktop and other clients
  - Connect to any MCP server with the built-in MCP client
  - Multi-agent synchronization for complex problem solving
- **Real-Time Tool Visualization:** See tool execution progress and results in real-time
- **Cost Management:** Track token usage and expenses with budget controls
- **Comprehensive Tool Suite:** File operations, search, command execution, and more
- **Enhanced UI:** Rich terminal interface with progress indicators and syntax highlighting
- **Context Optimization:** Smart conversation compaction and memory management
- **Agent Coordination:** Specialized agents with different roles can collaborate on tasks

## Installation

1. Clone this repository
2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Create a `.env` file with your API keys:

```
# Choose one or more providers
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Optional model selection
OPENAI_MODEL=gpt-4o
ANTHROPIC_MODEL=claude-3-opus-20240229
```

## Usage

### CLI Mode

Run the CLI with the default provider (determined from available API keys):

```bash
python claude.py chat
```

Specify a provider and model:

```bash
python claude.py chat --provider openai --model gpt-4o
```

Set a budget limit to manage costs:

```bash
python claude.py chat --budget 5.00
```

### MCP Server Mode

Run as a Model Context Protocol server:

```bash
python claude.py serve
```

Start in development mode with the MCP Inspector:

```bash
python claude.py serve --dev
```

Configure host and port:

```bash
python claude.py serve --host 0.0.0.0 --port 8000
```

Specify additional dependencies:

```bash
python claude.py serve --dependencies pandas numpy
```

Load environment variables from file:

```bash
python claude.py serve --env-file .env
```

### MCP Client Mode

Connect to an MCP server using Claude as the reasoning engine:

```bash
python claude.py mcp-client path/to/server.py
```

Specify a Claude model:

```bash
python claude.py mcp-client path/to/server.py --model claude-3-5-sonnet-20241022
```

Try the included example server:

```bash
# In terminal 1 - start the server
python examples/echo_server.py

# In terminal 2 - connect with the client
python claude.py mcp-client examples/echo_server.py
```

### Multi-Agent MCP Mode

Launch a multi-agent client with synchronized agents:

```bash
python claude.py mcp-multi-agent path/to/server.py
```

Use a custom agent configuration file:

```bash
python claude.py mcp-multi-agent path/to/server.py --config examples/agents_config.json
```

Example with the echo server:

```bash
# In terminal 1 - start the server
python examples/echo_server.py

# In terminal 2 - launch the multi-agent client
python claude.py mcp-multi-agent examples/echo_server.py --config examples/agents_config.json
```

## Available Tools

- **View:** Read files with optional line limits
- **Edit:** Modify files with precise text replacement
- **Replace:** Create or overwrite files
- **GlobTool:** Find files by pattern matching
- **GrepTool:** Search file contents using regex
- **LS:** List directory contents
- **Bash:** Execute shell commands

## Chat Commands

- **/help:** Show available commands
- **/compact:** Compress conversation history to save tokens
- **/version:** Show version information
- **/providers:** List available LLM providers
- **/cost:** Show cost and usage information
- **/budget [amount]:** Set a budget limit
- **/quit, /exit:** Exit the application

## Architecture

Claude Code Python Edition is built with a modular architecture:

```
/claude_code/
  /lib/
    /providers/      # LLM provider implementations
    /tools/          # Tool implementations
    /context/        # Context management
    /ui/             # UI components
    /monitoring/     # Cost tracking & metrics
  /commands/         # CLI commands
  /config/           # Configuration management
  /util/             # Utility functions
  claude.py          # Main CLI entry point
  mcp_server.py      # Model Context Protocol server
```

## Using with Model Context Protocol

### Using Claude Code as an MCP Server

Once the MCP server is running, you can connect to it from Claude Desktop or other MCP-compatible clients:

1. Install and run the MCP server:
   ```bash
   python claude.py serve
   ```

2. Open the configuration page in your browser:
   ```
   http://localhost:8000
   ```

3. Follow the instructions to configure Claude Desktop, including:
   - Copy the JSON configuration
   - Download the auto-configured JSON file
   - Step-by-step setup instructions

### Using Claude Code as an MCP Client

To connect to any MCP server using Claude Code:

1. Ensure you have your Anthropic API key in the environment or .env file
2. Start the MCP server you want to connect to
3. Connect using the MCP client:
   ```bash
   python claude.py mcp-client path/to/server.py
   ```
4. Type queries in the interactive chat interface

### Using Multi-Agent Mode

For complex tasks, the multi-agent mode allows multiple specialized agents to collaborate:

1. Create an agent configuration file or use the provided example
2. Start your MCP server
3. Launch the multi-agent client:
   ```bash
   python claude.py mcp-multi-agent path/to/server.py --config examples/agents_config.json
   ```
4. Use the command interface to interact with multiple agents:
   - Type a message to broadcast to all agents
   - Use `/talk Agent_Name message` for direct communication
   - Use `/agents` to see all available agents
   - Use `/history` to view the conversation history

## Contributing

1. Fork the repository
2. Create a feature branch
3. Implement your changes with tests
4. Submit a pull request

## License

MIT

## Acknowledgments

This project is inspired by Anthropic's Claude Code CLI tool, reimplemented in Python with additional features for enhanced visibility, cost management, and MCP server capabilities.# OpenAI Code Assistant

A powerful command-line and API-based coding assistant that uses OpenAI APIs with function calling and streaming.

## Features

- Interactive CLI for coding assistance
- Web API for integration with other applications
- Model Context Protocol (MCP) server implementation
- Replication support for high availability
- Tool-based architecture for extensibility
- Reinforcement learning for tool optimization
- Web client for browser-based interaction

## Installation

1. Clone the repository
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Set your OpenAI API key:
   ```bash
   export OPENAI_API_KEY=your_api_key
   ```

## Usage

### CLI Mode

Run the assistant in interactive CLI mode:

```bash
python cli.py
```

Options:
- `--model`, `-m`: Specify the model to use (default: gpt-4o)
- `--temperature`, `-t`: Set temperature for response generation (default: 0)
- `--verbose`, `-v`: Enable verbose output with additional information
- `--enable-rl/--disable-rl`: Enable/disable reinforcement learning for tool optimization
- `--rl-update`: Manually trigger an update of the RL model

### API Server Mode

Run the assistant as an API server:

```bash
python cli.py serve
```

Options:
- `--host`: Host address to bind to (default: 127.0.0.1)
- `--port`, `-p`: Port to listen on (default: 8000)
- `--workers`, `-w`: Number of worker processes (default: 1)
- `--enable-replication`: Enable replication across instances
- `--primary/--secondary`: Whether this is a primary or secondary instance
- `--peer`: Peer instances to replicate with (host:port), can be specified multiple times

### MCP Server Mode

Run the assistant as a Model Context Protocol (MCP) server:

```bash
python cli.py mcp-serve
```

Options:
- `--host`: Host address to bind to (default: 127.0.0.1)
- `--port`, `-p`: Port to listen on (default: 8000)
- `--dev`: Enable development mode with additional logging
- `--dependencies`: Additional Python dependencies to install
- `--env-file`: Path to .env file with environment variables

### MCP Client Mode

Connect to an MCP server using the assistant as the reasoning engine:

```bash
python cli.py mcp-client path/to/server.py
```

Options:
- `--model`, `-m`: Model to use for reasoning (default: gpt-4o)
- `--host`: Host address for the MCP server (default: 127.0.0.1)
- `--port`, `-p`: Port for the MCP server (default: 8000)

### Deployment Script

For easier deployment, use the provided script:

```bash
./deploy.sh --host 0.0.0.0 --port 8000 --workers 4
```

To enable replication:

```bash
# Primary instance
./deploy.sh --enable-replication --port 8000

# Secondary instance
./deploy.sh --enable-replication --secondary --port 8001 --peer 127.0.0.1:8000
```

### Web Client

To use the web client, open `web-client.html` in your browser. Make sure the API server is running.

## API Endpoints

### Standard API Endpoints

- `POST /conversation`: Create a new conversation
- `POST /conversation/{conversation_id}/message`: Send a message to a conversation
- `POST /conversation/{conversation_id}/message/stream`: Stream a message response
- `GET /conversation/{conversation_id}`: Get conversation details
- `DELETE /conversation/{conversation_id}`: Delete a conversation
- `GET /health`: Health check endpoint

### MCP Protocol Endpoints

- `GET /`: Health check (MCP protocol)
- `POST /context`: Get context for a prompt template
- `GET /prompts`: List available prompt templates
- `GET /prompts/{prompt_id}`: Get a specific prompt template
- `POST /prompts`: Create a new prompt template
- `PUT /prompts/{prompt_id}`: Update an existing prompt template
- `DELETE /prompts/{prompt_id}`: Delete a prompt template

## Replication

The replication system allows running multiple instances of the assistant with synchronized state. This provides:

- High availability
- Load balancing
- Fault tolerance

To set up replication:
1. Start a primary instance with `--enable-replication`
2. Start secondary instances with `--enable-replication --secondary --peer [primary-host:port]`

## Tools

The assistant includes various tools:
- Weather: Get current weather for a location
- View: Read files from the filesystem
- Edit: Edit files
- Replace: Write files
- Bash: Execute bash commands
- GlobTool: File pattern matching
- GrepTool: Content search
- LS: List directory contents
- JinaSearch: Web search using Jina.ai
- JinaFactCheck: Fact checking using Jina.ai
- JinaReadURL: Read and summarize webpages

## CLI Commands

- `/help`: Show help message
- `/compact`: Compact the conversation to reduce token usage
- `/status`: Show token usage and session information
- `/config`: Show current configuration settings
- `/rl-status`: Show RL tool optimizer status (if enabled)
- `/rl-update`: Update the RL model manually (if enabled)
- `/rl-stats`: Show tool usage statistics (if enabled)
', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"Protocol Implementations\"]}', '{"is_mcp":"YES","justification":"The repository explicitly mentions and implements the Model Context Protocol (MCP) as both a server and a client, enabling communication between AI models and other tools, aligning with the core purpose of MCP.","categories":["Agent Frameworks","API Frameworks","Function Calling"],"analysis":{"Architecture and Design Patterns":"The repository adopts a modular design. It uses distinct directories for providers, tools, context management, UI components, and monitoring. The documentation mentions CLI commands, MCP server mode, and MCP client mode, indicating a multi-faceted approach. The code supports both command-line interaction and API-based integration, suggesting a flexible architecture suitable for diverse use cases.","Implementation Details and Key Features":"Key features include multi-provider support (OpenAI, Anthropic), MCP server and client modes, multi-agent synchronization, real-time tool visualization, cost management, and a comprehensive tool suite. The implementation relies on Python and utilizes libraries specified in the requirements.txt. It integrates tools like file operations, search, and command execution, accessible through a rich terminal interface.","Integration Approaches and Protocols":"The repository integrates with LLM providers through API keys and model selection. The Model Context Protocol (MCP) is used for communication between the assistant and external clients or servers. Multi-agent synchronization facilitates complex problem-solving. The system provides API endpoints for managing conversations and accessing prompt templates, enabling seamless integration into existing workflows.","Technical Stack and Dependencies":"The repository primarily uses Python and lists dependencies in a `requirements.txt` file. This suggests reliance on libraries like OpenAI's Python library, Anthropic's libraries, and potentially others for API communication and data processing. The documentation mentions using `pip` for dependency installation, indicating a standard Python development environment.","Best Practices and Coding Standards":"The structure into modules such as `/lib/providers/` and `/lib/tools/` suggests good separation of concerns. There is version control with badge, and contributing guidelines are defined, suggesting a formal development process. The use of environment variables for API keys promotes security. However, a detailed analysis of the codebase would be necessary to assess adherence to coding standards.","Performance Considerations":"The documentation mentions context optimization through conversation compaction to save tokens, indicating a focus on reducing costs and improving response times. The API server mode supports worker processes, suggesting an effort to handle concurrent requests efficiently. Replication across instances aims to provide high availability and load balancing, crucial for sustained performance."}}', '2025-03-10 21:28:34+00', 'true', 'Servers', 'openai-mcp-server'), ('944762584', 'mcp-list', 'andrewhuhh/mcp-list', 'https://github.com/andrewhuhh/mcp-list', null, '2025-03-07 23:20:18+00', '2025-03-10 17:16:23+00', 'TypeScript', '0', '# MCP List

A community-curated collection of Model Context Protocols (MCPs). This web application allows users to browse, submit, and discuss various MCPs.

## What is an MCP?

An MCP (Model Context Protocol) is a standardized format for providing context and instructions to AI language models to achieve specific behaviors or capabilities.

## Features

- Browse community-submitted MCPs
- Submit new MCPs
- Vote and comment on existing MCPs
- Search and filter MCPs by categories', 'true', '"{\"Context Management\",\"Protocol Implementations\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository explicitly defines and curates Model Context Protocols (MCPs), offering features to browse, submit, and discuss them, aligning with the MCP definition provided.","categories":["Context Management","Prompt Engineering Tools","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The architecture appears to be a web application focused on MCP curation. Key design patterns likely involve data storage and retrieval for MCPs, user authentication and authorization for submissions, voting, and commenting. A search and filtering mechanism is also a key architectural component.","Implementation Details and Key Features":"The implementation details are centered around managing a collection of MCPs. Key features include browsing, submission with voting and commenting capabilities. The application likely uses a database to store MCP data and employs a frontend framework to render the user interface.","Integration Approaches and Protocols":"This repository itself defines and catalogues MCPs rather than implementing them directly. The focus is on providing a platform for the community to manage and discuss these protocols. Integration would involve consuming the MCPs listed within the application in other AI models or applications.","Technical Stack and Dependencies":"The technical stack is not explicitly mentioned, but based on the features, it likely includes a frontend framework like React or Vue.js, a backend framework like Node.js or Python/Flask, and a database (e.g., PostgreSQL or MongoDB) to store MCPs and related metadata. API endpoints are needed for data access.","Best Practices and Coding Standards":"Without access to the codebase, it's impossible to assess coding standards. However, a well-structured web application with clear separation of concerns, proper error handling, and secure coding practices would be expected for maintaining data integrity and security.","Performance Considerations":"Performance considerations likely focus on efficient data retrieval and rendering of MCPs, especially with a large number of submissions. Database indexing, caching, and optimized frontend rendering techniques would be important. Search functionality needs to be performant."}}', '2025-03-10 21:26:21+00', 'true', 'Servers', 'mcp-server-list'), ('944774530', 'figma-mcp', 'JayZeeDesign/figma-mcp', 'https://github.com/JayZeeDesign/figma-mcp', null, '2025-03-08 00:04:27+00', '2025-03-10 00:47:01+00', 'Python', '20', '# Figma MCP Python

[![PyPI version](https://badge.fury.io/py/figma-mcp.svg)](https://badge.fury.io/py/figma-mcp)

Allow your AI coding agents to access Figma files & prototypes directly.
You can DM me for any issues / improvements: https://x.com/jasonzhou1993

## Quick Installation with pipx

```bash
pipx install figma-mcp
```

### For Cursor:

1. In settings, add an MCP server using the command:
```shell
figma-mcp --figma-api-key=your_figma_key
```

2. OR Add a `.cursor/mcp.json` file in your project:

```json
{
  "mcpServers": {
    "figma-python": {
      "command": "figma-mcp",
      "args": [
        "--figma-api-key=your_figma_key"
      ]
    } 
  }
}
```


### For other IDEs like Windsurf, use an MCP configuration file (e.g., `mcp_config.json`):

```json
{
  "mcpServers": {
    "figma-python": {
      "command": "figma-mcp",
      "args": [
        "--figma-api-key=your_figma_key"
      ]
    } 
  }
}
```


## Install uv and set up the environment
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
uv venv
source .venv/bin/activate
uv sync
```

## Test locally
```bash
python -m figma_mcp.main
```


', 'true', '"{\"Protocol Implementations\",\"Developer Tooling\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository allows AI agents to access Figma files, which aligns with the MCP's purpose of enabling communication between models and external tools/contexts. It supports function calling within IDE environments.","categories":["AI Model Integration","Developer Tooling","Function Calling"],"analysis":{"Architecture and Design Patterns":"The repository implements a command-line interface (CLI) that acts as an MCP server. It uses a configuration file (`mcp.json` or `mcp_config.json`) to specify the server command and arguments, allowing IDEs like Cursor and Windsurf to integrate with Figma. The design promotes modularity by separating the Figma interaction logic from the IDE integration.","Implementation Details and Key Features":"The implementation revolves around a Python package `figma-mcp` which provides the core functionality. The key feature is enabling AI coding agents to access and interact with Figma files directly. It utilizes the Figma API, which requires a Figma API key for authentication. The repository structure includes a `main.py` to initiate the server.","Integration Approaches and Protocols":"The integration is primarily achieved through a local MCP server that is launched from the command line. IDEs such as Cursor and Windsurf are configured to communicate with this server. The repository documentation outlines how to configure these IDEs using MCP configuration files or directly in the IDE settings, establishing the communication protocol.","Technical Stack and Dependencies":"The technical stack primarily consists of Python. It uses `pipx` for installation and `uv` for environment management. Dependencies are managed via `uv sync`. The core functionality relies on the Figma API and likely utilizes libraries for making HTTP requests and parsing JSON data from the Figma API.","Best Practices and Coding Standards":"The documentation provides clear instructions on setting up the environment and configuring IDEs, which suggests an emphasis on usability. The usage of `uv` for environment management indicates attention to dependency isolation and reproducibility. The repository uses a standardized configuration file format (JSON) for defining MCP servers.","Performance Considerations":"Performance considerations are not explicitly mentioned, but accessing Figma files over the network could introduce latency. Optimizing API requests and caching data might be relevant. The repository should consider error handling for API rate limits and connection issues to maintain responsiveness and prevent service disruptions."}}', '2025-03-10 21:28:27+00', 'true', 'Servers', 'figma-mcp-server-944774530'), ('944950953', 'mcp-go', 'ecodeclub/mcp-go', 'https://github.com/ecodeclub/mcp-go', '遵守 MCP 协议，搞一个 GO 版本实现', '2025-03-08 10:00:17+00', '2025-03-08 13:37:14+00', null, '2', '# mcp-go
遵守 MCP 协议，搞一个 GO 版本实现
', 'true', '"{\"Protocol Implementations\",\"AI Model Integration\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states that it implements the Model Context Protocol (MCP) in Go. The primary purpose is to adhere to MCP specifications.","categories":["Protocol Implementations","AI Model Integration","Function Calling"],"analysis":{"Architecture and Design Patterns":"The architecture likely involves defining Go structs and interfaces that represent MCP message formats and interactions. Design patterns may include message queues or pub/sub for asynchronous communication between components. The implementation likely focuses on modularity to allow easy extension and customization.","Implementation Details and Key Features":"The implementation likely includes serialization and deserialization of MCP messages, error handling, and potentially request routing. Key features would focus on adherence to the MCP specification including context window management, function calling, and memory management interactions with other AI models and tools.","Integration Approaches and Protocols":"The integration approach likely uses standard networking protocols such as HTTP or gRPC for communication. The repository implements the MCP protocol to interact with AI models and other services, enabling them to exchange context and call functions. It likely uses standard Go libraries for networking.","Technical Stack and Dependencies":"The technical stack is primarily Go. Dependencies might include libraries for HTTP clients/servers, gRPC, serialization (e.g., JSON or Protocol Buffers), and potentially logging or metrics collection. The repository is likely built using the standard Go toolchain. No external databases or specialized hardware are expected.","Best Practices and Coding Standards":"The repository should follow standard Go coding conventions, including proper error handling, documentation, and testing. Code should be well-structured and modular to promote maintainability and readability. The implementation should be robust and handle edge cases gracefully. Adherence to Go's style guidelines is crucial.","Performance Considerations":"Performance considerations include efficient serialization/deserialization of messages, minimizing latency in communication, and managing resources effectively. Optimizations could involve using Go's concurrency features to handle multiple requests concurrently. Memory management is important for large context windows. Benchmarking and profiling are important for performance tuning."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'mcp-go-protocol'), ('944959776', 'mcpservers', 'toastts/mcpservers', 'https://github.com/toastts/mcpservers', null, '2025-03-08 10:25:37+00', '2025-03-10 18:17:53+00', 'HTML', '0', 'This is a [Next.js](https://nextjs.org) project bootstrapped with [`create-next-app`](https://nextjs.org/docs/app/api-reference/cli/create-next-app).

## Getting Started

First, run the development server:

```bash
npm run dev
# or
yarn dev
# or
pnpm dev
# or
bun dev
```

Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.

You can start editing the page by modifying `app/page.tsx`. The page auto-updates as you edit the file.

This project uses [`next/font`](https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load [Geist](https://vercel.com/font), a new font family for Vercel.

## Learn More

To learn more about Next.js, take a look at the following resources:

- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.
- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.

You can check out [the Next.js GitHub repository](https://github.com/vercel/next.js) - your feedback and contributions are welcome!

## Deploy on Vercel

The easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.

Check out our [Next.js deployment documentation](https://nextjs.org/docs/app/building-your-application/deploying) for more details.
', 'true', '"{\"API Frameworks\",\"Developer Tooling\",\"Web Automation\"]}', '{"is_mcp":"NO","justification":"The repository appears to be a standard Next.js project setup with no clear indication of implementing or relating to Model Context Protocol (MCP) specifications or AI model communication protocols.","categories":["API Frameworks","Developer Tooling","Cloud Infrastructure"],"analysis":{"Architecture and Design Patterns":"The project is based on Next.js, a React framework, employing a server-side rendering approach and following a component-based architecture. The 'app' directory structure suggests the use of Next.js's App Router feature. It likely incorporates common web development patterns such as routing and API endpoint creation.","Implementation Details and Key Features":"The core implementation relies on Next.js features like automatic code splitting, optimized font loading via `next/font`, and API route handling. The 'app/page.tsx' file serves as the main entry point for the application's landing page. The project can be deployed on Vercel, using its serverless functions.","Integration Approaches and Protocols":"The project utilizes HTTP for communication between the client and server. It likely integrates with Vercel's platform for deployment and serverless functions. It leverages Next.js's API routes for backend functionality, allowing integration with external services via API calls.","Technical Stack and Dependencies":"The project's technical stack includes Next.js, React, and potentially Vercel for deployment. Dependencies are managed using npm, yarn, pnpm, or bun. The code likely uses TypeScript based on the .tsx extension. The font, Geist, is optimized using Next.js.","Best Practices and Coding Standards":"The project appears to follow Next.js recommended coding practices for file structure and component design. It leverages Next.js's built-in features for performance optimization and font loading. Following Vercel's deployment guidelines is implied for streamlined deployment process.","Performance Considerations":"The project benefits from Next.js features like automatic code splitting and server-side rendering for improved performance. Font optimization using `next/font` contributes to faster page load times. Serverless deployment on Vercel provides scalability and reduces server management overhead."}}', '2025-03-10 21:26:08+00', 'true', 'Servers', 'mcpservers-servers'), ('944976593', 'fastapi_mcp', 'tadata-org/fastapi_mcp', 'https://github.com/tadata-org/fastapi_mcp', 'A zero-configuration tool for automatically exposing FastAPI endpoints as Model Context Protocol (MCP) tools.', '2025-03-08 11:15:43+00', '2025-03-10 18:25:29+00', 'Python', '111', '# FastAPI-MCP

A zero-configuration tool for automatically exposing FastAPI endpoints as Model Context Protocol (MCP) tools.

[![PyPI version](https://badge.fury.io/py/fastapi-mcp.svg)](https://pypi.org/project/fastapi-mcp/)
[![Python Versions](https://img.shields.io/pypi/pyversions/fastapi-mcp.svg)](https://pypi.org/project/fastapi-mcp/)

## Features

- **Direct integration** - Mount an MCP server directly to your FastAPI app
- **Zero configuration** required - just point it at your FastAPI app and it works
- **Automatic discovery** of all FastAPI endpoints and conversion to MCP tools
- **Preserving schemas** of your request models and response models
- **Preserve documentation** of all your endpoints, just as it is in Swagger
- **Extend** - Add custom MCP tools alongside the auto-generated ones

## Installation

We recommend using [uv](https://docs.astral.sh/uv/), a fast Python package installer:

```bash
uv add fastapi-mcp
```

Alternatively, you can install with pip:

```bash
pip install fastapi-mcp
```

## Basic Usage

The simplest way to use FastAPI-MCP is to add an MCP server directly to your FastAPI application:

```python
from fastapi import FastAPI
from fastapi_mcp import add_mcp_server

# Your FastAPI app
app = FastAPI()

# Mount the MCP server to your app
add_mcp_server(
    app,                    # Your FastAPI app
    mount_path="/mcp",      # Where to mount the MCP server
    name="My API MCP",      # Name for the MCP server
)
```

That's it! Your auto-generated MCP server is now available at `https://app.base.url/mcp`.

## Advanced Usage

FastAPI-MCP provides several ways to customize and control how your MCP server is created and configured. Here are some advanced usage patterns:

```python
from fastapi import FastAPI
from fastapi_mcp import add_mcp_server

app = FastAPI()

mcp_server = add_mcp_server(
    app,                                    # Your FastAPI app
    mount_path="/mcp",                      # Where to mount the MCP server
    name="My API MCP",                      # Name for the MCP server
    describe_all_responses=True,            # False by default. Include all possible response schemas in tool descriptions, instead of just the successful response.
    describe_full_response_schema=True      # False by default. Include full JSON schema in tool descriptions, instead of just an LLM-friendly response example.
)

# Optionally add custom tools in addition to existing APIs.
@mcp_server.tool()
async def get_server_time() -> str:
    """Get the current server time."""
    from datetime import datetime
    return datetime.now().isoformat()
```

## Examples

See the [examples](examples) directory for complete examples.

## Connecting to the MCP Server

Once your FastAPI app with MCP integration is running, you can connect to it with any MCP client, such as Claude:

1. Run your application
2. In Claude, use the URL of your MCP server endpoint (e.g., `http://localhost:8000/mcp`)
3. Claude will discover all available tools and resources automatically

## Development and Contributing

If you're interested in contributing to FastAPI-MCP:

```bash
# Clone the repository
git clone https://github.com/tadata-org/fastapi_mcp.git
cd fastapi_mcp

# Create a virtual environment and install dependencies with uv
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv add -e ".[dev]"

# Run tests
uv run pytest
```

For more details about contributing, see [CONTRIBUTING.md](CONTRIBUTING.md).

## Requirements

- Python 3.10+
- uv

## License

MIT License. Copyright (c) 2024 Tadata Inc.

## About

Developed and maintained by [Tadata Inc.](https://github.com/tadata-org)
', 'true', '"{\"API Frameworks\",\"Function Calling\",\"Protocol Implementations\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements Model Context Protocol (MCP) by automatically exposing FastAPI endpoints as MCP tools. This includes auto-discovery, schema preservation, and documentation integration.","categories":["API Frameworks","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"FastAPI-MCP utilizes a decorator-based approach to integrate with FastAPI applications. It mounts an MCP server to the FastAPI app, allowing automatic discovery and conversion of endpoints into MCP tools. The design emphasizes zero configuration and ease of use, providing a direct integration pathway.","Implementation Details and Key Features":"The core implementation involves automatically discovering FastAPI endpoints and translating them into MCP tool definitions. Key features include preserving schemas of request and response models, preserving endpoint documentation, and providing the ability to extend with custom MCP tools beyond the auto-generated ones.","Integration Approaches and Protocols":"The primary integration approach involves mounting an MCP server to a FastAPI application at a specified path. It supports connection with any MCP client, like Claude, using the server endpoint URL. The tool focuses on automating the process of making FastAPI APIs available through the MCP protocol.","Technical Stack and Dependencies":"The technical stack primarily consists of Python 3.10+ and FastAPI. It utilizes `uv` for package management. The core logic likely relies on inspecting FastAPI's endpoint definitions and converting them to MCP-compatible formats, possibly involving schema introspection and documentation parsing.","Best Practices and Coding Standards":"The repository encourages the use of virtual environments and provides clear instructions for development and contribution, indicating adherence to standard Python coding practices. The use of `uv` suggests a focus on performance. Type hints and docstrings improve code readability and maintainability.","Performance Considerations":"The 'zero configuration' and automatic discovery features suggest a focus on developer experience. The choice of `uv` over `pip` indicates a priority on installation speed. Performance considerations within the MCP server itself, such as efficient schema handling, may be important for real-world deployments."}}', '2025-03-10 21:28:52+00', 'true', 'Servers', 'fastapi-mcp-tools'), ('945009479', 'OpenMCP', 'shareAI-lab/OpenMCP', 'https://github.com/shareAI-lab/OpenMCP', '开源免费的Agent MCP公共性建议标准', '2025-03-08 12:54:17+00', '2025-03-10 17:03:12+00', null, '15', '## OpenMCP

OpenMCP (Open Model Context Protocol)是一种开放协议，用于标准化应用程序如何向大型语言模型（LLMs）提供上下文。可以将 OpenMCP 想象为 AI 应用的 USB 接口。正如 USB 提供了一种标准化的方式将您的设备连接到各种外设和配件，OpenMCP 也提供了一种标准化的方式，将 AI 模型连接到不同的数据源，软件和硬件。

该项目的愿景在于架起大模型 Agent 到 Anything 之间的桥梁。为 Agent 应用的上下游开发者提供易于开发的协议级框架，让大模型更快赋能到软件硬件的生态中。

OpenMCP通用Agent协议 QQ 交流群：782833642

 

## 仓库使用规则实例
想法放在/Idea/下，使用Markdown
MCP Server工作放在/SDK-Sample/适用领域/，使用Markdown，例如SDK-Sample\Blender-Agent-SDK\你的项目
 

## 目标和价值
- 让全球开发者可以通过查找需要的MCP Server来快速的配置自己的AI应用，让开发AI逐步进化成积木式配置AI
- 让全球开发者可以通过提交自己的MCP Server快速融入全球AI开发者生态 
- 所有通过MCP Server参与到AI生态建设的，都更加容易被基座大模型识别，提升GEO能力', 'true', '"{\"Agent Frameworks\",\"Protocol Implementations\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it's an open protocol for standardizing how applications provide context to LLMs, aligning with the Model Context Protocol (MCP) definition.","categories":["Protocol Implementations","Agent Frameworks","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The repository aims to establish a standardized protocol (OpenMCP) for AI applications to interact with large language models (LLMs) using context. The design focuses on creating a universal interface, similar to USB, facilitating seamless connection of AI models to diverse data sources and software/hardware systems. The intent is to enable modular AI development through composable MCP servers.","Implementation Details and Key Features":"The repository structure includes directories for ideas and SDK samples, suggesting a focus on protocol specifications and example implementations. Key features revolve around enabling developers to integrate their AI applications with various MCP servers for context enrichment. The emphasis is on simplifying integration, allowing developers to quickly configure AI applications with relevant data and functionalities.","Integration Approaches and Protocols":"The core integration approach involves the OpenMCP protocol, which serves as a standardized way for applications to provide context to LLMs. It aims to bridge the gap between AI agents and various external resources. The protocol's purpose is to facilitate communication between AI models and external components, improving the model's ability to access and use relevant information.","Technical Stack and Dependencies":"The description lacks specific details on the technical stack, but the usage of Markdown for documentation and sample code hints at reliance on text-based configuration and code. The 'SDK-Sample' directory suggests code examples, possibly in languages commonly used in AI development. Dependencies would likely vary based on the specific MCP Server implementation.","Best Practices and Coding Standards":"The 'Idea' directory suggests a collaborative approach to protocol development, encouraging community contributions and feedback. The repository structure promotes modularity and separation of concerns, with dedicated directories for ideas and example implementations. The project aims to establish a common standard, encouraging interoperability and reusability across AI applications.","Performance Considerations":"The documentation doesn't explicitly address performance considerations. However, the choice of protocol and implementation details within MCP servers would likely influence performance. Optimization strategies might involve minimizing latency, efficient data retrieval, and effective context window management to ensure real-time responsiveness in AI applications."}}', '2025-03-10 21:28:31+00', 'true', 'Servers', 'openmcp-agent-servers'), ('945018825', 'mult-fetch-mcp-server', 'lmcc-dev/mult-fetch-mcp-server', 'https://github.com/lmcc-dev/mult-fetch-mcp-server', null, '2025-03-08 13:21:29+00', '2025-03-10 16:50:59+00', 'TypeScript', '1', '# mult-fetch-mcp-server

This project implements an MCP-compliant client and server for communication between AI assistants and external tools.

[English](./README.md) | [中文文档](./README.zh.md)

## Project Structure

```
fetch-mcp/
├── src/                # Source code directory
│   ├── lib/            # Library files
│   │   ├── BrowserFetcher.ts # Browser mode fetcher
│   │   ├── NodeFetcher.ts    # Node.js mode fetcher
│   │   ├── server/           # Server-related modules
│   │   │   ├── index.ts      # Server entry point
│   │   │   ├── browser.ts    # Browser management
│   │   │   ├── fetcher.ts    # Web fetching logic
│   │   │   ├── logger.ts     # Logging utilities
│   │   │   ├── tools.ts      # Tool registration and handling
│   │   │   └── types.ts      # Server type definitions
│   │   ├── i18n/             # Internationalization support
│   │   │   ├── index.ts      # i18n configuration
│   │   │   └── logger.ts     # i18n logger utilities
│   │   └── types.ts          # Common type definitions
│   ├── client.ts       # MCP client implementation
│   └── mcp-server.ts   # MCP server main entry
├── index.ts            # Server entry point
├── tests/              # Test files
│   ├── test-mcp.ts     # MCP functionality tests
│   ├── test-mini4k.ts  # Specific website tests
│   └── test-direct-client.ts # Direct client call tests
└── dist/               # Compiled files
    ├── index.js        # Compiled entry point
    ├── src/            # Compiled source code
    └── tests/          # Compiled test files
```

## MCP Specification

The Model Context Protocol (MCP) defines two main transport methods:

1. **Standard Input/Output (Stdio)**: The client starts the MCP server as a child process, and they communicate through standard input (stdin) and standard output (stdout).
2. **Server-Sent Events (SSE)**: Used to pass messages between client and server.

This project implements the Standard Input/Output (Stdio) transport method.

## Features

- Implementation based on the official MCP SDK
- Support for Standard Input/Output (Stdio) transport
- Multiple web scraping methods (HTML, JSON, text, Markdown)
- Intelligent mode switching: automatically switches between standard requests and browser mode
- Detailed debug logs output to standard error stream
- Support for Chinese and English bilingual internationalization
- Modular design for easy maintenance and extension

## Installation

### Local Installation

```bash
npm install
```

### Global Installation

```bash
npm install -g mult-fetch-mcp-server
```

Or use npx to run directly (no installation required):

```bash
npx mult-fetch-mcp-server
```

## Integration with Claude

To integrate this tool with Claude desktop, you need to add server configuration:

### Configuration File Location

- **MacOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`
- **Windows**: `%APPDATA%/Claude/claude_desktop_config.json`

### Configuration Examples

#### Method 1: Using npx (Recommended)

This method is the simplest, doesn't require specifying the full path, and is suitable for global installation or direct use with npx:

```json
{
  "mcpServers": {
    "mult-fetch-mcp-server": {
      "command": "npx",
      "args": ["mult-fetch-mcp-server"],
      "env": {
        "MCP_LANG": "en"  // Set language to English, options: "zh" or "en"
      }
    }
  }
}
```

#### Method 2: Specifying Full Path

If you need to use a specific installation location, you can specify the full path:

```json
{
  "mcpServers": {
    "mult-fetch-mcp-server": {
      "command": "path-to/bin/node",
      "args": ["path-to/mult-fetch-mcp-server/dist/index.js"],
      "env": {
        "MCP_LANG": "en"  // Set language to English, options: "zh" or "en"
      }
    }
  }
}
```

Please replace `path-to/bin/node` with the path to the Node.js executable on your system, and replace `path-to/mult-fetch-mcp-server` with the actual path to this project.

### Usage

After configuration, restart Claude desktop, and you can use the following tools in your conversation:

- `fetch_html`: Get HTML content of a webpage
- `fetch_json`: Get JSON data
- `fetch_txt`: Get plain text content
- `fetch_markdown`: Get Markdown formatted content

## Build

```bash
npm run build
```

## Run Server

```bash
npm run server
# or
node dist/index.js
# if globally installed, you can run directly
mult-fetch-mcp-server
# or use npx
npx mult-fetch-mcp-server
```

## Run Client

```bash
npm run client <method> <params_json>
# example
npm run client fetch_html '{"url": "https://example.com", "debug": true}'
```

## Run Tests

```bash
# Run MCP functionality tests
npm run test:mcp

# Run mini4k.com website tests
npm run test:mini4k

# Run direct client call tests
npm run test:direct
```

## Language Settings

This project supports Chinese and English bilingual internationalization. You can set the language using environment variables:

### Using Environment Variables

Set the `MCP_LANG` environment variable to control the language:

```bash
# Set to English
export MCP_LANG=en
npm run server

# Set to Chinese
export MCP_LANG=zh
npm run server

# Windows system
set MCP_LANG=zh
npm run server
```

Using environment variables ensures that all related processes (including the MCP server) use the same language settings.

### Default Language

By default, the system will choose a language according to the following priority:
1. `MCP_LANG` environment variable
2. Operating system language (if it starts with "zh", use Chinese)
3. English (as the final fallback option)

## Debugging

This project follows the MCP protocol specification and does not output any logs by default to avoid interfering with JSON-RPC communication. Debug information is controlled through call parameters:

### Using the debug Parameter

Set the `debug: true` parameter when calling a tool:

```json
{
  "url": "https://example.com",
  "debug": true
}
```

Debug messages are sent to the standard error stream (stderr) using the following format:

```
[MCP-SERVER] MCP server starting...
[CLIENT] Fetching URL: https://example.com
```

## Parameter Handling

This project handles parameters in the following ways:

- **debug**: Passed through call parameters, each request can individually control whether to enable debug output
- **MCP_LANG**: Retrieved from environment variables, controls the language settings of the entire server

## Usage

### Creating a Client

```typescript
import { Client } from '@modelcontextprotocol/sdk/client/index.js';
import { StdioClientTransport } from '@modelcontextprotocol/sdk/client/stdio.js';
import path from 'path';
import { fileURLToPath } from 'url';

// Get the directory path of the current file
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Create client transport layer
const transport = new StdioClientTransport({
  command: 'node',
  args: [path.resolve(__dirname, 'dist/index.js')],
  stderr: 'inherit',
  env: {
    ...process.env  // Pass all environment variables, including MCP_LANG
  }
});

// Create client
const client = new Client({
  name: "example-client",
  version: "1.0.0"
});

// Connect to transport layer
await client.connect(transport);

// Use client
const result = await client.callTool({
  name: 'fetch_html',
  arguments: {
    url: 'https://example.com',
    debug: true  // Control debug output through parameters
  }
});

if (result.isError) {
  console.error('Fetch failed:', result.content[0].text);
} else {
  console.log('Fetch successful!');
  console.log('Content preview:', result.content[0].text.substring(0, 500));
}
```

### Supported Tools

- `fetch_html`: Get HTML content of a webpage
- `fetch_json`: Get JSON data
- `fetch_txt`: Get plain text content
- `fetch_markdown`: Get Markdown formatted content

### Resources Support

The server includes support for the resources/list and resources/read methods, but currently no resources are defined in the implementation. The resource system is designed to provide access to project files and documentation, but this feature is not fully implemented yet.

#### Resource Usage Example

```typescript
// Example: List available resources
const resourcesResult = await client.listResources({});
console.log('Available resources:', resourcesResult);

// Note: Currently this will return empty lists for resources and resourceTemplates
```

### Supported Prompt Templates

The server provides the following prompt templates:

- `fetch-website`: Get website content, supporting different formats and browser mode
- `extract-content`: Extract specific content from a website, supporting CSS selectors and data type specification
- `debug-fetch`: Debug website fetching issues, analyze possible causes and provide solutions

#### Prompt Template Usage

1. Use `prompts/list` to get a list of available prompt templates
2. Use `prompts/get` to get specific prompt template content

```typescript
// Example: List available prompt templates
const promptsResult = await client.listPrompts({});
console.log('Available prompts:', promptsResult);

// Example: Get website content prompt
const fetchPrompt = await client.getPrompt({
  name: "fetch-website",
  arguments: {
    url: "https://example.com",
    format: "html",
    useBrowser: "false"
  }
});
console.log('Fetch website prompt:', fetchPrompt);

// Example: Debug website fetching issues
const debugPrompt = await client.getPrompt({
  name: "debug-fetch",
  arguments: {
    url: "https://example.com",
    error: "Connection timeout"
  }
});
console.log('Debug fetch prompt:', debugPrompt);
```

### Parameter Options

Each tool supports the following parameters:

#### Basic Parameters
- `url`: URL to fetch (required)
- `headers`: Custom request headers (optional, default is {})
- `proxy`: Proxy server URL in format http://host:port or https://host:port (optional)

#### Network Control Parameters
- `timeout`: Timeout in milliseconds (optional, default is 30000)
- `maxRedirects`: Maximum number of redirects to follow (optional, default is 10)
- `noDelay`: Whether to disable random delay between requests (optional, default is false)
- `useSystemProxy`: Whether to use system proxy (optional, default is true)

#### Mode Control Parameters
- `useBrowser`: Whether to use browser mode (optional, default is false)
- `useNodeFetch`: Whether to force using Node.js mode (optional, default is false, mutually exclusive with `useBrowser`)
- `autoDetectMode`: Whether to automatically detect and switch to browser mode (optional, default is true)

#### Browser Mode Specific Parameters
- `waitForSelector`: Selector to wait for in browser mode (optional, default is 'body')
- `waitForTimeout`: Timeout to wait in browser mode in milliseconds (optional, default is 5000)
- `scrollToBottom`: Whether to scroll to the bottom of the page in browser mode (optional, default is false)
- `saveCookies`: Whether to save cookies in browser mode (optional, default is true)
- `closeBrowser`: Whether to close the browser instance (optional, default is false)

#### Debug Parameters
- `debug`: Whether to enable debug output (optional, default is false)

### Special Usage

#### Closing Browser Without Fetching
To close the browser instance without performing any fetch operation:
```json
{
  "url": "about:blank",
  "closeBrowser": true
}
```

#### Proxy Priority
The proxy is determined in the following order:
1. Command line specified proxy
2. `proxy` parameter in the request
3. Environment variables (if `useSystemProxy` is true)
4. Git configuration (if `useSystemProxy` is true)

If `proxy` is set, `useSystemProxy` will be automatically set to false.

### Debug Output

When `debug: true` is set, the logs will be output to stderr with the following prefixes:
- `[MCP-SERVER]`: Logs from the MCP server
- `[NODE-FETCH]`: Logs from the Node.js fetcher
- `[BROWSER-FETCH]`: Logs from the browser fetcher
- `[CLIENT]`: Logs from the client

## License

MIT', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states that it implements an MCP-compliant client and server, supporting communication between AI assistants and external tools via Stdio transport. This aligns with the MCP's purpose of facilitating model-tool interaction.","categories":["Protocol Implementations","AI Model Integration","Function Calling"],"analysis":{"Architecture and Design Patterns":"The repository follows a modular design, separating concerns into different directories such as `src/lib` for core functionalities, `src/lib/server` for server-related modules, and `src/client.ts` for the MCP client implementation. The project utilizes TypeScript and incorporates internationalization support. Overall, the architecture promotes maintainability and extensibility through a well-defined structure.","Implementation Details and Key Features":"The implementation focuses on providing web scraping functionalities through various tools like `fetch_html`, `fetch_json`, `fetch_txt`, and `fetch_markdown`. It supports both Node.js and browser-based fetching, with intelligent mode switching. The project leverages the official MCP SDK and incorporates features like detailed debug logs and parameter handling for customization. It offers resource and prompt template support, although some features are not fully implemented.","Integration Approaches and Protocols":"The primary integration approach is through the Model Context Protocol (MCP), specifically using the Standard Input/Output (Stdio) transport method. Integration with Claude is demonstrated via configuration examples. The project uses JSON-RPC for communication and supports configuring language settings through environment variables. The design allows external tools to interact seamlessly with AI models.","Technical Stack and Dependencies":"The project uses Node.js as its runtime environment. The primary language is TypeScript. Dependencies include `@modelcontextprotocol/sdk` for MCP implementation, and potentially libraries for web scraping and internationalization, although they are not explicitly listed. The `package.json` would provide more specific version and dependency information. The system uses `npm` for package management.","Best Practices and Coding Standards":"The project demonstrates several best practices, including modular design, internationalization support, and detailed debug logging controlled by call parameters. Usage of TypeScript promotes type safety and code maintainability. The presence of tests indicates a commitment to code quality. The code includes usage examples and integration instructions.","Performance Considerations":"The intelligent mode switching between standard requests and browser mode aims to optimize performance based on the specific website being scraped. Network control parameters like `timeout` and `maxRedirects` provide some performance tuning capabilities. The optional disabling of random delays can affect performance and should be used cautiously. Further profiling would be needed for a complete performance analysis."}}', '2025-03-10 21:26:21+00', 'true', 'Servers', 'mult-fetch-mcp'), ('945034453', 'mcp-server-template', 'jatinsandilya/mcp-server-template', 'https://github.com/jatinsandilya/mcp-server-template', null, '2025-03-08 14:05:28+00', '2025-03-09 14:12:48+00', 'TypeScript', '7', '# MCP Server Template 🛠️

A starter template for building your own Model Context Protocol (MCP) server. This template provides the basic structure and setup needed to create custom MCPs that can be used with Cursor or Claude Desktop.

## Features

- Basic MCP server setup with TypeScript
- Sample tool implementation
- Ready-to-use project structure
- Built with [@modelcontextprotocol/sdk](https://docs.anthropic.com/en/docs/agents-and-tools/mcp)

## Project Structure

```
mcp-server-template/
├── index.ts        # Main server implementation
├── package.json    # Project dependencies
├── tsconfig.json   # TypeScript configuration
└── build/         # Compiled JavaScript output
```

## Getting Started

1. Clone this template:
```bash
git clone [your-repo-url] my-mcp-server
cd my-mcp-server
```

2. Install dependencies:
```bash
pnpm install
```

3. Build the project:
```bash
pnpm run build
```

This will generate the `/build/index.js` file - your compiled MCP server script.

## Using with Cursor

1. Go to Cursor Settings -> MCP -> Add new MCP server
2. Configure your MCP:
   - Name: [choose your own name]
   - Type: command
   - Command: `node ABSOLUTE_PATH_TO_MCP_SERVER/build/index.js`

## Using with Claude Desktop

Add the following MCP config to your Claude Desktop configuration:

```json
{
  "mcpServers": {
    "your-mcp-name": {
      "command": "node",
      "args": ["ABSOLUTE_PATH_TO_MCP_SERVER/build/index.js"]
    }
  }
}
```

## Development

The template includes a sample tool implementation in `index.ts`. To create your own MCP:

1. Modify the server configuration in `index.ts`:
```typescript
const server = new McpServer({
  name: "your-mcp-name",
  version: "0.0.1",
});
```

2. Define your custom tools using the `server.tool()` method:
```typescript
server.tool(
  "your-tool-name",
  "Your tool description",
  {
    // Define your tool's parameters using Zod schema
    parameter: z.string().describe("Parameter description"),
  },
  async ({ parameter }) => {
    // Implement your tool's logic here
    return {
      content: [
        {
          type: "text",
          text: "Your tool's response",
        },
      ],
    };
  }
);
```

3. Build and test your implementation:
```bash
npm run build
```

## Contributing

Feel free to submit issues and enhancement requests!

## License

MIT
', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository is explicitly a template for building Model Context Protocol (MCP) servers, designed for use with Cursor and Claude Desktop, indicating direct MCP implementation.","categories":["Protocol Implementations","Agent Frameworks","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The repository provides a basic server template built with TypeScript, designed for easy customization. It follows a modular design where tool implementations are added to the main server instance. The project structure is straightforward, separating source code from compiled JavaScript output to enhance maintainability and ease of deployment.","Implementation Details and Key Features":"The core implementation revolves around the `@modelcontextprotocol/sdk`. Key features include a pre-configured MCP server setup, a sample tool implementation, and a ready-to-use project structure. Developers can define custom tools with descriptions and parameter schemas using Zod. The server enables registering tools to handle specific functions.","Integration Approaches and Protocols":"The repository provides integration examples for Cursor and Claude Desktop using command-line execution. Integration involves configuring these platforms to execute the compiled MCP server script using `node`. The MCP server interacts with the environment via standard input/output streams or specific API endpoints, facilitating communication and data exchange.","Technical Stack and Dependencies":"The primary technical stack includes TypeScript, Node.js, and `pnpm` as the package manager. The `@modelcontextprotocol/sdk` library is a central dependency, facilitating MCP functionality. Zod is used for defining parameter schemas.  The runtime environment requires Node.js to execute the JavaScript code.","Best Practices and Coding Standards":"The template uses TypeScript, promoting type safety and code maintainability. The project structure is well-organized with clear separation of concerns. The documentation encourages developers to follow a structured approach for defining and implementing tools. Error handling and input validation using Zod schemas are implemented to ensure robustness.","Performance Considerations":"Performance can be influenced by the complexity of the implemented tools and the overhead of the Node.js runtime. Efficient tool implementations and optimized Zod schemas are crucial for minimizing latency. Scaling the MCP server may involve using process managers or load balancing strategies to handle concurrent requests. Code profiling and optimization techniques can improve performance."}}', '2025-03-10 21:28:27+00', 'true', 'Servers', 'mcp-server-template-945034453'), ('945036417', 'tfmcp', 'nwiizo/tfmcp', 'https://github.com/nwiizo/tfmcp', '🌍 Terraform Model Context Protocol (MCP) Tool - An experimental CLI tool that enables AI assistants to manage and operate Terraform environments. Supports reading Terraform configurations, analyzing plans, applying configurations, and managing state with Claude Desktop integration. ⚡️', '2025-03-08 14:10:29+00', '2025-03-10 09:03:04+00', 'Rust', '66', '# tfmcp: Terraform Model Context Protocol Tool

*⚠️  This project is experimental. Features may change without notice. Use with caution! ⚠️*

tfmcp is a command-line tool that helps you interact with Terraform via the Model Context Protocol (MCP). It allows LLMs to manage and operate your Terraform environments, including:

## 🎮 Demo

See tfmcp in action with Claude Desktop:

![tfmcp Demo with Claude Desktop](.github/images/tfmcp-demo.gif)

- Reading Terraform configuration files
- Analyzing Terraform plan outputs
- Applying Terraform configurations
- Managing Terraform state
- Creating and modifying Terraform configurations

## 🎉 Latest Release

The first stable release of tfmcp (v0.1.1) is now available on Crates.io! You can easily install it using Cargo:

```bash
cargo install tfmcp
```

## Features

- 🚀 **Terraform Integration**  
  Deeply integrates with the Terraform CLI to analyze and execute operations.

- 📄 **MCP Server Capabilities**  
  Runs as a Model Context Protocol server, allowing AI assistants to access and manage Terraform.

- ⚡️ **Blazing Fast**  
  High-speed processing powered by the Rust ecosystem.

- 🛠️ **Automatic Setup**  
  Automatically creates sample Terraform projects when needed, ensuring smooth operation even for new users.

## Installation

### From Source
```bash
# Clone the repository
git clone https://github.com/nwiizo/tfmcp
cd tfmcp

# Build and install
cargo install --path .
```

### From Crates.io
```bash
cargo install tfmcp
```

## Requirements

- Rust (edition 2021)
- Terraform CLI installed and available in PATH
- Claude Desktop (for AI assistant integration)

## Usage

```bash
$ tfmcp --help
✨ A CLI tool to manage Terraform configurations and operate Terraform through the Model Context Protocol (MCP).

Usage: tfmcp [OPTIONS] [COMMAND]

Commands:
  mcp       Launch tfmcp as an MCP server
  analyze   Analyze Terraform configurations
  help      Print this message or the help of the given subcommand(s)

Options:
  -c, --config <PATH>    Path to the configuration file
  -d, --dir <PATH>       Terraform project directory
  -V, --version          Print version
  -h, --help             Print help
```

### Integrating with Claude Desktop

To use tfmcp with Claude Desktop:

1. If you haven't already, install tfmcp:
   ```bash
   cargo install tfmcp
   ```

2. Find the path to your installed tfmcp executable:
   ```bash
   which tfmcp
   ```

3. Add the following configuration to `~/Library/Application\ Support/Claude/claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "tfmcp": {
      "command": "/path/to/your/tfmcp",  // Replace with the actual path from step 2
      "args": ["mcp"],
      "env": {
        "HOME": "/Users/yourusername",  // Replace with your username
        "PATH": "/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin",
        "TERRAFORM_DIR": "/path/to/your/terraform/project"  // Optional: specify your Terraform project
      }
    }
  }
}
```

4. Restart Claude Desktop and enable the tfmcp tool.

5. tfmcp will automatically create a sample Terraform project in `~/terraform` if one doesn't exist, ensuring Claude can start working with Terraform right away. The sample project is based on the examples included in the `example/demo` directory of this repository.

## Logs and Troubleshooting

The tfmcp server logs are available at:
```
~/Library/Logs/Claude/mcp-server-tfmcp.log
```

Common issues and solutions:

- **Claude can't connect to the server**: Make sure the path to the tfmcp executable is correct in your configuration
- **Terraform project issues**: tfmcp automatically creates a sample Terraform project if none is found
- **Method not found errors**: MCP protocol support includes resources/list and prompts/list methods

## Environment Variables

- `TERRAFORM_DIR`: Set this to specify a custom Terraform project directory. If not set, tfmcp will use the directory provided by command line arguments, configuration files, or fall back to `~/terraform`. You can also change the project directory at runtime using the `set_terraform_directory` tool.
- `TFMCP_LOG_LEVEL`: Set to `debug`, `info`, `warn`, or `error` to control logging verbosity.
- `TFMCP_DEMO_MODE`: Set to `true` to enable demo mode with additional safety features.

## Security Considerations

When using tfmcp, please be aware of the following security considerations:

- tfmcp executes Terraform commands on your behalf, which can create, modify, or destroy infrastructure
- In production environments, consider using appropriate IAM permissions and role boundaries
- Review all Terraform plans before applying them, especially when generated by an AI
- Sensitive information in your Terraform state might be accessible to the AI assistants

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## Roadmap

Here are some planned improvements and future features for tfmcp:

### Completed
- [x] **Basic Terraform Integration**  
  Core integration with Terraform CLI for analyzing and executing operations.

- [x] **MCP Server Implementation**  
  Initial implementation of the Model Context Protocol server for AI assistants.

- [x] **Automatic Project Creation**  
  Added functionality to automatically create sample Terraform projects when needed.

- [x] **Claude Desktop Integration**  
  Support for seamless integration with Claude Desktop.

- [x] **Core MCP Methods**  
  Implementation of essential MCP methods including resources/list and prompts/list.

- [x] **Error Handling Improvements**  
  Better error handling and recovery mechanisms for robust operation.

- [x] **Dynamic Project Directory Switching**  
  Added ability to change the active Terraform project directory without restarting the service.

- [x] **Crates.io Publication**  
  Published the package to Crates.io for easy installation via Cargo.

### In Progress
- [ ] **Enhanced Terraform Analysis**  
  Implement deeper parsing and analysis of Terraform configurations, plans, and state files.

- [ ] **Multi-Environment Support**  
  Add support for managing multiple Terraform environments, workspaces, and modules.

- [ ] **Security Enhancements**  
  Improve security features including better authentication and authorization mechanisms.

- [ ] **Expanded MCP Protocol Support**  
  Implement additional MCP methods and capabilities for richer integration with AI assistants.

- [ ] **Interactive TUI**  
  Develop a terminal-based user interface for easier local usage and debugging.

- [ ] **Cost Estimation**  
  Integrate with cloud provider pricing APIs to provide cost estimates for Terraform plans.

- [ ] **Performance Optimization**  
  Optimize resource usage and response times for large Terraform projects.

- [ ] **Integration with Other AI Platforms**  
  Extend beyond Claude to support other AI assistants and platforms.

- [ ] **Comprehensive Testing Framework**  
  Expand test coverage including integration tests with real Terraform configurations.

- [ ] **Plugin System**  
  Develop a plugin architecture to allow extensions of core functionality.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
', 'true', '"{\"Agent Frameworks\",\"Protocol Implementations\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements the Model Context Protocol (MCP) to allow AI assistants, specifically Claude, to manage and operate Terraform environments. It provides an MCP server and handles Terraform configurations.","categories":["Agent Frameworks","Protocol Implementations","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The architecture centers around a CLI tool that acts as an MCP server, enabling AI agents to interact with Terraform. It uses Rust's robust ecosystem for performance. The design incorporates modular components for Terraform configuration analysis, planning, and execution, facilitating seamless integration with AI assistants. The project leverages standard command-line interface patterns for user interaction.","Implementation Details and Key Features":"The tool implements core MCP methods for AI agents to manage Terraform environments. Key features include reading Terraform configurations, analyzing plan outputs, applying configurations, and managing state. It automatically creates sample Terraform projects for ease of use. Rust provides high-speed processing and seamless integration with Terraform CLI. The configuration allows specifying a Terraform project directory.","Integration Approaches and Protocols":"tfmcp integrates with Claude Desktop using a configuration file to define the command and arguments for the MCP server. It establishes a communication channel between the AI assistant and the Terraform environment. The project supports `resources/list` and `prompts/list` MCP methods and is designed to potentially support other AI platforms in the future.","Technical Stack and Dependencies":"The primary language is Rust (edition 2021). Key dependencies include the Terraform CLI, which must be installed and available in the system's PATH. Integration with Claude Desktop requires its installation and configuration. The project uses Cargo for building and managing dependencies and relies on standard libraries for file system operations and process execution.","Best Practices and Coding Standards":"The project adheres to Rust's coding standards, emphasizing memory safety and performance. It uses standard CLI argument parsing and configuration management. The code includes comprehensive error handling and logging, aiding in troubleshooting and maintaining system stability. The repository includes a clear contribution guide and a well-defined roadmap for future enhancements.","Performance Considerations":"The choice of Rust aims to ensure high-speed processing for Terraform operations. Performance optimizations are planned for large Terraform projects. Efficient resource management and asynchronous processing are likely employed to minimize response times. The tool's architecture is designed to support scalability and handle complex Terraform configurations effectively."}}', '2025-03-10 21:28:52+00', 'true', 'Servers', 'tfmcp-ai-terraform'), ('945099804', 'mcp-client-dumb', 'anaisbetts/mcp-client-dumb', 'https://github.com/anaisbetts/mcp-client-dumb', 'The dumbest MCP client', '2025-03-08 16:55:46+00', '2025-03-09 02:39:25+00', 'JavaScript', '2', '# The Dumbest MCP Client

The world's simplest example of a client for interacting with Claude AI using the Model Context Protocol (MCP) to enable tool usage with YouTube context.

## Overview

This project provides a command-line interface for sending prompts to Anthropic's Claude AI model while giving it access to YouTube-related tools through the Model Context Protocol. The client connects Claude to an MCP YouTube server, allowing the AI to retrieve and process information from YouTube videos when responding to user prompts.

## Prerequisites

- Node.js (v18+)
- Bun runtime
- Anthropic API key

## Installation

1. Clone the repository:
   ```
   git clone https://github.com/anaisbetts/mcp-client.git
   cd mcp-client
   ```

2. Install dependencies:
   ```
   bun install
   ```

3. Set up your Anthropic API key:
   ```
   export ANTHROPIC_API_KEY=your_api_key_here
   ```
   
   On Windows, use:
   ```
   $env:ANTHROPIC_API_KEY=your_api_key_here
   ```

## Usage

Run the client with a prompt:

```
bun start "Your prompt here"
```

Example:

```
bun start "Find me information about the latest SpaceX launch from this YouTube video: https://www.youtube.com/watch?v=example"
```

## Development

- Run in development mode with hot reloading:
  ```
  bun dev
  ```

- Build the project:
  ```
  bun build
  ```

- Run TypeScript type checking:
  ```
  bun typecheck
  ```

- Run tests:
  ```
  bun test
  ```

## How It Works

1. The client initializes an Anthropic client with your API key
2. It starts an MCP YouTube server process
3. The client connects to the MCP server and retrieves available tools
4. These tools are converted to Anthropic's tool format
5. Your prompt is sent to Claude along with the available tools
6. If Claude uses a tool, the request is forwarded to the MCP server
7. The tool response is sent back to Claude for further processing
8. The final response is displayed in the console

## License

ISC

## Dependencies

- @anthropic-ai/sdk: For interacting with Claude AI
- @modelcontextprotocol/sdk: Model Context Protocol SDK
- @anaisbetts/mcp-youtube: YouTube tools for MCP
- spawn-rx: For spawning processes
', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"Protocol Implementations\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly mentions Model Context Protocol (MCP) and interacting with Claude AI using MCP for tool usage with YouTube context. It also uses @modelcontextprotocol/sdk.","categories":["AI Model Integration","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The architecture consists of a client interacting with an MCP server. The client leverages the Anthropic API to send prompts and tool information. The system appears to use a command-line interface for user interaction. The design pattern employed seems to be a client-server model, with the client initiating requests and the server providing responses and tools.","Implementation Details and Key Features":"The client is implemented using Node.js and the Bun runtime. Key features include initializing an Anthropic client, starting an MCP YouTube server process, connecting to the MCP server, retrieving available tools, and converting these tools into Anthropic's format.  It allows Claude to access and utilize tools provided via the MCP.","Integration Approaches and Protocols":"The primary integration approach revolves around the Model Context Protocol (MCP). The client integrates with Anthropic's Claude AI through its SDK and communicates with a separate MCP server. The protocol facilitates the use of external tools by the AI model, specifically designed for YouTube context in this case.","Technical Stack and Dependencies":"The technical stack includes Node.js (v18+), Bun runtime, and TypeScript. Key dependencies are @anthropic-ai/sdk for interacting with Claude, @modelcontextprotocol/sdk for MCP functionality, @anaisbetts/mcp-youtube for YouTube tools, and spawn-rx for process management. The project uses Bun for package management, running scripts, and type checking.","Best Practices and Coding Standards":"The repository includes scripts for running tests (bun test) and type checking (bun typecheck), suggesting a focus on code quality. The project uses standard tools for development (e.g., git for version control, npm/bun for dependency management). The documentation is clear and concise, providing instructions for installation and usage.","Performance Considerations":"Performance considerations aren't explicitly addressed, but the use of Bun runtime might suggest an intent to improve startup time and overall execution speed. The client's interaction with the MCP server and Claude API introduces network latency. The efficiency of tool execution on the MCP server is also a factor for overall performance."}}', '2025-03-10 21:54:07+00', 'true', 'Clients', 'mcp-client-dumb'), ('945152231', 'open-mcp-client', 'CopilotKit/open-mcp-client', 'https://github.com/CopilotKit/open-mcp-client', null, '2025-03-08 19:22:42+00', '2025-03-10 18:01:06+00', 'TypeScript', '579', 'https://github.com/user-attachments/assets/f72e1f7d-3c84-4429-a465-23dff3d3bd63


# Getting Started

## Set Up Environment Variables:

```sh
touch .env
```

Add the following inside `.env` at the root:

```sh
LANGSMITH_API_KEY=lsv2_...
```

Next, create another `.env` file inside the `agent` folder:

```sh
cd agent
touch .env
```

Add the following inside `agent/.env`:

```sh
OPENAI_API_KEY=sk-...
LANGSMITH_API_KEY=lsv2_...
```

## Development

We recommend running the **frontend and agent separately** in different terminals to debug errors and logs:

```bash
# Terminal 1 - Frontend
pnpm run dev-frontend

# Terminal 2 - Agent
pnpm run dev-agent
```

Alternatively, you can run both services together with:

```bash
pnpm run dev
```

Then, open [http://localhost:3000](http://localhost:3000) in your browser.

## Architecture

The codebase is split into two main parts:

1. `/agent` **folder** – A LangGraph agent that connects to MCP servers and calls their tools.
2. `/app` **folder** – A frontend application using CopilotKit for UI and state synchronization.
', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"API Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository name 'open-mcp-client' and the description of the agent connecting to MCP servers indicate it implements or relates to Model Context Protocol (MCP) specifications. It also utilizes function calling via LangGraph.","categories":["Agent Frameworks","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The architecture is divided into two primary components: a LangGraph agent residing in the `/agent` directory and a frontend application located in the `/app` directory. This separation allows for independent development and debugging of the agent and the UI. The agent interacts with MCP servers, while the frontend leverages CopilotKit for UI management.","Implementation Details and Key Features":"The agent is built using LangGraph, suggesting a graph-based approach to managing agent workflows and interactions with tools offered by MCP servers. The frontend utilizes CopilotKit, implying features like state synchronization and UI components tailored for agent interaction. Environment variables manage API keys for OpenAI and LangSmith.","Integration Approaches and Protocols":"The repository integrates with MCP servers through a defined protocol, likely involving API calls to access tools and retrieve context. The LangGraph agent manages the interaction flow, potentially handling authentication, request formatting, and response parsing. The specific protocol details are not explicitly provided but are implied through server interaction.","Technical Stack and Dependencies":"The technical stack includes LangGraph for agent development, CopilotKit for the frontend, and potentially OpenAI for language model integration. Dependencies are managed using `pnpm`, as indicated by the `pnpm run dev` commands. The agent likely relies on standard HTTP libraries for communication with MCP servers. Environment variables store keys.","Best Practices and Coding Standards":"The project promotes separation of concerns by dividing the codebase into distinct agent and frontend components. Using environment variables for sensitive information like API keys is a good practice. The provided development commands encourage separate execution for easier debugging. No code samples are available for assessment.","Performance Considerations":"Performance considerations are not explicitly addressed in the provided documentation. However, the use of LangGraph might enable optimization of agent workflows, potentially leading to faster response times. The performance of the agent is dependent on the MCP server's response time and the efficiency of the LangGraph implementation."}}', '2025-03-10 21:26:16+00', 'true', 'Clients', 'open-mcp-client'), ('945161703', 'mcp-on-vercel', 'vercel-labs/mcp-on-vercel', 'https://github.com/vercel-labs/mcp-on-vercel', null, '2025-03-08 19:52:36+00', '2025-03-10 18:26:07+00', 'TypeScript', '73', '# Run an MCP Server on Vercel

## Usage

Update `api/server.ts` with your tools, prompts, and resources following the [MCP TypeScript SDK documentation](https://github.com/modelcontextprotocol/typescript-sdk/tree/main?tab=readme-ov-file#server).

## Notes for running on Vercel

- Requires a Redis attached to the project under process.env.REDIS_URL
- Make sure you have [Fluid compute](https://vercel.com/docs/functions/fluid-compute) enabled for efficient execution
- After enabling Fluid compute, open `vercel.json` and adjust max duration to 800 if you using a Vercel Pro or Enterprise account
- [Deploy the MCP template](https://vercel.com/templates/other/model-context-protocol-mcp-with-vercel-functions)

## Sample Client

`script/test-client.mjs` contains a sample client to try invocations.

```sh
node scripts/test-client.mjs https://mcp-on-vercel.vercel.app
```
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Serverless Computing\"]}', '{"is_mcp":"YES","justification":"The repository description and documentation explicitly mention Model Context Protocol (MCP) and provide instructions on running an MCP server on Vercel, utilizing the MCP TypeScript SDK.","categories":["Protocol Implementations","API Frameworks","Serverless Computing"],"analysis":{"Architecture and Design Patterns":"The repository's architecture revolves around deploying an MCP server on Vercel, leveraging serverless functions. It utilizes the MCP TypeScript SDK to define the server's behavior, including tool integrations and prompt handling. The design emphasizes ease of deployment and integration with existing Vercel infrastructure, promoting a modular approach to context management.","Implementation Details and Key Features":"The core implementation resides in `api/server.ts`, where developers define their tools, prompts, and resources following the MCP TypeScript SDK. A key feature is the integration with Redis for state management and data persistence. The repository also leverages Vercel's fluid compute for efficient execution, allowing for longer function durations.","Integration Approaches and Protocols":"The repository primarily integrates with the MCP protocol through the MCP TypeScript SDK. It utilizes HTTP endpoints for client interactions. The `test-client.mjs` script demonstrates how to interact with the deployed MCP server, showcasing the integration approach and request-response patterns.","Technical Stack and Dependencies":"The technical stack includes Node.js for server-side logic, TypeScript for type safety, the MCP TypeScript SDK for MCP integration, and Redis as a data store. Vercel functions provide the serverless execution environment. Dependencies are managed via `package.json` and installed using npm or yarn.","Best Practices and Coding Standards":"The repository promotes using the MCP TypeScript SDK, which likely enforces certain coding standards and best practices for MCP implementation. The use of TypeScript encourages type safety and maintainability. The inclusion of a test client demonstrates a commitment to testing and validation.","Performance Considerations":"Performance is addressed through Vercel's fluid compute, allowing for longer function execution times. Redis integration provides efficient data access and caching. Optimizations are needed within the `api/server.ts` to avoid long running tasks within the serverless function and optimize context window handling."}}', '2025-03-10 21:28:31+00', 'true', 'Servers', 'mcp-vercel-server'), ('945174897', 'mcp-command-proxy', 'Hormold/mcp-command-proxy', 'https://github.com/Hormold/mcp-command-proxy', null, '2025-03-08 20:37:04+00', '2025-03-10 17:49:59+00', 'TypeScript', '5', '# MCP Command Proxy

An MCP (Model Context Protocol) server that acts as a proxy for CLI commands, specifically designed for Expo development but adaptable for any command-line application.

## How to use in Cursor (Expo example)

1. Go to the directory of your Expo project
2. Run `npx mcp-command-proxy --prefix "ExpoServer" --command "expo start" --port 8383`
3. Go to Cursor settings -> MCP -> +Add new MCP server, like this:  
![add_mcp_server](guide/add_mcp.png)
4. Set the name to "ExpoServer", Type to "SSE", URL to `http://localhost:8383/sse`
5. Click "Save" and you should now be able to use the MCP server in Cursor. Like this:
![mcp_server_in_cursor](guide/result.png)

Recommended to use the `--port 8383` flag to avoid conflicts with other servers.
Also, you can add following instruction to .cursorrules file:
```
You can use MCP getRecentLogs tool to get the most recent logs from Expo server. And if needed, you can send key presses to the running process using sendKeyPress tool.
```


## Features

- **Command Proxying**: Run any CLI command through the MCP server
- **Log Collection**: Capture and store logs from running processes (configurable buffer size)
- **Key Press Forwarding**: Forward key presses from client to the running process
- **Transparent Experience**: The end user sees the command output exactly as if they ran it directly
- **Interactive Commands**: Works with interactive CLI tools like Expo
- **MCP Integration**: Built using the MCP SDK for easy integration with Claude and other MCP-enabled AI assistants

## How It Works

1. The server starts a specified command in a pseudo-terminal (PTY)
2. All stdout/stderr output is:
   - Streamed to the client in real-time
   - Stored in a circular buffer (configurable size, default 300 lines)
3. Key presses from the client are forwarded to the running process
4. The server provides tools to:
   - View collected logs
   - Send key presses to the process
   - Get the current state of the process

## Use Cases

- **Expo Development**: Run `expo start` and interact with it while collecting logs
- **Build Processes**: Monitor build processes and analyze logs
- **Long-running Services**: Monitor services and keep recent log history
- **Remote Command Execution**: Execute and monitor commands from remote clients

## Requirements

- Node.js 18+ 
- TypeScript
- pnpm (recommended) or npm

## Installation

```bash
# Install dependencies
pnpm install

# Build the project
pnpm build

# Run directly
pnpm start -- --prefix "MyServer" --command "expo start"

# Or install globally
pnpm install -g
mcp-command-proxy --prefix "MyServer" --command "expo start"
```

## Usage

### Basic Usage

```bash
# Using the CLI
mcp-command-proxy --prefix "ExpoServer" --command "expo start"

# Or programmatically
import { createServer } from 'mcp-command-proxy';

const server = await createServer({
  prefix: 'ExpoServer',
  command: 'expo start',
  bufferSize: 500,
  port: 8080
});

// To stop the server later
server.stop();
```

### Options

- `--prefix, -p`: Name/prefix for the server (default: "CommandProxy")
- `--command, -c`: Command to run (required)
- `--buffer-size, -b`: Number of log lines to keep in memory (default: 300)
- `--port`: Port for HTTP server (default: 8080)
- `--help, -h`: Show help

### MCP Integration

This server implements the following MCP tools:

1. `getRecentLogs`: Returns the most recent logs from the buffer
   - Parameters: 
     - `limit` (optional): Number of logs to return (default: 100)
     - `types` (optional): Types of logs to include (stdout, stderr, system) (default: all)

2. `sendKeyPress`: Sends a key press to the running process
   - Parameters:
     - `key`: Key to send (e.g. "enter", "a", "1", "space")

3. `getProcessStatus`: Returns the current status of the process
   - Parameters: None

## Examples

### Running Expo Start

```bash
mcp-command-proxy -p "ExpoServer" -c "expo start" -b 500
```

### Using with Claude

1. Configure Claude to connect to this MCP server (SSE endpoint: http://localhost:8080/sse)
2. Ask Claude to run Expo or other commands
3. Claude can analyze logs and help troubleshoot issues

## Development

```bash
# Clone the repository
git clone https://github.com/hormold/mcp-command-proxy.git
cd mcp-command-proxy

# Install dependencies
pnpm install

# Build the project
pnpm build

# Run in development mode
pnpm dev
```

## License

MIT ', 'true', '"{\"Protocol Implementations\",\"Agent Frameworks\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements a Model Context Protocol (MCP) server. It provides tools like `getRecentLogs` and `sendKeyPress` to interact with a running process via an MCP-enabled client like Cursor.","categories":["Protocol Implementations","Developer Tooling","API Frameworks"],"analysis":{"Architecture and Design Patterns":"The repository implements a proxy server that wraps CLI commands, enabling interaction via MCP. It uses a pseudo-terminal (PTY) to run commands and captures stdout/stderr. A circular buffer stores log data, and an HTTP server exposes MCP endpoints for log retrieval and key press forwarding. The design emphasizes real-time streaming of command output and interactive command support.","Implementation Details and Key Features":"The core feature is command proxying, allowing remote interaction with CLI tools. It collects logs from running processes and forwards key presses from a client. Key features include a configurable log buffer, real-time output streaming, and MCP tool implementations for retrieving logs and sending key presses. TypeScript and Node.js are used for implementation.","Integration Approaches and Protocols":"The repository integrates with MCP-enabled clients like Cursor and Claude via Server-Sent Events (SSE). It exposes endpoints for log retrieval (`getRecentLogs`) and key press injection (`sendKeyPress`). The server acts as an intermediary, translating MCP requests into actions on the proxied command-line application. Integration requires configuring the MCP client with the server's SSE endpoint.","Technical Stack and Dependencies":"The project relies on Node.js (version 18+), TypeScript, and potentially pnpm or npm for package management. It likely uses libraries for creating pseudo-terminals (PTYs), handling HTTP requests (e.g., Express), and implementing the MCP protocol. The use of TypeScript suggests a focus on type safety and maintainability.","Best Practices and Coding Standards":"The project uses TypeScript, indicating adherence to type-safe coding practices. The documentation includes installation instructions, usage examples, and API details, promoting usability. The modular design, with clear separation of concerns between command execution, log management, and MCP endpoint handling, reflects good software engineering principles. The use of a circular buffer is efficient.","Performance Considerations":"The circular buffer size is configurable to manage memory usage. Streaming command output in real-time minimizes latency. The choice of Node.js provides non-blocking I/O, which enhances concurrency and responsiveness. Performance can be affected by network latency between the client and server, and the overhead of the PTY."}}', '2025-03-10 21:28:34+00', 'true', 'Servers', 'mcp-command-proxy'), ('945181573', 'firebase-mcp', 'gannonh/firebase-mcp', 'https://github.com/gannonh/firebase-mcp', 'Model Context Protocol (MCP) server to interact with Firebase services.', '2025-03-08 21:00:09+00', '2025-03-10 15:37:38+00', 'TypeScript', '5', '# Firebase MCP Server

![Project Logo](./assets/logo.png)

## Overview

The [Model Context Protocol (MCP)](https://github.com/modelcontextprotocol) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. This MCP server provides a standardized interface to interact with Firebase services including:

- **Authentication**: User management and verification
- **Firestore**: Document database operations
- **Storage**: File storage and retrieval

The server exposes Firebase services through MCP tools, making them accessible to LLM clients including [Claude Desktop](https://claude.ai/download), [Cursor](https://www.cursor.com/), [Roo Code](https://github.com/RooVetGit/Roo-Code), and [Cline](https://cline.bot/), while handling authentication and connection management.

## Setup

> The easiest way to install the Firebase MCP server is to simply feed your LLM client (like Cline) the [llms-install.md](./llms-install.md) file.

### 1. Firebase Configuration

- Go to [Firebase Console](https://console.firebase.google.com)
- Navigate to Project Settings > Service Accounts
- Click "Generate new private key"
- Save the JSON file securely

### 2. Environment Variables

The server requires the following environment variables:

- `SERVICE_ACCOUNT_KEY_PATH`: Path to your Firebase service account key JSON file (required)
- `FIREBASE_STORAGE_BUCKET`: Bucket name for Firebase Storage (optional)
  - If not provided, defaults to `[projectId].appspot.com`

### 3. Install MCP Server

Add the server configuration to your MCP settings file:

- Claude Desktop: `~/Library/Application Support/Claude/claude_desktop_config.json`
- Cursor: `[project root]/.cursor/mcp.json`
- Roo Code (VS Code Extension): (`~/Library/Application Support/Code/User/globalStorage/rooveterinaryinc.roo-cline/settings/cline_mcp_settings.json`)
- Cline (VS Code Extension): `~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`

MCP Servers can be installed manually or at runtime via npx (recommended). How you install determines your configuration:

#### Configure for npx

   ```json
   {
     "firebase-mcp": {
       "command": "npx",
       "args": [
         "-y",
         "@gannonh/firebase-mcp"
       ],
       "env": {
         "SERVICE_ACCOUNT_KEY_PATH": "/absolute/path/to/serviceAccountKey.json",
         "FIREBASE_STORAGE_BUCKET": "your-project-id.firebasestorage.app"
       }
     }
   }
   ```

#### Configure for local installation

   ```json
   {
     "firebase-mcp": {
       "command": "node",
       "args": [
         "/absolute/path/to/firebase-mcp/dist/index.js"
       ],
       "env": {
         "SERVICE_ACCOUNT_KEY_PATH": "/absolute/path/to/serviceAccountKey.json",
         "FIREBASE_STORAGE_BUCKET": "your-project-id.firebasestorage.app"
       }
     }
   }
   ```

#### Manual Installation

##### Install Dependencies

   ```bash
   git clone https://github.com/gannonh/firebase-mcp
   cd firebase-mcp
   npm install
   ```
   
##### Build the Project

   ```bash
   npm run build
   ```

### Test your Installation

To make sure everything is working, simply prompt your client: `Please run through and test all of your Firebase MCP tools.`

## Features

### Authentication Tools

- `auth_get_user`: Get user details by ID or email

  ```typescript
  {
    identifier: string // User ID or email address
  }
  ```

### Firestore Tools

- `firestore_add_document`: Add a document to a collection

  ```typescript
  {
    collection: string,
    data: object
  }
  ```

- `firestore_list_collections`: List available collections

  ```typescript
  {
    documentPath?: string, // Optional parent document path
    limit?: number,        // Default: 20
    pageToken?: string     // For pagination
  }
  ```

- `firestore_list_documents`: List documents with optional filtering

  ```typescript
  {
    collection: string,
    filters?: Array<{
      field: string,
      operator: string,
      value: any
    }>,
    limit?: number,
    pageToken?: string
  }
  ```

- `firestore_get_document`: Get a specific document

  ```typescript
  {
    collection: string,
    id: string
  }
  ```

- `firestore_update_document`: Update an existing document

  ```typescript
  {
    collection: string,
    id: string,
    data: object
  }
  ```

- `firestore_delete_document`: Delete a document

  ```typescript
  {
    collection: string,
    id: string
  }
  ```

### Storage Tools

- `storage_list_files`: List files in a directory

  ```typescript
  {
    directoryPath?: string, // Optional path, defaults to root
    pageSize?: number,      // Number of items per page, defaults to 10
    pageToken?: string      // Token for pagination
  }
  ```

- `storage_get_file_info`: Get file metadata and download URL

  ```typescript
  {
    filePath: string // Path to the file in storage
  }
  ```

## Development

### Building

```bash
npm run build
```

### Testing

The project uses Jest for testing. Tests can be run against Firebase emulators to avoid affecting production data.

1. **Install Firebase Emulators**

   ```bash
   npm install -g firebase-tools
   firebase init emulators
   ```

2. **Start Emulators**

   ```bash
   firebase emulators:start
   ```

3. **Run Tests**

   ```bash
   npm run test:emulator
   ```

### Architecture

The server is structured into three main components:

```
src/
├── index.ts              # Server entry point
└── lib/
    └── firebase/
        ├── authClient.ts       # Authentication operations
        ├── firebaseConfig.ts   # Firebase configuration
        ├── firestoreClient.ts  # Firestore operations
        └── storageClient.ts    # Storage operations
```

Each client module implements specific Firebase service operations and exposes them as MCP tools.

## Contributing

1. Fork the repository
2. Create a feature branch
3. Implement changes with tests
4. Submit a pull request

## License

MIT License - see [LICENSE](LICENSE) file for details

## Related Resources

- [Model Context Protocol](https://github.com/modelcontextprotocol)
- [Firebase Documentation](https://firebase.google.com/docs)
- [Firebase Admin SDK](https://firebase.google.com/docs/admin/setup)

## Troubleshooting

### Common Issues

#### "The specified bucket does not exist" Error

If you encounter this error when trying to access Firebase Storage:

1. Check that your Firebase project has Storage enabled
   - Go to the Firebase Console
   - Navigate to Storage
   - Complete the initial setup if you haven't already

2. Verify the correct bucket name
   - The default bucket name is usually `[projectId].appspot.com`
   - Some projects use `[projectId].firebasestorage.app` instead
   - You can find your bucket name in the Firebase Console under Storage

3. Set the `FIREBASE_STORAGE_BUCKET` environment variable
   - Add the correct bucket name to your MCP configuration
   - Example: `"FIREBASE_STORAGE_BUCKET": "your-project-id.firebasestorage.app"`

#### "Firebase is not initialized" Error

If you see this error:

1. Verify your service account key path
   - Make sure the path in `SERVICE_ACCOUNT_KEY_PATH` is correct and absolute
   - Check that the file exists and is readable

2. Check service account permissions
   - Ensure the service account has the necessary permissions for the Firebase services you're using
   - For Storage, the service account needs the Storage Admin role

#### JSON Parsing Errors

If you see errors about invalid JSON:

1. Make sure there are no `console.log` statements in the code
   - All logging should use `console.error` to avoid interfering with the JSON communication
   - The MCP protocol uses stdout for JSON communication

2. Check for syntax errors in your requests
   - Verify that all parameters are correctly formatted
   - Check for typos in field names
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states its purpose is to implement a Model Context Protocol (MCP) server to interface with Firebase services, which aligns directly with the MCP specifications.","categories":["Protocol Implementations","AI Model Integration","Function Calling"],"analysis":{"Architecture and Design Patterns":"The server architecture is divided into modules for Firebase Authentication, Firestore, and Storage. Each module exposes specific Firebase service operations as MCP tools, enabling LLMs to interact with these services. The structure promotes modularity and separation of concerns, facilitating maintainability and scalability. The server uses environment variables for configuration.","Implementation Details and Key Features":"The server implements several MCP tools, including `auth_get_user`, `firestore_*` operations (add, list, get, update, delete documents), and `storage_*` operations (list files, get file info). These tools enable LLMs to interact with Firebase services. The server uses TypeScript and Node.js. The project uses Jest for testing.","Integration Approaches and Protocols":"The server integrates with LLM clients (Claude, Cursor, Roo Code, Cline) by exposing Firebase services as MCP tools. The integration requires configuring the LLM client with the server details. The server uses JSON for communication with LLM clients via standard output and uses environment variables for secure configuration, such as the path to the Firebase service account key.","Technical Stack and Dependencies":"The server uses Node.js and TypeScript. It depends on the Firebase Admin SDK for interacting with Firebase services. The `firebase-tools` package is also used for local testing and emulators. Dependencies are managed using npm. The server relies on environment variables for configuring the Firebase service account and storage bucket.","Best Practices and Coding Standards":"The repository includes a contributing guide and emphasizes testing. The server uses environment variables to manage configurations and sensitive information like the Firebase service account key. The documentation advises using `console.error` instead of `console.log` to avoid interfering with JSON communication in the MCP protocol, which ensures correct data parsing.","Performance Considerations":"The performance of the server depends on the Firebase services and the efficiency of the queries. Pagination is implemented for listing documents and files to limit the amount of data retrieved at once. The server relies on Firebase's infrastructure for scaling and handling requests. The testing guide recommends using Firebase emulators to avoid impacting production data."}}', '2025-03-10 21:28:38+00', 'true', 'Servers', 'firebase-mcp-server'), ('945197439', 'localutils-mcp-server', 'arjshiv/localutils-mcp-server', 'https://github.com/arjshiv/localutils-mcp-server', null, '2025-03-08 21:58:46+00', '2025-03-09 23:17:12+00', 'JavaScript', '2', '# Local Utilities MCP Server

A Model Context Protocol (MCP) server that provides access to various local system utilities. This server can be used with Cursor and other MCP-compatible clients to provide quick access to system information.

## Features

The server provides the following utilities:

- **Time and Date**: Get the current local time and date in various formats
- **Hostname**: Get the system's hostname
- **Public IP**: Get the machine's public IP address
- **Directory Listing**: List the contents of a specified directory
- **Node.js Version**: Get the currently running Node.js version
- **Port Checker**: Check what process is running on a specific port

## Installation

### Global Installation

```bash
pnpm add -g localutils-mcp-server
```

### Using with npx

You can also run the server directly using npx without installing it globally:

```bash
npx localutils-mcp-server
```

## Usage

### Starting the Server

If installed globally:

```bash
localutils-mcp
```

With npx:

```bash
npx localutils-mcp-server
```

### Using with Cursor

The server can be used with Cursor by configuring it as an MCP server in Cursor's settings.

1. Open Cursor settings
2. Navigate to the MCP section
3. Add a new MCP server with the following configuration:
   ```json
   {
     "name": "localutils",
     "command": "npx",
     "args": ["localutils-mcp-server"]
   }
   ```

### Using the MCP Inspector

You can test the server using the MCP Inspector:

```bash
pnpm run inspector
```

This will start the MCP Inspector at http://localhost:5173.

## Available Tools

### `get_time_and_date`

Returns the current local time and date in various formats.

### `get_hostname`

Returns the hostname of the machine running the MCP server.

### `get_public_ip`

Returns the public IP address of the machine running the MCP server.

### `list_directory`

Lists the contents of a specified directory.

**Parameters:**
- `path` (string, required): Directory path to list

### `get_node_version`

Returns the Node.js version information of the environment running the MCP server.

### `check_port`

Checks what process is running on a specific port.

**Parameters:**
- `port` (number or string, required): Port number to check (1-65535). String values will be automatically converted to numbers.

**Example Response (macOS/Linux):**
```json
{
  "processes": [
    {
      "command": "node",
      "pid": "12345",
      "user": "username",
      "fd": "12u",
      "type": "IPv4",
      "device": "0x1234567890",
      "size": "0t0",
      "node": "TCP",
      "name": "*:3000 (LISTEN)"
    }
  ],
  "message": "Found 1 process(es) using port 3000"
}
```

## Development

### Building

```bash
pnpm run build
```

### Running in Development Mode

```bash
pnpm run dev
```

### Testing

```bash
pnpm test
```

### Git Workflow

This repository includes a pre-commit hook that automatically builds the server before each commit. This ensures that the build files are always up-to-date in the repository.

The build folder is included in the git repository to make it easier to use the package with npx without having to build it first.

To set up the pre-commit hook after cloning the repository:

```bash
pnpm install
```

This will install dependencies and set up the pre-commit hook via Husky.

## License

MIT ', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements a Model Context Protocol (MCP) server, offering system utilities access via MCP-compatible clients like Cursor. It advertises itself as MCP server.","categories":["Protocol Implementations","Developer Tooling","System Automation"],"analysis":{"Architecture and Design Patterns":"The repository implements a server that exposes local system utilities following the Model Context Protocol (MCP). It uses a command-line interface to start the server and offers multiple utilities. The design is modular, with each utility implemented as a separate function. The project leverages npm for package management and includes build and testing scripts.","Implementation Details and Key Features":"Key features include providing access to time, hostname, public IP, directory listing, Node.js version, and port checking utilities. The server is implemented using Node.js, offering functions like `get_time_and_date`, `get_hostname`, `get_public_ip`, `list_directory`, `get_node_version`, and `check_port`. It supports both global installation and direct execution via `npx`.","Integration Approaches and Protocols":"The server is designed to integrate with MCP-compatible clients like Cursor. Integration involves configuring the client with the server's command and arguments. The server communicates through the Model Context Protocol, allowing clients to call its utilities. The `MCP Inspector` allows manual testing of the server by calling the available tools.","Technical Stack and Dependencies":"The technical stack primarily consists of Node.js and npm (or pnpm). Key dependencies include the libraries needed for building the utilities and handling MCP communication. The project uses `pnpm` for package management and includes a pre-commit hook using Husky. The build process utilizes Node.js and related tools.","Best Practices and Coding Standards":"The repository includes a pre-commit hook to ensure the server is built before each commit. The documentation is clear and concise. The project includes testing to validate the functionality of the utilities. The inclusion of a license file (MIT) indicates attention to open-source best practices.","Performance Considerations":"The server is designed to provide quick access to system information, suggesting a focus on low latency. The project uses Node.js, known for its non-blocking I/O, which can improve performance. Performance may depend on the underlying system utilities being accessed, but the Node.js part should not be a bottleneck."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'localutils-mcp-server'), ('945213178', 'mcp-node-fetch', 'mcollina/mcp-node-fetch', 'https://github.com/mcollina/mcp-node-fetch', null, '2025-03-08 23:03:13+00', '2025-03-10 10:03:52+00', 'TypeScript', '8', '# MCP Node Fetch

An MCP server that enables fetching web content using the Node.js [undici](https://github.com/nodejs/undici) library.

## Features

- Fetch content from any URL using various HTTP methods
- Support for headers and request body
- Return content in various formats (text, JSON, binary, HTML fragments)
- Handle errors gracefully
- Configure timeout and redirect behavior

## MCP Tools

This server provides the following MCP tools:

### `fetch-url`

Fetches content from a URL and returns it.

Parameters:
- `url` (string, required): The URL to fetch
- `method` (string, optional): HTTP method (default: "GET")
- `headers` (object, optional): HTTP headers to include
- `body` (string, optional): Request body for POST/PUT requests
- `timeout` (number, optional): Request timeout in milliseconds
- `responseType` (string, optional): How to parse the response ("text", "json", "binary", "html-fragment")
- `fragmentSelector` (string, optional): CSS selector to extract specific HTML fragments (when responseType is "html-fragment")
- `followRedirects` (boolean, optional): Whether to follow redirects (default: true)

### `extract-html-fragment`

Extracts specific HTML content from a webpage using CSS selectors and optionally navigates to anchor points.

Parameters:
- `url` (string, required): The URL to fetch
- `selector` (string, required): CSS selector for the HTML fragment to extract
- `anchorId` (string, optional): Optional anchor ID to locate a specific fragment
- `method` (string, optional): HTTP method (default: "GET")
- `headers` (object, optional): HTTP headers to include
- `body` (string, optional): Request body for POST requests
- `timeout` (number, optional): Request timeout in milliseconds
- `followRedirects` (boolean, optional): Whether to follow redirects (default: true)

### `check-status`

Checks if a URL is accessible without downloading the full content.

Parameters:
- `url` (string, required): The URL to check
- `timeout` (number, optional): Request timeout in milliseconds


## Claude for Desktop Configuration

To use with Claude for Desktop, add this to your `claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "node-fetch": {
      "command": "node",
      "args": ["dist/index.js"]
    }
  }
}
```

## License

MIT
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it is an MCP server providing tools like `fetch-url`, `extract-html-fragment`, and `check-status`. It includes a configuration example for Claude for Desktop's `mcpServers` setting.","categories":["Protocol Implementations","API Frameworks","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The repository implements a server that exposes web fetching functionalities as MCP tools. It utilizes Node.js and the `undici` library for making HTTP requests. The design follows a modular approach, with distinct tools for fetching URLs, extracting HTML fragments, and checking URL status, each accessible via the MCP protocol.","Implementation Details and Key Features":"The core feature is the `fetch-url` tool, allowing retrieval of web content with configurable HTTP methods, headers, and response types. The `extract-html-fragment` tool enables extracting specific HTML sections using CSS selectors. Error handling and configuration options for timeouts and redirects are also key aspects.","Integration Approaches and Protocols":"The repository is designed for seamless integration into environments supporting MCP. It provides clear instructions for integrating with Claude for Desktop. It utilizes JSON for configuration and data exchange within the MCP framework. The server is accessible via standard Node.js mechanisms.","Technical Stack and Dependencies":"The project's technical stack consists primarily of Node.js. It leverages the `undici` library for making HTTP requests, offering a performant and reliable fetching mechanism. The code is likely transpiled to run on various Node.js versions using a build process (indicated by the 'dist/index.js' entry point).","Best Practices and Coding Standards":"The use of `undici` suggests adherence to Node.js best practices for HTTP requests. The documented configuration options demonstrate an awareness of security considerations like request timeouts and redirect handling. The MIT license promotes open-source collaboration and responsible usage.","Performance Considerations":"The `undici` library is known for its performance and efficiency in handling HTTP requests. The server allows configuring request timeouts to prevent indefinite delays. The `check-status` tool offers a lightweight alternative for quickly checking URL accessibility without downloading the entire content, optimizing performance."}}', '2025-03-10 21:28:38+00', 'true', 'Servers', 'mcp-node-fetch'), ('945224573', 'MCPBeeAgentProject', 'deghislain/MCPBeeAgentProject', 'https://github.com/deghislain/MCPBeeAgentProject', null, '2025-03-08 23:53:22+00', '2025-03-10 16:35:57+00', 'Python', '0', '# MCP Stock Application

This project demonstrates how the MCP protocol can be used to facilitate tool calling for AI agents. It consists of the following components:

- **Server:** `MCP_stock_app_server`
- **Client:** `MCP_stock_app_client`
- **Tools Module:** `stock_tools`
- **User Interface:** Built with Streamlit

### Running the Application

To start the application, use the following command:

```sh
poetry run streamlit run MCP_stock_app_client.py 
from the following path MCPStockPythonProject/mcpstockpythonproject ', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"Protocol Implementations\"]}', '{"is_mcp":"YES","justification":"The repository implements an application showcasing the Model Context Protocol (MCP) for AI agent tool calling, with client-server architecture and a stock tool module.","categories":["Agent Frameworks","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The architecture follows a client-server model. The client interacts with the server, which utilizes the MCP protocol to enable tool calling functionality. The application utilizes a dedicated 'stock_tools' module, suggesting a modular design approach for tool integration. Streamlit is used to build the User Interface, offering interactive data display.","Implementation Details and Key Features":"The key feature is demonstrating tool calling via MCP. The server component likely handles the execution of tools based on requests from the client. The 'stock_tools' module probably contains functions for retrieving stock data or performing related actions. The Streamlit UI provides an interface for users to interact with the application.","Integration Approaches and Protocols":"The repository integrates components using the MCP protocol for communication between the client and server, particularly for facilitating tool use. The Streamlit UI is integrated as a front-end for user interaction. The application likely leverages external APIs or data sources through the 'stock_tools' module to fetch stock information.","Technical Stack and Dependencies":"The repository utilizes Python as its primary language. The Streamlit library is used for building the user interface. The presence of a `poetry.lock` file suggests the use of Poetry for dependency management. The MCP protocol itself is a core dependency, though its specific implementation details are not fully detailed.","Best Practices and Coding Standards":"The provided documentation lacks detailed insights into coding standards. The use of Poetry for dependency management indicates an effort to maintain a reproducible environment. The modular design with a separate 'stock_tools' module promotes code organization and reusability. Further analysis of the source code is required for deeper assessment.","Performance Considerations":"Performance considerations are not explicitly addressed in the provided description. The client-server architecture may introduce network latency. The performance of the tool calling mechanism depends on the efficiency of the 'stock_tools' module and the underlying data sources. Optimizing the UI and backend data processing can be crucial for overall application performance."}}', '2025-03-10 21:26:21+00', 'true', 'Servers', 'mcp-bee-agent'), ('945235930', 'mcp4go', 'mcp4go/mcp4go', 'https://github.com/mcp4go/mcp4go', 'A comprehensive Go SDK for the Model Context Protocol (MCP) - simplifying AI application development by abstracting away protocol complexities.', '2025-03-09 00:46:04+00', '2025-03-10 17:01:49+00', 'Go', '3', '<p align="center">
<a href="https://github.com/mcp4go/mcp4go/actions"><img src="https://github.com/mcp4go/mcp4go/workflows/Go/badge.svg?v=1231" alt="Build Status"></a>
<a href="https://pkg.go.dev/github.com/mcp4go/mcp4go"><img src="https://pkg.go.dev/badge/github.com/mcp4go/mcp4go?v=1231" alt="GoDoc"></a>
<a href="https://codecov.io/gh/mcp4go/mcp4go"><img src="https://codecov.io/gh/mcp4go/mcp4go/master/graph/badge.svg?v=1231" alt="codeCov"></a>
<a href="https://goreportcard.com/report/github.com/mcp4go/mcp4go"><img src="https://goreportcard.com/badge/github.com/mcp4go/mcp4go?v=1231" alt="Go Report Card"></a>
<a href="https://github.com/mcp4go/mcp4go/blob/main/LICENSE"><img src="https://img.shields.io/github/license/mcp4go/mcp4go?v=1231" alt="License"></a>
<a href="https://github.com/avelino/awesome-go"><img src="https://awesome.re/mentioned-badge.svg?v=1231" alt="Awesome Go"></a>
</p>

# MCP4Go

MCP4Go is a Go implementation of the Model Context Protocol (MCP), designed to simplify the development of AI applications by abstracting away protocol complexities.

## Features

- Complete MCP protocol implementation in pure Go
- High-level abstractions for common MCP resources
- Pluggable architecture for custom extensions
- Comprehensive documentation and examples
- Production-ready with robust error handling

## Installation

MCP4Go requires Go 1.18 or later. Install it using Go modules:

```bash
go get github.com/mcp4go/mcp4go
```


## Protocol Implementation
MCP4Go provides a complete implementation of the Model Context Protocol with support for:
- JSON-RPC communication
- Resource lifecycle management
- Prompt engineering
- Tool definitions and invocations
- Sampling parameters
- Logging and diagnostics

## License
This project is licensed under the MIT License
## Contributing
Contributions are welcome! Please see our Contributing Guide for more information.
', 'true', '"{\"Protocol Implementations\",\"AI Model Integration\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it's a Go SDK for the Model Context Protocol (MCP), aiming to simplify AI application development by abstracting protocol complexities.","categories":["Protocol Implementations","AI Model Integration","Function Calling"],"analysis":{"Architecture and Design Patterns":"The repository implements the Model Context Protocol (MCP) using a Go SDK. It features a pluggable architecture designed to accommodate custom extensions, suggesting a modular and extensible design. The architecture abstracts the complexities of the underlying protocol, providing a higher-level interface for developers to interact with MCP functionalities.","Implementation Details and Key Features":"MCP4Go provides a comprehensive implementation of the Model Context Protocol, including JSON-RPC communication and resource lifecycle management. Key features include support for prompt engineering, tool definitions and invocations, sampling parameters, and logging and diagnostics. The implementation uses Go modules for dependency management and offers robust error handling.","Integration Approaches and Protocols":"The repository focuses on integrating AI models through the Model Context Protocol (MCP). It utilizes JSON-RPC for communication between components. The SDK provides abstractions for defining and invoking tools, managing resources, and configuring sampling parameters, streamlining the integration process for AI applications.","Technical Stack and Dependencies":"The primary technical stack consists of Go (version 1.18 or later). It relies on Go modules for managing dependencies. The project utilizes libraries for JSON-RPC communication. The presence of build status badges, code coverage, and Go Report Card suggests a focus on code quality and maintainability.","Best Practices and Coding Standards":"The repository includes a contributing guide, indicating an emphasis on community contributions and standardized development practices. The use of linters and code formatting tools (as suggested by the Go Report Card) helps ensure code consistency and adherence to best practices. Comprehensive documentation is also provided.","Performance Considerations":"The performance considerations are not explicitly stated but can be inferred. The use of Go suggests an emphasis on concurrency and efficient resource utilization. Production-ready error handling and comprehensive logging and diagnostics can contribute to the performance and stability of applications using the SDK. "}}', '2025-03-10 21:26:16+00', 'true', 'Clients', 'mcp-go-sdk'), ('945242713', 'cryo-mcp', 'z80dev/cryo-mcp', 'https://github.com/z80dev/cryo-mcp', 'A Python package for accessing Cryo datasets via Claude Code', '2025-03-09 01:17:35+00', '2025-03-10 17:24:04+00', 'Python', '83', '# Cryo MCP 🧊

A Model Completion Protocol (MCP) server for the [Cryo](https://github.com/paradigmxyz/cryo) blockchain data extraction tool. 

Cryo MCP allows you to access Cryo's powerful blockchain data extraction capabilities via an API server that implements the MCP protocol, making it easy to query blockchain data from any MCP-compatible client.

## For LLM Users: SQL Query Workflow Guide

When using this MCP server to run SQL queries on blockchain data, follow this workflow:

1. **Download data** with `query_dataset`:
   ```python
   result = query_dataset(
       dataset="blocks",  # or "transactions", "logs", etc.
       blocks="15000000:15001000",  # or use blocks_from_latest=100
       output_format="parquet"  # important: use parquet for SQL
   )
   files = result.get("files", [])  # Get the returned file paths
   ```

2. **Explore schema** with `get_sql_table_schema`:
   ```python
   # Check what columns are available in the file
   schema = get_sql_table_schema(files[0])
   # Now you can see all columns, data types, and sample data
   ```

3. **Run SQL** with `query_sql`:
   ```python
   # Option 1: Simple table reference (DuckDB will match the table name to file)
   sql_result = query_sql(
       query="SELECT block_number, timestamp, gas_used FROM blocks",
       files=files  # Pass the files from step 1
   )
   
   # Option 2: Using read_parquet() with explicit file path
   sql_result = query_sql(
       query=f"SELECT block_number, timestamp, gas_used FROM read_parquet('{files[0]}')",
       files=files  # Pass the files from step 1
   )
   ```

Alternatively, use the combined approach with `query_blockchain_sql`:
```python
# Option 1: Simple table reference
result = query_blockchain_sql(
    sql_query="SELECT * FROM blocks",
    dataset="blocks",
    blocks_from_latest=100
)

# Option 2: Using read_parquet()
result = query_blockchain_sql(
    sql_query="SELECT * FROM read_parquet('/path/to/file.parquet')",  # Path doesn't matter
    dataset="blocks",
    blocks_from_latest=100
)
```

For a complete working example, see [examples/sql_workflow_example.py](examples/sql_workflow_example.py).

## Features

- **Full Cryo Dataset Access**: Query any Cryo dataset through an API server
- **MCP Integration**: Works seamlessly with MCP clients
- **Flexible Query Options**: Support for all major Cryo filtering and output options
- **Block Range Options**: Query specific blocks, latest block, or relative ranges
- **Contract Filtering**: Filter data by contract address 
- **Latest Block Access**: Easy access to the latest Ethereum block data
- **Multiple Output Formats**: JSON, CSV, and Parquet support
- **Schema Information**: Get detailed dataset schemas and sample data
- **SQL Queries**: Run SQL queries directly against downloaded blockchain data

## Installation (Optional)

This is not required if you will run the tool with `uvx` directly.

```bash
# install with UV (recommended)
uv tool install cryo-mcp
```

## Requirements

- Python 3.8+
- uv
- A working installation of [Cryo](https://github.com/paradigmxyz/cryo)
- Access to an Ethereum RPC endpoint
- DuckDB (for SQL query functionality)

## Quick Start

### Usage with Claude Code

1. Run `claude mcp add` for an interactive prompt.
2. Enter `uvx` as the command to run.
3. Enter `cryo-mcp --rpc-url <ETH_RPC_URL> [--data-dir <DATA_DIR>]` as the args
4. Alternatively, provide `ETH_RPC_URL` and `CRYO_DATA_DIR` as environment variables instead.

New instances of `claude` will now have access to cryo as configured to hit your RPC endpoint and store data in the specified directory.

## Available Tools

Cryo MCP exposes the following MCP tools:

### `list_datasets()`

Returns a list of all available Cryo datasets.

Example:
```python
client.list_datasets()
```

### `query_dataset()`

Query a Cryo dataset with various filtering options.

Parameters:
- `dataset` (str): The name of the dataset to query (e.g., 'blocks', 'transactions', 'logs')
- `blocks` (str, optional): Block range specification (e.g., '1000:1010')
- `start_block` (int, optional): Start block number (alternative to blocks)
- `end_block` (int, optional): End block number (alternative to blocks)
- `use_latest` (bool, optional): If True, query the latest block
- `blocks_from_latest` (int, optional): Number of blocks from latest to include
- `contract` (str, optional): Contract address to filter by
- `output_format` (str, optional): Output format ('json', 'csv', 'parquet')
- `include_columns` (list, optional): Columns to include alongside defaults
- `exclude_columns` (list, optional): Columns to exclude from defaults

Example:
```python
# Get transactions from blocks 15M to 15.01M
client.query_dataset('transactions', blocks='15M:15.01M')

# Get logs for a specific contract from the latest 100 blocks
client.query_dataset('logs', blocks_from_latest=100, contract='0x1234...')

# Get just the latest block
client.query_dataset('blocks', use_latest=True)
```

### `lookup_dataset()`

Get detailed information about a specific dataset, including schema and sample data.

Parameters:
- `name` (str): The name of the dataset to look up
- `sample_start_block` (int, optional): Start block for sample data
- `sample_end_block` (int, optional): End block for sample data
- `use_latest_sample` (bool, optional): Use latest block for sample
- `sample_blocks_from_latest` (int, optional): Number of blocks from latest for sample

Example:
```python
client.lookup_dataset('logs')
```

### `get_latest_ethereum_block()`

Returns information about the latest Ethereum block.

Example:
```python
client.get_latest_ethereum_block()
```

### SQL Query Tools

Cryo MCP includes several tools for running SQL queries against blockchain data:

### `query_sql()`

Run a SQL query against downloaded blockchain data.

Parameters:
- `query` (str): SQL query to execute
- `files` (list, optional): List of parquet file paths to query. If None, will use all files in the data directory.
- `include_schema` (bool, optional): Whether to include schema information in the result

Example:
```python
# Run against all available files
client.query_sql("SELECT * FROM read_parquet('/path/to/blocks.parquet') LIMIT 10")

# Run against specific files
client.query_sql(
    "SELECT * FROM read_parquet('/path/to/blocks.parquet') LIMIT 10",
    files=['/path/to/blocks.parquet']
)
```

### `query_blockchain_sql()`

Query blockchain data using SQL, automatically downloading any required data.

Parameters:
- `sql_query` (str): SQL query to execute
- `dataset` (str, optional): The dataset to query (e.g., 'blocks', 'transactions')
- `blocks` (str, optional): Block range specification
- `start_block` (int, optional): Start block number
- `end_block` (int, optional): End block number
- `use_latest` (bool, optional): If True, query the latest block
- `blocks_from_latest` (int, optional): Number of blocks before the latest to include
- `contract` (str, optional): Contract address to filter by
- `force_refresh` (bool, optional): Force download of new data even if it exists
- `include_schema` (bool, optional): Include schema information in the result

Example:
```python
# Automatically downloads blocks data if needed, then runs the SQL query
client.query_blockchain_sql(
    sql_query="SELECT block_number, gas_used, timestamp FROM blocks ORDER BY gas_used DESC LIMIT 10",
    dataset="blocks",
    blocks_from_latest=100
)
```

### `list_available_sql_tables()`

List all available tables that can be queried with SQL.

Example:
```python
client.list_available_sql_tables()
```

### `get_sql_table_schema()`

Get the schema for a specific parquet file.

Parameters:
- `file_path` (str): Path to the parquet file

Example:
```python
client.get_sql_table_schema("/path/to/blocks.parquet")
```

### `get_sql_examples()`

Get example SQL queries for different blockchain datasets.

Example:
```python
client.get_sql_examples()
```

## Configuration Options

When starting the Cryo MCP server, you can use these command-line options:

- `--rpc-url URL`: Ethereum RPC URL (overrides ETH_RPC_URL environment variable)
- `--data-dir PATH`: Directory to store downloaded data (overrides CRYO_DATA_DIR environment variable, defaults to ~/.cryo-mcp/data/)

## Environment Variables

- `ETH_RPC_URL`: Default Ethereum RPC URL to use when not specified via command line
- `CRYO_DATA_DIR`: Default directory to store downloaded data when not specified via command line

## Advanced Usage

### SQL Queries Against Blockchain Data

Cryo MCP allows you to run powerful SQL queries against blockchain data, combining the flexibility of SQL with Cryo's data extraction capabilities:

#### Two-Step SQL Query Flow

You can split data extraction and querying into two separate steps:

```python
# Step 1: Download data and get file paths
download_result = client.query_dataset(
    dataset="transactions",
    blocks_from_latest=1000,
    output_format="parquet"
)

# Step 2: Use the file paths to run SQL queries
file_paths = download_result.get("files", [])
client.query_sql(
    query=f"""
    SELECT 
        to_address as contract_address, 
        COUNT(*) as tx_count,
        SUM(gas_used) as total_gas,
        AVG(gas_used) as avg_gas
    FROM read_parquet('{file_paths[0]}')
    WHERE to_address IS NOT NULL
    GROUP BY to_address
    ORDER BY total_gas DESC
    LIMIT 20
    """,
    files=file_paths
)
```

#### Combined SQL Query Flow

For convenience, you can also use the combined function that handles both steps:

```python
# Get top gas-consuming contracts
client.query_blockchain_sql(
    sql_query="""
    SELECT 
        to_address as contract_address, 
        COUNT(*) as tx_count,
        SUM(gas_used) as total_gas,
        AVG(gas_used) as avg_gas
    FROM read_parquet('/path/to/transactions.parquet')
    WHERE to_address IS NOT NULL
    GROUP BY to_address
    ORDER BY total_gas DESC
    LIMIT 20
    """,
    dataset="transactions",
    blocks_from_latest=1000
)

# Find blocks with the most transactions
client.query_blockchain_sql(
    sql_query="""
    SELECT 
        block_number, 
        COUNT(*) as tx_count
    FROM read_parquet('/path/to/transactions.parquet')
    GROUP BY block_number
    ORDER BY tx_count DESC
    LIMIT 10
    """,
    dataset="transactions",
    blocks="15M:16M"
)

# Analyze event logs by topic
client.query_blockchain_sql(
    sql_query="""
    SELECT 
        topic0, 
        COUNT(*) as event_count
    FROM read_parquet('/path/to/logs.parquet')
    GROUP BY topic0
    ORDER BY event_count DESC
    LIMIT 20
    """,
    dataset="logs",
    blocks_from_latest=100
)
```

**Note**: For SQL queries, always use `output_format="parquet"` when downloading data to ensure optimal performance with DuckDB. When using `query_blockchain_sql`, you should refer to the file paths directly in your SQL using the `read_parquet()` function.

### Querying with Block Ranges

Cryo MCP supports the full range of Cryo's block specification syntax:

```python
# Using block numbers
client.query_dataset('transactions', blocks='15000000:15001000')

# Using K/M notation
client.query_dataset('logs', blocks='15M:15.01M')

# Using offsets from latest 
client.query_dataset('blocks', blocks_from_latest=100)
```

### Contract Filtering

Filter logs and other data by contract address:

```python
# Get all logs for USDC contract
client.query_dataset('logs', 
                    blocks='16M:16.1M', 
                    contract='0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48')
```

### Column Selection

Include only the columns you need:

```python
# Get just block numbers and timestamps
client.query_dataset('blocks', 
                    blocks='16M:16.1M', 
                    include_columns=['number', 'timestamp'])
```

## Development

### Project Structure

```
cryo-mcp/
├── cryo_mcp/           # Main package directory
│   ├── __init__.py     # Package initialization
│   ├── server.py       # Main MCP server implementation
│   ├── sql.py          # SQL query functionality
├── tests/              # Test directory
│   ├── test_*.py       # Test files
├── pyproject.toml      # Project configuration
├── README.md           # Project documentation
```

### Run Tests

`uv run pytest`

## License

MIT

## Credits

- Built on top of the amazing [Cryo](https://github.com/paradigmxyz/cryo) tool by Paradigm
- Uses the [MCP protocol](https://github.com/mcp-team/mcp) for API communication
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it implements a Model Completion Protocol (MCP) server for the Cryo blockchain data extraction tool, facilitating queries from MCP-compatible clients.","categories":["Protocol Implementations","API Frameworks","Blockchain"],"analysis":{"Architecture and Design Patterns":"The architecture revolves around providing an API server interface to the Cryo data extraction tool using the Model Completion Protocol (MCP). It employs a modular design, separating data extraction (Cryo), API serving (MCP server), and SQL querying functionalities. The core pattern seems to be a service-oriented architecture focused on data retrieval and transformation.","Implementation Details and Key Features":"Key features include accessing Cryo datasets via an API, supporting multiple query options like block ranges and contract filtering, and offering SQL query execution against downloaded blockchain data. The implementation details involve defining MCP tools for dataset listing, querying, and schema retrieval, as well as using DuckDB for SQL operations on Parquet files.","Integration Approaches and Protocols":"The repository integrates Cryo with an API server implementing the Model Completion Protocol (MCP). The primary integration is via command-line execution of Cryo to extract blockchain data. SQL integration is achieved using DuckDB, allowing for querying data in Parquet format. The tool is designed to be used with clients supporting the MCP protocol, like Claude Code.","Technical Stack and Dependencies":"The technical stack comprises Python 3.8+, uv (likely for package management), Cryo, DuckDB, and an Ethereum RPC endpoint. Dependencies include libraries for handling API requests and responses, interacting with Cryo for data extraction, and using DuckDB for SQL querying of blockchain data stored in Parquet format.","Best Practices and Coding Standards":"The documentation provides clear examples and usage instructions, promoting ease of use. The project structure includes separate directories for the main package and tests, indicating a focus on maintainability. However, there's no explicit mention of linting or code formatting tools, so it's not possible to determine adherence to coding standards.","Performance Considerations":"The documentation recommends using Parquet as the output format for SQL queries, suggesting a focus on optimizing performance with DuckDB. The use of block ranges and column selection allows for targeted data extraction, which improves efficiency. Caching mechanisms or other performance optimizations are not explicitly discussed, but the modular architecture could support such improvements."}}', '2025-03-10 21:28:23+00', 'true', 'Servers', 'cryo-mcp-code-access'), ('945265954', 'mcp-create', 'tesla0225/mcp-create', 'https://github.com/tesla0225/mcp-create', null, '2025-03-09 02:57:35+00', '2025-03-10 07:20:24+00', 'TypeScript', '9', '# MCP Create Server

A dynamic MCP server management service that creates, runs, and manages Model Context Protocol (MCP) servers dynamically. This service itself functions as an MCP server and launches/manages other MCP servers as child processes, enabling a flexible MCP ecosystem.

## Key Features

- Dynamic creation and execution of MCP server code
- Support for TypeScript only (JavaScript and Python support planned for future releases)
- Tool execution on child MCP servers
- Server code updates and restarts
- Removal of unnecessary servers

## Installation

**Note: Docker is the recommended way to run this service**

### Docker Installation (Recommended)

```bash
# Build Docker image
docker build -t mcp-create .

# Run Docker container
docker run -it --rm mcp-create
```

### Manual Installation (TypeScript Only)

```bash
# Clone repository
git clone https://github.com/tesla0225/mcp-create.git
cd mcp-create

# Install dependencies
npm install

# Build
npm run build

# Run
npm start
```

## Integration with Claude Desktop

Add the following to your Claude Desktop configuration file (`claude_desktop_config.json`):

```json
{
  "mcpServers": {
    "mcp-create": {
      "command": "docker",
      "args": ["run", "-i", "--rm", "mcp-create"]
    }
  }
}
```

## Available Tools

| Tool Name | Description | Input Parameters | Output |
|-----------|-------------|-----------------|--------|
| create-server-from-template | Create MCP server from template | language: string | { serverId: string, message: string } |
| execute-tool | Execute tool on server | serverId: string<br>toolName: string<br>args: object | Tool execution result |
| get-server-tools | Get list of server tools | serverId: string | { tools: ToolDefinition[] } |
| delete-server | Delete server | serverId: string | { success: boolean, message: string } |
| list-servers | Get list of running servers | none | { servers: string[] } |

## Usage Examples

### Creating a New Server

```json
{
  "name": "create-server-from-template",
  "arguments": {
    "language": "typescript"
  }
}
```

### Executing a Tool

```json
{
  "name": "execute-tool",
  "arguments": {
    "serverId": "ba7c9a4f-6ba8-4cad-8ec8-a41a08c19fac",
    "toolName": "echo",
    "args": {
      "message": "Hello, dynamic MCP server!"
    }
  }
}
```

## Technical Specifications

- Node.js 18 or higher
- TypeScript (required)
- Dependencies:
  - @modelcontextprotocol/sdk: MCP client/server implementation
  - child_process (Node.js built-in): Child process management
  - fs/promises (Node.js built-in): File operations
  - uuid: Unique server ID generation

## Security Considerations

- **Code Execution Restrictions:** Consider sandboxing as the service executes arbitrary code
- **Resource Limitations:** Set limits on memory, CPU usage, number of files, etc.
- **Process Monitoring:** Monitor and forcibly terminate zombie or runaway processes
- **Path Validation:** Properly validate file paths to prevent directory traversal attacks

## License

MIT
', 'true', '"{\"Protocol Implementations\",\"Agent Frameworks\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly mentions Model Context Protocol (MCP) and provides a service for dynamically managing MCP servers, indicating a clear implementation or relation to MCP specifications.","categories":["Agent Frameworks","Protocol Implementations","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The architecture revolves around a central MCP server managing child MCP servers. It uses a dynamic creation and execution model, allowing for flexible server management. The service supports tool execution on these child servers and implements mechanisms for server updates, restarts, and removal. Docker is recommended for deployment, suggesting a containerized approach to service management.","Implementation Details and Key Features":"The implementation is primarily in TypeScript, utilizing Node.js. Key features include dynamic MCP server creation from templates, tool execution on child servers, and server lifecycle management (start, stop, update). The service provides API endpoints for creating, executing tools on, listing, and deleting MCP servers, facilitating dynamic interaction with deployed servers.","Integration Approaches and Protocols":"The repository integrates with the Model Context Protocol (MCP) standard, exposing an API for managing MCP servers. It is intended to be used with systems such as Claude Desktop, as shown in the configuration example. Communication is achieved through the MCP protocol, allowing external systems to interact with and manage the dynamically created servers.","Technical Stack and Dependencies":"The core technical stack consists of Node.js and TypeScript. It relies on the `@modelcontextprotocol/sdk` for MCP implementation, `child_process` for managing child processes, `fs/promises` for file system operations, and `uuid` for unique server ID generation. Docker support is provided for containerization, enabling easier deployment and management of the service.","Best Practices and Coding Standards":"The repository emphasizes security considerations like code execution restrictions, resource limitations, process monitoring, and path validation, reflecting a concern for secure code execution. Usage of Docker and npm package manager indicates adherence to standard deployment and dependency management practices. The use of TypeScript promotes type safety and maintainability.","Performance Considerations":"The dynamic nature of server creation and tool execution implies a need for performance optimization. The documentation highlights the importance of resource limitations for child processes, suggesting awareness of potential performance bottlenecks. The service architecture benefits from concurrency through the use of child processes, enabling parallel execution of multiple servers and tools."}}', '2025-03-10 21:28:34+00', 'true', 'Servers', 'mcp-create-server'), ('945281833', 'mcp-community', 'Mirascope/mcp-community', 'https://github.com/Mirascope/mcp-community', 'Easily implement, deploy, and use MCP servers', '2025-03-09 03:58:09+00', '2025-03-10 06:16:25+00', 'Python', '9', '# Model Context Protocol Community

Coming soon...
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository name \"mcp-community\" and description \"Easily implement, deploy, and use MCP servers\" directly indicate implementation and support for the Model Context Protocol.","categories":["Protocol Implementations","API Frameworks","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"Due to limited documentation, the specific architecture and design patterns are currently unknown. However, the project description suggests a focus on ease of implementation and deployment, implying a modular design. Further details are required to assess design patterns.","Implementation Details and Key Features":"Implementation details are not available due to the lack of documentation. The core feature seems to be enabling easy implementation, deployment, and usage of MCP servers, which are likely designed to manage model context.","Integration Approaches and Protocols":"The repository focuses on the Model Context Protocol. Integration is likely facilitated through a standardized API, allowing other applications and services to interact with the MCP server. The specific communication protocols utilized are not defined in the available documentation.","Technical Stack and Dependencies":"The technical stack is currently unknown due to the absence of documentation. It would be necessary to inspect the code or available documentation to know which programming languages, frameworks, and libraries the implementation depends on.","Best Practices and Coding Standards":"Information about coding standards cannot be determined with the available information. Best practices adopted within the repository are also unavailable. Further investigation of code would be required to assess coding standards.","Performance Considerations":"Performance considerations are not explicitly mentioned in the provided description or documentation. Optimization strategies that may be implemented for this tool are unknown without further documentation or investigation."}}', '2025-03-10 21:54:04+00', 'true', 'Servers', 'mcp-servers-community'), ('945281966', 'codemcp', 'ezyang/codemcp', 'https://github.com/ezyang/codemcp', 'Aider-style MCP for Claude Desktop', '2025-03-09 03:58:34+00', '2025-03-10 18:17:50+00', 'Python', '242', '# codemcp

A multi-purpose MCP for coding with Claude Sonnet.  It is specifically
intended to be used with Claude Desktop, where you can purchase Claude Pro and
pay only a flat monthly fee for as much usage up to Anthropic's rate limit, as
opposed to potentially uncapped cost from API usage.

Currently, this MCP only provides the ability to directly read/write files on
your filesystem, based off of Claude Code's tools.  However, it takes an
opinionated approach to implementing this functionality based on the coding
use case:

- Git is mandatory; we generate a commit for every edit so you can easily use
  Git to rollback if the AI does something bad.  The MCP will ONLY write
  to Git tracked files, so you are guaranteed to be able to rollback if
  necessary.

- You must specifically opt-in a repository to being editable with codemcp by
  creating a codemcp.toml file at its base directory.  We will refuse to write
  all other files.

Major missing functionality that I plan to implement ASAP:

- Improve the system prompt

- Linter/autoformatter integration

- Typecheck/build integration

- Test runner integration

- Scrape webpage and add to context

- Explicitly add file to context

- Make file executable tool

- A few more of Claude Code's tools: glob, memory, notebook

Things I NEVER intend to implement, for philosophical reasons:

- Bash tool (instead, I want you to explicitly whitelist commands that are OK
  for the agent to run)

I might write an API-driven version of this tool for when you hit the rate
limit, but it might be better to just get someone to clone an open source
version of Claude Desktop and then iterate off of that.

This tool was bootstrapped into developing itself in three hours.  I'm still
working out Sonnet 3.7's quirks for Python projects, so apologies for any
naughty code.

## Installation

From source:

```bash
uv venv
source .venv/bin/activate
uv sync
```

and then in `claude_desktop_config.json`

```json
{
  "mcpServers": {
    "codemcp": {
      "command": "/Users/ezyang/Dev/codemcp/.venv/bin/python",
      "args": [
        "-m",
        "codemcp"
      ]
    }
  }
}
```

TODO: uvx instructions

There is a pypi package but it is out of date until we get out of rapid
development, install from source for now.

## Usage

Create a project and put this in your system prompt:

```
Before doing anything, first init project PATH_TO_PROJECT.
```

Then chat with Claude about what changes you want to make to the project.

## Configuration

codemcp uses a TOML configuration file located at `~/.codemcprc`. Currently supported configuration options:

```toml
[logger]
verbosity = "INFO"  # Can be DEBUG, INFO, WARNING, ERROR, or CRITICAL
```

In your repository, there is also a config file `codemcp.toml`.

```
global_prompt = """
Before beginning work on this feature, write a short haiku.  Do this only once.
"""
```

This prompt will be loaded when you initialize the project in chats.

## Logging

Logs are written to `~/.codemcp/codemcp.log`. The log level can be set in the configuration file or overridden with environment variables:

- Set the log level in config: `verbosity = "DEBUG"` in `~/.codemcprc`
', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository implements an MCP specifically designed for Claude Desktop, facilitating file read/write operations with Git integration and configuration via TOML files. It enhances Claude's capabilities by providing controlled access to the filesystem.","categories":["AI Model Integration","Function Calling","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The architecture revolves around a server that Claude Desktop interacts with using a multi-purpose context protocol. The design pattern is centered around file system access with enforced Git version control. It adopts a configuration-driven approach, using TOML files for global and project-specific settings, defining behavior and constraints.","Implementation Details and Key Features":"Key features include mandatory Git integration for every edit, restricted write access only to Git-tracked files, and project-specific configuration via `codemcp.toml`. The implementation uses Python and integrates with Claude Desktop through a specified command and arguments in `claude_desktop_config.json`.","Integration Approaches and Protocols":"The repository integrates with Claude Desktop using a context protocol server configuration. It utilizes the `claude_desktop_config.json` to establish the connection. Communication occurs through command-line arguments to the Python script. It intends to incorporate linting, type checking, and test runner integration.","Technical Stack and Dependencies":"The technical stack includes Python, TOML for configuration, and Git for version control. It utilizes `uv` for virtual environment management. Dependencies likely include libraries for file system interaction, Git command execution, and TOML parsing.","Best Practices and Coding Standards":"The repository enforces a coding standard of Git commit tracking for every file edit. It uses TOML configuration files for clarity and separation of concerns. It aims to incorporate linting and auto-formatting to improve code quality, but the current state is acknowledged as potentially 'naughty'.","Performance Considerations":"Performance considerations are implied by the focus on minimizing API usage and leveraging local file system access. The design avoids unbounded API costs. The implementation aims for efficiency by integrating directly with Claude Desktop, but performance optimization details are not explicitly described."}}', '2025-03-10 21:26:08+00', 'true', 'Servers', 'codemcp-claude-desktop'), ('945309383', 'mongodb-lens', 'furey/mongodb-lens', 'https://github.com/furey/mongodb-lens', '🍃🔎 MongoDB Lens: Full Featured MCP Server for MongoDB Database Analysis', '2025-03-09 05:39:54+00', '2025-03-10 12:56:26+00', 'JavaScript', '6', '# MongoDB Lens

**MongoDB Lens** is a local Model Context Protocol (MCP) server with full featured access to MongoDB databases using natural language via LLMs to perform queries, run aggregations, optimize performance, and more.

## Contents

- [Quick Start](#quick-start)
- [Features](#features)
- [Configuration](#configuration)
- [Tutorial](#tutorial)
- [Disclaimer](#disclaimer)

## Quick Start

- Clone repository
- [Install](#installation) dependencies
- [Configure](#mcp-client-setup) your MCP Client (e.g. [Claude Desktop](#usage-with-claude-desktop))
- Start exploring your MongoDB databases with [natural language queries](#example-queries)

## Features

MongoDB Lens exposes the following capabilities through MCP:

- [Resources](#resources)
- [Tools](#tools)
- [Prompts](#prompts)

### Resources

- Database listings
- Collection metadata
- Collection statistics
- Schema inference
- Index information
- Server status and metrics
- Replica set configuration
- Collection validation rules
- Database users and roles
- Stored JavaScript functions

### Tools

- **list-databases**: View all accessible MongoDB databases
- **current-database**: Show the current database context
- **use-database**: Switch to a specific database context
- **list-collections**: Explore collections in the current database
- **find-documents**: Run queries with filters, projections, and sorting
- **count-documents**: Count documents matching specified criteria
- **aggregate-data**: Execute aggregation pipelines
- **get-stats**: Retrieve database or collection statistics
- **analyze-schema**: Automatically infer collection schemas
- **create-index**: Create new indexes for performance optimization
- **explain-query**: Analyze query execution plans
- **distinct-values**: Extract unique values for any field
- **validate-collection**: Check for data inconsistencies
- **create-collection**: Create new collections with custom options
- **drop-collection**: Remove collections from the database
- **rename-collection**: Rename existing collections
- **modify-document**: Insert, update, or delete specific documents
- **export-data**: Export query results in JSON or CSV format
- **map-reduce**: Run MapReduce operations for complex data processing
- **bulk-operations**: Perform multiple operations efficiently

### Prompts

- **query-builder**: Interactive guidance for constructing MongoDB queries
- **aggregation-builder**: Step-by-step creation of aggregation pipelines
- **schema-analysis**: Detailed collection schema analysis with recommendations
- **index-recommendation**: Get personalized index suggestions based on query patterns
- **mongo-shell**: Generate MongoDB shell commands with explanations
- **inspector-guide**: Get help using MongoDB Lens with MCP Inspector
- **data-modeling**: Expert advice on MongoDB schema design for specific use cases
- **query-optimizer**: Optimization recommendations for slow queries
- **security-audit**: Database security analysis and improvement recommendations
- **backup-strategy**: Customized backup and recovery recommendations
- **migration-guide**: Step-by-step MongoDB version migration plans

## Configuration

- [Installation](#installation)
- [MongoDB Connection String](#mongodb-connection-string)
- [MCP Server Logging](#mcp-server-logging)
- [MCP Client Setup](#mcp-client-setup)

### Installation

Depending on whether you want to run with Node.js or Docker, follow the appropriate instructions below.

#### Docker Installation

1. Navigate to the cloned repository directory:<br>
    ```console
    cd /path/to/mongodb-lens
    ```
1. Build the Docker image:<br>
    ```console
    docker build -t mongodb-lens .
    ```
1. Check the installation runs (tip: press <kbd>Ctrl</kbd>+<kbd>C</kbd> to exit):<br>
    ```console
    docker run --rm -i --network=host mongodb-lens
    ```

#### Node.js Installation

1. Navigate to the cloned repository directory:<br>
    ```console
    cd /path/to/mongodb-lens
    ```
1. Ensure [Node](https://nodejs.org/en/download) running (tip: use [Volta](https://volta.sh)):<br>`$ node -v` >= `22.*`
1. Install Node.js dependencies:<br>
    ```console
    npm ci
    ```
1. Check the installation runs (tip: press <kbd>Ctrl</kbd>+<kbd>C</kbd> to exit):<br>
    ```console
    node mongodb-lens.js
    ```

### MongoDB Connection String

The server accepts a MongoDB connection string as its only argument:

```txt
mongodb://[username:password@]host[:port][/database][?options]
```

Example URIs:

- Local connection: `mongodb://localhost:27017`
- Connection with credentials and DB name: `mongodb://username:password@hostname:27017/mydatabase`
- Connection with DB name and options: `mongodb://hostname:27017/mydatabase?retryWrites=true&w=majority`

Example Docker connection string usage:

```console
docker run --rm -i --network=host mongodb-lens mongodb://your-connection-string
```

Example Node.js connection string usage:

```console
node mongodb-lens.js mongodb://your-connection-string
```

If no connection string is provided, the server will attempt to connect to a local MongoDB instance on the default port i.e. `mongodb://localhost:27017`.

### MCP Server Logging

To enable verbose MCP Server logging for debugging purposes, set the environment variable `VERBOSE_LOGGING` to `true`.

Example Node.js usage:

```console
VERBOSE_LOGGING=true node mongodb-lens.js mongodb://your-connection-string
```

Example Docker usage:

```console
docker run --rm -i --network=host -e VERBOSE_LOGGING='true' mongodb-lens mongodb://your-connection-string
```

### MCP Client Setup

- [Usage with Claude Desktop](#usage-with-claude-desktop)
- [Usage with MCP Inspector](#usage-with-mcp-inspector)
- [Usage with Other MCP Clients](#usage-with-other-mcp-clients)

#### Usage with Claude Desktop

To use MongoDB Lens with Claude Desktop:

1. Install [Claude Desktop](https://claude.ai/download)
1. Create and/or open `claude_desktop_config.json`:
    - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`
    - Windows: `%APPDATA%\Claude\claude_desktop_config.json`
1. Add the MongoDB Lens server configuration:
    - Example Docker configuration:<br>
        ```json
        {
          "mcpServers": {
            "mongodb-lens": {
              "command": "docker",
              "args": [
                "run",
                "--rm",
                "-i",
                "--network=host",
                "mongodb-lens",
                "mongodb://your-connection-string"
              ],
              "env": {
                "VERBOSE_LOGGING": "[true|false]"
              }
            }
          }
        }
        ```
      - Replace `mongodb://your-connection-string` with your MongoDB connection string
      - Set `VERBOSE_LOGGING` to `true` for verbose MCP Server logs
    - Example Node.js configuration:<br>
        ```json
        {
          "mcpServers": {
            "mongodb-lens": {
              "command": "/absolute/path/to/node",
              "args": [
                "/absolute/path/to/mongodb-lens.js",
                "mongodb://your-connection-string"
              ],
              "env": {
                "VERBOSE_LOGGING": "<true|false>"
              }
            }
          }
        }
        ```
      - Replace `/absolute/path/to/node` with the full path to `node`
      - Replace `/absolute/path/to/mongodb-lens.js` with the full file path [`mongodb-lens.js`](./mongodb-lens.js)
      - Replace `mongodb://your-connection-string` with your MongoDB connection string
      - Set `VERBOSE_LOGGING` to `true` for verbose MCP Server logs
1. Restart Claude Desktop
1. Start a conversation with Claude and ask about your MongoDB data
    - Claude will show a hammer icon indicating available tools
    - Ask questions like "What databases do I have?" or "Show me the schema for the users collection"

#### Usage with MCP Inspector

The [MCP Inspector](https://github.com/modelcontextprotocol/inspector) is a development tool specifically designed for testing and debugging MCP servers. It provides a visual interface to explore resources, run tools, and via MongoDB Lens understand your MongoDB database.

To use MongoDB Lens with MCP Inspector:

1. Navigate to the cloned repository directory:<br>
    ```console
    cd /path/to/mongodb-lens
    ```
1. Run Inspector via `npx`:<br>
    ```console
    npx @modelcontextprotocol/inspector node mongodb-lens.js mongodb://your-connection-string
    ```
1. The Inspector will start both a proxy server (default port 3000) and a web UI (default port 5173)
1. Open your browser to http://localhost:5173 to access the Inspector interface
1. You can customize the ports if needed:<br>
    ```console
    CLIENT_PORT=8080 SERVER_PORT=9000 npx @modelcontextprotocol/inspector node mongodb-lens.js
    ```
1. The Inspector supports the full range of MongoDB Lens capabilities, including autocompletion for collection names and query fields.

For more, see: [MCP Inspector](https://modelcontextprotocol.io/docs/tools/inspector)

#### Usage with Other MCP Clients

MongoDB Lens can be used with any MCP-compatible client.

For more, see: [MCP Documentation: Example Clients](https://modelcontextprotocol.io/clients)

## Tutorial

This tutorial will guide you through setting up a MongoDB instance with sample data and using MongoDB Lens to interact with it through natural language queries.

- [Setting Up MongoDB](#setting-up-mongodb)
- [Importing Sample Data](#importing-sample-data)
- [Example Queries](#example-queries)

### Setting Up MongoDB

1. Run MongoDB Docker container:<br>
   ```console
   docker run --name mongodb-sampledata -d -p 27017:27017 mongo:6
   ```
1. Verify the container is running:
   ```console
   docker ps | grep mongodb-sampledata
   ```

### Importing Sample Data

MongoDB provides several sample datasets, which we'll use to explore MongoDB Lens.

1. Download the sample dataset:
   ```console<br>
   curl -LO https://atlas-education.s3.amazonaws.com/sampledata.archive
   ```
2. Copy the sample data into your MongoDB container:<br>
   ```console
   docker cp sampledata.archive mongodb-sampledata:/tmp/
   ```
3. Restore the sample data:<br>
   ```console
   docker exec -it mongodb-sampledata mongorestore --archive=/tmp/sampledata.archive
   ```

This will import several sample databases including:

- `sample_airbnb` - Airbnb listings and reviews
- `sample_analytics` - Customer and account data
- `sample_geospatial` - Geographic data
- `sample_mflix` - Movie data
- `sample_restaurants` - Restaurant data
- `sample_supplies` - Supply chain data
- `sample_training` - Training data for various applications
- `sample_weatherdata` - Weather measurements

### Connecting MongoDB Lens

Set your [MCP Client](#mcp-client-setup) to connect to MongoDB Lens with the connection string:

```txt
mongodb://localhost:27017/mongodb-sampledata
```

### Example Queries

With your MCP Client running and connected to MongoDB Lens, try these queries on the sample dataset:

#### Exploring Databases and Collections

- "List all available databases"
- "What's in the sample_mflix database? Switch to it"
- "Show me all collections in the current database"
- "How many documents are in the movies collection?"

#### Movie Data Queries (sample_mflix)

- "Find the top 5 movies by IMDB rating"
- "What are the most common movie genres?"
- "Show me movies directed by Christopher Nolan"
- "Find movies released in the 1990s with a rating above 8.5"
- "What's the average runtime of action movies?"
- "Who are the top 10 most prolific actors in the database?"

#### Airbnb Data Analysis (sample_airbnb)

- "Switch to sample_airbnb database"
- "What types of properties are listed on Airbnb?"
- "What's the average price of listings in Brooklyn?"
- "Find the top 5 most reviewed listings"
- "What neighborhoods have the most listings?"
- "Analyze the distribution of prices across different property types"

#### Weather Data Queries (sample_weatherdata)

- "Switch to sample_weatherdata database"
- "What's the schema of the data collection?"
- "Find the highest temperature recorded in the dataset"
- "What's the average pressure reading across all measurements?"
- "Show me readings where callLetters is 'SHIP'"

#### Advanced Operations

- "Switch back to the sample_mflix database"
- "Create an index on the title field in the movies collection"
- "Analyze the schema of the movies collection and suggest improvements"
- "Build an aggregation pipeline to show the count of movies by year and genre"
- "Find distinct countries where movies were produced"
- "Export a CSV with the top 50 rated movies including title, year, and rating"

## Disclaimer

MongoDB Lens:

- is licensed under the [MIT License](./LICENSE).
- is not affiliated with or endorsed by MongoDB, Inc.
- is written with the assistance of AI and may contain errors.
- is intended for educational and experimental purposes only.
- is provided as-is with no warranty or support—use at your own risk.
', 'true', '"{\"Protocol Implementations\",\"AI Model Integration\",\"Database Systems\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it's a Model Context Protocol (MCP) server for MongoDB database analysis, providing tools, resources, and prompts accessible via MCP clients like Claude Desktop and MCP Inspector.","categories":["AI Model Integration","Function Calling","Database Systems"],"analysis":{"Architecture and Design Patterns":"MongoDB Lens acts as a local MCP server. It exposes MongoDB database functionalities through resources, tools, and prompts, enabling interaction with MongoDB using natural language. Clients communicate with the server using the MCP protocol.  The server connects directly to a MongoDB instance using a connection string.","Implementation Details and Key Features":"Key features include database listings, schema inference, query execution, and data aggregation. It exposes 'Tools' like 'list-databases', 'find-documents', and 'aggregate-data' through MCP. 'Prompts' provide interactive guidance for constructing queries and analyzing schemas. Supports installation via Docker and Node.js.","Integration Approaches and Protocols":"MongoDB Lens integrates with various MCP-compatible clients like Claude Desktop and MCP Inspector. It uses the MCP protocol for communication, facilitating natural language interaction with MongoDB. Integration involves configuring the MCP client with the Lens server's address and connection string.","Technical Stack and Dependencies":"The primary dependency is MongoDB. It can be run using Node.js, requiring Node version 22 or higher and npm. Docker provides an alternative deployment option. The repository depends on MCP for enabling natural language interactions with a database.","Best Practices and Coding Standards":"The documentation provides clear instructions for installation and configuration. Logging can be enabled using the `VERBOSE_LOGGING` environment variable. The project includes a disclaimer emphasizing that it's intended for educational and experimental purposes and may contain errors.","Performance Considerations":"The repository provides tools like `explain-query` and `create-index` for optimizing query performance in MongoDB. The `bulk-operations` tool suggests a focus on efficient data manipulation. The documentation also mentions the importance of using a proper connection string for MongoDB."}}', '2025-03-10 21:54:04+00', 'true', 'Servers', 'mongodb-lens-mcp-server'), ('945313106', 'redash-mcp', 'suthio/redash-mcp', 'https://github.com/suthio/redash-mcp', null, '2025-03-09 05:52:59+00', '2025-03-10 01:59:11+00', 'JavaScript', '8', '# Redash MCP Server

Model Context Protocol (MCP) server for integrating Redash with AI assistants like Claude.

## Features

- Connect to Redash instances via the Redash API
- List available queries and dashboards as resources
- Execute queries and retrieve results
- Create and manage queries (create, update, archive)
- List data sources for query creation
- Get dashboard details and visualizations

## Prerequisites

- Node.js (v18 or later)
- npm or yarn
- Access to a Redash instance
- Redash API key

## Environment Variables

The server requires the following environment variables:

- `REDASH_URL`: Your Redash instance URL (e.g., https://redash.example.com)
- `REDASH_API_KEY`: Your Redash API key

Optional variables:
- `REDASH_TIMEOUT`: Timeout for API requests in milliseconds (default: 30000)
- `REDASH_MAX_RESULTS`: Maximum number of results to return (default: 1000)

## Installation

1. Clone this repository:
   ```bash
   git clone https://github.com/suthio/redash-mcp.git
   cd redash-mcp
   ```

2. Install dependencies:
   ```bash
   npm install
   ```

3. Create a `.env` file with your Redash configuration:
   ```
   REDASH_URL=https://your-redash-instance.com
   REDASH_API_KEY=your_api_key
   ```

4. Build the project:
   ```bash
   npm run build
   ```

5. Start the server:
   ```bash
   npm start
   ```

## Usage with Claude for Desktop

To use this MCP server with Claude for Desktop, configure it in your Claude for Desktop configuration file:

**macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`
**Windows**: `%APPDATA%\Claude\claude_desktop_config.json`

Add the following configuration (edit paths as needed):

```json
{
  "mcpServers": {
    "redash": {
      "command": "node",
      "args": ["/absolute/path/to/redash-mcp/dist/index.js"],
      "env": {
        "REDASH_API_KEY": "your-api-key",
        "REDASH_URL": "https://your-redash-instance.com"
      }
    }
  }
}
```

## Available Tools

### Query Management
- `list-queries`: List all available queries in Redash
- `get-query`: Get details of a specific query 
- `create-query`: Create a new query in Redash
- `update-query`: Update an existing query in Redash
- `archive-query`: Archive (soft-delete) a query
- `list-data-sources`: List all available data sources

### Query Execution
- `execute-query`: Execute a query and return results

### Dashboard Management
- `list-dashboards`: List all available dashboards
- `get-dashboard`: Get dashboard details and visualizations 
- `get-visualization`: Get details of a specific visualization

## Development

Run in development mode:
```bash
npm run dev
```

## Version History

- v1.1.0: Added query management functionality (create, update, archive)
- v1.0.0: Initial release

## License

MIT
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository implements a Model Context Protocol (MCP) server specifically designed to integrate Redash with AI assistants like Claude, enabling access to Redash queries and dashboards.","categories":["AI Model Integration","Context Management","API Frameworks"],"analysis":{"Architecture and Design Patterns":"The application uses a server-based architecture to expose Redash functionalities through the Model Context Protocol. It leverages the Redash API to interact with Redash instances. The server is designed to be configured via environment variables, promoting flexibility and ease of deployment. The design supports various tools for query and dashboard management and execution.","Implementation Details and Key Features":"The implementation is based on Node.js, utilizing npm for package management. Key features include listing queries and dashboards, executing queries, creating/updating/archiving queries, listing data sources, and retrieving dashboard details. The server exposes these functionalities as tools accessible via MCP, facilitating interaction with AI assistants like Claude.","Integration Approaches and Protocols":"The primary integration approach involves implementing a Model Context Protocol server that acts as an intermediary between Redash and AI assistants. The server communicates with Redash via its API using HTTP requests. The provided `claude_desktop_config.json` snippet demonstrates how to configure Claude for Desktop to use the MCP server.","Technical Stack and Dependencies":"The technical stack primarily consists of Node.js (v18 or later) as the runtime environment. Dependencies are managed via npm. The application depends on access to a Redash instance and its API, requiring environment variables like `REDASH_URL` and `REDASH_API_KEY` for configuration.","Best Practices and Coding Standards":"The documentation includes instructions for installation, configuration using environment variables, and usage with Claude for Desktop. The repository uses `npm` which helps standardize the development process. Clear instructions for setting up `.env` files indicate an awareness of security best practices for managing API keys.","Performance Considerations":"The server provides optional configuration for request timeouts (`REDASH_TIMEOUT`) and result limits (`REDASH_MAX_RESULTS`), indicating consideration for performance and resource management. These options allow users to fine-tune the server's behavior based on the Redash instance's performance and the needs of the AI assistant."}}', '2025-03-10 21:28:34+00', 'true', 'Servers', 'redash-mcp-server'), ('945353787', 'databutton-mcp', 'databutton/databutton-mcp', 'https://github.com/databutton/databutton-mcp', 'Databutton MCP Server', '2025-03-09 08:03:57+00', '2025-03-10 07:25:07+00', 'TypeScript', '5', '# Databutton MCP Server

Databutton's (https://www.databutton.com/) AI agent builds and deploys frontends and backends in React and Python APIs/MCPs, for building beautiful business apps with unbounded complexity.

The agent has a planning mode for generating an initial plan for your app.

This MCP server is for doing doing initial app planning and creating a good starting point for an app.

## Development

Install dependencies:

```bash
npm install
```

Build the server:

```bash
npm run build
```

For development with auto-rebuild:

```bash
npm run watch
```

## Installation

To use with Claude Desktop, add the server config:

On MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`
On Windows: `%APPDATA%/Claude/claude_desktop_config.json`

```json
{
  "mcpServers": {
    "databutton": {
      "command": "/path/to/databutton/build/index.js"
    }
  }
}
```

### Debugging

Since MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:

```bash
npm run inspector
```

The Inspector will provide a URL to access debugging tools in your browser.
', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly mentions 'MCP Server' and provides instructions for integrating it with Claude Desktop using the Model Context Protocol. It also references the MCP Inspector for debugging.","categories":["Agent Frameworks","Function Calling","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The repository appears to implement a server that acts as an intermediary between an AI agent and other components, likely frontends and backends built with React and Python APIs. It facilitates initial app planning and provides a starting point for application development, aligning with a modular design to interface with AI agents.","Implementation Details and Key Features":"The key feature is the implementation of an MCP server for initial app planning. It supports building and deploying frontends and backends, and can be used to develop complex business applications. The server configuration is designed for integration with Claude Desktop, suggesting a focus on AI-assisted application development.","Integration Approaches and Protocols":"The repository utilizes the Model Context Protocol (MCP) for communication. Integration is facilitated by configuring the 'claude_desktop_config.json' file to point to the MCP server's executable. Debugging is supported via stdio communication and the MCP Inspector tool, indicating standard input/output streams.","Technical Stack and Dependencies":"The repository relies on Node.js and npm for development and build processes. The code base is likely written in JavaScript or TypeScript, given the use of npm and the build process. It likely depends on libraries for inter-process communication (stdio) to interact with AI agents.","Best Practices and Coding Standards":"The repository includes a clear build process using npm scripts. There are instructions for development with auto-rebuild and debugging, suggesting an awareness of developer experience. The documentation encourages using the MCP Inspector for debugging, which improves ease of integration and overall understanding.","Performance Considerations":"The initial planning stage implemented by the MCP server is likely designed to optimize the development process. The performance of the application built will depend on the specific code generated by the AI agent. The documentation doesn't mention specific runtime performance optimizations."}}', '2025-03-10 21:54:04+00', 'true', 'Servers', 'databutton-mcp-server'), ('945389006', 'mcp-server-auto-commit', 'jatinsandilya/mcp-server-auto-commit', 'https://github.com/jatinsandilya/mcp-server-auto-commit', 'An mcp server that auto commits changes and creates commit message in the form of conventional commits (https://www.conventionalcommits.org/en/v1.0.0/)', '2025-03-09 09:58:15+00', '2025-03-10 17:50:36+00', 'TypeScript', '9', '# MCP Server to Auto commit changes  🛠️

This implementation provides a Git changes analyzer that generates commit messages using OpenAI's GPT models.

## Demo

![](./assets/auto_commit_demo_with_ai.gif)

## Features

- Analyzes git changes in your repository (both staged and unstaged)
- Generates conventional commit messages using GPT-4o-mini
- Provides detailed summaries of:
  - 📝 Modified files
  - ✨ Newly added files
  - 🗑️ Deleted files
  - 📄 Detailed changes (up to 10 lines per file)
- Built with [@modelcontextprotocol/sdk](https://docs.anthropic.com/en/docs/agents-and-tools/mcp)
- Adds an auto-commit signature to each commit

## Project Structure

```
mcp-server-auto-commit/
├── index.ts        # Main server implementation with git analysis tool
├── package.json    # Project dependencies
├── tsconfig.json   # TypeScript configuration
└── build/         # Compiled JavaScript output
```

## Prerequisites

- Node.js installed
- OpenAI API key
- Git repository to analyze
- pnpm package manager

## Getting Started

1. Clone this repository:
```bash
git clone https://github.com/jatinsandilya/mcp-server-auto-commit.git
cd mcp-server-auto-commit
```

2. Install dependencies:
```bash
pnpm install
```

3. Set up your OpenAI API key using one of these methods:
   - Set as an environment variable: `OPENAI_API_KEY=your-api-key`
   - Pass as a command line argument: `--key your-api-key`
   - Add to a `.env` file in the project root

4. Build the project:
```bash
pnpm run build
```

This will generate the `/build/index.js` file - your compiled MCP server script.

## Using with Cursor

1. Go to Cursor Settings -> MCP -> Add new MCP server
2. Configure your MCP:
   - Name: git-auto-commit
   - Type: command
   - Command: `node ABSOLUTE_PATH_TO_MCP_SERVER/build/index.js --key your-api-key`
   (Replace `your-api-key` with your actual OpenAI API key if not set in environment)

## Using with Claude Desktop

Add the following MCP config to your Claude Desktop configuration:

```json
{
  "mcpServers": {
    "git-auto-commit": {
      "command": "node",
      "args": ["ABSOLUTE_PATH_TO_MCP_SERVER/build/index.js", "--key", "your-api-key"]
    }
  }
}
```

## Available Tools

### git-changes-commit-message

This tool analyzes the current git changes in your repository and generates a commit message using OpenAI's GPT-4o-mini model. It provides:

- List of modified files with status indicators
- List of newly added files
- List of deleted files
- Detailed changes for each file (limited to 10 lines per file for readability)
- A generated commit message following conventional commits format
- An auto-commit signature

Usage parameters:
- `autoCommitPath`: Optional path to analyze specific directory/file. If not provided, uses current working directory.

## Development

The implementation in `index.ts` showcases:

1. Setting up the MCP server with proper configuration
2. Handling command line arguments and environment variables
3. Integrating with OpenAI's API using GPT-4o-mini model
4. Git operations using child processes
5. Error handling and fallback mechanisms
6. Detailed change analysis and formatting

To modify or extend the implementation:

1. Update the server configuration in `index.ts`:
```typescript
const server = new McpServer({
  name: "git-auto-commit",
  version: "0.0.1",
});
```

2. The tool is defined using `server.tool()` with proper parameter validation using Zod schema.

3. Build and test your changes:
```bash
pnpm run build
```

## Contributing

Feel free to submit issues and enhancement requests!

## License

MIT
', 'true', '"{\"Agent Frameworks\",\"Developer Tooling\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository description and documentation explicitly state that it is an MCP server and utilizes the @modelcontextprotocol/sdk for generating commit messages using OpenAI's GPT models based on git changes.","categories":["AI Model Integration","Developer Tooling","API Frameworks"],"analysis":{"Architecture and Design Patterns":"The repository implements a Model Context Protocol (MCP) server designed to automatically generate commit messages based on Git changes. It uses a command-line interface (CLI) and environment variables for configuration. The design leverages OpenAI's GPT models for generating commit messages and the conventional commits format for structuring them.","Implementation Details and Key Features":"The implementation centers around the `index.ts` file, which sets up the MCP server, handles command-line arguments, and integrates with the OpenAI API. Key features include analyzing Git changes, generating commit messages, providing summaries of modified files, and using GPT-4o-mini for generating commit messages. The tool is defined using `server.tool()`.","Integration Approaches and Protocols":"The repository integrates with OpenAI's GPT models via API calls. It implements the Model Context Protocol (MCP) for communication with tools like Cursor and Claude Desktop. The system analyzes Git changes using child processes. It follows conventional commit message guidelines. The repository provides explicit instructions on how to integrate with Cursor and Claude Desktop.","Technical Stack and Dependencies":"The technical stack primarily consists of Node.js, TypeScript, and the @modelcontextprotocol/sdk. Key dependencies include OpenAI's API for generating commit messages. The project uses `pnpm` as a package manager. Other dependencies likely include libraries for interacting with Git and handling command-line arguments.","Best Practices and Coding Standards":"The repository utilizes TypeScript, which promotes type safety and code maintainability. It also includes error handling and fallback mechanisms. Conventional commits are implemented for improved commit message structure. The code is structured with a clear separation of concerns, as demonstrated by the `index.ts` file handling server logic.","Performance Considerations":"The repository limits the number of lines analyzed per file (10) to enhance performance. It also provides options to analyze specific directories or files to reduce the scope of analysis. Caching and asynchronous processing might be considered to further optimize performance when interacting with the OpenAI API."}}', '2025-03-10 21:28:38+00', 'true', 'Servers', 'mcp-server-auto-commit'), ('945399185', 'MCPheonix', 'jmanhype/MCPheonix', 'https://github.com/jmanhype/MCPheonix', 'A simplified implementation of the Model Context Protocol (MCP) server using Elixir's Phoenix Framework.', '2025-03-09 10:30:11+00', '2025-03-10 16:38:32+00', 'Elixir', '37', '# MCPheonix

A simplified implementation of the Model Context Protocol (MCP) server using Elixir's Phoenix Framework.

## Overview

MCPheonix is an intelligent, self-healing, distributed AI event system using Model Context Protocol and Elixir's Phoenix Framework. It provides a server that implements the Model Context Protocol, allowing AI models to interact with your application data and functionality through a unified interface.

## Features

- Server-Sent Events (SSE) stream for real-time notifications
- JSON-RPC endpoint for client requests
- Simple resource system
- Event publish/subscribe mechanism
- Basic tool invocation
- Flux image generation integration
- Dart task management integration
- Extensible MCP server architecture
- Self-healing distributed architecture via Cloudflare Durable Objects
- Edge computing capabilities through Cloudflare Workers

## Self-Healing Distributed Architecture

MCPheonix implements a sophisticated self-healing system through its integration with Cloudflare Durable Objects and Workers, creating a distributed, resilient architecture that can withstand failures and ensure continuous operation.

### Key Components

- **Durable Objects**: Stateful serverless components running on Cloudflare's global edge network that maintain consistency even across failures.
- **Edge Distribution**: Critical application state is replicated across Cloudflare's global network, ensuring availability even during regional outages.
- **Automatic Recovery**: If an instance becomes unavailable, the system automatically recreates it with consistent state from durable storage.
- **Real-time Communication**: WebSocket support enables instant recovery coordination and state synchronization across the distributed system.
- **Event-Driven Architecture**: Components react to state changes through a publish/subscribe model, allowing the system to self-heal and adapt to failures.

For detailed information on the implementation, see the [Cloudflare Integration](docs/cloudflare_integration.md) documentation.

## Getting Started

### Prerequisites

- Elixir 1.14 or higher
- Erlang 25 or higher
- Phoenix 1.7.0 or higher
- Python 3.9+ (for Flux and Dart integration)
- Node.js 18+ (for Dart MCP server)
- Cloudflare account (for Durable Objects integration)

### Installation

1. Clone the repository
```bash
git clone https://github.com/yourusername/mcpheonix.git
cd mcpheonix
```

2. Install dependencies
```bash
mix deps.get
```

3. Configure the Cloudflare integration
   - Create a Cloudflare Worker using the template in `cloudflare/durable-objects-worker.js`
   - Deploy it to your Cloudflare account
   - Set the environment variables:
     - `CLOUDFLARE_WORKER_URL`: URL of your deployed worker
     - `CLOUDFLARE_ACCOUNT_ID`: Your Cloudflare account ID
     - `CLOUDFLARE_API_TOKEN`: API token with Workers and DO permissions

4. Configure the Flux integration (if using image generation)
   - Set up the Flux CLI environment as described in the [Flux Integration](docs/flux_integration.md) section

5. Configure the Dart integration (if using task management)
   - Set up the Dart MCP server as described in the [Dart Integration](docs/dart_integration.md) section

6. Start the server
```bash
mix phx.server
```

The server will be available at http://localhost:4001.

### Adding Custom MCP Servers

MCPheonix is designed to work with multiple MCP servers. This system includes a flexible infrastructure for integrating custom MCP servers through:

1. **Simple JSON Configuration**: Define your server settings in `priv/config/mcp_servers.json`:
```json
{
  "mcpServers": {
    "your_server_id": {
      "command": "/path/to/executable",
      "args": ["arg1", "arg2"],
      "env": {
        "ENV_VAR1": "value1",
        "ENV_VAR2": "value2"
      },
      "tools": {
        "your_tool": {
          "description": "Description of your tool",
          "parameters": [
            { "name": "param1", "type": "string", "description": "Parameter description" }
          ]
        }
      }
    }
  }
}
```

2. **Automatic Server Management**: Servers are automatically loaded and managed during application startup.

For comprehensive implementation details, including the Elixir architecture, server lifecycle management, and protocol handling, see the [Adding MCP Servers](docs/adding_mcp_servers.md) documentation.

## MCP Endpoints

- **SSE Stream**: `GET /mcp/stream`
  - Establishes a Server-Sent Events stream for receiving real-time notifications
  - Returns a client ID in the response headers

- **JSON-RPC**: `POST /mcp/rpc`
  - Accepts JSON-RPC 2.0 requests
  - Client ID can be provided in the `x-mcp-client-id` header or will be generated if missing

## Built-in Capabilities

### Resources
', 'true', '"{\"Protocol Implementations\",\"Agent Frameworks\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states that it's a simplified implementation of the Model Context Protocol (MCP) server using Elixir's Phoenix Framework.","categories":["Protocol Implementations","Agent Frameworks","API Frameworks"],"analysis":{"Architecture and Design Patterns":"MCPheonix uses a self-healing distributed architecture implemented via Cloudflare Durable Objects and Workers. It employs Server-Sent Events (SSE) for real-time notifications and a JSON-RPC endpoint for client requests. The system is designed to be extensible, supporting the integration of custom MCP servers through JSON configuration and automatic server management.","Implementation Details and Key Features":"The system includes a simple resource system and an event publish/subscribe mechanism. It supports basic tool invocation and offers integrations with Flux image generation and Dart task management. The core is written in Elixir using the Phoenix framework. Durable Objects ensure application state replication and automatic recovery.","Integration Approaches and Protocols":"The repository integrates with Cloudflare Durable Objects and Workers for edge computing and self-healing capabilities. It also provides integration with external services through JSON-RPC. The system utilizes SSE for real-time communication. Integration with external MCP servers is facilitated through a JSON configuration.","Technical Stack and Dependencies":"The technical stack includes Elixir 1.14+, Erlang 25+, and Phoenix 1.7.0+. It also depends on Python 3.9+ for Flux and Dart integration, Node.js 18+ for the Dart MCP server, and a Cloudflare account for Durable Objects. Dependencies are managed using Mix.","Best Practices and Coding Standards":"The repository promotes modularity through its extensible MCP server architecture. It leverages Phoenix framework conventions. Configuration is managed through JSON files, and server management is automated during application startup, promoting maintainability.  The use of Cloudflare Durable Objects suggests a focus on distributed systems best practices.","Performance Considerations":"The use of Cloudflare's edge network with Durable Objects and Workers allows for low-latency responses.  SSE is used for real-time updates.  The self-healing architecture enhances availability and reduces downtime. The Elixir/Erlang foundation supports concurrent request handling and efficient resource utilization."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'mcphoenix-mcp-server'), ('945460300', 'duckduckgo-mcp-server', 'zhsama/duckduckgo-mcp-server', 'https://github.com/zhsama/duckduckgo-mcp-server', null, '2025-03-09 13:32:08+00', '2025-03-10 05:06:06+00', 'TypeScript', '12', '# duckduckgo-search MCP Server

English | [中文](README_zh.md)

A Model Context Protocol server for DuckDuckGo Search

This is a TypeScript-based MCP server that provides DuckDuckGo search functionality. It demonstrates core MCP concepts through:

- Integration with DuckDuckGo Search
- Easy-to-use search tool interface
- Rate limiting and error handling support

## Features

### Search Tool

- `duckduckgo_search` - Perform web searches using DuckDuckGo API
  - Required parameter: `query` (search query, max 400 characters)
  - Optional parameter: `count` (number of results, 1-20, default 10)
  - Optional parameter: `safeSearch` (safety level: strict/moderate/off, default moderate)
  - Returns formatted Markdown search results

### Rate Limits

- Maximum 1 request per second
- Maximum 15000 requests per month

## Development

### Prerequisites

- Node.js >= 18
- pnpm >= 8.0.0

### Installation

```bash
# Install pnpm if not already installed
npm install -g pnpm

# Install project dependencies
pnpm install
```

### Build and Run

Build the server:

```bash
pnpm run build
```

For development with auto-rebuild:

```bash
pnpm run watch
```

## Setup in Claude Desktop

To use with Claude Desktop, add the server config:

On MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`
On Windows: `%APPDATA%/Claude/claude_desktop_config.json`

```json
# online
{
  "mcpServers": {
    "duckduckgo-search": {
        "command": "npx",
        "args": [
          "-y",
          "duckduckgo-mpc-server"
        ]
    }
  }
}

# local
{
  "mcpServers": {
    "duckduckgo-search": {
      "command": "node",
      "args": [
        "/path/to/duckduckgo-search/build/index.js"
      ]
    }
  }
}
```
![image](https://github.com/user-attachments/assets/6906e280-9dbb-4bb5-a537-d9e45e666084)
![image](https://github.com/user-attachments/assets/867a70ae-082f-45ab-a623-869bfd6c31eb)

### Debugging

Since MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:

```bash
pnpm run inspector
```

The Inspector will provide a URL to access debugging tools in your browser.
', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"Agent Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository is explicitly described as a Model Context Protocol (MCP) server for DuckDuckGo search, implementing core MCP concepts and providing a search tool interface for use with Claude Desktop.","categories":["Protocol Implementations","API Frameworks","Function Calling"],"analysis":{"Architecture and Design Patterns":"The repository implements a server that adheres to the Model Context Protocol, allowing AI models to access DuckDuckGo search functionality. The architecture appears to be straightforward, with a focus on providing a simple search tool interface. It is built with rate limiting and error handling support, suggesting a focus on reliability and responsible usage.","Implementation Details and Key Features":"The key feature is the `duckduckgo_search` tool, which takes a query and returns formatted Markdown search results. The implementation includes rate limits (1 request/second, 15000/month) to prevent abuse. The server interacts with the DuckDuckGo API to perform searches, handling parameters like query, count, and safe search level.","Integration Approaches and Protocols":"The server integrates with AI models, specifically Claude Desktop, using the Model Context Protocol (MCP). This protocol facilitates communication between the AI model and the DuckDuckGo search tool. Configuration details for Claude Desktop are provided, including how to specify the server's command and arguments.","Technical Stack and Dependencies":"The server is built using TypeScript and requires Node.js >= 18 and pnpm >= 8.0.0. Dependencies are managed by pnpm. The build process involves compiling the TypeScript code into JavaScript. The use of TypeScript suggests a focus on type safety and maintainability.","Best Practices and Coding Standards":"The presence of a build script, a watch script for development, and rate limiting suggest good development practices. The use of TypeScript helps enforce coding standards and improve code quality. The inclusion of debugging tools and an MCP Inspector also points to a focus on maintainability.","Performance Considerations":"Rate limiting is implemented to prevent abuse and ensure fair usage of the DuckDuckGo API. The server is designed to handle a limited number of requests per second and per month. The description does not explicitly mention optimization techniques for search speed, but the relatively simple architecture likely contributes to good performance."}}', '2025-03-10 21:28:31+00', 'true', 'Servers', 'duckduckgo-mcp-server'), ('945566771', 'hyper-mcp-shell', 'BigSweetPotatoStudio/hyper-mcp-shell', 'https://github.com/BigSweetPotatoStudio/hyper-mcp-shell', null, '2025-03-09 18:10:24+00', '2025-03-10 14:12:10+00', 'JavaScript', '3', '```
npx -y hyper-mcp-shell
```

## Usage

### hyperchat安装方式  [下载地址](https://github.com/BigSweetPotatoStudio/HyperChat)

![image](https://github.com/user-attachments/assets/c40feb2c-48cf-4965-b743-7ccd12f6b207)


## 普通安装
command:  npx

args:  -y hyper-mcp-shell


## 使用图片

![e679a8b4e0924b882753edd3d2392a2d](https://github.com/user-attachments/assets/981a7047-ab87-4fb3-a77a-c5d5f2a815cf)
![4fd6cf2aff7f2e13ed1993e297129db4](https://github.com/user-attachments/assets/9e1067b8-ef1c-4d7b-8d68-31053f1001d8)
![c98448b8e31e062f7fd7e6a07c9d487a](https://github.com/user-attachments/assets/d67dd8f9-4658-4e79-986a-1bbe4d855c92)
', 'true', '"{\"Agent Frameworks\",\"Function Calling\",\"Context Management\"]}', '{"is_mcp":"YES","justification":"The repository name includes \"mcp-shell,\" and the documentation references a tool called HyperChat, implying a focus on model interaction and potentially tool usage within a shell environment.","categories":["AI Model Integration","Function Calling","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The architecture appears to revolve around a shell environment that interacts with an AI model via a tool called HyperChat. The design leverages npm and command-line execution using `npx`, suggesting a modular and potentially extensible design. Image attachments imply a user interface component, possibly for interaction and visualization. The system might employ a client-server model, with the shell acting as a client to the HyperChat backend.","Implementation Details and Key Features":"The primary feature seems to be invoking HyperChat via the command line using `npx hyper-mcp-shell`. The `-y` flag suggests automatically accepting prompts, indicating a scripted or automated usage pattern. The inclusion of image attachments indicates a visual component, possibly for presenting results or enabling user interaction. The exact implementation details are unclear due to limited information.","Integration Approaches and Protocols":"The integration approach relies on command-line execution and npm for dependency management. The protocol used for communication between the shell and HyperChat is not explicitly defined but likely involves standard input/output (stdin/stdout) or inter-process communication (IPC). The use of image attachments suggests that the system supports visual output as part of the integration.","Technical Stack and Dependencies":"The technical stack includes npm, Node.js (implied by `npx`), and likely JavaScript or TypeScript. HyperChat is a key dependency, but its internal stack is unknown. The system likely depends on common command-line utilities and potentially libraries for image handling and display. The specific versions of these dependencies are not specified in the provided data.","Best Practices and Coding Standards":"There isn't sufficient information to assess coding standards or best practices. The use of npm for package management is a positive indication. However, the lack of detailed documentation makes it difficult to determine adherence to best practices related to code structure, error handling, and security. Further examination of the actual code is needed.","Performance Considerations":"Performance considerations are not explicitly addressed in the available documentation. However, the reliance on command-line execution and potentially IPC suggests that performance might be a factor, particularly if the interaction with HyperChat involves computationally intensive tasks. Optimizations might involve caching, asynchronous processing, or efficient data transfer mechanisms."}}', '2025-03-10 21:54:04+00', 'true', 'Servers', 'hyper-mcp-shell'), ('945567855', 'gmail-mcp-client-server', 'karimdabbouz/gmail-mcp-client-server', 'https://github.com/karimdabbouz/gmail-mcp-client-server', 'MCP client and server to hook up Claude to Gmail.', '2025-03-09 18:13:11+00', '2025-03-10 17:32:14+00', 'Python', '0', '


## Important: Authenticating with Gmail

Manual OAuth authentication via the browser is required to work with Gmail. To do so, run server.py once as a standalone script with the credentials.json file next to the server.py script. It will then ask you to authenticate using your browser. After authenticating, the session is saved as token.pickle in the root of this project. You can then use this pickle file to start the client and server via the client.py file.

Information on authentication with Gmail and getting the credentials.json file: https://developers.google.com/gmail/api/quickstart/python', 'true', '"{\"Function Calling\",\"Protocol Implementations\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states \"MCP client and server\" and its purpose is to connect Claude to Gmail, which aligns with the MCP concept of connecting models with external tools.","categories":["Function Calling","AI Model Integration","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The repository follows a client-server architecture where the server likely interacts with the Gmail API, and the client interfaces with the AI model (Claude). The client-server pattern allows for clear separation of concerns and scalable interaction between the AI model and the external service. This architecture facilitates interaction of Large Language Models with external services.","Implementation Details and Key Features":"The core feature is integrating Claude with Gmail. The server handles Gmail API interactions, while the client manages communication with Claude. A key implementation detail is the use of OAuth 2.0 for authentication. The storage of tokens in pickle files suggests a simple persistence mechanism for managing user sessions, but it may raise security concerns.","Integration Approaches and Protocols":"The repository employs the Gmail API to interact with Gmail. OAuth 2.0 is used for authentication, as evidenced by the documentation's reference to obtaining `credentials.json` and the creation of `token.pickle`. The integration likely involves sending and receiving email data between Claude and Gmail via API calls.","Technical Stack and Dependencies":"The repository is written in Python and depends on the Gmail API client library. The use of pickle for token storage suggests a reliance on Python's built-in serialization capabilities. The documentation highlights the need for `credentials.json`, indicating a dependency on Google Cloud Platform or a similar service for API access.","Best Practices and Coding Standards":"The documentation points to Google's Gmail API quickstart guide, indicating an adherence to Google's recommended authentication practices. However, storing tokens in pickle files, while convenient, is not generally considered a secure practice for production environments. Other coding standards are not ascertainable without access to the codebase.","Performance Considerations":"Performance considerations include API rate limits imposed by the Gmail API, especially with the client-server architecture enabling multiple requests. Efficient handling of API requests and data transfer is crucial. OAuth token refresh mechanisms should be handled correctly to maintain continuous operation. The serialization and deserialization of data could also impact performance."}}', '2025-03-10 21:26:16+00', 'true', 'Clients', 'gmail-mcp-claude'), ('945583139', 'fastapi_mcp_sse', 'panz2018/fastapi_mcp_sse', 'https://github.com/panz2018/fastapi_mcp_sse', 'A working example to create a FastAPI server with SSE-based MCP support', '2025-03-09 18:55:52+00', '2025-03-10 13:54:02+00', 'Python', '5', '# FastAPI MCP SSE

<p align="center">
  <strong>English</strong> | <a href="/README.zh-CN.md">简体中文</a>
</p>

A Server-Sent Events (SSE) implementation using FastAPI framework with [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) integration.

## What is MCP?

The [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) is an open standard that enables AI models to interact with external tools and data sources. MCP solves several key challenges in AI development:

- **Context limitations**: Allows models to access up-to-date information beyond their training data
- **Tool integration**: Provides a standardized way for models to use external tools and APIs
- **Interoperability**: Creates a common interface between different AI models and tools
- **Extensibility**: Makes it easy to add new capabilities to AI systems without retraining

This project demonstrates how to implement MCP using Server-Sent Events (SSE) in a FastAPI web application.

## Description

This project demonstrates how to implement Server-Sent Events (SSE) using the FastAPI framework while integrating Model Context Protocol (MCP) functionality. The key feature is the seamless integration of MCP's SSE capabilities within a full-featured FastAPI web application that includes custom routes.

## Features

- Server-Sent Events (SSE) implementation with MCP
- FastAPI framework integration with custom routes
- Unified web application with both MCP and standard web endpoints
- Customizable route structure
- Clean separation of concerns between MCP and web functionality

## Architecture

This project showcases a modular architecture that:

1. Integrates MCP SSE endpoints (`/sse` and `/messages/`) into a FastAPI application
2. Provides standard web routes (`/`, `/about`, `/status`, `/docs`, `/redoc`)
3. Demonstrates how to maintain separation between MCP functionality and web routes

## Installation & Usage Options

### Prerequisites

Install [UV Package Manager](https://docs.astral.sh/uv/) - A fast Python package installer written in Rust:

```cmd
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```

### Option 1: Quick Run Without Installation

Run the application directly without cloning the repository using UV's execution tool:

```cmd
uvx --from git+https://github.com/panz2018/fastapi_mcp_sse.git start
```

### Option 2: Full Installation

#### Create Virtual Environment

Create an isolated Python environment for the project:

```cmd
uv venv
```

#### Activate Virtual Environment

Activate the virtual environment to use it:

```cmd
.venv\Scripts\activate
```

#### Install Dependencies

Install all required packages:

```cmd
uv pip install -r pyproject.toml
```

#### Start the Integrated Server

Launch the integrated FastAPI server with MCP SSE functionality:

```cmd
python src/server.py
```

or

```cmd
uv run start
```

### Available Endpoints

After starting the server (using either Option 1 or Option 2), the following endpoints will be available:

- Main server: http://localhost:8000
- Standard web routes:
  - Home page: http://localhost:8000/
  - About page: http://localhost:8000/about
  - Status API: http://localhost:8000/status
  - Documentation (Swagger UI): http://localhost:8000/docs
  - Documentation (ReDoc): http://localhost:8000/redoc
- MCP SSE endpoints:
  - SSE endpoint: http://localhost:8000/sse
  - Message posting: http://localhost:8000/messages/

### Debug with MCP Inspector

For testing and debugging MCP functionality, use the MCP Inspector:

```cmd
mcp dev ./src/weather.py
```

### Connect to MCP Inspector

1. Open MCP Inspector at http://localhost:5173
2. Configure the connection:
   - Set Transport Type to `SSE`
   - Enter URL: http://localhost:8000/sse
   - Click `Connect`

### Test the Functions

1. Navigate to `Tools` section
2. Click `List Tools` to see available functions:
   - `get_alerts` : Get weather alerts
   - `get_forcast` : Get weather forecast
3. Select a function
4. Enter required parameters
5. Click `Run Tool` to execute

## Extending the Application

### Adding Custom Routes

The application structure makes it easy to add new routes using FastAPI's APIRouter:

1. Define new route handlers in routes.py using the APIRouter:

   ```python
   @router.get("/new-route")
   async def new_route():
       return {"message": "This is a new route"}
   ```

2. All routes defined with the router will be automatically included in the main application

### Customizing MCP Integration

The MCP SSE functionality is integrated in server.py through:

- Creating an SSE transport
- Setting up an SSE handler
- Adding MCP routes to the FastAPI application

## Integration with [Continue](https://www.continue.dev/)

To use this MCP server with the Continue VS Code extension, add the following configuration to your Continue settings:

```json
{
  "experimental": {
    "modelContextProtocolServers": [
      {
        "transport": {
          "name": "weather",
          "type": "sse",
          "url": "http://localhost:8000/sse"
        }
      }
    ]
  }
}
```
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository description and documentation explicitly mention Model Context Protocol (MCP) and its integration using Server-Sent Events (SSE) with FastAPI. It also mentions MCP Inspector and Continue integration.","categories":["Protocol Implementations","API Frameworks","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The architecture employs a modular design, separating MCP SSE endpoints from standard web routes within a FastAPI application. It integrates MCP SSE functionality into FastAPI using custom routes, promoting clean separation between MCP and web concerns. The project uses FastAPI's APIRouter for easy addition of new routes, improving modularity and maintainability.","Implementation Details and Key Features":"The project implements MCP using Server-Sent Events (SSE) in a FastAPI web application. Key features include SSE endpoints for MCP communication, integration with MCP Inspector for debugging, and customizable routes using FastAPI's APIRouter. It supports adding custom routes and configuring the application for use with the Continue VS Code extension.","Integration Approaches and Protocols":"The repository integrates MCP using Server-Sent Events (SSE) for real-time communication between AI models and the FastAPI server. It showcases how to connect to MCP Inspector for testing and debugging. Integration with the Continue VS Code extension is provided through a specific configuration snippet, showing integration with external tools.","Technical Stack and Dependencies":"The project relies on FastAPI for building the web application and handling HTTP requests. It uses Server-Sent Events (SSE) for MCP communication. UV is used as the package manager and environment manager. Dependencies are managed using pyproject.toml, ensuring reproducibility of the environment.","Best Practices and Coding Standards":"The repository structure separates MCP functionality from standard web routes. Documentation is provided in both English and Chinese, promoting accessibility and collaboration. Example configurations for integrating with external tools (MCP Inspector, Continue) enhance usability. The usage of UV promotes consistent environment setups.","Performance Considerations":"Using SSE allows for real-time, server-pushed updates, which can be more efficient than traditional polling. FastAPI's asynchronous capabilities provide concurrency and improve the overall responsiveness of the API. Choosing UV as the package manager is likely to improve installation speed over pip."}}', '2025-03-10 21:54:04+00', 'true', 'Servers', 'fastapi-mcp-sse'), ('945610929', 'createveai-nexus-server', 'spgoodman/createveai-nexus-server', 'https://github.com/spgoodman/createveai-nexus-server', 'Createve.AI Nexus Server: MCP and API bridge for advanced LLM and agent use', '2025-03-09 20:17:28+00', '2025-03-10 15:34:47+00', 'Python', '1', '# Createve.AI Nexus Server

A versatile server that combines REST API and Model Context Protocol (MCP) functionality in a single unified platform.

## Features

- **Dual Protocol Support**: 
  - REST API for traditional HTTP-based integration
  - MCP for AI assistant integration (via the TypeScript client server)
- **Dynamic API Loading**: Load API modules from a configurable directory
- **API Key Authentication**: Secure all endpoints with API key authentication
- **Queue System**: Handle long-running processes with a robust queue system
- **Auto-Reload**: Automatically reload when API files change
- **OpenAPI Documentation**: Interactive API documentation with Swagger UI
- **File Type Handling**: Base64 encoding/decoding for image and file types
- **Logging & Error Handling**: Comprehensive logging and structured error responses
- **State Persistence**: Queue state persistence for recovery after restart
- **Containerization**: Compatible with Docker and systemd

## Documentation

Comprehensive documentation is available in the `docs` directory:

- [Architecture Overview](docs/architecture.md) - Details of the server architecture
- [Custom API Guide](docs/custom-api-guide.md) - How to create custom API modules
- [API Usage Guide](docs/api-usage.md) - How to use the API endpoints
- [MCP Client Server](docs/mcp-server.md) - Using the TypeScript MCP client server

## Configuration

The config is stored in config.yaml. This contains settings for both the API server and MCP functionality.

```yaml
apiserver:
  host: localhost # Host to listen on
  port: 43080 # Port to listen on
  debug: true # Debug to console
  log: true # Log to file
  log_file: logs/apiserver.log # Log file path
  apis_dir: custom_apis # Directory containing API modules to load

security:
  api_keys: # API keys for authentication
    - key: "sk-apiservertest1"
      description: "API Key 1"
```

An alternative config file can be specified when launching the server with `python main.py --config ./config.yaml`

## API Definitions

The APIs are loaded from the defined custom_apis folder, each in a subfolder structure that contains an __init__.py file which will load the relevant classes from the same/sub-folders.

The API definitions are inspired by ComfyUI's custom nodes, and use the same structures for defining inputs and outputs. These APIs are exposed both as REST endpoints and MCP tools.

```python
class TextAnalyzer:
    """Text analyzer for sentiment and statistics."""
    
    CATEGORY = "text"
    
    @classmethod    
    def INPUT_TYPES(cls):
        return {
            "required": {
                "text": ("STRING", {"multiline": True}),
            },
            "optional": {
                "include_sentiment": ("BOOLEAN", {"default": True})
            }
        }
    
    RETURN_TYPES = ("DICT",)
    RETURN_NAMES = ("analysis_results",)
    FUNCTION = "analyze_text"
    
    def analyze_text(self, text, include_sentiment=True):
        # Implementation here
        return (result,)
```

## Installation

Clone this repo and run start.sh to set up a python virtual environment and start the server.

```bash
git clone https://github.com/spgoodman/createveai-nexus-server.git
cd createveai-nexus-server
chmod +x start.sh
./start.sh
```

## Running as a service

Recommended: Use docker compose to start the server as a docker container.

```bash
docker-compose up -d
```

Alternative: Use service.sh to install as a systemd service on Ubuntu.

```bash
# Install as service
./service.sh install -U user -G group

# Uninstall service
./service.sh uninstall
```

## Command Line Options

The server can be started with the following command line options:

```bash
python main.py [--config CONFIG_PATH] [--host HOST] [--port PORT] [--reload]
```

## Using the MCP Functionality

The MCP functionality allows AI assistants to interact with the server using the Model Context Protocol. This enables assistants to:

1. Discover available tools (derived from API endpoints)
2. Execute tools with parameters
3. Access resources like queue status and documentation

### TypeScript MCP Client Server

The TypeScript MCP client server provides a local interface to a remote Nexus server:

- Runs locally on the client machine
- Connects to the Nexus server via REST API
- Better security with local API key management
- See [MCP Client Server](docs/mcp-server.md) for detailed documentation

## Testing

The server includes test scripts for verifying functionality:

```bash
# Test API functionality
python test_api.py
```

## License

This project is licensed under the [Apache Licence v2](LICENCE)
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Agent Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements Model Context Protocol (MCP) functionality, enabling AI assistants to discover and execute tools derived from API endpoints. It also includes a TypeScript MCP client server for secure local interaction.","categories":["API Frameworks","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The architecture combines a REST API server with MCP functionality within a single platform. It employs a modular design, loading APIs from a configurable directory. The server supports API key authentication and a queue system for handling long-running processes. It utilizes dynamic API loading and auto-reloading features. Docker and systemd support are provided for deployment.","Implementation Details and Key Features":"Key features include dual protocol support (REST and MCP), dynamic API loading, and API key authentication. The implementation includes a queue system for asynchronous task processing, automatic reloading of API modules upon changes, and Swagger UI for interactive API documentation. File type handling (Base64 encoding/decoding) and structured logging are also implemented.","Integration Approaches and Protocols":"The repository uses a REST API for traditional HTTP-based integration, and MCP for AI assistant integration. The APIs are inspired by ComfyUI's custom nodes, defining inputs and outputs using similar structures. The TypeScript MCP client server facilitates secure communication with the server via REST, managing API keys locally.","Technical Stack and Dependencies":"The server is built using Python, leveraging frameworks for REST API development. YAML is used for configuration. It utilizes libraries for API handling, queue management, and logging. The MCP client-side component is implemented in TypeScript, suggesting a dependency on Node.js and associated packages. Docker and systemd are supported for containerization and service management, respectively.","Best Practices and Coding Standards":"The repository incorporates best practices such as API key authentication for security and comprehensive logging for debugging and monitoring. The use of a queue system contributes to system stability and responsiveness. The provision of OpenAPI documentation via Swagger UI enhances usability and discoverability. Containerization and service management scripts are provided for streamlined deployment.","Performance Considerations":"The queue system helps manage long-running processes asynchronously, improving server responsiveness. Auto-reloading of API modules minimizes downtime during development. Using a TypeScript client-server for MCP functionality can improve security by managing API keys locally. Containerization allows for consistent and scalable deployments. There are no specific performance optimization techniques explicitly mentioned in the documentation."}}', '2025-03-10 21:26:24+00', 'true', 'Servers', 'nexus-mcp-server'), ('945613474', 'mcp_starter_template', 'lmarte17/mcp_starter_template', 'https://github.com/lmarte17/mcp_starter_template', 'Starter Template for MCP development', '2025-03-09 20:25:21+00', '2025-03-10 18:38:39+00', 'TypeScript', '0', '# MCP Starter Template

A starter template for building Model Context Protocol (MCP) servers. This template provides the basic structure and boilerplate code needed to create custom MCP tools that can be integrated with AI assistants like Claude.

## What is MCP?

The Model Context Protocol (MCP) allows AI assistants to interact with external tools and services. Using this template, you can build servers that expose custom functionality to AI models, enabling them to perform actions like:

- Fetching data from APIs
- Performing calculations
- Accessing databases
- Controlling external systems
- And much more!

## Prerequisites

- Node.js (v18 or later recommended)
- npm or yarn

## Getting Started

1. Clone this repository to use as a starting point:

```bash
git clone https://github.com/lmarte17/mcp_starter_template.git
cd mcp_starter_template
```

2. **Delete the .git directory** to start with a clean Git history for your project:

```bash
# On macOS/Linux
rm -rf .git

# On Windows (Command Prompt)
rmdir /s /q .git
```

3. Initialize a new Git repository (optional):

```bash
git init
```

4. Install dependencies:

```bash
npm install
# or
yarn install
```

5. Build the TypeScript code:

```bash
npm run build
# or
yarn build
```

6. Run the server:

```bash
npm start
# or
yarn start
```

## Project Structure

- `src/index.ts` - The main entry point for your MCP server
- `src/` - Directory for your server's source code
- `build/` - Output directory for compiled code (created after building)

## Creating Your Own Tools

The template includes an example tool to help you get started. To create your own tools:

1. Open `src/index.ts`
2. Locate the tools section (marked with comments)
3. Add your own tool definitions following the example pattern:

```typescript
server.tool(
  "your_tool_name", 
  "Description of what your tool does", 
  {
    // Define parameters with zod schemas
    param1: z.string().describe("Description of parameter 1"),
    param2: z.number().describe("Description of parameter 2")
  },
  async ({ param1, param2 }) => {
    // Implement your tool logic here
    
    // Return results according to MCP protocol
    return {
      content: [
        {
          type: "text", 
          text: "Your result output here"
        }
      ]
    };
  }
);
```

## Building and Testing

After making changes to your code, you need to rebuild the project:

```bash
npm run build
# or
yarn build
```

This will compile the TypeScript code into JavaScript in the `dist/` directory.

## Connecting to AI Assistants

MCP servers can be used with compatible AI assistants. To connect your server:

1. Run your server locally
2. Set up the appropriate configuration in your AI assistant's interface
3. Use the MCP protocol to communicate between the assistant and your server

## Tips for Development

- Use TypeScript interfaces to define your API response types
- Leverage the `makeApiRequest` helper function for external API calls
- Structure your tools to be reusable and composable
- Use proper error handling for robust operation

## Resources

- [MCP Documentation](https://github.com/anthropics/model-context-protocol) - Official MCP documentation
- [Zod Documentation](https://github.com/colinhacks/zod) - For schema validation
- [TypeScript Documentation](https://www.typescriptlang.org/docs/) - For TypeScript references

## License

[MIT License](LICENSE)
', 'true', '"{\"Protocol Implementations\",\"AI Model Integration\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository is explicitly described as a starter template for building Model Context Protocol (MCP) servers, designed to enable AI assistants to interact with external tools and services.","categories":["Protocol Implementations","Agent Frameworks","Function Calling"],"analysis":{"Architecture and Design Patterns":"The repository provides a basic structure for an MCP server, outlining the main entry point and a directory structure. It uses a modular approach, enabling developers to add custom tools. The template promotes reusability and composability of tools, adhering to separation of concerns. It follows a standard server-client architecture implicitly.","Implementation Details and Key Features":"The template is implemented using Node.js and TypeScript. Key features include an example tool definition, instructions for creating custom tools using Zod schemas for parameter validation, and instructions to implement tool logic. The code demonstrates how to format results according to the MCP protocol. Error handling practices are encouraged.","Integration Approaches and Protocols":"The repository focuses on integration with AI assistants using the Model Context Protocol (MCP). It describes how to connect the server to an AI assistant's interface and use the MCP for communication. The `makeApiRequest` function (mentioned in tips) allows the server to interact with external APIs, enabling a broader range of functionalities.","Technical Stack and Dependencies":"The technical stack includes Node.js (v18 or later recommended) and TypeScript. Dependencies include npm or yarn for package management and Zod for schema validation. The use of TypeScript allows for static typing and improved code maintainability. The `build` directory stores the compiled JavaScript code.","Best Practices and Coding Standards":"The template encourages the use of TypeScript interfaces for API response types, proper error handling, and structuring tools for reusability. Code is structured logically into `src` and `build` directories to promote maintainability and separation of concerns. Instructions suggest deleting the existing `.git` directory, implying a clean project start is expected.","Performance Considerations":"While performance is not explicitly addressed, the use of Node.js implies an event-driven, non-blocking architecture, suitable for handling concurrent requests. Reusable and composable tools can help improve code efficiency. The template recommends proper error handling which is essential to maintaining stable performance under unexpected conditions."}}', '2025-03-10 21:41:32+00', 'true', 'Servers', 'mcp-starter-template'), ('945663290', 'mcp_py_sse_browser_control', 'matdmiller/mcp_py_sse_browser_control', 'https://github.com/matdmiller/mcp_py_sse_browser_control', 'This is a reference design for a MCP server that hosts a web page that connects back to the server via sse and allows Claude to execute javascript on the page.', '2025-03-09 23:20:30+00', '2025-03-10 18:23:01+00', 'Python', '0', '# Browser JavaScript Evaluator

This project provides a way to execute JavaScript code in a browser from Python using MCP (Model Control Protocol).

# <span style="color: red;">⚠️ WARNING ⚠️</span>

**This MCP server can execute arbitrary JavaScript in your browser. This can be dangerous. Be aware of the implications of this before using this plugin. Use at your own risk.**


## Architecture

The project is split into two main components:

1. **Web Server (`web_server.py`)**: 
   - Handles browser connections via Server-Sent Events (SSE)
   - Provides a REST API endpoint for executing JavaScript
   - Returns results synchronously

2. **MCP Server (`mcp_server.py`)**: 
   - Manages the lifecycle of the web server
   - Provides MCP tools for executing JavaScript
   - Communicates with the web server via REST API calls

## How It Works

1. The MCP server starts the web server during its lifecycle initialization
2. A browser connects to the web server via SSE
3. When an MCP tool is called, it makes a REST API call to the web server
4. The web server sends the JavaScript code to the browser via SSE
5. The browser executes the code and sends the result back to the web server
6. The web server returns the result to the MCP server
7. The MCP server returns the result to the caller

## Usage

### Add the MCP server to your Claude config

```json
  ...
  "browser-js-evaluator": {
      "command": "uv",
      "args": [
        "--directory",
        "/path/to/this/repo/mcp_py_sse_browser_control",
        "run",
        "browser_server.py"
      ]
    }
  }
  ...
```

When you launch Claude Desktop this will start both the MCP server and the web server. Then open a browser and navigate to http://127.0.0.1:8000 to connect to the web server.

### Using the MCP Tools

The MCP server provides two tools:

1. `execute_javascript`: Execute arbitrary JavaScript code in the browser
2. `add_numbers`: Add two numbers together using JavaScript in the browser

These tools can be called from any MCP client.

## Development

To run the web server independently (for testing):

```bash
python web_server.py
```

This will start only the web server without the MCP integration.', 'true', '"{\"Protocol Implementations\",\"Agent Frameworks\",\"Browser & Web Automation\"]}', '{"is_mcp":"YES","justification":"The repository implements an MCP server that hosts a web page, communicates via SSE, and allows a model (Claude) to execute JavaScript in the browser using MCP tools. It explicitly uses the Model Control Protocol.","categories":["Protocol Implementations","Agent Frameworks","API Frameworks"],"analysis":{"Architecture and Design Patterns":"The architecture consists of two primary components: a web server handling browser connections via SSE and providing a REST API, and an MCP server managing the web server's lifecycle and providing MCP tools. The design employs a client-server pattern with SSE for real-time communication. This approach separates the concerns of web server functionality and MCP tool provision.","Implementation Details and Key Features":"The web server (`web_server.py`) uses Server-Sent Events for browser communication and provides a REST API endpoint for JavaScript execution. The MCP server (`mcp_server.py`) manages the web server's lifecycle and offers MCP tools, such as `execute_javascript` and `add_numbers`. These tools allow executing arbitrary JavaScript code and performing calculations in the browser.","Integration Approaches and Protocols":"The project integrates browser functionality with an MCP server using Server-Sent Events for real-time updates from the server to the browser. It utilizes a REST API for communication between the MCP server and the web server. The MCP server then exposes functionalities as tools compliant with the Model Control Protocol standard.","Technical Stack and Dependencies":"The repository uses Python for both the web server and the MCP server. It leverages Server-Sent Events (SSE) for browser communication. It also appears to utilize a REST API for internal communication. The \"uv\" command in the Claude config suggests the use of the uv package installer and runner.","Best Practices and Coding Standards":"The documentation includes a warning regarding the security implications of executing arbitrary JavaScript code in the browser, highlighting a focus on security awareness. The separation of concerns between the web server and MCP server suggests adherence to modular design principles. Further assessment would require code inspection.","Performance Considerations":"The use of Server-Sent Events enables real-time communication, potentially improving the responsiveness of the system. However, executing JavaScript code in the browser adds a dependency on the browser's performance. The synchronous nature of the REST API calls could introduce bottlenecks if the JavaScript execution takes a long time."}}', '2025-03-10 21:26:08+00', 'true', 'Servers', 'mcp-py-sse-browser'), ('945671144', 'mcp', 'ibirman/mcp', 'https://github.com/ibirman/mcp', 'Repository for MCP (Multi-Cloud Platform) project', '2025-03-09 23:51:40+00', '2025-03-10 17:13:45+00', 'HTML', '0', 'Hello
', 'true', '"{\"Cloud Infrastructure\",\"Developer Tooling\",\"API Frameworks\"]}', '{"is_mcp":"NO","justification":"The repository description mentions \"Multi-Cloud Platform,\" which does not align with the Model Context Protocol focus on AI model communication and context management. No evidence of MCP is found.","categories":["Cloud Infrastructure","API Frameworks","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"Given the \"Multi-Cloud Platform\" description, the architecture likely involves distributed systems and cloud-native design patterns. API gateways, microservices, and containerization are probable elements. The design aims to provide a unified interface for interacting with services across multiple cloud providers.","Implementation Details and Key Features":"Implementation details are not readily available but could include features for managing resources and deployments across various cloud providers. API frameworks are likely used to abstract away the complexities of each cloud provider's API, offering a consistent interface to users.","Integration Approaches and Protocols":"Integration would likely involve RESTful APIs or gRPC for communication between components. The platform might leverage cloud provider-specific SDKs and APIs for resource management. Standard protocols like OAuth or JWT might be used for authentication and authorization.","Technical Stack and Dependencies":"The technical stack likely includes a programming language such as Python, Java, or Go. Dependencies could include cloud provider SDKs (AWS SDK, Azure SDK, GCP SDK), API frameworks (Flask, Spring Boot, or Gin), and containerization technologies (Docker, Kubernetes).","Best Practices and Coding Standards":"Best practices would include adherence to cloud-native principles, such as immutability and infrastructure as code. Coding standards would likely include proper error handling, logging, and security best practices. The platform may follow the twelve-factor app methodology.","Performance Considerations":"Performance considerations would involve optimizing resource utilization, minimizing latency, and ensuring scalability. Load balancing and caching strategies are likely employed. The platform might use cloud provider-specific monitoring tools to track performance metrics."}}', '2025-03-10 21:26:16+00', 'true', 'Servers', 'mcp-servers-platform'), ('945678498', 'mcp-server-airbnb', 'openbnb-org/mcp-server-airbnb', 'https://github.com/openbnb-org/mcp-server-airbnb', null, '2025-03-10 00:20:50+00', '2025-03-10 03:54:52+00', 'JavaScript', '3', '# Airbnb MCP Server
[![smithery badge](https://smithery.ai/badge/@openbnb-org/mcp-server-airbnb)](https://smithery.ai/server/@openbnb-org/mcp-server-airbnb)

MCP Server for searching Airbnb and get listing details.

## Tools

1. `airbnb_search`
   - Search for Airbnb listings
   - Required Input: `location` (string)
   - Optional Inputs:
     - `placeId` (string)
     - `checkin` (string, YYYY-MM-DD)
     - `checkout` (string, YYYY-MM-DD)
     - `adults` (number)
     - `children` (number)
     - `infants` (number)
     - `pets` (number)
     - `minPrice` (number)
     - `maxPrice` (number)
     - `cursor` (string)
     - `ignoreRobotsText` (boolean)
   - Returns: Array of listings with details like name, price, location, etc.

2. `airbnb_listing_details`
   - Get detailed information about a specific Airbnb listing
   - Required Input: `id` (string)
   - Optional Inputs:
     - `checkin` (string, YYYY-MM-DD)
     - `checkout` (string, YYYY-MM-DD)
     - `adults` (number)
     - `children` (number)
     - `infants` (number)
     - `pets` (number)
     - `ignoreRobotsText` (boolean)
   - Returns: Detailed listing information including description, host details, amenities, pricing, etc.

## Features

- Respects Airbnb's robots.txt rules
- Uses cheerio for HTML parsing
- No API key required
- Returns structured JSON data
- Reduces context load by flattening and picking data

## Setup


### Installing on Claude Desktop
Before starting make sure [Node.js](https://nodejs.org/) is installed on your desktop for `npx` to work.
1. Go to: Settings > Developer > Edit Config

2. Add the following to your `claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "airbnb": {
      "command": "npx",
      "args": [
        "-y",
        "@openbnb/mcp-server-airbnb"
      ]
    }
  }
}
```

To ignore robots.txt for all requests, use this version with `--ignore-robots-txt` args

```json
{
  "mcpServers": {
    "airbnb": {
      "command": "npx",
      "args": [
        "-y",
        "@openbnb/mcp-server-airbnb",
        "--ignore-robots-txt"
      ]
    }
  }
}
```
3. Restart Claude Desktop and plan your next trip that include Airbnbs!

### Other Option: Installing via Smithery

To install mcp-server-airbnb for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@openbnb-org/mcp-server-airbnb):

```bash
npx -y @smithery/cli install @openbnb-org/mcp-server-airbnb --client claude
```

## Build (for devs)

```bash
npm install
npm run build
```

## License

This MCP server is licensed under the MIT License.

## Disclaimer

Airbnb is a trademark of Airbnb, Inc.
OpenBnB is not related to Airbnb, Inc. or its subsidiaries
', 'true', '"{\"Function Calling\",\"API Frameworks\",\"Plugin Ecosystems\"]}', '{"is_mcp":"YES","justification":"The repository is explicitly described as an MCP server for Airbnb, providing tool definitions and installation instructions for Claude Desktop. It facilitates communication between the AI model and the Airbnb search context.","categories":["Function Calling","API Frameworks","Agent Frameworks"],"analysis":{"Architecture and Design Patterns":"The repository implements a server designed to expose Airbnb search functionalities as tools for an AI model. It utilizes a command-line interface and configuration files to integrate with platforms like Claude Desktop. The server's architecture focuses on providing structured JSON responses to facilitate easy consumption by AI models, ensuring seamless integration and data handling.","Implementation Details and Key Features":"The implementation leverages `cheerio` for HTML parsing, allowing the extraction of data from Airbnb's website. Key features include respecting `robots.txt` rules, returning structured JSON data, and reducing context load by flattening and picking relevant data. The server provides two main tools: `airbnb_search` and `airbnb_listing_details`, enabling the AI model to search and retrieve details of Airbnb listings.","Integration Approaches and Protocols":"The repository provides explicit instructions on integrating with Claude Desktop through configuration files (`claude_desktop_config.json`). It utilizes a command-line interface with `npx` to run the server, enabling communication between the AI model and the Airbnb search functionalities. The server's API exposes functionalities as tools callable by AI models.","Technical Stack and Dependencies":"The technical stack includes Node.js for server-side execution and `cheerio` for HTML parsing. Dependencies are managed via `npm`. The server uses `npx` to install and run, simplifying the deployment process. The use of `npx` allows execution without global installation, streamlining the setup for users.","Best Practices and Coding Standards":"The repository follows best practices by respecting Airbnb's `robots.txt` rules, ensuring ethical scraping practices. It returns data in structured JSON format, which is a standard for API communication. It employs data flattening and picking to reduce context load, enhancing performance. Clear installation instructions and a MIT license indicate good project management.","Performance Considerations":"Performance is considered by reducing context load through data flattening and picking, which minimizes the amount of data sent to the AI model. By respecting `robots.txt`, the server avoids overloading Airbnb's servers, ensuring sustainable usage. The use of `cheerio` optimizes HTML parsing, enhancing extraction speed and efficiency."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'mcp-server-airbnb'), ('945694930', 'mcp-client-server', 'willccbb/mcp-client-server', 'https://github.com/willccbb/mcp-client-server', 'An MCP Server that's also an MCP Client. Useful for letting Claude develop and test MCPs without needing to reset the application.', '2025-03-10 01:22:06+00', '2025-03-10 17:35:23+00', 'TypeScript', '98', '# MCP Test Client

An MCP middleware that acts as both a server (to Claude) and a client (to servers under test) for testing MCP servers during development.

## Architecture

The MCP Test Client has a dual role:
- It's a **server** registered with Claude that exposes tools for testing
- It's a **client** that connects to and tests other MCP servers

```
┌─────────────┐          ┌───────────────────┐          ┌────────────────┐
│             │  Tools   │                   │  Client  │                │
│   Claude    │─────────>│  MCP Test Client  │─────────>│  Server Under  │
│             │          │                   │          │     Test       │
└─────────────┘          └───────────────────┘          └────────────────┘
```

This architecture lets you test MCP servers without registering them directly with Claude.

## Features

- Deploy MCP servers to test environments
- Call individual tools with custom arguments
- Run automated test suites
- View server logs
- Test servers before formal registration with Claude

## Implementation

The MCP Test Client is implemented with:

- **Process Management**: Spawns and manages MCP server processes
- **MCP SDK Client**: Uses the official MCP SDK to communicate with servers
- **Custom Transport**: Implements a custom transport for stdio communication
- **Test Execution**: Runs tests and validates responses
- **CLI Interface**: Provides an interactive testing interface

The current implementation is Phase 1 of the design plan, with future enhancements planned for Phases 2 and 3.

## Installation

```bash
# Install dependencies
npm install

# Build the TypeScript project
npm run build
```

## Usage

### As an MCP Server

The MCP Test Client is registered with Claude via the `claude-mcp-local` script. You can use the following tools:

1. Deploy a server:
```typescript
mcp__mcp-test__mcp_test_deploy_server({
  name: "my-server",
  source_path: "/path/to/server",
  env_vars: {
    "API_KEY": "${API_KEY}"
  }
})
```

2. Call a tool on a deployed server:
```typescript
mcp__mcp-test__mcp_test_call_tool({
  server_name: "my-server",
  tool_name: "tool_name",
  arguments: {
    // Tool-specific arguments
  }
})
```

3. Run tests against a server:
```typescript
mcp__mcp-test__mcp_test_run_tests({
  server_name: "my-server"
})
```

4. View server logs:
```typescript
mcp__mcp-test__mcp_test_get_logs({
  server_name: "my-server",
  lines: 100
})
```

5. List deployed servers:
```typescript
mcp__mcp-test__mcp_test_list_servers({})
```

6. Stop a server:
```typescript
mcp__mcp-test__mcp_test_stop_server({
  server_name: "my-server"
})
```

### As a CLI Tool

Run the CLI interface for testing:

```bash
# Use npm script
npm run test

# Or run directly
node dist/test-runner.js
```

This provides an interactive menu for deploying, testing, and managing MCP servers.

## Development Workflow

The MCP Test Client supports this workflow:

1. Develop an MCP server in the playground directory
2. Deploy it to the test environment with MCP Test Client
3. Test functionality, call individual tools, and debug issues
4. Fix and iterate until the server works correctly
5. Migrate the server to mcp-servers/ when ready
6. Register with Claude through claude-mcp-local

## Future Enhancements

Planned enhancements include:

- **Phase 2**: Docker-based container management, comprehensive test suites
- **Phase 3**: Migration tools, more advanced test validation

See `notes/mcp_test_client_design.md` for the complete design document.
', 'true', '"{\"Protocol Implementations\",\"Developer Tooling\",\"AI Testing Frameworks\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements an MCP server and client for testing purposes. It interacts with Claude using MCP, deploys test servers, calls tools, and runs tests, directly relating to MCP specifications.","categories":["AI Model Integration","API Frameworks","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The repository adopts a dual role as both an MCP server and client, designed for testing MCP servers during development. It utilizes a clear architecture with distinct components for process management, client communication, and test execution. The design includes modularity to easily manage interactions and future enhancements, with phased implementation planned.","Implementation Details and Key Features":"Implemented using process management for spawning servers, an MCP SDK client for communication, and custom transport over stdio. Key features include the ability to deploy servers, call tools with custom arguments, run test suites, and view server logs. The interactive CLI interface offers a convenient way to manage and test MCP servers.","Integration Approaches and Protocols":"The repository integrates with Claude using the MCP protocol, enabling deployment and testing of servers. It employs a custom transport mechanism for stdio communication. Integration points involve deploying servers, calling tools, and running tests, as demonstrated in the provided code snippets for interacting with the test client.","Technical Stack and Dependencies":"The technical stack includes TypeScript, indicated by the `npm run build` command, along with `npm` for managing dependencies. The usage of the official MCP SDK is a core dependency. The repository utilizes a custom transport mechanism implying further internal dependencies related to stdio communication handling.","Best Practices and Coding Standards":"The presence of a design document (`notes/mcp_test_client_design.md`) indicates some attention to design and planning. The modular architecture and phased implementation suggest an intention to follow good coding practices. The TypeScript implementation implies the use of static typing for maintainability and reducing errors.","Performance Considerations":"The use of stdio for communication might introduce performance limitations compared to other transport methods. Process management for server spawning could also impact performance depending on the number and complexity of servers. Future enhancements in Docker-based container management may address some performance concerns by improving resource utilization and isolation."}}', '2025-03-10 21:28:27+00', 'true', 'Clients', 'mcp-client-server-dev'), ('945734901', 'mcprotocol', 'nobodywasishere/mcprotocol', 'https://github.com/nobodywasishere/mcprotocol', 'LSProtocol but for MCP', '2025-03-10 03:26:31+00', '2025-03-10 15:39:02+00', 'Crystal', '1', '# mcprotocol

TODO: Write a description here

## Installation

1. Add the dependency to your `shard.yml`:

   ```yaml
   dependencies:
     mcprotocol:
       github: nobodywasishere/mcprotocol
   ```

2. Run `shards install`

## Usage

```crystal
require "mcprotocol"
```

TODO: Write usage instructions here

## Development

TODO: Write development instructions here

## Contributing

1. Fork it (<https://github.com/nobodywasishere/mcprotocol/fork>)
2. Create your feature branch (`git checkout -b my-new-feature`)
3. Commit your changes (`git commit -am 'Add some feature'`)
4. Push to the branch (`git push origin my-new-feature`)
5. Create a new Pull Request

## Contributors

- [Margret Riegert](https://github.com/nobodywasishere) - creator and maintainer
', 'true', '"{\"Not Categorized\"]}', '{"is_mcp":"YES","justification":"The repository name and description explicitly mention Model Context Protocol (MCP), suggesting direct implementation or relation. The description states 'LSProtocol but for MCP'.","categories":["Protocol Implementations","AI Model Integration","Context Management"],"analysis":{"Architecture and Design Patterns":"The documentation provides minimal architectural details. Given the name 'mcprotocol', it likely implements a client-server communication pattern or message passing system designed to facilitate interactions conforming to Model Context Protocol specifications. Further investigation of implementation would be needed to confirm specific architectural patterns.","Implementation Details and Key Features":"The provided documentation focuses on installation and contribution guidelines. Actual implementation details and key features related to MCP are currently absent in this preliminary documentation. The presence of 'require \"mcprotocol\"' implies a library or framework structure.","Integration Approaches and Protocols":"Based on the description, the repository aims to implement a protocol similar to LSProtocol, but adapted for MCP. The absence of specific integration examples makes it difficult to detail concrete integration approaches. Further analysis is necessary to determine what protocols are involved.","Technical Stack and Dependencies":"The repository uses Crystal, as indicated by the `shard.yml` file. This file also suggests the use of `shards` for dependency management. Other dependencies are not explicitly mentioned in the current documentation and need to be discovered by analyzing the code base.","Best Practices and Coding Standards":"The repository includes standard contribution guidelines encouraging forking, branch creation, and pull requests. No specific coding standards or best practices are explicitly documented beyond the usual collaborative workflow on GitHub. Further review of the code would be needed to assess the quality.","Performance Considerations":"There is no information in the available documentation regarding performance considerations. Performance optimization strategies, memory management techniques, or scalability aspects are not yet addressed. Further investigation of the implementation would be required to understand the approach to performance."}}', '2025-03-10 21:26:24+00', 'true', 'Servers', 'mcprotocol-for-mcp'), ('945753194', 'mcp-turso', 'nbbaier/mcp-turso', 'https://github.com/nbbaier/mcp-turso', 'MCP server for interacting with Turso-hosted LibSQL databases', '2025-03-10 04:22:01+00', '2025-03-10 17:21:11+00', 'TypeScript', '3', '# mcp-turso

A Model Context Protocol (MCP) server that provides access to the Turso-hosted LibSQL databases. Currently, the server provides the following functionality:

-  Retrieving a list of tables in a database
-  Retrieview the database schema
-  Retrieving the schema of a table
-  Performing SELECT queries

## Configuration

### With Claude Desktop

Add this to your `claude_desktop_config.json`:

```json
{
   "mcpServers": [
      "turso": {
         "command": "npx",
         "args": ["-y", "mcp-turso"],
         "env": {
            "TURSO_DATABASE_URL": "your_url",
            "TURSO_AUTH_TOKEN": "your_token"
         }
      }
   ]
}
```

You will need an existing database to continue. If you don’t have one, [create one](https://docs.turso.tech/quickstart). To get the database URL via the Turso CLI, run:

```bash
turso db show --url <database-name>
```

Then get the database authentication token:

```bash
turso db tokens create <database-name>
```

Add those values to your configuration as shown above.

## Server Capabilities

The server provides the following tools:

-  `list_tables`
   -  Get a list of all the tables in the database
   -  No input
   -  Returns: an array of table names
-  `get_db_schema`
   -  Get the schemas of all tables in the database
   -  No input
   -  Returns: an array of SQL creation statements
-  `describe_table`

   -  View schema information for a specific table

   -  Input: - `table_name` (string): Name of table to describe
   -  Returns: Array of column definitions with names and types

-  `query`
   -  Execute a SELECT query to read data from the database
   -  Input:
      -  `query` (string): The SELECT SQL query to execute
   -  Returns: Query results as an object of type `{ columns: string[]; rows: Record<string, unknown>[]; rowCount: number; }`

## License

MIT License - see the [LICENSE](LICENSE) file for details.
', 'true', '"{\"Protocol Implementations\",\"Database Systems\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states that it is an MCP server and provides functionality like listing tables, retrieving schemas, and executing SELECT queries on a Turso-hosted LibSQL database.","categories":["Protocol Implementations","API Frameworks","Database Systems"],"analysis":{"Architecture and Design Patterns":"The repository implements a server that acts as an intermediary between AI models and a Turso database. It likely follows a client-server architecture, exposing an API that adheres to the Model Context Protocol. The design patterns employed probably include request-response and potentially a data mapper pattern for translating database results.","Implementation Details and Key Features":"The server provides tools for retrieving database schemas, listing tables, and executing SELECT queries. It uses environment variables for configuration, including the database URL and authentication token. The `query` tool allows executing arbitrary SQL queries. The server exposes a set of functions tailored for LLMs to interact with the Turso database.","Integration Approaches and Protocols":"The repository integrates with Claude Desktop using a defined configuration structure. It communicates with the Turso database using LibSQL. The primary integration point is the Model Context Protocol, enabling AI models to interact with the database through a standardized interface, facilitating tool use by the LLM.","Technical Stack and Dependencies":"Based on the description, the repository likely uses Node.js and depends on packages for interacting with Turso/LibSQL databases and potentially an HTTP server framework like Express.js or similar. The use of 'npx' suggests that the code is written in JavaScript or TypeScript.","Best Practices and Coding Standards":"The provided configuration examples use JSON format for configurations, which indicates adherence to standard data representation practices. The repository is MIT licensed, which is a widely used permissive open-source license. The documentation is clear and concise, outlining setup steps and tool usage.","Performance Considerations":"Performance considerations might include optimizing SQL query execution, caching database schemas, and managing database connections efficiently. The network latency between the MCP server and the Turso database could impact performance. Input validation and sanitization of SQL queries is essential to prevent injection attacks and ensure stability."}}', '2025-03-10 21:26:16+00', 'true', 'Servers', 'mcp-turso-server'), ('945821830', 'linkedin-mcp-server', 'Dishant27/linkedin-mcp-server', 'https://github.com/Dishant27/linkedin-mcp-server', 'Model Context Protocol (MCP) server for LinkedIn API integration', '2025-03-10 07:18:59+00', '2025-03-10 16:32:19+00', 'TypeScript', '3', '# LinkedInMCP: Revolutionizing LinkedIn API Interactions

## 🚀 Project Background

**LinkedInMCP** is an innovative Model Context Protocol (MCP) server designed to transform how developers interact with LinkedIn's API. Born from the need for more flexible and powerful LinkedIn data integration, this project provides a robust, extensible framework for advanced LinkedIn data retrieval and interaction.

## 🌟 Project Vision

Recognizing the limitations of traditional LinkedIn API approaches, this solution creates a more intelligent, context-aware method of working with professional network data.

## ✨ Key Innovations

- **Intelligent Context Management**: Leverage MCP's advanced context handling
- **Secure, Typed Interactions**: Type-safe LinkedIn API interactions
- **Extensible Architecture**: Easy to expand and customize
- **Standardized Protocol**: Follows Model Context Protocol specifications

## 🛠️ Technologies

- TypeScript
- Model Context Protocol
- LinkedIn API
- OAuth 2.0
- Zod for Type Validation
- Axios for HTTP Requests

## 🔍 Core Capabilities

- Advanced People Search
- Detailed Profile Retrieval
- Job Market Intelligence
- Messaging Capabilities
- Secure Authentication Workflow

## 🚀 Comprehensive Setup Guide

### Prerequisites
- Node.js 16+
- LinkedIn Developer Account
- MCP-Compatible LLM (Claude, OpenAI, etc.)

### 1. LinkedIn Developer Setup

1. Go to [LinkedIn Developer Portal](https://www.linkedin.com/developers/)
2. Create a new application
3. Obtain Client ID and Client Secret

### 2. Project Installation

```bash
git clone https://github.com/Dishant27/linkedin-mcp-server.git
cd linkedin-mcp-server
npm install
```

### 3. Configuration Files

#### `.env` File
Create a `.env` file in the project root:
```
LINKEDIN_CLIENT_ID=your_client_id_here
LINKEDIN_CLIENT_SECRET=your_client_secret_here
```

#### `claude_desktop_config.json` (for Claude Desktop)
```json
{
  "mcpServers": {
    "linkedin": {
      "command": "node",
      "args": ["/absolute/path/to/linkedin-mcp-server/dist/index.js"],
      "env": {
        "LINKEDIN_CLIENT_ID": "your_client_id",
        "LINKEDIN_CLIENT_SECRET": "your_client_secret"
      }
    }
  }
}
```

### 4. LLM Configuration Example

#### Claude.ai / Claude Desktop MCP Integration

```json
{
  "tools": [
    {
      "name": "search-people",
      "description": "Search for LinkedIn profiles",
      "parameters": {
        "type": "object",
        "properties": {
          "keywords": {
            "type": "string",
            "description": "Keywords to search for in profiles"
          },
          "currentCompany": {
            "type": "array",
            "items": {"type": "string"},
            "description": "Filter by current company"
          },
          "industries": {
            "type": "array",
            "items": {"type": "string"},
            "description": "Filter by industries"
          },
          "location": {
            "type": "string", 
            "description": "Filter by location"
          }
        }
      }
    },
    {
      "name": "get-profile",
      "description": "Retrieve detailed LinkedIn profile information",
      "parameters": {
        "type": "object",
        "properties": {
          "publicId": {
            "type": "string",
            "description": "Public ID of the LinkedIn profile"
          },
          "urnId": {
            "type": "string", 
            "description": "URN ID of the LinkedIn profile"
          }
        }
      }
    }
  ]
}
```

### 5. Typical Workflow

1. Start the MCP Server:
```bash
npm run build
npm start
```

2. Example LLM Interactions:
```
> Find software engineers in San Francisco working at tech companies

# The LLM will use the search-people tool to:
# 1. Search LinkedIn profiles
# 2. Filter by location (San Francisco)
# 3. Filter by industry (Technology)
# 4. Return relevant profile details
```

## 📦 Generated Project Structure

```
linkedin-mcp-server/
│
├── src/
│   ├── index.ts         # Main server entry point
│   ├── auth.ts          # LinkedIn authentication handler
│   └── client.ts        # LinkedIn API interaction client
│
├── dist/                # Compiled JavaScript files
│   ├── index.js
│   ├── auth.js
│   └── client.js
│
├── .env                 # Secret environment variables
├── .env.example         # Template for environment variables
├── package.json         # Project dependencies and scripts
├── tsconfig.json        # TypeScript configuration
└── README.md            # Project documentation
```

## 🤝 Contributing

Inspired by the project? Contributions are welcome!

1. Fork the repository
2. Create your feature branch
3. Commit your changes
4. Push to the branch
5. Open a Pull Request

## 📄 License

Distributed under the MIT License.

---

**Disclaimer**: This project is an independent innovation and is not officially affiliated with LinkedIn or Microsoft.
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository implements a Model Context Protocol (MCP) server to interact with the LinkedIn API. It supports context management and function calling, crucial aspects of MCP. The documentation explicitly mentions 'Model Context Protocol'.","categories":["Protocol Implementations","API Frameworks","Function Calling"],"analysis":{"Architecture and Design Patterns":"The architecture follows a client-server model. The server, built with TypeScript, exposes endpoints that adhere to the Model Context Protocol. Key components include authentication handling (OAuth 2.0), API interaction clients, and a main entry point. The design emphasizes modularity and extensibility, facilitating customization and expansion of capabilities.","Implementation Details and Key Features":"Implementation focuses on enabling LLMs to interact with LinkedIn's API using defined functions. Key features include advanced people search, profile retrieval, and messaging capabilities. The server uses Zod for type validation and Axios for HTTP requests. The server provides a secure and typed interaction layer on top of the LinkedIn API.","Integration Approaches and Protocols":"The repository utilizes MCP for LLM integration, enabling function calling. LLMs like Claude.ai can be configured to use tools exposed by the server, such as 'search-people' and 'get-profile'. The integration uses JSON schemas to define tool parameters, ensuring structured communication between the LLM and the MCP server.","Technical Stack and Dependencies":"The technical stack consists of TypeScript, Node.js, LinkedIn API, OAuth 2.0, Zod, and Axios. TypeScript provides type safety, while Node.js serves as the runtime environment. OAuth 2.0 handles authentication, and Axios manages HTTP requests to the LinkedIn API. Zod is used for schema validation.","Best Practices and Coding Standards":"The repository employs TypeScript, promoting type safety and code maintainability. The project structure is well-organized, with clear separation of concerns. Environment variables are used to manage sensitive data like API keys. The use of Zod for validation ensures that data conforms to expected schemas before processing.","Performance Considerations":"The repository's performance depends on the efficiency of the LinkedIn API and the server's ability to handle concurrent requests. Caching strategies and optimized API calls could improve performance. Network latency and API rate limits are potential bottlenecks. Efficient data serialization/deserialization is also crucial."}}', '2025-03-10 21:54:04+00', 'true', 'Servers', 'linkedin-mcp-server'), ('945822749', 'mcp_generate_images', 'chenyeju295/mcp_generate_images', 'https://github.com/chenyeju295/mcp_generate_images', '可用于cursor 集成 mcp server ', '2025-03-10 07:21:04+00', '2025-03-10 12:12:45+00', 'Python', '4', '# AI 图像生成服务

基于 Together AI 的图像生成服务，专门设计用于与 Cursor MCP 服务集成。支持自定义图片大小、保存路径等功能。

## 功能特点

- 支持高质量图像生成
- 自动重试和错误处理
- 支持批量生成多张图片
- 完整的路径和权限验证
- 详细的错误提示
- 异步处理支持

## 环境准备

### 1. Python 环境

- Python 3.10+ 
- 下载地址： https://www.python.org/downloads/

- 推荐使用 pyenv 管理 Python 版本：
```bash
# macOS 安装 pyenv
brew install pyenv

# 安装 Python
pyenv install 3.13.2
pyenv global 3.13.2

```
- Nodejs 环境

- 下载地址： https://nodejs.org/zh-cn  

### 2. uv 包管理工具
uv 是一个快速的 Python 包管理器，需要先安装：

```bash
# macOS 安装 uv
brew install uv

# 或者使用 pip 安装
pip install uv
```

### 3. Together AI API 密钥
1. 访问 [Together AI API Keys](https://api.together.ai/settings/api-keys)
2. 注册/登录账号
3. 创建新的 API 密钥
4. 复制密钥并保存，格式如：`YOUR_API_KEY`

### 4. Cursor 
- 下载并安装 [Cursor IDE](https://cursor.sh/)
- 确保 Cursor 已正确配置 Python 环境

## 安装配置

1. 克隆项目：
```bash
git clone [https://github.com/chenyeju295/mcp_generate_images.git]

```

2. 安装依赖：
```bash
python3 -m pip install fastmcp requests
```
出现证书问题可以使用：
```bash
python3 -m pip install fastmcp requests --trusted-host pypi.org --trusted-host files.pythonhosted.org --upgrade --force-reinstall --no-cache-dir
```
3. 配置 API 密钥：

在 `mcp_server.py` 中修改 `TOGETHER_API_KEY`：
```python
TOGETHER_API_KEY = "your_api_key_here"  # 替换为你的 Together AI API 密钥
```

4. 配置服务：

在 `mcp_server.py` 中可以修改以下配置：

```python
CONFIG = {
    "api": {
        "url": "https://api.together.xyz/v1/images/generations",
        "model": "black-forest-labs/FLUX.1-schnell-Free",
        "timeout": 30,
        "max_retries": 3,
        "retry_delay": 5
    },
    "image": {
        "max_width": 1024,
        "max_height": 1024,
        "default_width": 1024,
        "default_height": 1024,
        "default_steps": 2,
        "max_batch_size": 4
    },
    "output": {
        "base_folder": "你的默认保存路径",
        "allowed_extensions": [".png", ".jpg", ".jpeg"],
        "default_extension": ".png"
    }
}
```

## 运行服务

1. 开发模式运行（带调试界面）：
- 运行检查是否成功:
```bash
uv run --with fastmcp fastmcp dev /Users/username/Documents/mcp_generate_images/mcp_server.py
```
   
2. 生产模式运行：
```bash
uv run --with fastmcp fastmcp run /Users/username/Documents/mcp_generate_images/mcp_server.py
```

3. 如果端口被占用，可以指定其他端口：
```bash
PORT=5174 uv run --with fastmcp fastmcp dev /Users/username/Documents/mcp_generate_images/mcp_server.py
```

## 使用说明

### 在 Cursor IDE 中使用

1. 确保服务正在运行
2. 在 Cursor 中引入MCP：
- 添加mcp 服务：

```bash
uv run --with fastmcp fastmcp run /Users/username/Documents/mcp_generate_images/mcp_server.py
```
- 如图（运行成功）：
![image.png](./images/image.png)

3. 在 Cursor 中使用：
- 在composer 的 agent 模式下，直接输入相关的提示：
![image.png](./images/image_2.png)


## 错误排查

如果遇到问题，请检查：

1. 服务是否正常运行
2. 保存路径是否正确（必须是绝对路径）
3. 目录权限是否正确
4. 网络连接是否正常
5. API 密钥是否有效
6. Python 环境是否正确配置
7. uv 是否正确安装
8. 依赖包是否完整安装
', 'true', '"{\"AI Model Integration\",\"Function Calling\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states integration with Cursor MCP service and utilizes the fastmcp library, indicating implementation and interaction with Model Context Protocol specifications.","categories":["AI Model Integration","Function Calling","Plugin Ecosystems"],"analysis":{"Architecture and Design Patterns":"The repository implements a server that interfaces with the Together AI image generation API. The server appears to expose functionality that can be triggered by the Cursor IDE via MCP. Configuration is managed through a dictionary allowing for adjustments to the API endpoint, image parameters, and output settings. It supports both development and production modes.","Implementation Details and Key Features":"The core implementation involves making API calls to Together AI's image generation endpoint based on specified configurations. Key features include handling image size, saving images to a specified path with path validation, error handling (retries, exceptions), asynchronous processing, and batch image generation. The repository uses `fastmcp` to enable MCP integration.","Integration Approaches and Protocols":"The primary integration approach is through the Model Context Protocol (MCP), facilitated by the `fastmcp` library. This enables the Cursor IDE to invoke the image generation service as a plugin. The service exposes specific functions or endpoints that Cursor can call to generate images based on user prompts or other relevant inputs.","Technical Stack and Dependencies":"The repository relies on Python 3.10+, along with the `requests` library for making HTTP requests to the Together AI API and `fastmcp` for MCP integration. It also utilizes `uv` as the package manager. The image generation itself depends on the Together AI API and its associated models. It also supports NodeJS environment.","Best Practices and Coding Standards":"The documentation suggests using `pyenv` for managing Python versions. The configuration is done via a clearly defined dictionary. Error handling with retry mechanisms is implemented.  The repository provides clear instructions for setting up the environment, installing dependencies, and running the service in different modes. However, further code inspection is needed to assess adherence to general Python coding standards.","Performance Considerations":"The service includes retry mechanisms and timeout settings to handle potential API issues. The configuration includes parameters like `max_batch_size` which can affect performance. Asynchronous processing is supported but the details need more investigation. Image dimensions and the number of steps used in image generation will significantly affect processing time."}}', '2025-03-10 21:54:07+00', 'true', 'Servers', 'mcp-generate-images'), ('945846068', 'nix-mcp-server', 'aloshy-ai/nix-mcp-server', 'https://github.com/aloshy-ai/nix-mcp-server', 'Nix flake for MCP Server (AI) daemon for Linux and MacOS', '2025-03-10 08:12:27+00', '2025-03-10 17:29:23+00', 'Nix', '0', '# nix-inkustrator

[![](https://img.shields.io/badge/aloshy.🅰🅸-000000.svg?style=for-the-badge)](https://aloshy.ai)
[![Powered By Nix](https://img.shields.io/badge/NIX-POWERED-5277C3.svg?style=for-the-badge&logo=nixos)](https://nixos.org)
[![Platform](https://img.shields.io/badge/MACOS-ONLY-000000.svg?style=for-the-badge&logo=apple)](https://github.com/aloshy-ai/nix-inkustrator)
[![Build Status](https://img.shields.io/badge/BUILD-PASSING-success.svg?style=for-the-badge&logo=github)](https://github.com/aloshy-ai/nix-inkustrator/actions)
[![License](https://img.shields.io/badge/LICENSE-GPL3-yellow.svg?style=for-the-badge)](https://www.gnu.org/licenses/gpl-3.0.en.html)

A Nix package that provides Inkustrator customizations for Inkscape on macOS, making it more familiar to Adobe Illustrator users.

## Features

- Installs and configures Inkscape with Inkustrator customizations
- Provides an Illustrator-like interface and keyboard shortcuts
- Creates a proper macOS application bundle
- Supports both Apple Silicon and Intel Macs
- Integrates with Nix, NixOS, and nix-darwin

## Installation

### Using nix-darwin

Add the following to your `flake.nix`:

```nix
{
  inputs = {
    nix-inkustrator.url = "github:aloshy-ai/nix-inkustrator";
  };

  outputs = { self, darwin, nix-inkustrator, ... }: {
    darwinConfigurations."your-hostname" = darwin.lib.darwinSystem {
      modules = [
        nix-inkustrator.darwinModules.default
        {
          programs.inkustrator.enable = true;
        }
      ];
    };
  };
}
```

### Using Home Manager

Add the following to your Home Manager configuration:

```nix
{
  inputs = {
    nix-inkustrator.url = "github:aloshy-ai/nix-inkustrator";
  };

  outputs = { self, home-manager, nix-inkustrator, ... }: {
    homeConfigurations."your-username" = home-manager.lib.homeManagerConfiguration {
      modules = [
        nix-inkustrator.homeManagerModules.default
        {
          programs.inkustrator.enable = true;
        }
      ];
    };
  };
}
```

### Direct Installation

You can also install it directly using:

```bash
nix profile install github:aloshy-ai/nix-inkustrator
```

## Usage

After installation:

1. Launch Inkustrator from your Applications folder
2. The interface will be customized to match Adobe Illustrator's layout
3. Keyboard shortcuts will match Illustrator's defaults
4. Your settings will be preserved between updates

## Credits

- Original Inkustrator project by [Lucas Gabriel Moreno](https://github.com/lucasgabmoreno/inkustrator)
- Nix packaging by [aloshy-ai](https://github.com/aloshy-ai)

## License

This project is licensed under the GPL-3.0 License - see the original [Inkustrator repository](https://github.com/lucasgabmoreno/inkustrator) for details. ', 'true', '"{\"Developer Tooling\",\"System Automation\",\"Workflow Orchestration\"]}', '{"is_mcp":"NO","justification":"The repository provides a Nix package for Inkustrator, a customization for Inkscape. It does not appear to implement or relate to Model Context Protocol (MCP) specifications or involve AI models.","categories":["Developer Tooling","System Automation","Workflow Orchestration"],"analysis":{"Architecture and Design Patterns":"The repository uses a Nix flake to package and configure Inkustrator, a customization for Inkscape. It leverages Nix's declarative package management to ensure reproducible builds and deployments across different environments, including macOS. The architecture focuses on modularity, allowing users to easily enable or disable Inkustrator through Nix modules.","Implementation Details and Key Features":"The implementation involves creating a macOS application bundle for Inkscape with Inkustrator customizations. Key features include Illustrator-like interface and keyboard shortcuts for Inkscape, support for both Apple Silicon and Intel Macs, and seamless integration with Nix, NixOS, and nix-darwin. The flake provides Nix modules for nix-darwin and Home Manager for easy integration.","Integration Approaches and Protocols":"The repository integrates with Nix, nix-darwin, and Home Manager to manage Inkustrator's installation and configuration. It utilizes Nix's module system to define Inkustrator's settings and dependencies. Integration is achieved through declarative configuration files, allowing users to specify their desired state. No specific communication protocols are directly implemented.","Technical Stack and Dependencies":"The technical stack primarily consists of Nix and Inkscape. It relies on Nix for package management, build automation, and environment configuration. Key dependencies include Inkscape and its associated libraries, as well as Inkustrator's customization files. The repository uses Nix flakes to manage dependencies and ensure reproducibility.","Best Practices and Coding Standards":"The repository adheres to Nix's declarative configuration principles, promoting reproducibility and consistency. It utilizes Nix modules to encapsulate Inkustrator's configuration, making it easy to enable or disable. The project follows standard Nix conventions for flake structure and module definitions, enhancing maintainability.","Performance Considerations":"Performance considerations are primarily related to Inkscape's performance and the efficiency of the Nix build process. The repository aims to minimize build times by leveraging Nix's caching mechanisms. Optimizations focus on ensuring that Inkustrator's customizations do not negatively impact Inkscape's performance."}}', '2025-03-10 21:26:16+00', 'true', 'Servers', 'nix-mcp-server-flake'), ('945904536', 'adx-mcp-server', 'pab1it0/adx-mcp-server', 'https://github.com/pab1it0/adx-mcp-server', 'A Model Context Protocol (MCP) server that enables AI assistants to query and analyze Azure Data Explorer databases through standardized interfaces.', '2025-03-10 10:04:07+00', '2025-03-10 16:41:55+00', 'Python', '2', '# Azure Data Explorer MCP Server

A [Model Context Protocol][mcp] (MCP) server for Azure Data Explorer.

This provides access to your Azure Data Explorer clusters and databases through standardized MCP interfaces, allowing AI assistants to execute KQL queries and explore your data.

[mcp]: https://modelcontextprotocol.io

## Features

- [x] Execute KQL queries against Azure Data Explorer
- [x] Discover and explore database resources
  - [x] List tables in the configured database
  - [x] View table schemas
  - [x] Sample data from tables
- [x] Authentication support
  - [x] Client credentials from environment variables

- [x] Provide interactive tools for AI assistants

The list of tools is configurable, so you can choose which tools you want to make available to the MCP client.
This is useful if you don't use certain functionality or if you don't want to take up too much of the context window.

## Usage

1. Create a service account in Azure Data Explorer with appropriate permissions, or ensure you have access through your Azure account.

2. Configure the environment variables for your ADX cluster, either through a `.env` file or system environment variables:

```env
# Required: Azure Data Explorer configuration
ADX_CLUSTER_URL=https://yourcluster.region.kusto.windows.net
ADX_DATABASE=your_database

# Optional: Azure authentication (if not using default credentials)
AZURE_TENANT_ID=your_tenant_id
AZURE_CLIENT_ID=your_client_id
AZURE_CLIENT_SECRET=your_client_secret
```

3. Add the server configuration to your client configuration file. For example, for Claude Desktop:

```json
{
  "mcpServers": {
    "adx": {
      "command": "uv",
      "args": [
        "--directory",
        "<full path to adx-mcp-server directory>",
        "run",
        "src/adx-mcp-server/main.py"
      ],
      "env": {
        "ADX_CLUSTER_URL": "https://yourcluster.region.kusto.windows.net",
        "ADX_DATABASE": "your_database"
      }
    }
  }
}
```

> Note: if you see `Error: spawn uv ENOENT` in Claude Desktop, you may need to specify the full path to `uv` or set the environment variable `NO_UV=1` in the configuration.

## Development

Contributions are welcome! Please open an issue or submit a pull request if you have any suggestions or improvements.

This project uses [`uv`](https://github.com/astral-sh/uv) to manage dependencies. Install `uv` following the instructions for your platform:

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

You can then create a virtual environment and install the dependencies with:

```bash
uv venv
source .venv/bin/activate  # On Unix/macOS
.venv\Scripts\activate     # On Windows
uv pip install -e .
```

## Project Structure

The project has been organized with a `src` directory structure:

```
adx-mcp-server/
├── src/
│   └── adx_mcp_server/
│       ├── __init__.py      # Package initialization
│       ├── server.py        # MCP server implementation
│       ├── main.py          # Main application logic
├── pyproject.toml           # Project configuration
└── README.md                # This file
```

### Testing

The project includes a comprehensive test suite that ensures functionality and helps prevent regressions.

Run the tests with pytest:

```bash
# Install development dependencies
uv pip install -e ".[dev]"

# Run the tests
pytest

# Run with coverage report
pytest --cov=src --cov-report=term-missing

# Or use the convenience script
./run_tests.sh
./run_tests.sh --coverage  # With coverage
```

Tests are organized into:

- Configuration validation tests
- Server functionality tests
- Error handling tests
- Main application tests

When adding new features, please also add corresponding tests.

### Tools

| Tool | Category | Description |
| --- | --- | --- |
| `execute_query` | Query | Execute a KQL query against Azure Data Explorer |
| `list_tables` | Discovery | List all tables in the configured database |
| `get_table_schema` | Discovery | Get the schema for a specific table |
| `sample_table_data` | Discovery | Get sample data from a table with optional sample size |

## Example Usage

Here's how an AI assistant might use this MCP server to interact with your Azure Data Explorer instance:

```
User: I need to analyze some data in Azure Data Explorer. Can you help me?

AI: I can help you analyze data in Azure Data Explorer. Let me first check what tables are available in your database.
[AI uses the list_tables tool]

AI: I found several tables in your database. Here are the available tables:
- logs
- metrics
- users
- events

What kind of analysis would you like to perform?

User: I want to see if there were any errors in the logs in the last 24 hours.

AI: I'll query the logs table for errors in the last 24 hours.
[AI uses the execute_query tool with:
  "logs | where Timestamp > ago(24h) and Level == 'Error' | summarize count() by Source"]

AI: Here are the error counts by source from the last 24 hours:
| Source      | count |
|-------------|-------|
| WebService  | 42    |
| DataProcessor| 17   |
| API         | 5     |

Would you like me to investigate any of these error sources in more detail?
```

## License

MIT

---

[mcp]: https://modelcontextprotocol.io
', 'true', '"{\"AI Model Integration\",\"Protocol Implementations\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements a Model Context Protocol (MCP) server for Azure Data Explorer, enabling AI assistants to interact with ADX databases through standardized interfaces. The documentation also links to the official MCP specification.","categories":["Protocol Implementations","AI Model Integration","Function Calling"],"analysis":{"Architecture and Design Patterns":"The repository adopts a server-client architecture where the MCP server acts as an intermediary between AI assistants (clients) and Azure Data Explorer. It uses a modular design, separating server logic, main application, and database interaction. The project structure facilitates organized development with clear separation of concerns, enhancing maintainability.","Implementation Details and Key Features":"The implementation focuses on executing KQL queries, discovering database resources (listing tables, viewing schemas), and providing sample data. Authentication is supported using client credentials from environment variables. Tools are configurable, allowing administrators to choose which tools are exposed to the AI client, thus controlling the context window size.","Integration Approaches and Protocols":"The integration approach leverages the Model Context Protocol to enable AI assistants to interact with Azure Data Explorer. The server exposes standardized interfaces, allowing AI clients to send requests and receive responses. It uses environment variables for configuration and supports authentication via Azure service principals.","Technical Stack and Dependencies":"The repository utilizes Python as its primary language. It leverages `uv` for dependency management. The main dependencies are related to interacting with Azure Data Explorer and implementing the MCP server functionality. The `pyproject.toml` file defines the project's dependencies, including testing libraries like `pytest`.","Best Practices and Coding Standards":"The repository includes a comprehensive test suite using `pytest`, covering configuration validation, server functionality, and error handling. This suggests a commitment to testing and quality assurance. The project structure promotes modularity and separation of concerns, which are considered best practices in software development. The use of a virtual environment ensures dependency isolation.","Performance Considerations":"The documentation includes configuration options to control which tools are exposed, suggesting an awareness of context window limitations which directly impact performance of the calling LLM. No specific code-level optimizations are explicitly described but potential improvements might include caching query results, optimizing KQL queries, and improving data serialization/deserialization."}}', '2025-03-10 21:26:21+00', 'true', 'Servers', 'adx-mcp-server'), ('946037221', 'mcp-server-deepseek', 'tizee/mcp-server-deepseek', 'https://github.com/tizee/mcp-server-deepseek', 'A MCP server provides access to DeepSeek-R1's reasoning capabilities for LLMs', '2025-03-10 14:09:40+00', '2025-03-10 17:35:22+00', 'Python', '0', '# mcp-server-deepseek

A [Model Context Protocol (MCP)](https://modelcontextprotocol.io) server that provides access to DeepSeek-R1's reasoning capabilities, allowing non-reasoning models to generate better responses with enhanced thinking.

- read [R1+Sonnet set SOTA on aider’s polyglot benchmark](https://aider.chat/2025/01/24/r1-sonnet.html)

## Overview

This server acts as a bridge between LLM applications and DeepSeek's reasoning capabilities. It exposes DeepSeek-R1's reasoning content through an MCP tool, which can be used by any MCP-compatible client.

The server is particularly useful for:

- Enhancing responses from models without native reasoning capabilities
- Accessing DeepSeek-R1's thinking process for complex problem solving
- Adding structured reasoning to Claude or other LLMs that support MCP

## Features

- **Access to DeepSeek-R1**: Connects to DeepSeek's API to leverage their reasoning model
- **Structured Thinking**: Returns reasoning in a structured `<thinking>` format
- **Integration with MCP**: Fully compatible with the Model Context Protocol
- **Error Handling**: Robust error handling with detailed logging

## Installation

### Prerequisites

- Python 3.13 or higher
- An API key for DeepSeek

### Setup

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/mcp-server-deepseek.git
   cd mcp-server-deepseek
   ```

2. Create a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. Install the package:
   ```bash
   pip install -e .
   ```

4. Create a `.env` file with your DeepSeek API credentials:
   ```bash
   cp .env.example .env
   ```

5. Edit the `.env` file with your API key and model details:
   ```
   MCP_SERVER_DEEPSEEK_MODEL_NAME=deepseek-reasoner
   MCP_SERVER_DEEPSEEK_API_KEY=your_api_key_here
   MCP_SERVER_DEEPSEEK_API_BASE_URL=https://api.deepseek.com
   ```

## Usage

### Running the Server

You can run the server directly:

```bash
mcp-server-deepseek
```

Or use the development mode with the MCP Inspector:

```bash
make dev
```

### MCP Tool

The server exposes a single tool:

#### `think_with_deepseek_r1`

This tool sends a prompt to DeepSeek-R1 and returns its reasoning content.

**Arguments:**
- `prompt` (string): The full user prompt to process

**Returns:**
- String containing DeepSeek-R1's reasoning wrapped in `<thinking>` tags

### Example Usage

When used with Claude or another LLM that supports MCP, you can trigger the thinking process by calling the tool:

```
Please use the think_with_deepseek_r1 tool with the following prompt:
"How can I optimize a neural network for time series forecasting?"
```

## Development

### Testing

For development and testing, use the MCP Inspector:

```bash
npx @modelcontextprotocol/inspector uv run mcp-server-deepseek
```

### Logging

Logs are stored in `~/.cache/mcp-server-deepseek/server.log`

The log level can be configured using the `LOG_LEVEL` environment variable (defaults to `DEBUG`).

## Troubleshooting

### Common Issues

- **API Key Issues**: Ensure your DeepSeek API key is correctly set in the `.env` file
- **Timeout Errors**: Complex prompts may cause timeouts. Try simplifying your prompt
- **Missing Reasoning**: Some queries might not generate reasoning content. Try rephrasing

### Error Logs

Check the logs for detailed error messages:

```bash
cat ~/.cache/mcp-server-deepseek/server.log
```

## License

MIT

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## Acknowledgements

- Thanks to the DeepSeek team for their powerful reasoning model
- Built with the [Model Context Protocol](https://modelcontextprotocol.io) framework
', 'true', '"{\"Protocol Implementations\",\"AI Model Integration\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it's a Model Context Protocol server providing DeepSeek-R1 reasoning capabilities. It integrates with other LLMs via the MCP protocol and exposes a `think_with_deepseek_r1` tool.","categories":["AI Model Integration","Function Calling","Protocol Implementations"],"analysis":{"Architecture and Design Patterns":"The server acts as a bridge, exposing DeepSeek-R1's reasoning through an MCP tool. It follows a client-server architecture where LLM applications are clients and this server provides a specific reasoning service. It leverages API calls to DeepSeek and exposes a single, well-defined tool.","Implementation Details and Key Features":"The server offers access to DeepSeek-R1 for structured reasoning. It returns reasoning content wrapped in `<thinking>` tags, integrates with the MCP standard, and includes error handling. It's implemented in Python and uses environment variables for configuration, including the DeepSeek API key.","Integration Approaches and Protocols":"Integration is achieved via the Model Context Protocol, allowing other LLMs like Claude to leverage DeepSeek's reasoning capabilities. The server exposes a tool that accepts a prompt and returns the reasoning result. It is designed to be used in conjunction with the MCP Inspector for development and testing.","Technical Stack and Dependencies":"The server is built using Python 3.13 or higher. Key dependencies likely include libraries for making API requests to DeepSeek and potentially libraries for handling the MCP protocol itself. The documentation mentions using a virtual environment and `pip` for dependency management.","Best Practices and Coding Standards":"The repository includes a `.env.example` file for managing API keys, promoting security. The documentation emphasizes error handling and provides instructions for logging and troubleshooting. The presence of a `Makefile` suggests the use of standardized build and development processes.","Performance Considerations":"The documentation mentions that complex prompts can cause timeouts, suggesting that prompt engineering and potentially rate limiting are relevant performance considerations. The server's performance depends on the DeepSeek API's response time. Caching mechanisms might improve performance."}}', '2025-03-10 21:26:36+00', 'true', 'Servers', 'mcp-server-deepseek'), ('946044325', 'mcp-google-calendar', 'arashiyama/mcp-google-calendar', 'https://github.com/arashiyama/mcp-google-calendar', 'Google Calendar Model Context Prococol Server', '2025-03-10 14:21:20+00', '2025-03-10 17:59:16+00', 'JavaScript', '1', '# Google Calendar MCP Server

This is a Model Context Protocol (MCP) server implementation for Google Calendar. It allows AI assistants and applications to access and manipulate Google Calendar data in a standardized way.

## Features

- Full CRUD operations for Google Calendar events
- Support for recurring events and exceptions
- Batch operations for multiple requests
- Event reminders and notifications
- Advanced search and filtering
- Webhook notifications for calendar changes
- Database persistence for tokens and webhooks

## Support the Project

If you find this project useful, consider supporting the developer:

[Buy Me A Coffee](https://buymeacoffee.com/jonathancare)

## Setup Instructions

1. Create a Google Cloud project and enable the Google Calendar API:
   - Go to the [Google Cloud Console](https://console.cloud.google.com/)
   - Create a new project
   - Enable the Google Calendar API
   - Create OAuth 2.0 credentials (you'll need a client ID and client secret)
   - Configure the OAuth consent screen

2. Install dependencies:
   ```
   npm install
   ```

3. Update the `.env` file with your Google API credentials:
   ```
   GOOGLE_CLIENT_ID=your_client_id_here
   GOOGLE_CLIENT_SECRET=your_client_secret_here
   GOOGLE_REDIRECT_URI=http://localhost:3000/auth/google/callback
   ```

4. Start the server:
   ```
   npm start
   ```

5. Open http://localhost:3000 in your browser and authorize the application with Google.

## Connecting to Claude

To use this MCP server with Claude:

1. Claude will start the server automatically using the shell script command
2. Add the following MCP configuration to Claude:

```json
{
  "mcpServers": {
    "googleCalendar": {
      "name": "Google Calendar MCP",
      "version": "1.0.0",
      "command": "/Users/jonc/code/mcp-google-calendar/start-mcp.sh",
      "description": "MCP server for Google Calendar access",
      "url": "http://localhost:3000",
      "mcpProtocolVersion": "0.1"
    }
  }
}
```

3. In Claude settings, go to Model Context Protocol > Add MCP server
4. Enter the details above (or import a JSON file with these contents)
5. Claude will now be able to manage your Google Calendar

## MCP Endpoints

- `/mcp/definition` - GET endpoint that returns the capabilities of this MCP server
- `/mcp/execute` - POST endpoint for executing actions on Google Calendar

## Supported Actions

### Core Calendar Operations
- `list_calendars` - List all available calendars
- `list_events` - List calendar events with filtering options
- `create_event` - Create a new calendar event
- `get_event` - Get details for a specific event
- `update_event` - Update an existing calendar event
- `delete_event` - Delete a calendar event
- `find_duplicates` - Identify potential duplicate events in your calendar
- `advanced_search_events` - Advanced search with complex filtering

### Recurring Events
- `list_recurring_instances` - List all instances of a recurring event
- `create_event_exception` - Create an exception to a specific instance of a recurring event
- `delete_event_instance` - Delete a specific instance of a recurring event

### Batch Operations
- `batch_operations` - Execute multiple calendar operations in a single request

### Notifications
- `manage_webhooks` - Set up, list, or delete notification webhooks

All event-related actions support an optional `calendarId` parameter to work with different calendars. If not specified, the primary calendar is used by default.

## Example Usage

### List Calendars
```json
{
  "action": "list_calendars",
  "parameters": {}
}
```

### List Events
```json
{
  "action": "list_events",
  "parameters": {
    "calendarId": "primary",
    "timeMin": "2023-01-01T00:00:00Z",
    "maxResults": 10
  }
}
```

### Create Event
```json
{
  "action": "create_event",
  "parameters": {
    "calendarId": "primary",
    "summary": "Team Meeting",
    "description": "Weekly team sync",
    "location": "Conference Room A",
    "start": {
      "dateTime": "2023-01-15T09:00:00-07:00",
      "timeZone": "America/Los_Angeles"
    },
    "end": {
      "dateTime": "2023-01-15T10:00:00-07:00",
      "timeZone": "America/Los_Angeles"
    }
  }
}
```

### Get Event
```json
{
  "action": "get_event",
  "parameters": {
    "calendarId": "primary",
    "eventId": "event_id_here"
  }
}
```

### Update Event
```json
{
  "action": "update_event",
  "parameters": {
    "calendarId": "primary",
    "eventId": "event_id_here",
    "summary": "Updated Meeting Title",
    "description": "This event has been updated",
    "location": "Conference Room B",
    "start": {
      "dateTime": "2023-01-15T10:00:00-07:00",
      "timeZone": "America/Los_Angeles"
    },
    "end": {
      "dateTime": "2023-01-15T11:00:00-07:00",
      "timeZone": "America/Los_Angeles"
    }
  }
}
```

### Delete Event
```json
{
  "action": "delete_event",
  "parameters": {
    "calendarId": "primary",
    "eventId": "event_id_here"
  }
}
```

### Find Duplicate Events
```json
{
  "action": "find_duplicates",
  "parameters": {
    "calendarId": "primary",
    "timeMin": "2023-01-01T00:00:00Z",
    "timeMax": "2023-12-31T23:59:59Z",
    "similarityThreshold": 0.7
  }
}
```

### List Recurring Instances
```json
{
  "action": "list_recurring_instances",
  "parameters": {
    "calendarId": "primary",
    "eventId": "recurring_event_id_here",
    "timeMin": "2023-01-01T00:00:00Z",
    "maxResults": 25
  }
}
```

### Create Event Exception
```json
{
  "action": "create_event_exception",
  "parameters": {
    "calendarId": "primary",
    "recurringEventId": "recurring_event_id_here",
    "originalStartTime": "2023-01-15T09:00:00-07:00",
    "summary": "Special Team Meeting",
    "location": "Virtual Meeting Room",
    "reminders": {
      "useDefault": false,
      "overrides": [
        { "method": "email", "minutes": 30 },
        { "method": "popup", "minutes": 15 }
      ]
    }
  }
}
```

### Delete Event Instance
```json
{
  "action": "delete_event_instance",
  "parameters": {
    "calendarId": "primary",
    "recurringEventId": "recurring_event_id_here",
    "originalStartTime": "2023-01-22T09:00:00-07:00"
  }
}
```

### Batch Operations
```json
{
  "action": "batch_operations",
  "parameters": {
    "operations": [
      {
        "action": "get_event",
        "parameters": {
          "calendarId": "primary",
          "eventId": "event_id_1"
        }
      },
      {
        "action": "create_event",
        "parameters": {
          "summary": "New Event",
          "start": {
            "dateTime": "2023-02-15T10:00:00-07:00",
            "timeZone": "America/Los_Angeles"
          },
          "end": {
            "dateTime": "2023-02-15T11:00:00-07:00",
            "timeZone": "America/Los_Angeles"
          }
        }
      },
      {
        "action": "delete_event",
        "parameters": {
          "eventId": "event_id_2"
        }
      }
    ]
  }
}
```

### Advanced Search Events
```json
{
  "action": "advanced_search_events",
  "parameters": {
    "calendarId": "primary",
    "timeRange": {
      "start": "2023-01-01T00:00:00Z",
      "end": "2023-12-31T23:59:59Z"
    },
    "textSearch": "meeting",
    "location": "conference",
    "attendees": ["jane@example.com"],
    "status": "confirmed",
    "isRecurring": true,
    "maxResults": 50
  }
}
```

### Manage Webhooks
```json
{
  "action": "manage_webhooks",
  "parameters": {
    "operation": "create",
    "address": "https://your-server.com/webhook/calendar"
  }
}
```

## Webhook Notifications

This MCP server supports real-time notifications through webhooks. When events in your calendar change, notifications are sent to the registered webhook endpoints.

### Setting up Webhooks

1. Create a publicly accessible HTTPS endpoint that can receive POST requests
2. Register your endpoint using the `manage_webhooks` action with the "create" operation
3. Your server will start receiving notifications when calendar events change

### Webhook Notification Format

Notifications are sent as JSON payloads with the following structure:

```json
{
  "type": "calendar_update",
  "events": [
    {
      "id": "event_id",
      "summary": "Event Title",
      "start": { "dateTime": "2023-01-15T09:00:00-07:00" },
      "end": { "dateTime": "2023-01-15T10:00:00-07:00" },
      "status": "confirmed",
      "updated": "2023-01-10T12:34:56Z"
    }
  ],
  "channelId": "webhook_id",
  "timestamp": "2023-01-10T12:34:56Z"
}
```

The server also supports event reminder notifications:

```json
{
  "type": "event_reminder",
  "events": [
    {
      "id": "event_id",
      "summary": "Upcoming Event",
      "start": { "dateTime": "2023-01-15T09:00:00-07:00" },
      "end": { "dateTime": "2023-01-15T10:00:00-07:00" },
      "location": "Conference Room",
      "status": "confirmed"
    }
  ],
  "timestamp": "2023-01-15T08:45:00Z"
}
```', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository is explicitly described as a Model Context Protocol (MCP) server implementation for Google Calendar, enabling AI assistants to interact with calendar data. It defines MCP endpoints and actions.","categories":["Protocol Implementations","Function Calling","API Frameworks"],"analysis":{"Architecture and Design Patterns":"The architecture follows a client-server model, with the MCP server acting as a bridge between AI models and the Google Calendar API. It uses standard REST API principles for communication, with defined endpoints for MCP interactions and actions. The server abstracts the complexity of the Google Calendar API.","Implementation Details and Key Features":"The server provides full CRUD operations, supports recurring events and exceptions, and offers batch operations. Event reminders and advanced search capabilities are also implemented. Webhook notifications are supported to inform clients of calendar changes. Database persistence manages tokens and webhooks. The code supports actions like `list_calendars` and `create_event`.","Integration Approaches and Protocols":"The repository uses the Model Context Protocol (MCP) for integration with AI models. Integration involves configuring the AI model (e.g., Claude) with the server's address and supported actions. The server interacts with the Google Calendar API using OAuth 2.0 for authentication and authorization. Webhooks are used for real-time notifications.","Technical Stack and Dependencies":"The server uses Node.js and npm for package management. It depends on Google Calendar API libraries. The `.env` file stores API credentials and the setup instructions detail Google Cloud project configuration. Specific libraries for handling OAuth and interacting with the Google Calendar API are required.","Best Practices and Coding Standards":"The repository includes a `.env` file for storing sensitive API keys, promoting secure configuration management. The documentation provides clear setup instructions and example usage scenarios. Structured JSON payloads are used for requests and responses. There's a well-defined action-based structure.","Performance Considerations":"Batch operations optimize for efficient execution of multiple calendar operations. Webhook notifications provide real-time updates, reducing the need for frequent polling. The `advanced_search_events` action and the option for `maxResults` within the `list_events` action can also aid in performance."}}', '2025-03-10 21:26:36+00', 'true', 'Servers', 'mcp-google-calendar'), ('946066462', 'pdf-mcp-tools', 'alanhe421/pdf-mcp-tools', 'https://github.com/alanhe421/pdf-mcp-tools', null, '2025-03-10 14:58:34+00', '2025-03-10 15:58:48+00', 'JavaScript', '1', '# PDF MCP Server

## Features
- Remove pages from a PDF document

', 'true', '"{\"Developer Tooling\",\"API Frameworks\",\"Data Processing\"]}', '{"is_mcp":"NO","justification":"The repository provides basic PDF manipulation functionality (page removal). It lacks any evidence of implementing or interacting with Model Context Protocol specifications or related concepts like function calling or tool use.","categories":["Developer Tooling","Data Processing","API Frameworks"],"analysis":{"Architecture and Design Patterns":"The architecture appears simple, likely involving an API endpoint that receives a PDF document and page removal instructions. The code probably uses a library to parse and modify PDF files. Design patterns are likely basic, such as request-response for the API.","Implementation Details and Key Features":"The primary feature is removing pages from a PDF document. Implementation likely utilizes a PDF processing library. The absence of detailed documentation obscures further implementation specifics. The server probably exposes an API for interaction.","Integration Approaches and Protocols":"Based on the name, the repository might be intended as an API server. It potentially utilizes HTTP for communication. Specific integration methods depend on the client-side implementation, which is unknown from the available information.","Technical Stack and Dependencies":"The repository probably depends on a PDF processing library (e.g., PDFMiner, PyPDF2 if Python). The server-side implementation likely uses a web framework (e.g., Flask, FastAPI if Python). The specific language is unstated.","Best Practices and Coding Standards":"Without access to the codebase, it's impossible to assess adherence to best practices. Standard API design principles (e.g., input validation, error handling) would be expected. Use of version control and documentation practices would be helpful.","Performance Considerations":"Performance considerations depend heavily on the size of the PDF documents and the efficiency of the PDF processing library. Caching mechanisms might improve repeated requests. Optimization of memory usage is essential for handling large files."}}', '2025-03-10 21:26:24+00', 'true', 'Servers', 'pdf-mcp-tools'), ('946072840', 'x-mcp-server', 'siddheshutd/x-mcp-server', 'https://github.com/siddheshutd/x-mcp-server', 'This is an MCP server for the X Platform', '2025-03-10 15:09:28+00', '2025-03-10 15:57:18+00', 'TypeScript', '1', '# X MCP Server

This tool is an MCP server for interacting with the X platform via an MCP client such as Claude, Cursor AI, Windsurf AI, etc.

## Prerequisites

1. Node.js (>=18.0.0)
2. npm (>=8.0.0)
3. X Developer API keys

## Important Notes regarding to the rate limits of the X APIs
- There is a rate limit on the number of requests per minute for the X APIs, which is set by the X platform.
- You can refer to this page for knowing the limits: https://developer.x.com/en/portal/products

## Getting Started

1. Clone this repository:
   ```bash
   git clone <repository-url>
   ```

2. Install the dependencies:
   ```bash
   npm install
   ```

3. Build the project:
   ```bash
   npm run build
   ```
   This will compile the `index.ts` file in the `src` folder and create the `build` directory.

## Configuration

1. You will need your X Developer API keys to use this tool. You will need these 4 keys/tokens:
   - X API Key
   - X API Key Secret
   - X Access Token
   - X Access Token Secret

   In case you are not aware of how to get these keys/tokens, you can follow the instructions on the X Developer website: https://developer.twitter.com/en/docs/twitter-api/getting-started/getting-access-to-the-twitter-api

2. Set the environment variables:
   You will have to setup the MCP server for your client.

   1. For Claude:
      - Open Claude Desktop
      - Go to Settings -> Developer
      - Here you will see an option to configure MCP servers by clicking on it (Edit Config)
      - Claude will open the folder containing the config file. Open the config file which is named `claude_desktop_config.json`.
      - Add the following configuration:
        ```json
        {
            "x-mcp-server": {
                "command": "node",
                "args": [
                    "/ABSOLUTE/PATH/TO/PARENT/FOLDER/x-mcp-server/build/index.js"
                ],
                "env": {
                    "X_API_KEY": "your-x-api-key",
                    "X_API_KEY_SECRET": "your-x-api-key-secret",
                    "X_ACCESS_TOKEN": "your-x-access-token",
                    "X_ACCESS_TOKEN_SECRET": "your-x-access-token-secret"
                }
            }
        }
        ```
      - Save the config file.
      - Restart Claude Desktop.
      - You can refer the official Anthropic documentation for setting up an MCP server https://modelcontextprotocol.io/quickstart/server

   2. For Cursor AI:
      - You can follow the instructions in the Cursor documentation: https://docs.cursor.com/context/model-context-protocol

   3. For Windsurf AI:
      - Open a new chat in Cascade.
      - You should see a Configure MCP button below the Cascade's prompt input. Click on it.
      - It will open the config file in the editor and you can add the following configuration:
        ```json
        {
            "mcpServers": {
                "x-mcp-server": {
                    "command": "node",
                    "args": [
                        "/ABSOLUTE/PATH/TO/PARENT/FOLDER/x-mcp-server/build/index.js"
                    ],
                    "env": {
                        "X_API_KEY": "your-x-api-key",
                        "X_API_KEY_SECRET": "your-x-api-key-secret",
                        "X_ACCESS_TOKEN": "your-x-access-token",
                        "X_ACCESS_TOKEN_SECRET": "your-x-access-token-secret"
                    }
                }
            }
        }
        ```
      - Save file and click on the Refresh servers button below the Cascade's prompt input.

   4. For other MCP clients:
      - You can refer to the official MCP documentation: https://modelcontextprotocol.io/introduction

Now you can connect to this MCP server locally using your preferred MCP client and interact with the X platform.', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states that it is an MCP server designed to interact with the X platform, and it provides configuration instructions for various MCP clients like Claude and Cursor AI.","categories":["Protocol Implementations","API Frameworks","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The repository's architecture appears to be a server application built with Node.js. It is designed to receive requests from MCP clients, interact with the X platform's API, and return responses. The design pattern employed is likely a client-server model conforming to MCP specifications. The server handles authentication via API keys.","Implementation Details and Key Features":"The key implementation detail is the `index.js` file, which serves as the main entry point for the MCP server. It utilizes the X platform's API through configured environment variables. The server's main function is to translate MCP requests into X API calls, handle rate limits, and provide responses to the client.","Integration Approaches and Protocols":"The repository integrates with the X platform through its API, and with AI models/tools using the Model Context Protocol (MCP). It supports integration with Claude, Cursor AI, and Windsurf AI, as evidenced by provided configuration examples. The protocol defines how the AI models interact with the server to access context from the X platform.","Technical Stack and Dependencies":"The primary technical stack consists of Node.js and npm. The crucial dependencies are those related to networking (handling API requests), environment variable management, and any X platform API client libraries. The project uses typescript, so will require additional build dependencies.","Best Practices and Coding Standards":"The provided documentation includes steps for setting up environment variables to manage secrets like API keys. The build process indicates proper project structuring and compilation. However, further analysis of the source code would be needed to assess compliance with more general coding standards and secure coding practices.","Performance Considerations":"The documentation explicitly mentions the rate limits of the X APIs, indicating a consideration for performance. The server needs to handle rate limiting and potentially implement caching mechanisms to prevent exceeding these limits. The performance will depend on the network latency and efficiency of the X API interactions."}}', '2025-03-10 21:26:24+00', 'true', 'Servers', 'x-platform-mcp-server'), ('946079810', 'make-mcp-server', 'integromat/make-mcp-server', 'https://github.com/integromat/make-mcp-server', 'Make MCP Server', '2025-03-10 15:21:12+00', '2025-03-10 18:01:08+00', 'TypeScript', '4', '# Make MCP Server

A Model Context Protocol server that enables Make scenarios to be utilized as tools by AI assistants. This integration allows AI systems to trigger and interact with your Make automation workflows.

## How It Works

The MCP server:

-   Connects to your Make account and identifies all scenarios configured with "On-Demand" scheduling
-   Parses and resolves input parameters for each scenario, providing AI assistants with meaningful parameter descriptions
-   Allows AI assistants to invoke scenarios with appropriate parameters
-   Returns scenario output (currently in closed beta) as structured JSON, enabling AI assistants to properly interpret the results

## Benefits

-   Turn your Make scenarios into callable tools for AI assistants
-   Maintain complex automation logic in Make while exposing functionality to AI systems
-   Create bidirectional communication between your AI assistants and your existing automation workflows

## Usage with Claude Desktop

### Prerequisites

-   NodeJS
-   MCP Client (like Claude Desktop App)
-   Make API Key with `scenarios:read` and `scenarios:run` scopes

### Installation

To use this server with the Claude Desktop app, add the following configuration to the "mcpServers" section of your `claude_desktop_config.json`:

```json
{
    "mcpServers": {
        "make": {
            "command": "npx",
            "args": ["-y", "@makehq/mcp-server"],
            "env": {
                "MAKE_API_KEY": "<your-api-key>",
                "MAKE_ZONE": "<your-zone>",
                "MAKE_TEAM": "<your-team-id>"
            }
        }
    }
}
```

-   `MAKE_API_KEY` - You can generate an API key in your Make profile.
-   `MAKE_ZONE` - The zone your organization is hosted in (e.g., `eu2.make.com`).
-   `MAKE_TEAM` - You can find the ID in the URL of the Team page.
', 'true', '"{\"Protocol Implementations\",\"Agent Frameworks\",\"Workflow Orchestration\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements a Model Context Protocol (MCP) server designed to integrate Make automation scenarios as callable tools for AI assistants, aligning with MCP specifications.","categories":["Agent Frameworks","Function Calling","Workflow Orchestration"],"analysis":{"Architecture and Design Patterns":"The architecture centers around a server that connects to Make, parses scenario parameters, and allows AI assistants to invoke these scenarios. It acts as a bridge, translating requests from AI models into executable workflows within the Make environment. The design facilitates integration with AI assistants like Claude Desktop.","Implementation Details and Key Features":"The implementation involves parsing Make scenarios scheduled with 'On-Demand' triggers. It extracts input parameters and generates descriptions for AI assistants. A key feature is the ability to invoke scenarios with provided parameters and return the output in structured JSON format for AI interpretation. Configuration is managed via environment variables.","Integration Approaches and Protocols":"Integration occurs via the Model Context Protocol (MCP), enabling AI assistants to interact with Make scenarios as tools. The server exposes functionality that aligns with the MCP specification, allowing AI agents to invoke functions and receive structured results. The integration uses API keys for authentication.","Technical Stack and Dependencies":"The technical stack includes NodeJS. The command `npx` is used to execute the MCP server. The server depends on the Make API, requiring `scenarios:read` and `scenarios:run` scopes. The Claude Desktop configuration utilizes a `claude_desktop_config.json` file to define the MCP server settings.","Best Practices and Coding Standards":"The documentation emphasizes clear configuration through environment variables, promoting secure handling of sensitive information like the Make API key. The structured JSON output ensures that AI assistants can reliably interpret the results. The repository leverages a well-defined configuration file.","Performance Considerations":"Performance considerations are not explicitly detailed. However, the use of 'On-Demand' scenarios suggests an event-driven approach. Efficiency in parsing scenario parameters and translating requests will be crucial for minimizing latency. The network connection to Make and the complexity of the triggered scenarios will impact overall performance."}}', '2025-03-10 21:26:36+00', 'true', 'Servers', 'make-mcp-server'), ('946093988', 'MCP', 'dhiraj-instalily/MCP', 'https://github.com/dhiraj-instalily/MCP', 'Model Context Protocol (MCP) is an open protocol that standardizes how applications provide context to LLMs- https://docs.anthropic.com/en/docs/agents-and-tools/mcp', '2025-03-10 15:46:08+00', '2025-03-10 15:46:11+00', null, '0', '# MCP
Model Context Protocol (MCP) is an open protocol that standardizes how applications provide context to LLMs- https://docs.anthropic.com/en/docs/agents-and-tools/mcp
', 'true', '"{\"Protocol Implementations\",\"Context Management\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository name and description explicitly mention Model Context Protocol (MCP), confirming its involvement with the protocol and providing a link to official MCP documentation.","categories":["Protocol Implementations","AI Model Integration","Context Management"],"analysis":{"Architecture and Design Patterns":"Given the repository's focus on MCP, the architecture likely centers around defining standard interfaces and data structures for exchanging context information between applications and LLMs. Design patterns may include abstract factories for context providers and adapters for different LLM input formats. Specific architectures cannot be determined with the given information.","Implementation Details and Key Features":"Implementation would involve defining message formats, serialization methods, and client/server interactions according to MCP specifications. Key features would focus on context window management, standardization of the application/LLM interface, and potentially include supporting different context types (e.g., text, metadata, tool outputs). No specific feature can be extracted from the given info.","Integration Approaches and Protocols":"Integration involves adopting the MCP protocol for applications that want to provide context to LLMs. This would require applications to format their context data according to MCP's specifications and communicate it to the LLM. The protocol likely leverages standard communication methods like HTTP/REST or gRPC for exchanging context information.","Technical Stack and Dependencies":"Based on the documentation, the technical stack may use common languages (Python, JavaScript) for libraries and frameworks. Dependencies would vary based on the specific implementation, but might include libraries for network communication (e.g., requests, grpc), data serialization (e.g., JSON, Protocol Buffers), and potentially LLM SDKs (e.g., Anthropic's SDK) to handle the actual LLM interactions.","Best Practices and Coding Standards":"Best practices would emphasize adherence to the MCP specifications, clear and concise documentation, and robust error handling. Coding standards would promote maintainability, readability, and testability. The repository might implement unit tests to validate correct context formatting and integration tests to ensure seamless communication with LLMs and other tools.","Performance Considerations":"Performance considerations would include minimizing latency in context delivery, optimizing context size to fit within LLM's context window, and efficient data serialization. Caching mechanisms might be used to reduce repeated context retrieval. Strategies for handling large context sizes and potentially streaming context data to the LLM would also be important."}}', '2025-03-10 21:26:24+00', 'true', 'Servers', 'mcp-open-protocol'), ('946103461', 'gorse', 'dataontap/gorse', 'https://github.com/dataontap/gorse', 'Data On Tap Inc. is a Full MVNO operating network 302 100 in Canada. This is DOT's code repository. It includes advanced security and authentication, various connectivity tools, intelligence, including smart network reservations, eSIM/iSIM, bootstrap wireless connectivity, D2C satellite, frameworks and concepts to build on. OpenAPI 3.1. MCP server.', '2025-03-10 16:03:09+00', '2025-03-10 16:03:13+00', null, '0', '# gorse
Data On Tap Inc. is a Full MVNO operating network 302 100 in Canada. This is DOT's code repository. It includes advanced security and authentication, various connectivity tools, intelligence, including smart network reservations, eSIM/iSIM, bootstrap wireless connectivity, D2C satellite, frameworks and concepts to build on. OpenAPI 3.1. MCP server.
', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Security & Privacy\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly mentions \"MCP server\", indicating an implementation or component related to Model Context Protocol specifications within the context of a mobile virtual network operator.","categories":["Protocol Implementations","API Frameworks","Security & Privacy"],"analysis":{"Architecture and Design Patterns":"The repository appears to be structured around providing a set of connectivity and security tools for a Full MVNO. The architecture likely incorporates modular components for authentication, network reservations, and connectivity management. It seems to follow API-driven design principles, as evidenced by the mention of OpenAPI 3.1, enabling external integrations and communication.","Implementation Details and Key Features":"Key features include advanced security and authentication mechanisms, tools for managing various connectivity options (eSIM/iSIM, wireless, satellite), and smart network reservations. The implementation likely involves a combination of network protocols, API endpoints, and security frameworks to manage and control connectivity services. The \"MCP server\" component is a core implementation detail.","Integration Approaches and Protocols":"The repository utilizes OpenAPI 3.1 for defining and exposing its API, which allows for standardized integration with other systems and services. It supports various connectivity protocols including wireless and satellite. The use of MCP likely facilitates communication between the network and other relevant model-driven systems or components, potentially for optimization or automation.","Technical Stack and Dependencies":"Based on the description, the technical stack likely involves a mix of programming languages (unspecified), networking libraries, security frameworks, and potentially database systems for storing network configuration and user data. The OpenAPI 3.1 specification suggests the presence of a RESTful API. The eSIM/iSIM functionality will require specific hardware and software integrations.","Best Practices and Coding Standards":"The mention of OpenAPI 3.1 implies adherence to API design best practices and the use of established coding standards for building and documenting the API. The security and authentication features suggest a focus on secure coding practices. The repository structure and code quality need to be inspected to determine overall adherence to best practices.","Performance Considerations":"Performance considerations are likely important, especially for network reservations and real-time connectivity management. The system must be designed to handle a large number of concurrent connections and ensure low latency. Performance testing and optimization are critical for maintaining a reliable and responsive network service."}}', '2025-03-10 21:26:24+00', 'true', 'Servers', 'gorse-data-mcp-server'), ('946115199', 'mcpartsprivacypolicy', 'ganesharadhya009/mcpartsprivacypolicy', 'https://github.com/ganesharadhya009/mcpartsprivacypolicy', null, '2025-03-10 16:25:13+00', '2025-03-10 16:34:47+00', null, '0', '# mcpartsprivacypolicy', 'true', '"{]}', '{"is_mcp":"NO","justification":"Based on the limited information available (repository name and lack of description), there's no indication it involves communication protocols between AI models and tools, context window management, or other MCP-related functionalities.","categories":[],"analysis":{"Architecture and Design Patterns":"Due to the lack of information, it is not possible to determine the architecture or design patterns employed by this repository. The absence of a description makes it difficult to understand its structure and organization. Further investigation with access to the code is needed.","Implementation Details and Key Features":"Without a description or access to the repository's contents, identifying implementation details and key features is impossible. The repository name suggests something related to 'privacy policy' but lacks context about the technology stack. No specific technologies or programming languages can be identified.","Integration Approaches and Protocols":"The limited information prevents any analysis of integration approaches or protocols used. The repository's purpose and intended use cases are unclear, making it difficult to determine any potential external integrations or communication protocols it may utilize. No specific integration patterns can be inferred.","Technical Stack and Dependencies":"The technical stack and dependencies cannot be determined due to the absence of documentation or code. No specific information is provided about the libraries, frameworks, or tools used in this repository. Analysis of code files or dependency declarations would be needed to identify its components.","Best Practices and Coding Standards":"Without access to the source code, it is impossible to evaluate the adherence to best practices and coding standards. Considerations such as code quality, maintainability, and testing cannot be assessed. Analysis of code style, documentation practices, and test coverage would be necessary.","Performance Considerations":"Performance considerations cannot be evaluated without knowledge of the repository's functionality and usage patterns. Factors such as scalability, efficiency, and resource utilization cannot be assessed. Analysis of algorithms, data structures, and resource management techniques would be needed."}}', '2025-03-10 21:26:21+00', 'true', 'Servers', 'mcparts-privacy-server'), ('946123281', 'cursor-mcp', 'stickelt/cursor-mcp', 'https://github.com/stickelt/cursor-mcp', 'Repository for Cursor MCP project', '2025-03-10 16:40:31+00', '2025-03-10 16:43:30+00', null, '0', '# Cursor MCP

## Overview
Cursor MCP is a project designed to enhance multi-cloud platform integration with the Cursor IDE, providing developers with seamless access to cloud resources directly from their development environment.

## Features
- Integration with major cloud providers (AWS, Azure, GCP)
- Streamlined deployment workflows
- In-editor cloud resource management
- Real-time cloud status monitoring
- Credential management with secure storage

## Getting Started

### Prerequisites
- Cursor IDE (latest version)
- Node.js (v14+)
- Git
- Cloud provider accounts (as needed)

### Installation
```bash
# Clone the repository
git clone https://github.com/stickelt/cursor-mcp.git

# Navigate to the project directory
cd cursor-mcp

# Install dependencies
npm install
```

### Configuration
1. Copy `.env.example` to `.env`
2. Update the environment variables with your cloud credentials
3. Run the configuration wizard:
```bash
npm run configure
```

## Usage
Detailed usage instructions and examples will be provided as the project develops.

## Project Structure
```
cursor-mcp/
├── src/             # Source code
├── docs/            # Documentation
├── tests/           # Test suite
├── examples/        # Example integrations
└── scripts/         # Utility scripts
```

## Development

### Development Setup
```bash
# Set up development environment
npm run dev:setup

# Start development server
npm run dev
```

### Testing
```bash
# Run all tests
npm test

# Run specific test suite
npm test -- --suite=integration
```

## Contributing
Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License
This project is licensed under the MIT License - see the LICENSE file for details.

## Contact
Project Owner: [Your Name/Organization]

## Acknowledgements
- Cursor IDE team
- Contributors
- [Other acknowledgements]', 'true', '"{\"Developer Tooling\",\"Cloud Infrastructure\",\"API Frameworks\"]}', '{"is_mcp":"NO","justification":"The repository focuses on cloud platform integration with the Cursor IDE, not communication protocols between AI models, context management, or tool use typical of MCP.","categories":["Developer Tooling","Cloud Infrastructure","API Frameworks"],"analysis":{"Architecture and Design Patterns":"The repository outlines a structure centered around integrating Cursor IDE with various cloud platforms. It uses a modular approach, separating source code, documentation, tests, and examples. The architecture appears to be designed for extensibility to accommodate multiple cloud providers.","Implementation Details and Key Features":"Key features involve streamlining deployment workflows and in-editor cloud resource management. The project uses Node.js and likely employs APIs specific to each cloud provider (AWS, Azure, GCP) to manage resources. Credential management is implemented with secure storage.","Integration Approaches and Protocols":"The repository integrates with cloud platforms using provider-specific APIs. Configuration relies on environment variables. Integration with Cursor IDE is central, suggesting the use of Cursor IDE's extension API. The use of npm suggests a JavaScript/Node.js based integration approach.","Technical Stack and Dependencies":"The project primarily uses Node.js (v14+) and Git. Dependencies are managed using npm. The development environment suggests a standard JavaScript development stack. The `.env` file indicates the use of environment variables for configuration.","Best Practices and Coding Standards":"The repository encourages contributions via pull requests and adheres to the MIT License. A clear project structure is enforced, separating concerns. Documentation is provided for installation, configuration, and usage. Testing practices are incorporated using npm test.","Performance Considerations":"Performance is not explicitly addressed in the provided documentation. However, efficient resource management and optimized deployment workflows are implied. The use of Node.js allows for asynchronous operations which can improve responsiveness. Cloud provider APIs used will significantly affect performance."}}', '2025-03-10 21:26:21+00', 'true', 'Servers', 'cursor-mcp-servers'), ('946131172', 'stock_mcp_server', 'ingpoc/stock_mcp_server', 'https://github.com/ingpoc/stock_mcp_server', null, '2025-03-10 16:54:56+00', '2025-03-10 17:53:49+00', 'Python', '0', '# Indian Stock Analysis MCP Server

A Model Context Protocol (MCP) server for interacting with MongoDB stock data to provide portfolio recommendations and market insights, exclusively for the Indian stock market (NSE/BSE).

## Overview

This MCP server integrates with:
1. MongoDB database (`stock_data`) for direct stock data access
2. Alpha Vantage API for Indian market data (NSE/BSE only) with free tier limitations
3. Knowledge Graph for persistent Indian stock analysis

It enables Claude to access your Indian stock portfolio holdings, analyze stock performance, and provide personalized recommendations directly through the Claude desktop app, while building a persistent knowledge graph for improved context awareness.

## Features

- **Exclusive Indian Market Focus**: Designed specifically for NSE and BSE listed stocks
- **Direct MongoDB Access**: Queries MongoDB directly without going through an API layer
- **Knowledge Graph Integration**: Maintains persistent analysis data for Indian stocks
- **Alpha Vantage Rate Limiting**: Handles free tier API limitations with automatic rate limiting
- **Modular Architecture**: Clean, maintainable code structure with separation of concerns
- **Environment Configuration**: Uses .env for easy configuration
- **LLM-Optimized Data**: Automatically limits and simplifies data responses to be easily processed by Claude
- **Segmented Portfolio Analysis**: Processes large portfolios in smaller segments to prevent timeouts

## Installation

### Prerequisites

- Python 3.9+
- MongoDB running on localhost:27017 with database "stock_data"
- Alpha Vantage API key (free tier supported, get one at https://www.alphavantage.co/support/#api-key)
- Claude Desktop app (available at https://claude.ai/download)

### Setup

1. Clone this repository:
   ```bash
   git clone <repository-url>
   cd stock-mcp-server
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Create a .env file from the example:
   ```bash
   cp .env.example .env
   ```

4. Edit the .env file to configure your settings:
   ```
   # MongoDB Configuration
   MONGODB_URI=mongodb://localhost:27017
   MONGODB_DB_NAME=stock_data
   
   # Alpha Vantage API
   ALPHA_VANTAGE_API_KEY=your_api_key_here
   
   # Indian Stock Market Settings
   ALPHA_VANTAGE_DEFAULT_EXCHANGE=NSE  # NSE or BSE
   ```

## Usage

### Start the MCP Server

```bash
python server.py
```

### Configure Claude Desktop App

Claude Desktop uses a configuration file to connect to MCP servers. You'll need to create or edit this file to include your stock analysis server.

1. Create or edit the configuration file at:
   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`
   - Windows: `%APPDATA%\Claude\claude_desktop_config.json`

2. Add the stock analysis MCP server configuration:

```json
{
  "mcpServers": {
    "stock-analysis-mcp": {
      "command": "/path/to/your/python",
      "args": [
        "/path/to/stock_mcp_server/server.py"
      ],
      "cwd": "/path/to/stock_mcp_server",
      "env": {
        "ALPHA_VANTAGE_API_KEY": "your_api_key_here",
        "MONGODB_URI": "mongodb://localhost:27017",
        "MONGODB_DB_NAME": "stock_data",
        "MONGODB_HOLDINGS_COLLECTION": "holdings",
        "MONGODB_FINANCIALS_COLLECTION": "detailed_financials",
        "MONGODB_KNOWLEDGE_GRAPH_COLLECTION": "stock_knowledge_graph",
        "ALPHA_VANTAGE_BASE_URL": "https://www.alphavantage.co/query",
        "ALPHA_VANTAGE_RATE_LIMIT_MINUTE": "5",
        "ALPHA_VANTAGE_RATE_LIMIT_DAY": "500",
        "ALPHA_VANTAGE_DEFAULT_EXCHANGE": "NSE",
        "MCP_SERVER_NAME": "stock-analysis-mcp",
        "MCP_SERVER_VERSION": "0.1.0",
        "LOG_LEVEL": "INFO",
        "CACHE_ENABLED": "True",
        "CACHE_TTL": "3600"
      }
    }
  }
}
```

**Important Notes:**
- Use the full path to your Python executable (e.g., `/usr/bin/python3` or `C:\Python39\python.exe`)
- The server key `stock-analysis-mcp` must use hyphens, not underscores
- The `MCP_SERVER_NAME` in env must match the server key
- Make sure all paths are absolute paths

3. Start or restart Claude Desktop

4. Verify the server connection:
   - Open Claude Desktop
   - The tools icon (hammer) should appear in the interface
   - Start a new conversation (important - new tools may not appear in existing conversations)
   - Try asking about your stock portfolio

### Example Prompts

Once configured, you can ask Claude:

- "Can you provide a summary of my portfolio?"
- "Analyze segment 1 of my portfolio"
- "Analyze the next segment of my portfolio with segment_size=8"
- "Analyze segment 3 of my portfolio with detailed metrics"
- "What are the stocks in segment 2 of my portfolio?" 
- "Which sectors are most represented in my portfolio?"
- "Can you provide recommendations on my Indian stock portfolio?"
- "Analyze the banking stocks in my portfolio"
- "What market trends should I be aware of in the Indian market this quarter?"
- "Which stocks in my portfolio should I consider selling?"
- "What new NSE stocks would complement my current portfolio?"
- "Find me the best performing stocks on BSE this week"
- "Can you find information about Reliance Industries stock?"
- "What are the technical indicators for TCS stock?"
- "Search for HDFC related stocks in the Indian market"

## Troubleshooting

### MCP Tools Not Appearing

If the tools aren't appearing in Claude despite successful server startup:

1. **Check your logs**:
   ```bash
   tail -f ~/Library/Logs/Claude/mcp-server-stock-analysis-mcp.log
   ```

2. **Verify server naming**:
   - Ensure the server key in `claude_desktop_config.json` uses hyphens (`stock-analysis-mcp`)
   - Make sure the `MCP_SERVER_NAME` environment variable matches this name

3. **Start a new conversation** in Claude after making changes

4. **Enable developer mode** in Claude's settings

5. **Check paths**: Make sure all paths in the configuration are absolute and correct

6. **Full Python path**: Use the full path to your Python executable (e.g., run `which python` to find it)

7. **Check Claude app log**: 
   ```bash
   tail -f ~/Library/Logs/Claude/app.log
   ```

### Common Issues

1. **Python not found**: Ensure the Python path in the configuration is correct
2. **MongoDB connection issues**: Check that MongoDB is running on the configured URI
3. **Tool schema issues**: Make sure all tools have proper schema definitions
4. **Claude Desktop version**: Keep Claude Desktop updated to the latest version
5. **Server name mismatch**: Server name in code must match configuration file
6. **Large data responses**: If Claude struggles processing large datasets, use segmented analysis:
   - First get a portfolio summary with `get_portfolio_summary`
   - Then analyze one segment at a time with `portfolio_analysis` using the segment parameter
   - Use smaller segment sizes (3-5 stocks) for detailed analysis

### Handling Large Portfolios

For large portfolios (20+ stocks), the server implements a segmented approach:

1. **Get Portfolio Summary**:
   - Use `get_portfolio_summary` to see an overview of your portfolio
   - This shows total stocks, sectors, and recommended segmentation

2. **Analyze in Segments**:
   - Use `portfolio_analysis` with the `segment` parameter
   - Start with segment 1, then progress through each segment
   - Example: "Analyze segment 1 of my portfolio", then "Analyze segment 2..."

3. **Control Segment Size**:
   - Default segment size is 5 stocks
   - Use `segment_size` parameter to adjust (1-10 stocks per segment)
   - Example: "Analyze segment 1 with segment_size=3 for detailed analysis"

4. **Request Levels of Detail**:
   - Use `include_details=false` for quick overview (default)
   - Use `include_details=true` for comprehensive metrics and insights

## Testing

This repository includes a comprehensive test suite for validating the functionality of the MCP server.

### Available Tests

- `test_mcp_server.py`: A comprehensive test script that validates all endpoints and tools in the MCP server.
- `test_client.py`: A simple client for testing MCP server endpoints.
- `test_server_startup.sh`: Script to test the server startup process

### Running the Tests

#### Quick Startup Test

To verify the server can start properly:

```bash
./test_server_startup.sh
```

#### Comprehensive Test Script

To run the comprehensive test script:

```bash
python test_mcp_server.py
```

This will test all aspects of the server, including:
- MongoDB connection
- Database operations (portfolio holdings, knowledge graph)
- Indian stock symbol formatting
- Alpha Vantage API integration
- Trending stocks functionality
- MCP tools handler

#### Skip Alpha Vantage API Tests

If you want to skip the tests that make actual API calls to Alpha Vantage (to avoid rate limits):

```bash
python test_mcp_server.py --skip-alpha-vantage
```

### Test Results

The script will output detailed logs to both the console and a `test_mcp_server.log` file. At the end, you'll see a summary of test results:

```
=== Test Results Summary ===
database_connection: ✅ PASSED
portfolio_holdings: ✅ PASSED
knowledge_graph: ✅ PASSED
symbol_formatting: ✅ PASSED
alpha_vantage_api: ✅ PASSED
trending_stocks: ✅ PASSED
tools_handler: ✅ PASSED

Overall: 7/7 tests passed
```

### Troubleshooting Tests

If tests fail, check the following:

1. **Database Connection**: Ensure MongoDB is running on the configured URI (default: `mongodb://localhost:27017`) and the `stock_data` database exists.

2. **Alpha Vantage API**: Check that your API key is set in the `.env` file and has not exceeded rate limits.

3. **Data Availability**: Some tests require existing data in the MongoDB collections (e.g., portfolio holdings). Ensure your database has the necessary data.

4. **Network Issues**: The Alpha Vantage tests require internet connectivity.

### Exit Codes

- `0`: All tests passed
- `1`: One or more tests failed

This allows you to integrate the tests into continuous integration pipelines that check for non-zero exit codes.

## Indian Stock Market Notes

When working with the Indian stock market, keep in mind:

- NSE symbols are formatted as `NSE:SYMBOL` (e.g., `NSE:RELIANCE`)
- BSE symbols are formatted as `BSE:CODE` (e.g., `BSE:500325`)
- If no exchange is specified, the default is NSE
- Alpha Vantage free tier is limited to 5 calls per minute, 500 calls per day
- This server is exclusively designed for Indian stocks and will not process requests for other markets

## Architecture

The project follows a modular architecture for better maintainability and separation of concerns:

```
├── server.py           # Main entry point
├── src/                # Source code
│   ├── __init__.py     # Package initialization
│   ├── config.py       # Configuration module
│   ├── handlers/       # MCP handler implementations
│   │   ├── __init__.py
│   │   ├── resources.py  # Resource handlers
│   │   ├── tools.py      # Tool handlers for Indian stock analysis
│   │   └── prompts.py    # Prompt handlers
│   └── utils/          # Utility modules
│       ├── __init__.py
│       ├── database.py   # MongoDB operations
│       └── alpha_vantage.py  # Alpha Vantage API operations for Indian market
├── test_mcp_server.py  # Comprehensive test script
├── test_client.py      # Simple test client
├── requirements.txt    # Dependencies
└── .env.example        # Example environment configuration
```

## Tools

The server provides the following MCP tools, organized by category:

### Portfolio Analysis
- **Get Portfolio Holdings**: Retrieve current Indian stock portfolio with basic information
  - Parameters:
    - `limit`: Maximum number of holdings to return (default: 10, max: 50)
    - `summary`: Return simplified stock data (default: true)
- **Get Portfolio Summary**: Get a high-level overview of the portfolio 
  - Returns:
    - Total stocks count
    - Sector breakdown
    - Segmentation guide
    - Analysis recommendations
- **Analyze Portfolio**: Analyze portfolio holdings with metrics and recommendations
  - Parameters:
    - `limit`: Maximum stocks to analyze (default: 10, max: 30)
    - `include_details`: Include comprehensive metrics (default: false)
    - `segment`: Which segment of stocks to analyze (default: 1)
    - `segment_size`: Number of stocks per segment (default: 5, max: 10)

### Stock Recommendations
- **Get Stock Recommendations**: Get recommendations for Indian stocks to add based on financial metrics
- **Get Removal Recommendations**: Identify Indian stocks that should be removed from portfolio

### Market Trends
- **Get Market Trend Recommendations**: Find must-buy Indian stocks based on current market trends

### Knowledge Graph
- **Query Knowledge Graph**: Query the Indian stock knowledge graph for historical analysis and insights

### Market Data
- **Get Alpha Vantage Data**: Access Alpha Vantage API data for Indian stock market with free tier limitations 
- **Search Stock Symbol**: Search for Indian stock symbols by name or keywords

### Technical Analysis
- **Get Technical Analysis**: Get technical analysis indicators for an Indian stock (SMA, RSI)

## Knowledge Graph

The server maintains a knowledge graph in MongoDB (collection: `stock_knowledge_graph`) that stores:

- Historical analyses of Indian stocks
- Performance metrics and trends for NSE and BSE listed companies
- Portfolio inclusion/exclusion recommendations
- Technical and fundamental insights
- Indian market-specific trends and patterns

This provides Claude with persistent memory about your Indian stock portfolio and stocks of interest.

## Data Optimization for Claude

This server implements several strategies to optimize data for Claude's consumption:

1. **Automatic Data Limiting**: 
   - Default limits on query results to prevent overwhelming Claude with too much information
   - Portfolio holdings limited to 10 stocks by default (configurable up to 50)
   - Recommendations limited to 5 stocks by default

2. **Data Simplification**:
   - Portfolio holdings simplified to essential fields only (symbol, quantity, average price)
   - Full details available when needed for analysis but hidden from direct display

3. **Parameter Controls**:
   - Tools expose parameters to allow Claude to request more or less data as needed
   - Ability to toggle between summary and detailed views

4. **Segmented Processing**:
   - Portfolio analysis can be performed in segments to prevent large responses
   - Each segment processes a subset of stocks (default: 5 stocks per segment)
   - Allows Claude to analyze large portfolios without hitting connection timeouts

5. **Intelligent Internal Handling**:
   - Analysis functions use complete data internally while presenting simplified results
   - Market trend recommendations filter out stocks already in portfolio

These optimizations ensure Claude can process your stock data efficiently without being overwhelmed by excessive information.

## Alpha Vantage Free Tier Support

This server is optimized for Alpha Vantage's free tier API, which includes:
- Rate limiting to 5 API calls per minute
- Daily limit of 500 API calls
- Support for key endpoints:
  - `GLOBAL_QUOTE` - Current price information
  - `TIME_SERIES_DAILY` - Daily price history
  - `OVERVIEW` - Company information
  - `SYMBOL_SEARCH` - Finding Indian stock symbols
  - Basic technical indicators (SMA, RSI)

The server implements automatic rate limiting to ensure you stay within these limits.

## Configuration

The server uses the following environment variables (defined in `.env`):

### MongoDB Configuration
- `MONGODB_URI`: MongoDB connection string (default: mongodb://localhost:27017)
- `MONGODB_DB_NAME`: Database name (default: stock_data)
- `MONGODB_HOLDINGS_COLLECTION`: Holdings collection name (default: holdings)
- `MONGODB_FINANCIALS_COLLECTION`: Financials collection name (default: detailed_financials)
- `MONGODB_KNOWLEDGE_GRAPH_COLLECTION`: Knowledge graph collection name (default: stock_knowledge_graph)

### Alpha Vantage API
- `ALPHA_VANTAGE_API_KEY`: Your Alpha Vantage API key
- `ALPHA_VANTAGE_BASE_URL`: Alpha Vantage API URL (default: https://www.alphavantage.co/query)
- `ALPHA_VANTAGE_RATE_LIMIT_MINUTE`: API calls allowed per minute (default: 5)
- `ALPHA_VANTAGE_RATE_LIMIT_DAY`: API calls allowed per day (default: 500)
- `ALPHA_VANTAGE_DEFAULT_EXCHANGE`: Default exchange for Indian stocks (default: NSE)

### MCP Server Configuration
- `MCP_SERVER_NAME`: Server name (default: stock-analysis-mcp)
- `MCP_SERVER_VERSION`: Server version (default: 0.1.0)

### Logging Configuration
- `LOG_LEVEL`: Logging level (default: INFO)
- `LOG_FORMAT`: Log message format

### Cache Settings
- `CACHE_ENABLED`: Enable caching (default: True)
- `CACHE_TTL`: Cache time-to-live in seconds (default: 3600)

## License

[MIT License](LICENSE) ', 'true', '"{\"AI Model Integration\",\"Context Management\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository implements a Model Context Protocol (MCP) server specifically designed for interacting with a MongoDB database containing stock data, and the Claude desktop app for personalized stock recommendations.","categories":["AI Model Integration","Context Management","Function Calling"],"analysis":{"Architecture and Design Patterns":"The architecture is modular, separating concerns into handlers (resources, tools, prompts) and utilities (database, Alpha Vantage API). It uses a `.env` file for configuration. The project is structured to promote maintainability with designated modules for database interactions, API calls, and tool implementations. The tests further promote the stability of the software.","Implementation Details and Key Features":"The server provides several MCP tools including portfolio analysis, stock recommendations, market trend analysis, and knowledge graph querying. It integrates with MongoDB for stock data and a knowledge graph and uses the Alpha Vantage API for market data. Key features include direct MongoDB access, knowledge graph integration, and handling of Alpha Vantage API rate limits.","Integration Approaches and Protocols":"The project facilitates integration between Claude and a stock data backend. It leverages the Model Context Protocol (MCP) to connect the Claude desktop app to the custom stock analysis server. The configuration involves specifying command-line arguments and environment variables for the server, defined in a configuration JSON.","Technical Stack and Dependencies":"The technical stack includes Python 3.9+, MongoDB, and the Alpha Vantage API. Key Python dependencies listed in `requirements.txt` are used for interacting with MongoDB, handling environment variables, and making API requests. The server relies on Claude Desktop to communicate via MCP.","Best Practices and Coding Standards":"The repository uses environment variables for configuration, promoting security and portability. The modular architecture and separation of concerns contribute to maintainable code. The inclusion of a test suite with comprehensive tests indicates a commitment to code quality and reliability. It also implements data optimization techniques for LLM consumption.","Performance Considerations":"The server addresses performance through segmented portfolio analysis to prevent timeouts when processing large portfolios. It implements automatic rate limiting to comply with the Alpha Vantage API's free tier limitations. Caching mechanisms are included to reduce API calls. It also uses intelligent handling to reduce amount of data sent to the LLM."}}', '2025-03-10 21:26:36+00', 'true', 'Servers', 'stock-mcp-server'), ('946133386', 'beatport-mcp', 'larsenweigle/beatport-mcp', 'https://github.com/larsenweigle/beatport-mcp', 'MCP for the Beatport Developer API', '2025-03-10 16:59:04+00', '2025-03-10 17:01:46+00', null, '0', '# beatport-mcp
[MCP](https://modelcontextprotocol.io/introduction) for the Beatport Developer API
', 'true', '"{\"API Frameworks\",\"Protocol Implementations\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states it's an MCP implementation for the Beatport Developer API and links to the Model Context Protocol documentation.","categories":["API Frameworks","Protocol Implementations","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"The architecture likely centers around defining how AI models can interact with the Beatport Developer API. It probably uses a request-response pattern, with models sending requests and the API providing relevant data or actions in response. The design will focus on standardizing data formats and ensuring compatibility with the MCP.","Implementation Details and Key Features":"The implementation likely involves creating standardized API endpoints for common tasks and defining the expected data structures. Key features probably include authentication, data retrieval, and action execution (e.g., creating playlists). Adherence to the MCP requires standardized communication protocols.","Integration Approaches and Protocols":"Integration likely involves using standard HTTP/HTTPS protocols for communication. MCP promotes standardized methods for managing context and interactions. This may necessitate the use of specific data formats and request structures to facilitate seamless integration with AI models.","Technical Stack and Dependencies":"The technical stack likely includes Python (or another relevant language) along with standard libraries for HTTP requests and JSON parsing. Dependencies will likely include libraries for handling API authentication and data serialization/deserialization. It may also include libraries relevant to the Beatport API.","Best Practices and Coding Standards":"Best practices would involve adhering to the Model Context Protocol specifications. This includes using consistent data formats, robust error handling, and comprehensive documentation. Coding standards should ensure the code is maintainable and scalable for future integrations and features.","Performance Considerations":"Performance considerations involve optimizing API request processing and minimizing latency. Efficient data retrieval from the Beatport API is essential. Caching mechanisms may be employed to reduce the load on the Beatport API and improve response times. Scalability is key for handling many AI model requests."}}', '2025-03-10 21:26:21+00', 'true', 'Servers', 'beatport-mcp-api'), ('946137895', 'flux-images-mcp', 'tomcat65/flux-images-mcp', 'https://github.com/tomcat65/flux-images-mcp', null, '2025-03-10 17:07:02+00', '2025-03-10 17:09:37+00', 'JavaScript', '0', '# Flux Image MCP

## 🌟 Overview
Flux Image MCP is a **Model Context Protocol (MCP)** designed for **image generation and modification** using the **Flux Schnell** model on Replicate. With this tool, users can **generate AI-powered images** from text prompts and **modify existing images** through an interactive drag-and-drop interface.

---

## ✨ Features
✔️ **Generate images** using AI from a descriptive text prompt.
✔️ **Modify images** by applying AI transformations.
✔️ **Drag and drop** files into the terminal for easy modification.
✔️ **Interactive CLI** guides users step by step.
✔️ **Automatically saves** generated images in `images-from-prompt/`.
✔️ **Automatically saves** modified images in `modified_images/`.

---

## 🛠️ Installation

### 📌 Prerequisites
Ensure you have the following installed:
- [Node.js](https://nodejs.org) (latest LTS version recommended)
- TypeScript
- A Replicate API token

### 📥 Setup Instructions

1. **Clone the Repository**
   ```sh
   git clone <your-repo-url>
   cd flux-image-mcp
   ```

2. **Install Dependencies**
   ```sh
   npm install
   ```

3. **Set Up Environment Variables**
   Create a `.env` file and add your Replicate API token:
   ```sh
   echo "REPLICATE_API_TOKEN=your-token-here" > .env
   ```

4. **Run in Development Mode**
   ```sh
   npm run dev
   ```

---

## 🚀 Usage

### 🖼️ Generating an Image
1. Run the MCP server:
   ```sh
   node index.ts
   ```
2. Enter a **text prompt** when prompted (e.g., "A futuristic cyberpunk city at night").
3. The **generated image** will be saved in `images-from-prompt/`.

### ✨ Modifying an Existing Image
1. Run the MCP server:
   ```sh
   node index.ts
   ```
2. **Drag and drop** an image file when prompted **or** manually enter the file path.
3. Enter a **modification description** (e.g., "Make it look like a watercolor painting").
4. The **modified image** will be saved in `modified_images/`.

---

## 📦 Directory Structure

```sh
flux-image-mcp/
│── images/               # Original images to modify
│── modified_images/      # AI-modified images
│── images-from-prompt/   # AI-generated images
│── src/                 # Source code
│── index.ts             # Main MCP implementation
│── package.json         # Project dependencies
│── tsconfig.json        # TypeScript config
│── README.md            # Project documentation
```

---

## 🚢 Deployment
For production, build and run the project using:
```sh
npm run build
npm start
```

---

## 📜 License
This project is licensed under the **MIT License**. Feel free to modify and distribute it under the terms of the license.

## 🤝 Contributing
We welcome contributions! Feel free to **open issues** or submit **pull requests**.

## 📞 Contact
For questions or support, reach out via GitHub issues or email.

---

🚀 **Happy coding!** 🎨✨
', 'true', '"{\"AI Model Integration\",\"Function Calling\",\"Prompt Engineering Tools\"]}', '{"is_mcp":"YES","justification":"The repository explicitly states it is a Model Context Protocol (MCP) designed for image generation and modification using the Flux Schnell model on Replicate, aligning with the definition of MCP repositories.","categories":["AI Model Integration","Function Calling","Prompt Engineering Tools"],"analysis":{"Architecture and Design Patterns":"The architecture revolves around a command-line interface (CLI) that interacts with the Replicate API for image generation and modification. The design incorporates a simple directory structure for organizing original, generated, and modified images.  The application is designed to be run from a terminal, guiding the user through interactive steps.","Implementation Details and Key Features":"The implementation uses Node.js and TypeScript. Key features include generating images from text prompts, modifying existing images via prompts, and saving images in specific directories. The tool takes user prompts as input and leverages a model via API calls, showcasing a direct model integration approach. The process involves executing node commands to initiate the server.","Integration Approaches and Protocols":"The primary integration approach involves using the Replicate API to interact with the Flux Schnell model.  The application uses environment variables for API key management.  It utilizes standard file system operations for image storage and retrieval. The program uses standard input for user interaction and terminal-based prompts.","Technical Stack and Dependencies":"The technical stack consists of Node.js, TypeScript, and the Replicate API.  The `package.json` file would list specific versions of dependencies used.  The application's operation hinges on a valid Replicate API token for authentication and model access. The .env file holds these variables.","Best Practices and Coding Standards":"The documentation suggests using the latest LTS version of Node.js, indicating a focus on stability. Environment variables are used for sensitive information like API keys, which is a security best practice.  The repository structure is well-organized with separate directories for different image types. It uses a MIT license.","Performance Considerations":"Performance depends heavily on the Replicate API's response times and the Flux Schnell model's processing speed.  The CLI-based interaction minimizes client-side overhead. Image size and prompt complexity may impact latency.  There is no evidence of performance optimization techniques implemented in the current documentation."}}', '2025-03-10 21:26:16+00', 'true', 'Servers', 'flux-images-mcp'), ('946140133', 'SwiftMCP', 'Cocoanetics/SwiftMCP', 'https://github.com/Cocoanetics/SwiftMCP', 'Model Context Protocol Server for Swift', '2025-03-10 17:11:21+00', '2025-03-10 18:02:29+00', 'Swift', '4', '# SwiftMCP

SwiftMCP is a Swift library that makes supporting the Model Context Protocol (MCP) easy. It uses Swift macros to automatically extract function metadata and generate the necessary JSON-RPC interface for MCP communication.

## What is MCP?

The Model Context Protocol (MCP) is a standardized way for AI models to interact with external tools and services. SwiftMCP makes it simple to expose your Swift functions as MCP-compatible tools that can be called by AI models.

## MCP Transport Modes

MCP supports two transport modes:

- **stdio mode** ✅ - Fully implemented in SwiftMCP
  - Communication happens over standard input/output
  - Simple to implement and use for command-line tools
  - Perfect for local development and testing

- **HTTP+SSE mode** ⏳ - Not yet implemented
  - Communication over HTTP with Server-Sent Events
  - Better for networked applications and services
  - Coming in a future release

## Features

- **Simple Macro-Based API**: Just add `@MCPServer` and `@MCPTool` annotations to your code
- **Automatic Documentation Extraction**: Parameter names, types, and descriptions are extracted from your Swift documentation
- **JSON-RPC Interface**: Fully compliant with the MCP specification
- **Type Safety**: Leverages Swift's type system for safe parameter handling
- **Default Values Support**: Handles parameters with default values
- **Command-Line Interface**: Ready-to-use CLI for testing and integration

## Quick Start

Here's how to create an MCP-compatible server in just a few lines of code:

```swift
import SwiftMCP

// 1. Annotate your class with @MCPServer
@MCPServer(name: "MyCalculator", version: "1.0.0")
class Calculator {
    // 2. Add documentation comments that describe your function and parameters
    /// Adds two integers and returns their sum
    /// - Parameter a: First number to add
    /// - Parameter b: Second number to add
    /// - Returns: The sum of a and b
    // 3. Annotate your function with @MCPTool
    @MCPTool
    func add(a: Int, b: Int) -> Int {
        return a + b
    }
    
    /// Divides the numerator by the denominator
    /// - Parameter numerator: Number to be divided
    /// - Parameter denominator: Number to divide by (defaults to 1.0)
    /// - Returns: The quotient of numerator divided by denominator
    @MCPTool
    func divide(numerator: Double, denominator: Double = 1.0) -> Double {
        return numerator / denominator
    }
}

// 4. That's it! Your class now has MCP capabilities
let calculator = Calculator()

// Process MCP requests
let request = JSONRPCRequest(
    jsonrpc: "2.0",
    id: 1,
    method: "tools/call",
    params: [
        "name": AnyCodable("add"),
        "arguments": AnyCodable(["a": 5, "b": 3])
    ]
)

// The response will be a properly formatted MCP response
let response = calculator.handleRequest(request)
```

## Implementing the stdio Processing Loop

At present, the stdio processing loop needs to be implemented manually as shown in the demo app SwiftMCPDemo:

```swift
// Create an instance of the Calculator
let calculator = Calculator()

do {
    while true {
        if let input = readLine(),
           !input.isEmpty,
           let data = input.data(using: .utf8)
        {
            let request = try JSONDecoder().decode(SwiftMCP.JSONRPCRequest.self, from: data)
            
            // Handle the request
            if let response = calculator.handleRequest(request) {
                
                let data = try JSONEncoder().encode(response)
                let json = String(data: data, encoding: .utf8)!
                
                // Print the response and flush immediately
                print(json)
                fflush(stdout)
            }
        } else {
            // If no input is available, sleep briefly and try again
            Thread.sleep(forTimeInterval: 0.1)
        }
    }
}
catch
{
    fputs("\(error.localizedDescription)\n", stderr)
}
```

This loop:
1. Continuously reads from standard input
2. Decodes each line as a JSON-RPC request
3. Processes the request using your MCP server
4. Encodes and prints the response to standard output
5. Flushes stdout to ensure immediate delivery
6. Sleeps briefly if no input is available

## How It Works

SwiftMCP uses Swift macros to analyze your code at compile time:

1. **Documentation Extraction**: The `@MCPTool` macro extracts parameter names, types, and descriptions from your documentation comments
2. **Schema Generation**: It automatically generates JSON Schema for your function parameters
3. **Server Configuration**: The `@MCPServer` macro adds the necessary infrastructure to handle JSON-RPC requests

## JSON-RPC Interface

SwiftMCP implements the standard MCP JSON-RPC interface:

- `initialize`: Sets up the connection and returns server capabilities
- `tools/list`: Returns a list of available tools with their schemas
- `tools/call`: Calls a specific tool with the provided arguments

## Advanced Usage

### Custom Tool Descriptions

You can provide a custom description for a tool:

```swift
@MCPTool(description: "Custom description for this tool")
func myFunction(param: String) -> String {
    // ...
}
```

### Server Name and Version

Customize your server's name and version:

```swift
@MCPServer(name: "MyCustomServer", version: "2.5.0")
class MyServer {
    // ...
}
```

## License

This project is licensed under the BSD 2-clause License - see the LICENSE file for details. 
', 'true', '"{\"Protocol Implementations\",\"AI Model Integration\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The repository description explicitly states that it supports Model Context Protocol (MCP) and provides a Swift library for making Swift functions MCP-compatible, along with JSON-RPC interface implementation.","categories":["Protocol Implementations","API Frameworks","AI Model Integration"],"analysis":{"Architecture and Design Patterns":"SwiftMCP adopts a macro-based architecture to simplify the creation of MCP-compatible servers. The library uses Swift macros, such as `@MCPServer` and `@MCPTool`, to automatically extract metadata from code and generate the necessary JSON-RPC interface. This approach reduces boilerplate code and improves developer experience by automating schema generation and server configuration.","Implementation Details and Key Features":"The library provides a simple API based on Swift macros. It automatically extracts function metadata including parameter names, types, and descriptions from documentation. The implementation covers JSON-RPC interface compliance, type safety using Swift's type system, default values support for parameters, and includes a command-line interface for testing and integration. Only stdio transport mode is fully implemented.","Integration Approaches and Protocols":"SwiftMCP integrates with other systems using the Model Context Protocol (MCP) via JSON-RPC. It handles requests for initialization, listing available tools with schemas, and calling specific tools with provided arguments. Currently, it fully supports the `stdio` transport mode for communication. HTTP+SSE is planned for future releases. The server processes requests and returns responses in JSON format.","Technical Stack and Dependencies":"The repository is built using Swift and leverages Swift macros for code analysis and generation. It utilizes `JSONEncoder` and `JSONDecoder` for JSON serialization and deserialization. The library also uses `AnyCodable` to handle `Any` types. There are no explicit external dependencies listed other than standard Swift libraries.","Best Practices and Coding Standards":"The code adheres to Swift's coding standards, utilizing documentation comments for function and parameter descriptions. The use of macros promotes code reuse and reduces redundancy. The library implements type safety through Swift's type system. Error handling is implemented within the `stdio` processing loop. The code provides clear examples of how to use the library's API.","Performance Considerations":"The library supports stdio transport mode, which is suitable for local development and testing. The library relies on `JSONEncoder` and `JSONDecoder` for encoding and decoding JSON objects, which can introduce performance overhead. The manual implementation of the stdio processing loop could be further optimized for performance. HTTP+SSE transport mode, planned for future releases, might introduce other performance considerations."}}', '2025-03-10 21:26:36+00', 'true', 'Servers', 'swift-mcp-server'), ('946145880', 'mcp-server', 'groovyBugify/mcp-server', 'https://github.com/groovyBugify/mcp-server', 'Open-source MCP (Mission Control Panel) server for AWS - A powerful, region-independent backend service for AWS operations', '2025-03-10 17:22:09+00', '2025-03-10 17:28:50+00', 'TypeScript', '0', '# MCP Server

Open-source Mission Control Panel (MCP) server for AWS - A powerful, region-independent backend service for AWS operations.

## Features

- Region-independent AWS operations
- Credential validation and management
- Secure API endpoints
- Comprehensive error handling
- TypeScript support
- Built with security and scalability in mind

## Prerequisites

- Node.js (v16 or higher)
- npm or yarn
- AWS credentials (Access Key ID and Secret Access Key) or IAM role

## Installation

1. Clone the repository:
```bash
git clone https://github.com/groovyBugify/mcp-server.git
cd mcp-server
```

2. Install dependencies:
```bash
npm install
```

3. Create environment file:
```bash
cp .env.example .env
```

4. Configure your environment variables in `.env`

## Development

Start the development server:
```bash
npm run dev
```

## Build

Compile the TypeScript code:
```bash
npm run build
```

## Production

Start the production server:
```bash
npm start
```

## API Endpoints

### Validate AWS Credentials

```http
POST /api/aws/validate-credentials
Content-Type: application/json

{
  "region": "us-east-1" // optional, defaults to us-east-1
}
```

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.', 'true', '"{\"API Frameworks\",\"Cloud Infrastructure\",\"DevOps Tools\"]}', '{"is_mcp":"NO","justification":"While it uses the acronym MCP, this repository relates to a \"Mission Control Panel\" for AWS operations, not the Model Context Protocol for AI models. It focuses on AWS credential management and region-independent operations.","categories":["API Frameworks","Cloud Infrastructure","DevOps Tools"],"analysis":{"Architecture and Design Patterns":"The server follows a typical backend architecture, likely employing RESTful API principles for interacting with AWS services. It's designed to be region-independent, suggesting an abstraction layer to handle cross-region requests. Error handling is also a core architectural concern. The project utilizes environment variables for configuration, indicating a focus on deployment flexibility.","Implementation Details and Key Features":"The server implements API endpoints for validating AWS credentials. It uses Node.js and TypeScript. Key features include region-independent AWS operations, credential validation, and secure API endpoints. The implementation is designed with both security and scalability in mind, although specific scalability mechanisms are not detailed.","Integration Approaches and Protocols":"The server likely integrates with AWS services via the AWS SDK. It exposes API endpoints that accept JSON payloads via HTTP. The documented endpoint uses a POST request, demonstrating a client-server interaction model. The absence of specific protocol details suggests standard HTTP/HTTPS usage.","Technical Stack and Dependencies":"The technical stack includes Node.js (v16 or higher) and npm or yarn. The project utilizes TypeScript for development, which is compiled into JavaScript. AWS credentials or IAM roles are required for operation. The dependencies are likely to include packages for HTTP handling, AWS SDK integration, and potentially validation.","Best Practices and Coding Standards":"The use of TypeScript implies adherence to static typing and modern JavaScript coding standards. The `.env` file suggests a separation of configuration from code, which is a best practice. Comprehensive error handling is mentioned as a feature, indicating a focus on robust code. The presence of a LICENSE file promotes open-source collaboration.","Performance Considerations":"The repository description mentions scalability, indicating an awareness of performance. The use of Node.js implies an event-driven, non-blocking I/O model, which can enhance performance. However, specific performance optimization techniques (e.g., caching, connection pooling) are not explicitly mentioned."}}', '2025-03-10 21:26:16+00', 'true', 'Servers', 'mcp-aws-server'), ('946155816', 'gmail-imap-mcp', 'tonykipkemboi/gmail-imap-mcp', 'https://github.com/tonykipkemboi/gmail-imap-mcp', 'Gmail IMAP MCP server for integrating Gmail with Claude and other AI assistants', '2025-03-10 17:41:01+00', '2025-03-10 17:44:19+00', 'Python', '1', '# Gmail IMAP MCP Server

A Model Context Protocol (MCP) server for Gmail integration using IMAP. This server allows AI assistants to interact with Gmail accounts, providing functionality to read, search, and manage emails.

## Features

- OAuth2 authentication with Gmail
- Read emails from Gmail accounts
- Search emails with advanced query options
- View unread emails
- Send emails with attachments
- Manage labels (create, delete, list)
- Move emails between labels
- Download attachments
- Mark emails as read/unread
- Support for multiple Gmail accounts
- Integration with AI assistants through MCP

## Prerequisites

Before running the Gmail IMAP MCP server, ensure you have the following:

1. Python 3.12 or higher
2. Google Cloud Project with Gmail API enabled
3. OAuth 2.0 Client ID credentials

## Installation

### Install from source

1. Clone the repository:
   ```
   git clone https://github.com/yourusername/gmail-imap-mcp.git
   cd gmail-imap-mcp
   ```

2. Create and activate a virtual environment:
   ```
   python -m venv .venv
   # On Windows
   .venv\Scripts\activate
   # On Unix/MacOS
   source .venv/bin/activate
   ```

3. Install the package:
   ```
   pip install -e .
   ```

## Setup Google Cloud Project

1. Go to the [Google Cloud Console](https://console.cloud.google.com/)
2. Create a new project or select an existing one
3. Enable the Gmail API for your project:
   - Navigate to "APIs & Services" > "Library"
   - Search for "Gmail API" and enable it
4. Create OAuth 2.0 credentials:
   - Go to "APIs & Services" > "Credentials"
   - Click "Create Credentials" > "OAuth client ID"
   - Choose "Desktop app" as application type
   - Download the client configuration file
5. Save the downloaded file as `client_secret.json` in the credentials directory:
   ```
   mkdir -p ~/.gmail_imap_mcp_credentials
   # Move the downloaded file to ~/.gmail_imap_mcp_credentials/client_secret.json
   ```

## Architecture and Implementation Details

### Credential Storage

The Gmail IMAP MCP server stores OAuth2 credentials in the user's home directory at `~/.gmail_imap_mcp_credentials/`. This approach offers several advantages:

1. **Security**: Credentials are stored in a user-specific location rather than in the application directory
2. **Persistence**: Credentials persist across different sessions and application restarts
3. **Compatibility**: Avoids permission issues on read-only file systems

The credentials directory contains:
- `client_secret.json`: Your OAuth client credentials from Google Cloud Console
- Token files for each authenticated Gmail account (format: `token_{email_address}.json`)

### IMAP Implementation

The server uses Python's `imaplib2` library for IMAP operations with Gmail. Key implementation details include:

1. **Connection**: Secure connection to Gmail's IMAP server (`imap.gmail.com`) on port 993
2. **Authentication**: OAuth2 authentication using XOAUTH2 mechanism
3. **Email Retrieval**: Emails are retrieved using RFC822 format and parsed with Python's `email` module
4. **Label Management**: Gmail labels are managed through IMAP mailbox operations

### Email ID Format

Email IDs in the system follow this format:
```
email://message/{account}_{mailbox}_{id}
```

Where:
- `{account}`: The Gmail account address
- `{mailbox}`: The mailbox/label containing the email
- `{id}`: The unique IMAP ID of the email

This format allows the system to uniquely identify emails across different accounts and mailboxes.

## Usage

### Starting the Server

Run the Gmail IMAP MCP server:

```
gmail-imap-mcp
```

### Authenticating Gmail Accounts

1. Use the `authenticate-gmail` tool with your email address
2. Follow the OAuth2 authentication flow in your browser
3. Once authenticated, the server will store your credentials for future use

## Available Tools and Examples

The Gmail IMAP MCP server provides a comprehensive set of tools for interacting with Gmail accounts. Below is a detailed list of all available tools along with examples of how to use them.

### Authentication

#### 1. authenticate-gmail
Authenticate a Gmail account to use with the MCP server.

**Parameters:**
- `email`: Email address to authenticate

**Example:**
```json
{
  "name": "authenticate-gmail",
  "arguments": {
    "email": "your.email@gmail.com"
  }
}
```

### Email Retrieval and Search

#### 2. search-emails
Search for emails in a Gmail account using various search criteria.

**Parameters:**
- `account`: Email account to search in
- `mailbox`: Mailbox to search in (default: INBOX)
- `query`: Search query
- `limit`: Maximum number of emails to return (default: 10)

**Example - Search for emails from a specific sender:**
```json
{
  "name": "search-emails",
  "arguments": {
    "account": "your.email@gmail.com",
    "mailbox": "INBOX",
    "query": "from:sender@example.com",
    "limit": 5
  }
}
```

**Example - Search for emails with a specific subject:**
```json
{
  "name": "search-emails",
  "arguments": {
    "account": "your.email@gmail.com",
    "query": "subject:\"Meeting Invitation\""
  }
}
```

**Example - Search for emails with specific text in the body:**
```json
{
  "name": "search-emails",
  "arguments": {
    "account": "your.email@gmail.com",
    "query": "TEXT \"project update\""
  }
}
```

#### 3. get-unread-emails
Get unread emails from a Gmail account.

**Parameters:**
- `account`: Email account to get emails from
- `mailbox`: Mailbox to get emails from (default: INBOX)
- `limit`: Maximum number of emails to return (default: 10)

**Example:**
```json
{
  "name": "get-unread-emails",
  "arguments": {
    "account": "your.email@gmail.com",
    "limit": 20
  }
}
```

### Email Composition and Sending

#### 4. send-email
Send an email from a Gmail account with optional attachments and HTML content.

**Parameters:**
- `account`: Email account to send from
- `to`: Recipient email address(es), comma-separated for multiple
- `subject`: Email subject
- `body`: Plain text email body
- `cc`: Carbon copy recipients (optional)
- `bcc`: Blind carbon copy recipients (optional)
- `html_body`: HTML version of the email body (optional)
- `attachments`: List of attachment objects (optional)
  - Each attachment object requires:
    - `path`: Path to the file
    - `filename`: Custom filename (optional)
    - `content_type`: MIME type (optional)

**Example - Simple email:**
```json
{
  "name": "send-email",
  "arguments": {
    "account": "your.email@gmail.com",
    "to": "recipient@example.com",
    "subject": "Hello from Gmail MCP",
    "body": "This is a test email sent via the Gmail IMAP MCP server."
  }
}
```

**Example - Email with CC, BCC, and HTML content:**
```json
{
  "name": "send-email",
  "arguments": {
    "account": "your.email@gmail.com",
    "to": "recipient@example.com",
    "subject": "Meeting Agenda",
    "body": "Please find the agenda for our upcoming meeting.",
    "cc": "manager@example.com",
    "bcc": "archive@example.com",
    "html_body": "<h1>Meeting Agenda</h1><p>Please find the agenda for our <b>upcoming meeting</b>.</p>"
  }
}
```

**Example - Email with attachment:**
```json
{
  "name": "send-email",
  "arguments": {
    "account": "your.email@gmail.com",
    "to": "recipient@example.com",
    "subject": "Document Attached",
    "body": "Please find the attached document.",
    "attachments": [
      {
        "path": "/path/to/document.pdf",
        "filename": "important_document.pdf",
        "content_type": "application/pdf"
      }
    ]
  }
}
```

### Label Management

#### 5. create-label
Create a new label/mailbox in a Gmail account.

**Parameters:**
- `account`: Email account to create label in
- `label_name`: Name of the label to create

**Example:**
```json
{
  "name": "create-label",
  "arguments": {
    "account": "your.email@gmail.com",
    "label_name": "ProjectX"
  }
}
```

#### 6. delete-label
Delete a label/mailbox from a Gmail account.

**Parameters:**
- `account`: Email account to delete label from
- `label_name`: Name of the label to delete

**Example:**
```json
{
  "name": "delete-label",
  "arguments": {
    "account": "your.email@gmail.com",
    "label_name": "OldProject"
  }
}
```

#### 7. list-labels
List all labels/mailboxes in a Gmail account.

**Parameters:**
- `account`: Email account to list labels from

**Example:**
```json
{
  "name": "list-labels",
  "arguments": {
    "account": "your.email@gmail.com"
  }
}
```

### Email Organization

#### 8. move-email
Move an email from one label/mailbox to another.

**Parameters:**
- `account`: Email account
- `email_id`: Email ID to move (format: `email://message/{account}_{mailbox}_{id}`)
- `source_mailbox`: Source mailbox
- `target_mailbox`: Target mailbox

**Example:**
```json
{
  "name": "move-email",
  "arguments": {
    "account": "your.email@gmail.com",
    "email_id": "email://message/your.email@gmail.com_INBOX_12345",
    "source_mailbox": "INBOX",
    "target_mailbox": "ProjectX"
  }
}
```

### Attachment Handling

#### 9. download-attachment
Download an attachment from an email.

**Parameters:**
- `account`: Email account
- `email_id`: Email ID (format: `email://message/{account}_{mailbox}_{id}`)
- `attachment_index`: Index of the attachment to download (0-based)
- `mailbox`: Mailbox containing the email (default: INBOX)
- `download_dir`: Directory to save the attachment to (default: "downloads")

**Example:**
```json
{
  "name": "download-attachment",
  "arguments": {
    "account": "your.email@gmail.com",
    "email_id": "email://message/your.email@gmail.com_INBOX_12345",
    "attachment_index": 0,
    "download_dir": "my_attachments"
  }
}
```

### Email Status Management

#### 10. mark-as-read
Mark an email as read.

**Parameters:**
- `account`: Email account
- `email_id`: Email ID (format: `email://message/{account}_{mailbox}_{id}`)
- `mailbox`: Mailbox containing the email (default: INBOX)

**Example:**
```json
{
  "name": "mark-as-read",
  "arguments": {
    "account": "your.email@gmail.com",
    "email_id": "email://message/your.email@gmail.com_INBOX_12345"
  }
}
```

#### 11. mark-as-unread
Mark an email as unread.

**Parameters:**
- `account`: Email account
- `email_id`: Email ID (format: `email://message/{account}_{mailbox}_{id}`)
- `mailbox`: Mailbox containing the email (default: INBOX)

**Example:**
```json
{
  "name": "mark-as-unread",
  "arguments": {
    "account": "your.email@gmail.com",
    "email_id": "email://message/your.email@gmail.com_INBOX_12345"
  }
}
```

## Available Prompts

The server provides the following prompts for AI assistants to use:

### 1. summarize-emails
Creates a summary of recent emails.

**Parameters:**
- `account`: Email account to summarize
- `mailbox`: Mailbox to summarize (default: INBOX)
- `count`: Number of emails to summarize (default: 5)

**Example:**
```json
{
  "name": "summarize-emails",
  "arguments": {
    "account": "your.email@gmail.com",
    "mailbox": "INBOX",
    "count": 10
  }
}
```

## Integration with AI Assistants

The Gmail IMAP MCP server can be integrated with AI assistants that support the Model Context Protocol (MCP). Here's a typical workflow:

1. **Authentication**: The AI assistant uses the `authenticate-gmail` tool to authenticate the user's Gmail account.

2. **Email Management**: The assistant can retrieve, search, and manage emails using the various tools provided by the server.

3. **Email Composition**: The assistant can help draft and send emails based on user instructions.

4. **Email Organization**: The assistant can help organize emails by creating labels, moving emails between labels, and marking emails as read/unread.

5. **Email Summarization**: The assistant can summarize emails using the `summarize-emails` prompt.

## Connecting with AI Assistants

### Claude Desktop

To connect the Gmail IMAP MCP server with Claude Desktop:

1. Start the Gmail IMAP MCP server:
   ```bash
   python -m gmail_imap_mcp.server
   ```

2. Open Claude Desktop and navigate to Settings (gear icon)

3. Scroll down to the "Advanced" section and click on "Edit MCP Configuration"

4. Add the Gmail IMAP MCP server configuration:
   ```json
   {
     "servers": [
       {
         "name": "Gmail IMAP",
         "url": "http://localhost:8080",
         "tools": [
           "list-emails",
           "get-email",
           "search-emails",
           "send-email",
           "list-mailboxes",
           "create-label",
           "move-email",
           "mark-as-read",
           "download-attachment"
         ]
       }
     ]
   }
   ```

5. Click "Save" and restart Claude Desktop

6. You can now ask Claude to interact with your Gmail account, for example:
   - "Show me my unread emails"
   - "Send an email to [recipient] about [subject]"
   - "Create a new label called 'Important'"
   - "Move the email from [sender] to the 'Important' label"

### Windsurf IDE

To connect the Gmail IMAP MCP server with Windsurf IDE:

1. Start the Gmail IMAP MCP server:
   ```bash
   python -m gmail_imap_mcp.server
   ```

2. Open Windsurf IDE and navigate to Settings

3. Find the "AI Flow" or "MCP Configuration" section

4. Add the Gmail IMAP MCP server configuration:
   ```json
   {
     "servers": [
       {
         "name": "Gmail IMAP",
         "url": "http://localhost:8080",
         "tools": [
           "list-emails",
           "get-email",
           "search-emails",
           "send-email",
           "list-mailboxes",
           "create-label",
           "move-email",
           "mark-as-read",
           "download-attachment"
         ]
       }
     ]
   }
   ```

5. Save the settings and restart Windsurf if necessary

6. You can now ask Cascade (Windsurf's AI assistant) to interact with your Gmail account using the same commands as with Claude Desktop

## Common Use Cases

### 1. Email Triage
```
Assistant: I'll help you triage your unread emails.
User: Yes, please check my unread emails.
Assistant: [Uses get-unread-emails tool]
Assistant: You have 5 unread emails. The most urgent appears to be from your boss about the quarterly report due tomorrow.
User: Mark that as read and I'll look at it right away.
Assistant: [Uses mark-as-read tool]
```

### 2. Email Search and Organization
```
Assistant: Would you like me to find specific emails for you?
User: Yes, find all emails from john@example.com about the project budget.
Assistant: [Uses search-emails tool with query "from:john@example.com project budget"]
Assistant: I found 3 emails from John about the project budget. Would you like me to create a label for these?
User: Yes, create a "Budget" label and move them there.
Assistant: [Uses create-label tool followed by move-email tool for each email]
```

### 3. Email Composition
```
Assistant: Would you like me to draft an email for you?
User: Yes, write a follow-up email to the marketing team about our campaign results.
Assistant: [Drafts email content]
Assistant: Here's a draft. Would you like me to send it?
User: Yes, but add Sarah in CC.
Assistant: [Uses send-email tool with the drafted content and CC]
```

## Gmail-Specific Considerations

### Label Naming Conventions

Gmail has specific requirements for label names:

1. Label names are case-sensitive
2. Some special characters may not be allowed
3. System labels (like INBOX, Sent, Trash) cannot be created or deleted
4. Nested labels are represented with a forward slash (e.g., "Projects/ProjectX")

### Email ID Format

The email ID format used by this MCP server is:
```
email://message/{account}_{mailbox}_{id}
```

When using tools that require an email ID (like `mark-as-read` or `move-email`), make sure to use the complete resource URI returned by email retrieval tools.

## Security Considerations

- The server stores OAuth2 credentials locally in the `~/.gmail_imap_mcp_credentials` directory
- Never share your `client_secret.json` or token files
- The server only connects to Gmail's IMAP server using secure connections
- Email attachments are downloaded to the `downloads` directory by default
- Be cautious when using the server in shared environments to protect email data

## Troubleshooting

### Authentication Issues
- Ensure your `client_secret.json` is correctly placed in the `~/.gmail_imap_mcp_credentials` directory
- Check that you've enabled the Gmail API in your Google Cloud Project
- Try re-authenticating if your token has expired
- If you see "Read-only file system" errors, ensure the credentials directory is writable

### Connection Issues
- Verify your internet connection
- Ensure that your Google account doesn't have any security restrictions that might block IMAP access
- Check if you need to enable "Less secure app access" in your Google account settings

### Email Sending Issues
- Verify that your Gmail account allows SMTP access
- Check if you need to enable "Less secure app access" in your Google account settings
- Ensure attachments are not too large (Gmail has a 25MB limit)

### Label Management Issues
- If creating labels fails, check if the label already exists (case-sensitive)
- System labels cannot be created or deleted
- Ensure label names follow Gmail's naming conventions

### Email Movement Issues
- If moving emails between labels fails, ensure both source and target labels exist
- Check that the email ID format is correct
- Verify that you have sufficient permissions to modify the email

### Email ID Parsing Issues
- If operations on email IDs fail, ensure you're using the complete resource URI
- The system parses the last part of the URI as the actual email ID
- Format should be: `email://message/{account}_{mailbox}_{id}`

## License

[MIT License](LICENSE)

## Support

For issues and feature requests, please open an issue on the GitHub repository.', 'true', '"{\"Protocol Implementations\",\"Function Calling\",\"AI Model Integration\"]}', '{"is_mcp":"YES","justification":"The repository explicitly implements a Model Context Protocol (MCP) server for Gmail integration, enabling AI assistants to interact with and manage Gmail accounts. It provides tools for email retrieval, searching, and sending.","categories":["Protocol Implementations","AI Model Integration","Function Calling"],"analysis":{"Architecture and Design Patterns":"The server adopts a modular architecture focused on providing tools accessible via MCP. It utilizes Python's `imaplib2` for IMAP operations and stores OAuth2 credentials securely in the user's home directory. The system defines a specific email ID format for unique identification across different accounts and mailboxes. Server exposes tools like authenticate-gmail, search-email, and send-email.","Implementation Details and Key Features":"Key features include OAuth2 authentication, email searching, sending emails with attachments, label management, and email organization. The implementation leverages IMAP for Gmail interaction, using secure connections on port 993. It supports parsing emails using Python's `email` module, providing advanced search query options. The server offers tools for AI assistants to create email summaries.","Integration Approaches and Protocols":"The repository integrates with AI assistants such as Claude Desktop and Windsurf IDE via MCP. It provides configuration examples for connecting the server to these assistants, enabling them to use the defined tools. The system emphasizes a tool-based approach, offering a comprehensive set of tools for interacting with Gmail accounts. The integration is facilitated through a defined URL and a list of available tools.","Technical Stack and Dependencies":"The primary dependencies include Python 3.12 or higher, `imaplib2` for IMAP operations, and the `email` module for parsing emails. It requires a Google Cloud Project with the Gmail API enabled and OAuth 2.0 Client ID credentials. The system stores credentials as JSON files. Other dependecies and libraries are not specified, but implied through usage of `pip install -e .`.","Best Practices and Coding Standards":"The project follows some best practices such as using a virtual environment for dependency management and storing OAuth2 credentials in a user-specific directory for enhanced security. It also uses `imaplib2`, which supports secure connections. Clear documentation and examples are provided for each tool. However, specific coding standards and linting processes are not mentioned.","Performance Considerations":"The documentation doesn't explicitly address performance optimization. However, parameters such as 'limit' in search queries and 'count' in email summarization suggest an awareness of potentially large datasets. Asynchronous operations and caching mechanisms are not described in the provided documentation, but may be incorporated."}}', '2025-03-10 21:26:36+00', 'true', 'Servers', 'gmail-imap-mcp-server'), ('946167658', 'openapi-to-mcp', 'ashprabaker/openapi-to-mcp', 'https://github.com/ashprabaker/openapi-to-mcp', null, '2025-03-10 18:04:37+00', '2025-03-10 18:32:28+00', 'TypeScript', '0', '# OpenAPI to MCP

A CLI tool that converts any OpenAPI specification into an MCP (Model Context Protocol) server. This allows you to expose any OpenAPI-defined API as a tool to Claude Desktop or any MCP-compatible client.

## Installation

```bash
# Clone the repository
git clone [your-repo-url]
cd openapi-to-mcp

# Install dependencies
npm install

# Build the project
npm run build

# Link the CLI tool
npm link
```

## Usage

```bash
# Basic usage
openapi-to-mcp <path-or-url-to-openapi-spec>

# Example with a local file
openapi-to-mcp ./petstore.yaml

# Example with a URL
openapi-to-mcp https://raw.githubusercontent.com/OAI/OpenAPI-Specification/main/examples/v3.0/petstore.yaml

# Custom server name and version
openapi-to-mcp ./petstore.yaml -n "Pet Store API" -v "1.0.0"

# Custom base URL
openapi-to-mcp ./petstore.yaml -u "https://api.example.com"

# With custom headers
openapi-to-mcp ./petstore.yaml -H "Authorization: Bearer token123"
```

## Options

- `-n, --name <name>`: Set a custom name for the MCP server (defaults to the title from the OpenAPI spec)
- `-v, --version <version>`: Set a custom version for the server (defaults to the version from the OpenAPI spec)
- `-u, --base-url <url>`: Set a custom base URL for API requests (defaults to the first server URL in the OpenAPI spec)
- `-H, --header <header>`: Add a custom HTTP header to include with requests (format: "Name: Value")

## How It Works

1. The tool loads and parses the OpenAPI spec from a file or URL
2. It converts each OpenAPI operation into an MCP tool
3. Parameters are converted to Zod schemas for validation
4. When a tool is called, the corresponding API request is made
5. The response is returned to the MCP client

## Use with Claude Desktop

1. Run the tool with your OpenAPI spec: `openapi-to-mcp ./spec.yaml`
2. In Claude Desktop, add the MCP Server via the "Add Tool" button
3. Use the "Connect to local server" option and follow the prompts

## License

MIT ', 'true', '"{\"Protocol Implementations\",\"API Frameworks\",\"Function Calling\"]}', '{"is_mcp":"YES","justification":"The tool converts OpenAPI specifications into MCP servers, enabling APIs to function as tools within MCP-compatible clients like Claude Desktop. This directly implements the Model Context Protocol.","categories":["Function Calling","API Frameworks","Developer Tooling"],"analysis":{"Architecture and Design Patterns":"The tool follows a command-line interface (CLI) architecture. It parses OpenAPI specifications and transforms them into an MCP-compatible server. The design emphasizes simplicity and ease of use, enabling developers to quickly expose APIs as tools. It uses a modular approach by separating the parsing, conversion, and server functionalities.","Implementation Details and Key Features":"The core functionality revolves around converting OpenAPI operations into MCP tools. Parameters are converted to Zod schemas for validation ensuring type safety. It supports custom server names, versions, base URLs, and headers. The tool is implemented in JavaScript using Node.js and utilizes npm for dependency management and CLI functionality.","Integration Approaches and Protocols":"The tool integrates with MCP-compatible clients like Claude Desktop by exposing APIs as tools. It uses HTTP to facilitate communication. The integration approach is straightforward: start the MCP server and add it as a tool within the client application. The protocol leverages OpenAPI specifications.","Technical Stack and Dependencies":"The tool is built using Node.js and leverages npm for package management. Key dependencies include libraries for parsing OpenAPI specifications, creating Zod schemas for validation, and implementing the HTTP server. The specific libraries used for these functions are not specified in the provided documentation, but would be standard for such tasks.","Best Practices and Coding Standards":"The documentation highlights the use of version control (Git) and package management (npm), indicating adherence to standard development practices. The tool is structured to be easily buildable and linkable as a CLI utility. The provision of usage examples and options documentation promotes usability and discoverability.","Performance Considerations":"Performance considerations are not explicitly discussed. However, the conversion process from OpenAPI to MCP should be relatively fast, as it likely involves in-memory data transformations. The runtime performance of the exposed API will depend on the underlying API being proxied and the network latency. Optimization might be necessary for APIs with high traffic or complex operations."}}', '2025-03-10 21:34:28+00', 'true', 'Servers', 'openapi-to-mcp'), ('946168966', 'VisionOSPractice_MCP-PR-creation-demo', 'ChiaoteNi/VisionOSPractice_MCP-PR-creation-demo', 'https://github.com/ChiaoteNi/VisionOSPractice_MCP-PR-creation-demo', 'This is a copy of VisionOSPractice, used to demo how to utilize the MCP service with LLM to automatically create PRs.', '2025-03-10 18:07:03+00', '2025-03-10 18:30:48+00', 'Swift', '0', '# visionOS demonstrations
<img src=https://github.com/ChiaoteNi/visionOS-demonstrations/assets/40178645/2caf0047-17c1-4a91-a38c-40e3a0f5886f width=500 />

- This project is designed as a demonstration with the purpose of showcasing a diverse range of features and APIs related to app development for VisionOS
- The content is organized into several distinct parts, each focusing on different aspects. 
  The series will be released incrementally after several iOS@Taipei meetup events.
- As of now, the project has progressed to Part III.

### Part I:
  - There's a Notes.md file that includes the introductions regards to developing the app for the visionOS.
  - Other demonstrations are focused on the features and APIs for the Views and common UIs
    - `ToolsView.swift`:
      - The demo shows a potential way to create a button group with varying z offset.
      - You can extend the usage to create some control bar that is similar to the native keyboard in VisionOS.
    - `WindowBasedDemo`:
      - The glass background effect and z-offset are only supported from the visionOS instead of iOS.
      - Therefore, I made this demonstration to show a potential way to apply some features with these case via the ViewModifier.

### Part II:
  - Focus on features and APIs related to working with 3D models.
    - `Model3DAndRealityViewDemo`:
      - This demonstration highlights the differences between `Model3D` and `RealityView`.
      - There're 2 previews in this demonstration. 
        The other one is a simple demo about the potential usage for the Model3D since we control it in detail.
    - `InteractWithEntityDemo`:
      - It shows 2 different ways to add the entities to the RealityView:
        - add the entire scene with all entities inside the scene
        - add entities separately, so we can decide which entities we want to add instead all of them in a usda file
      - It also shows the way how we can interact with entities, and how to define which entity is allowed users to interact with
        - In this part, you'll find out that the venus entity is not interactable. 
          It's because I didn't add the `collision` component to it intentionally to mention what we should do if we want an entity to be touchable for users.
    - `AttachmentDemo`:
      - To show how to add an attachment to an entity.
      - Thanks to the discussion from the sharing session, we added a gesture to the entity in this demonstration to check the behavior of attachment.
        Please try moving the entity, and you'll notice that the attachment moves with it.
    - `AnimationDemo`:
      - To demonstrate how to apply animation to an entity in RealityView.

### Part III
  - Introduce features of 3D space drawing.
    - `Create3DPlanesDemo`:
      - It's a pre-practice demonstration for the `ThreeDimensionCanvasDemo`.
      - In this demonstration, I show how to create a plane in visionOS, which is used for making strokes in the subsequent `ThreeDimensionCanvasDemo`.
      <table>
        <tr>
          <th>
          Simplest Plate: Triangle
          </th>
          <th>
          Create a shape with triangles
          </th>
        </tr>
        <tr>
        <tr>
          <th>
          <img width="817" alt="Screenshot 2023-11-05 at 7 38 14 AM" src="https://github.com/ChiaoteNi/visionOS-demonstrations/assets/40178645/45f1ae74-a6e3-4800-aca3-70f00afac006">
          </th>
          <th>
          <img width="817" alt="Screenshot 2023-11-05 at 7 38 49 AM" src="https://github.com/ChiaoteNi/visionOS-demonstrations/assets/40178645/45ae3f96-a386-49d3-87f1-2b6d86e6fc6a">
          </th>
        </tr>
        <tr>
      </table>
    - `ThreeDimensionCanvasDemo`:
      - Demonstrations for showing how to draw lines and points in a spatial space, which include:
        - Create an entity without a USDZ file
        - SpatialEventGesture
        - Update mesh for an existing entity
        - Crate strokes in a spatial space
      - Also, I showed how to use multiple windows to create a separate control panel
      <table>
        <tr>
          <th>
          Points
          </th>
          <th>
          Strokes
          </th>
        </tr>
        <tr>
        <tr>
          <th>
          <img width="817" alt="Screenshot 2023-11-05 at 7 44 03 AM" src="https://github.com/ChiaoteNi/visionOS-demonstrations/assets/40178645/6839d8ab-7aef-4894-a2e6-0e4dd459fd3d">
          </th>
          <th>
          <img width="817" alt="Screenshot 2023-11-05 at 7 41 24 AM" src="https://github.com/ChiaoteNi/visionOS-demonstrations/assets/40178645/0e29baaa-b58c-4019-a34d-9a51f96c5b2e">
          </th>
        </tr>
        <tr>
      </table>

### Part IV:
- Immersive Space
  - Building on the implementation from Part III, extend it to use immersive space.
  - Additionally, improve the implementation of rendering the stroke in the Part IV demonstration.

<table>
        <tr>
        <tr>
          <th>
          <video src="https://github.com/ChiaoteNi/visionOS-demonstrations/assets/40178645/9f3320da-455d-42f0-b5ab-d498496556c0" width="300" />
          </th>
          <th>
          <img src="https://github.com/ChiaoteNi/visionOS-demonstrations/assets/40178645/4c5d0f10-ed1d-4a82-8ae3-ac9adc64b4b8" width="817" />
          </th>
        </tr>
        <tr>
      </table>

', 'true', '"{\"AI Model Integration\",\"Code Generation\",\"Developer Tooling\"]}', '{"is_mcp":"YES","justification":"The description mentions using an MCP service with an LLM to automatically create pull requests, suggesting the repo implements a communication protocol between a model and a development tool.","categories":["AI Model Integration","Developer Tooling","API Frameworks"],"analysis":{"Architecture and Design Patterns":"The repository appears to be structured as a series of demonstrations showcasing visionOS features. The demos are organized into parts, each focusing on different aspects of visionOS app development, such as UI elements, 3D model interaction, and spatial drawing. It uses a modular approach, presenting each feature in a separate demo.","Implementation Details and Key Features":"The code implements various visionOS features, including button groups with z-offset, glass background effects using ViewModifier, interaction with 3D models using Model3D and RealityView, attachment of objects to entities, animations in RealityView, and 3D space drawing using spatial events and mesh updates. It leverages Swift and visionOS APIs.","Integration Approaches and Protocols":"The repository description indicates integration with an MCP service, implying that it uses an API framework or protocol to enable communication between the visionOS application, an LLM, and a version control system for automated pull request generation. The specific protocol and API details are not fully elaborated within this documentation.","Technical Stack and Dependencies":"The technical stack includes Swift for the visionOS app development, RealityKit for 3D content rendering and interaction, and potentially external dependencies related to the MCP service and LLM integration for automated pull request creation. The code utilizes core visionOS SDK features. Details of external dependencies beyond visionOS are absent.","Best Practices and Coding Standards":"The code appears to follow standard Swift coding conventions. The project is structured logically with separate demos for each feature. The documentation provides guidance on the usage of specific visionOS APIs and components. No specific code linting or formatting information is available in the documentation.","Performance Considerations":"The project focuses on demonstrating visionOS features, and does not focus on specific optimization strategies. The demos related to 3D model rendering and spatial drawing will likely have performance implications that will need careful consideration during development. Usage of RealityView, Model3D, and mesh updates may benefit from performance analysis and optimization."}}', '2025-03-10 21:34:28+00', 'true', 'Servers', 'visionos-mcp-demo'), ('946177421', 'MCPolling', 'aelithron/MCPolling', 'https://github.com/aelithron/MCPolling', 'A full Minecraft plugin for conducting polls.', '2025-03-10 18:23:54+00', '2025-03-10 18:23:58+00', null, '0', '# MCPolling
A full Minecraft plugin for conducting polls.
', 'true', '"{\"Developer Tooling\",\"Plugin Ecosystems\",\"System Automation\"]}', '{"is_mcp":"NO","justification":"The repository describes a Minecraft plugin for conducting polls, unrelated to Model Context Protocol (MCP) specifications for AI models or context management.","categories":["Developer Tooling","Plugin Ecosystems","System Automation"],"analysis":{"Architecture and Design Patterns":"The architecture likely follows a modular design common in Minecraft plugins. It probably involves event listeners to capture player interactions and commands to initiate polls. The design might employ singleton patterns for managing poll state and data storage. Design patterns are likely basic and fit within the constraints of the Bukkit/Spigot API.","Implementation Details and Key Features":"The plugin likely utilizes the Bukkit or Spigot API to integrate with the Minecraft server. Key features probably include creating polls with configurable options, allowing players to vote, and displaying poll results. Implementation would involve handling player commands, managing poll data, and updating the in-game display.","Integration Approaches and Protocols":"The plugin integrates with the Minecraft server through the Bukkit or Spigot API. It doesn't use external protocols. All communication occurs within the Minecraft environment. Data persistence likely relies on simple file storage or a lightweight database managed within the plugin.","Technical Stack and Dependencies":"The plugin is developed using Java and the Bukkit or Spigot API. There are likely no other significant dependencies beyond these. Building the plugin typically involves using a build tool like Maven or Gradle to manage the Bukkit/Spigot API dependency.","Best Practices and Coding Standards":"Best practices would involve following standard Java coding conventions and utilizing the Bukkit/Spigot API effectively. The plugin might benefit from using a configuration file for defining poll settings and options. Proper error handling and input validation are also essential.","Performance Considerations":"Performance considerations would involve minimizing the plugin's impact on the Minecraft server's resources. Efficient data structures and algorithms should be used, especially when handling a large number of players or concurrent polls. Asynchronous tasks may be needed to prevent blocking the main server thread."}}', '2025-03-10 21:26:08+00', 'true', 'Servers', 'mcp-plugin-polling');